{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VLSP2020_RE_extract_test.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "H3ICier0JyzI",
        "PcS3CiYZxv8a",
        "jhOuiCNgaVqk",
        "chNxSZ0mx5wS",
        "jvsWfSzerWz5",
        "-wzBGIIgKYPD",
        "RQKOe59wfEcS",
        "kBr6Gx-Y2oj2",
        "8crlbk4MJTlx"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQDbz_q5irs0"
      },
      "source": [
        "# Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxh7foXeJtrf"
      },
      "source": [
        "## Unrar dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etKZzzHSF6_i"
      },
      "source": [
        "Please upload VLSP2020_RE_test.rar to Colab then */content* folder then unrar it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXjtAzVJHNI_",
        "outputId": "108b2f62-ff97-40ce-df63-264f53757142"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6x3Nl_tfBf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f378ca7d-3cc0-4102-b9c6-cd903c11a851"
      },
      "source": [
        "!pip install unrar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unrar\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/0b/53130ccd483e3db8c8a460cb579bdb21b458d5494d67a261e1a5b273fbb9/unrar-0.4-py3-none-any.whl\n",
            "Installing collected packages: unrar\n",
            "Successfully installed unrar-0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnlpfnYxiSiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7a2b0f-3a52-4340-be45-f2d5b73591ee"
      },
      "source": [
        "!unrar x VLSP2020_RE_test.rar"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from VLSP2020_RE_test.rar\n",
            "\n",
            "Creating    VLSP2020_RE_test                                          OK\n",
            "Creating    VLSP2020_RE_test/23352901.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352901.conll/CURATION_USER (1).tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352905.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352905.conll/CURATION_USER (1).tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352906.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352906.conll/CURATION_USER (1).tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352907.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352907.conll/CURATION_USER (1).tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352911.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352911.conll/CURATION_USER (1).tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352914.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352914.conll/CURATION_USER (1).tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352918.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352918.conll/CURATION_USER (1).tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352923.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352923.conll/CURATION_USER (1).tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352924.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352924.conll/CURATION_USER (1).tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352926.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352926.conll/CURATION_USER (1).tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352929.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352929.conll/CURATION_USER (1).tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352933.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352933.conll/CURATION_USER (1).tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352934.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352934.conll/CURATION_USER (1).tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352936.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352936.conll/CURATION_USER (1).tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352937.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352937.conll/CURATION_USER (1).tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352938.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352938.conll/CURATION_USER (1).tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352939.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352939.conll/CURATION_USER (1).tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352940.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352940.conll/CURATION_USER (1).tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352941.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352941.conll/CURATION_USER (1).tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352944.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352944.conll/CURATION_USER (1).tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352947.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352947.conll/CURATION_USER (1).tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352948.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352948.conll/CURATION_USER (1).tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352949.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352949.conll/CURATION_USER (1).tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352951.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352951.conll/CURATION_USER (1).tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352954.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352954.conll/CURATION_USER (1).tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352959.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352959.conll/CURATION_USER (1).tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352961.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352961.conll/CURATION_USER (1).tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352963.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352963.conll/CURATION_USER (1).tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352964.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352964.conll/CURATION_USER (1).tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352965.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352965.conll/CURATION_USER (1).tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352966.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352966.conll/CURATION_USER (1).tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352967.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352967.conll/CURATION_USER (1).tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352972.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352972.conll/CURATION_USER (1).tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352975.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352975.conll/CURATION_USER (1).tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352977.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352977.conll/CURATION_USER (1).tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352978.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352978.conll/CURATION_USER (1).tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352979.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352979.conll/CURATION_USER (1).tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352989.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352989.conll/CURATION_USER (1).tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352992.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352992.conll/CURATION_USER (1).tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352994.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352994.conll/CURATION_USER (1).tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352995.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352995.conll/CURATION_USER (1).tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352997.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352997.conll/CURATION_USER (1).tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352999.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352999.conll/CURATION_USER (1).tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353000.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353000.conll/CURATION_USER (1).tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353001.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353001.conll/CURATION_USER (1).tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353004.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353004.conll/CURATION_USER (1).tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353007.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353007.conll/CURATION_USER (1).tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353014.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353014.conll/CURATION_USER (1).tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353015.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353015.conll/CURATION_USER (1).tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353017.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353017.conll/CURATION_USER (1).tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353018.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353018.conll/CURATION_USER (1).tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353022.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353022.conll/CURATION_USER (1).tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353026.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353026.conll/CURATION_USER (1).tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353030.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353030.conll/CURATION_USER (1).tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353031.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353031.conll/CURATION_USER (1).tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353032.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353032.conll/CURATION_USER (1).tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353034.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353034.conll/CURATION_USER (1).tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353036.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353036.conll/CURATION_USER (1).tsv        \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353037.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353037.conll/CURATION_USER (1).tsv        \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24382548.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24382548.txt/CURATION_USER (1).tsv          \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24386181.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24386181.txt/CURATION_USER (1).tsv          \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24386853.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24386853.txt/CURATION_USER (1).tsv          \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24396110.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24396110.txt/CURATION_USER (1).tsv          \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24396579.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24396579.txt/CURATION_USER (1).tsv          \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24396882.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24396882.txt/CURATION_USER (1).tsv          \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24398354.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24398354.txt/CURATION_USER (1).tsv          \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24398606.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24398606.txt/CURATION_USER (1).tsv          \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24402906.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24402906.txt/CURATION_USER (1).tsv          \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24403633.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24403633.txt/CURATION_USER (1).tsv          \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24410798.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24410798.txt/CURATION_USER (1).tsv          \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24413768.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24413768.txt/CURATION_USER (1).tsv          \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24415062.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24415062.txt/CURATION_USER (1).tsv          \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24416404.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24416404.txt/CURATION_USER (1).tsv          \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24419138.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24419138.txt/CURATION_USER (1).tsv          \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24419297.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24419297.txt/CURATION_USER (1).tsv          \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24423042.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24423042.txt/CURATION_USER (1).tsv          \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24425264.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24425264.txt/CURATION_USER (1).tsv          \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24427769.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24427769.txt/CURATION_USER (1).tsv          \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24430244.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24430244.txt/CURATION_USER (1).tsv          \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24430640.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24430640.txt/CURATION_USER (1).tsv          \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24430933.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24430933.txt/CURATION_USER (1).tsv          \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24434007.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24434007.txt/CURATION_USER (1).tsv          \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24434088.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24434088.txt/CURATION_USER (1).tsv          \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24437568.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24437568.txt/CURATION_USER (1).tsv          \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24438042.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24438042.txt/CURATION_USER (1).tsv          \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24440102.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24440102.txt/CURATION_USER (1).tsv          \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24446278.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24446278.txt/CURATION_USER (1).tsv          \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24449031.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24449031.txt/CURATION_USER (1).tsv          \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24451925.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24451925.txt/CURATION_USER (1).tsv          \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24452575.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24452575.txt/CURATION_USER (1).tsv          \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24454904.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24454904.txt/CURATION_USER (1).tsv          \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24457046.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24457046.txt/CURATION_USER (1).tsv          \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24457804.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24457804.txt/CURATION_USER (1).tsv          \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24460828.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24460828.txt/CURATION_USER (1).tsv          \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24466614.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24466614.txt/CURATION_USER (1).tsv          \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24467261.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24467261.txt/CURATION_USER (1).tsv          \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24467995.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24467995.txt/CURATION_USER (1).tsv          \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24468281.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24468281.txt/CURATION_USER (1).tsv          \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24471464.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24471464.txt/CURATION_USER (1).tsv          \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24473311.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24473311.txt/CURATION_USER (1).tsv          \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24476024.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24476024.txt/CURATION_USER (1).tsv          \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24477503.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24477503.txt/CURATION_USER (1).tsv          \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24477819.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24477819.txt/CURATION_USER (1).tsv          \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24483786.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24483786.txt/CURATION_USER (1).tsv          \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24484476.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24484476.txt/CURATION_USER (1).tsv          \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24487539.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24487539.txt/CURATION_USER (1).tsv          \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24492634.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24492634.txt/CURATION_USER (1).tsv          \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24495542.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24495542.txt/CURATION_USER (1).tsv          \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24497042.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24497042.txt/CURATION_USER (1).tsv          \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24503940.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24503940.txt/CURATION_USER (1).tsv          \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24506379.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24506379.txt/CURATION_USER (1).tsv          \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24506693.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24506693.txt/CURATION_USER (1).tsv          \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24509762.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24509762.txt/CURATION_USER (1).tsv          \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24518119.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24518119.txt/CURATION_USER (1).tsv          \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24518855.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24518855.txt/CURATION_USER (1).tsv          \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24520486.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24520486.txt/CURATION_USER (1).tsv          \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24521205.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24521205.txt/CURATION_USER (1).tsv          \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24527838.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24527838.txt/CURATION_USER (1).tsv          \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24530851.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24530851.txt/CURATION_USER (1).tsv          \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24546530.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24546530.txt/CURATION_USER (1).tsv          \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24550113.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24550113.txt/CURATION_USER (1).tsv          \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24557491.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24557491.txt/CURATION_USER (1).tsv          \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24561887.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24561887.txt/CURATION_USER (1).tsv          \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24566358.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24566358.txt/CURATION_USER (1).tsv          \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24568398.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24568398.txt/CURATION_USER (1).tsv          \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24569400.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24569400.txt/CURATION_USER (1).tsv          \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24570211.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24570211.txt/CURATION_USER (1).tsv          \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24572848.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24572848.txt/CURATION_USER (1).tsv          \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24578615.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24578615.txt/CURATION_USER (1).tsv          \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24579908.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24579908.txt/CURATION_USER (1).tsv          \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24580249.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24580249.txt/CURATION_USER (1).tsv          \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24583634.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24583634.txt/CURATION_USER (1).tsv          \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24585044.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24585044.txt/CURATION_USER (1).tsv          \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24587840.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24587840.txt/CURATION_USER (1).tsv          \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24589198.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24589198.txt/CURATION_USER (1).tsv          \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24589556.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24589556.txt/CURATION_USER (1).tsv          \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24596366.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24596366.txt/CURATION_USER (1).tsv          \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24597278.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24597278.txt/CURATION_USER (1).tsv          \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24599825.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24599825.txt/CURATION_USER (1).tsv          \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24600969.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24600969.txt/CURATION_USER (1).tsv          \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24601755.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24601755.txt/CURATION_USER (1).tsv          \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24603220.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24603220.txt/CURATION_USER (1).tsv          \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24607573.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24607573.txt/CURATION_USER (1).tsv          \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24608560.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24608560.txt/CURATION_USER (1).tsv          \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24609160.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24609160.txt/CURATION_USER (1).tsv          \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24609184.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24609184.txt/CURATION_USER (1).tsv          \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24610918.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24610918.txt/CURATION_USER (1).tsv          \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24612134.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24612134.txt/CURATION_USER (1).tsv          \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24615775.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24615775.txt/CURATION_USER (1).tsv          \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24616349.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24616349.txt/CURATION_USER (1).tsv          \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24620897.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24620897.txt/CURATION_USER (1).tsv          \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24623163.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24623163.txt/CURATION_USER (1).tsv          \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24624726.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24624726.txt/CURATION_USER (1).tsv          \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24629498.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24629498.txt/CURATION_USER (1).tsv          \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24630076.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24630076.txt/CURATION_USER (1).tsv          \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24630208.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24630208.txt/CURATION_USER (1).tsv          \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24631914.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24631914.txt/CURATION_USER (1).tsv          \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24632539.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24632539.txt/CURATION_USER (1).tsv          \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24636919.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24636919.txt/CURATION_USER (1).tsv          \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24640923.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24640923.txt/CURATION_USER (1).tsv          \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24642763.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24642763.txt/CURATION_USER (1).tsv          \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24644081.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24644081.txt/CURATION_USER (1).tsv          \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24645143.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24645143.txt/CURATION_USER (1).tsv          \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24645829.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24645829.txt/CURATION_USER (1).tsv          \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24647189.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24647189.txt/CURATION_USER (1).tsv          \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24648109.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24648109.txt/CURATION_USER (1).tsv          \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24651289.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24651289.txt/CURATION_USER (1).tsv          \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24657217.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24657217.txt/CURATION_USER (1).tsv          \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24659167.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24659167.txt/CURATION_USER (1).tsv          \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24660008.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24660008.txt/CURATION_USER (1).tsv          \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24660307.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24660307.txt/CURATION_USER (1).tsv          \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24661858.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24661858.txt/CURATION_USER (1).tsv          \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24663125.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24663125.txt/CURATION_USER (1).tsv          \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24663382.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24663382.txt/CURATION_USER (1).tsv          \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24665411.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24665411.txt/CURATION_USER (1).tsv          \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24667794.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24667794.txt/CURATION_USER (1).tsv          \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24672816.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24672816.txt/CURATION_USER (1).tsv          \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24673482.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24673482.txt/CURATION_USER (1).tsv          \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24676707.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24676707.txt/CURATION_USER (1).tsv          \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24677601.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24677601.txt/CURATION_USER (1).tsv          \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24678234.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24678234.txt/CURATION_USER (1).tsv          \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24681341.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24681341.txt/CURATION_USER (1).tsv          \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24681368.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24681368.txt/CURATION_USER (1).tsv          \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24681732.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24681732.txt/CURATION_USER (1).tsv          \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24683119.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24683119.txt/CURATION_USER (1).tsv          \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24684290.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24684290.txt/CURATION_USER (1).tsv          \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24685628.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24685628.txt/CURATION_USER (1).tsv          \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24686546.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24686546.txt/CURATION_USER (1).tsv          \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24688913.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24688913.txt/CURATION_USER (1).tsv          \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24689923.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24689923.txt/CURATION_USER (1).tsv          \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24691721.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24691721.txt/CURATION_USER (1).tsv          \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24696981.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24696981.txt/CURATION_USER (1).tsv          \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24697334.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24697334.txt/CURATION_USER (1).tsv          \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24701718.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24701718.txt/CURATION_USER (1).tsv          \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24703481.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24703481.txt/CURATION_USER (1).tsv          \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24705236.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24705236.txt/CURATION_USER (1).tsv          \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24705340.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24705340.txt/CURATION_USER (1).tsv          \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24710465.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24710465.txt/CURATION_USER (1).tsv          \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24710822.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24710822.txt/CURATION_USER (1).tsv          \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24711089.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24711089.txt/CURATION_USER (1).tsv          \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24711109.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24711109.txt/CURATION_USER (1).tsv          \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24712774.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24712774.txt/CURATION_USER (1).tsv          \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24714805.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24714805.txt/CURATION_USER (1).tsv          \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24715735.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24715735.txt/CURATION_USER (1).tsv          \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24717794.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24717794.txt/CURATION_USER (1).tsv          \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24718610.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24718610.txt/CURATION_USER (1).tsv          \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24721608.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24721608.txt/CURATION_USER (1).tsv          \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24731470.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24731470.txt/CURATION_USER (1).tsv          \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24735645.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24735645.txt/CURATION_USER (1).tsv          \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24737869.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24737869.txt/CURATION_USER (1).tsv          \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24738161.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24738161.txt/CURATION_USER (1).tsv          \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24738794.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24738794.txt/CURATION_USER (1).tsv          \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24739126.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24739126.txt/CURATION_USER (1).tsv          \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24742457.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24742457.txt/CURATION_USER (1).tsv          \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24743184.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24743184.txt/CURATION_USER (1).tsv          \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24743776.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24743776.txt/CURATION_USER (1).tsv          \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24744439.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24744439.txt/CURATION_USER (1).tsv          \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24749898.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24749898.txt/CURATION_USER (1).tsv          \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24752878.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24752878.txt/CURATION_USER (1).tsv          \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24752981.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24752981.txt/CURATION_USER (1).tsv          \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24753325.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24753325.txt/CURATION_USER (1).tsv          \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24754804.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24754804.txt/CURATION_USER (1).tsv          \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24755376.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24755376.txt/CURATION_USER (1).tsv          \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24760038.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24760038.txt/CURATION_USER (1).tsv          \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24765248.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24765248.txt/CURATION_USER (1).tsv          \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24765250.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24765250.txt/CURATION_USER (1).tsv          \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24767906.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24767906.txt/CURATION_USER (1).tsv          \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24768650.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24768650.txt/CURATION_USER (1).tsv          \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24768997.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24768997.txt/CURATION_USER (1).tsv          \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24769741.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24769741.txt/CURATION_USER (1).tsv          \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24777727.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24777727.txt/CURATION_USER (1).tsv          \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24778288.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24778288.txt/CURATION_USER (1).tsv          \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24778760.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24778760.txt/CURATION_USER (1).tsv          \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24782798.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24782798.txt/CURATION_USER (1).tsv          \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24789938.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24789938.txt/CURATION_USER (1).tsv          \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24793543.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24793543.txt/CURATION_USER (1).tsv          \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24794285.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24794285.txt/CURATION_USER (1).tsv          \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24796218.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24796218.txt/CURATION_USER (1).tsv          \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24796868.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24796868.txt/CURATION_USER (1).tsv          \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24797448.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24797448.txt/CURATION_USER (1).tsv          \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24797533.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24797533.txt/CURATION_USER (1).tsv          \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24798725.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24798725.txt/CURATION_USER (1).tsv          \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24799096.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24799096.txt/CURATION_USER (1).tsv          \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24800851.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24800851.txt/CURATION_USER (1).tsv          \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24802191.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24802191.txt/CURATION_USER (1).tsv          \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24804667.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24804667.txt/CURATION_USER (1).tsv          \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24805251.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24805251.txt/CURATION_USER (1).tsv          \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24808362.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24808362.txt/CURATION_USER (1).tsv          \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24808463.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24808463.txt/CURATION_USER (1).tsv          \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24808720.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24808720.txt/CURATION_USER (1).tsv          \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24809032.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24809032.txt/CURATION_USER (1).tsv          \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24817845.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24817845.txt/CURATION_USER (1).tsv          \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24823386.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24823386.txt/CURATION_USER (1).tsv          \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24823817.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24823817.txt/CURATION_USER (1).tsv          \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24825034.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24825034.txt/CURATION_USER (1).tsv          \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24826498.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24826498.txt/CURATION_USER (1).tsv          \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24827011.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24827011.txt/CURATION_USER (1).tsv          \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24827089.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24827089.txt/CURATION_USER (1).tsv          \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24829505.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24829505.txt/CURATION_USER (1).tsv          \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24830846.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24830846.txt/CURATION_USER (1).tsv          \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24831053.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24831053.txt/CURATION_USER (1).tsv          \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24832737.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24832737.txt/CURATION_USER (1).tsv          \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24834287.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24834287.txt/CURATION_USER (1).tsv          \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24839422.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24839422.txt/CURATION_USER (1).tsv          \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24843696.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24843696.txt/CURATION_USER (1).tsv          \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24844657.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24844657.txt/CURATION_USER (1).tsv          \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24847781.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24847781.txt/CURATION_USER (1).tsv          \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24848294.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24848294.txt/CURATION_USER (1).tsv          \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24848490.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24848490.txt/CURATION_USER (1).tsv          \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24848521.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24848521.txt/CURATION_USER (1).tsv          \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24849193.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24849193.txt/CURATION_USER (1).tsv          \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24849538.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24849538.txt/CURATION_USER (1).tsv          \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24851327.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24851327.txt/CURATION_USER (1).tsv          \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24854363.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24854363.txt/CURATION_USER (1).tsv          \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24854400.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24854400.txt/CURATION_USER (1).tsv          \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24856273.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24856273.txt/CURATION_USER (1).tsv          \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24856833.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24856833.txt/CURATION_USER (1).tsv          \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24857007.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24857007.txt/CURATION_USER (1).tsv          \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24857924.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24857924.txt/CURATION_USER (1).tsv          \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24867415.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24867415.txt/CURATION_USER (1).tsv          \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24867954.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24867954.txt/CURATION_USER (1).tsv          \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24870101.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24870101.txt/CURATION_USER (1).tsv          \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24870792.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24870792.txt/CURATION_USER (1).tsv          \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24870899.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24870899.txt/CURATION_USER (1).tsv          \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24872304.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24872304.txt/CURATION_USER (1).tsv          \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24874547.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24874547.txt/CURATION_USER (1).tsv          \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24875238.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24875238.txt/CURATION_USER (1).tsv          \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24881113.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24881113.txt/CURATION_USER (1).tsv          \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24881311.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24881311.txt/CURATION_USER (1).tsv          \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24882485.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24882485.txt/CURATION_USER (1).tsv          \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24888805.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24888805.txt/CURATION_USER (1).tsv          \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24891267.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24891267.txt/CURATION_USER (1).tsv          \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24894135.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24894135.txt/CURATION_USER (1).tsv          \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24894691.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24894691.txt/CURATION_USER (1).tsv          \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24900537.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24900537.txt/CURATION_USER (1).tsv          \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24900662.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24900662.txt/CURATION_USER (1).tsv          \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24900741.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24900741.txt/CURATION_USER (1).tsv          \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24900743.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24900743.txt/CURATION_USER (1).tsv          \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24901248.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24901248.txt/CURATION_USER (1).tsv          \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24903191.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24903191.txt/CURATION_USER (1).tsv          \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3ICier0JyzI"
      },
      "source": [
        "## Install Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcS3CiYZxv8a"
      },
      "source": [
        "### Install VNCoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzsgsAb4uET6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f9348b4-0999-4b69-ba92-78c92034d7b2"
      },
      "source": [
        "# Install the vncorenlp python wrapper\n",
        "!pip install vncorenlp"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vncorenlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n",
            "\u001b[K     || 2.7MB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2020.12.5)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp37-none-any.whl size=2645936 sha256=c9476ca0c370ad73ac2983a1a7708b9ab853855d16b80d640a15505f95937c28\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxVE9cR6yZ3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef7c35c-5e51-4e49-9f7c-fcc54befe593"
      },
      "source": [
        "# Download VnCoreNLP-1.1.1.jar & all of its  component (i.e. RDRSegmenter, pos, ner, deprel) \n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!mkdir -p vncorenlp/models/dep\n",
        "!mkdir -p vncorenlp/models/ner\n",
        "!mkdir -p vncorenlp/models/postagger\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/dep/vi-dep.xz\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger\n",
        "\n",
        "\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "\n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/\n",
        "\n",
        "!mv vi-dep.xz vncorenlp/models/dep/\n",
        "\n",
        "!mv vi-500brownclusters.xz vncorenlp/models/ner/\n",
        "!mv vi-ner.xz vncorenlp/models/ner/\n",
        "!mv vi-pretrainedembeddings.xz vncorenlp/models/ner/\n",
        "\n",
        "!mv vi-tagger vncorenlp/models/postagger/\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-09 12:16:06--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: VnCoreNLP-1.1.1.jar\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M   125MB/s    in 0.2s    \n",
            "\n",
            "2021-05-09 12:16:08 (125 MB/s) - VnCoreNLP-1.1.1.jar saved [27412575/27412575]\n",
            "\n",
            "--2021-05-09 12:16:08--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: vi-vocab\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-05-09 12:16:08 (22.8 MB/s) - vi-vocab saved [526544/526544]\n",
            "\n",
            "--2021-05-09 12:16:08--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: wordsegmenter.rdr\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2021-05-09 12:16:09 (13.1 MB/s) - wordsegmenter.rdr saved [128508/128508]\n",
            "\n",
            "--2021-05-09 12:16:09--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/dep/vi-dep.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16048864 (15M) [application/octet-stream]\n",
            "Saving to: vi-dep.xz\n",
            "\n",
            "vi-dep.xz           100%[===================>]  15.30M  75.6MB/s    in 0.2s    \n",
            "\n",
            "2021-05-09 12:16:10 (75.6 MB/s) - vi-dep.xz saved [16048864/16048864]\n",
            "\n",
            "--2021-05-09 12:16:10--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5599844 (5.3M) [application/octet-stream]\n",
            "Saving to: vi-500brownclusters.xz\n",
            "\n",
            "vi-500brownclusters 100%[===================>]   5.34M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-09 12:16:10 (47.2 MB/s) - vi-500brownclusters.xz saved [5599844/5599844]\n",
            "\n",
            "--2021-05-09 12:16:10--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9956876 (9.5M) [application/octet-stream]\n",
            "Saving to: vi-ner.xz\n",
            "\n",
            "vi-ner.xz           100%[===================>]   9.50M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-09 12:16:11 (81.1 MB/s) - vi-ner.xz saved [9956876/9956876]\n",
            "\n",
            "--2021-05-09 12:16:11--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 57313672 (55M) [application/octet-stream]\n",
            "Saving to: vi-pretrainedembeddings.xz\n",
            "\n",
            "vi-pretrainedembedd 100%[===================>]  54.66M   137MB/s    in 0.4s    \n",
            "\n",
            "2021-05-09 12:16:14 (137 MB/s) - vi-pretrainedembeddings.xz saved [57313672/57313672]\n",
            "\n",
            "--2021-05-09 12:16:14--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29709468 (28M) [application/octet-stream]\n",
            "Saving to: vi-tagger\n",
            "\n",
            "vi-tagger           100%[===================>]  28.33M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-09 12:16:15 (195 MB/s) - vi-tagger saved [29709468/29709468]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXfVgT46BB-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5699ccd-73fb-4e67-cf99-e441d8ff5c9c"
      },
      "source": [
        "import unicodedata\n",
        "from vncorenlp import VnCoreNLP\n",
        "\n",
        "# To perform word segmentation, POS tagging, NER and then dependency parsing\n",
        "annotator1 = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx2g') \n",
        "\n",
        "# To perform word segmentation, POS tagging and then NER\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg,pos,ner\", max_heap_size='-Xmx2g') \n",
        "# To perform word segmentation and then POS tagging\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg,pos\", max_heap_size='-Xmx2g') \n",
        "# To perform word segmentation only\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
        "# Input \n",
        "text = unicodedata.normalize(\"NFD\", \"Thanh Thy\")\n",
        "\n",
        "\n",
        "# To perform word segmentation only\n",
        "word_segmented_text = annotator1.tokenize(text) \n",
        "\n",
        "print(*word_segmented_text, sep=\"\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Thanh', 'Thuy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhOuiCNgaVqk"
      },
      "source": [
        "### Install Underthesea"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IujpzlPaKfo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0f2930-4ec3-4d8d-8533-a9f5f9990df0"
      },
      "source": [
        "!pip install underthesea"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting underthesea\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/5f/03ab9091b88e7851aa92da33f8eea6f111423cc1194cf1636c63c1fff3d0/underthesea-1.3.1-py3-none-any.whl (7.5MB)\n",
            "\u001b[K     || 7.5MB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.41.1)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     || 245kB 38.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from underthesea) (0.22.2.post1)\n",
            "Collecting torch<=1.5.1,>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/cf/007b6de316c9f3d4cb315a60c308342cc299e464167f5ebc369e93b5e23a/torch-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (753.2MB)\n",
            "\u001b[K     || 753.2MB 20kB/s \n",
            "\u001b[?25hCollecting python-crfsuite>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     || 747kB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.13)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.0.1)\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     || 51kB 5.3MB/s \n",
            "\u001b[?25hCollecting transformers<=3.5.1,>=3.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     || 1.3MB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch<=1.5.1,>=1.1.0->underthesea) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2020.12.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     || 901kB 47.8MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     || 1.1MB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (2019.12.20)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/ac/f5ba028f0f097d855e1541301e946d4672eb0f30b6e25cb2369075f916d2/tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     || 2.9MB 56.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (20.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers<=3.5.1,>=3.5.0->underthesea) (56.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<=3.5.1,>=3.5.0->underthesea) (2.4.7)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=d2bd02bfafa8f545e973fd67348d007972969793bf8efcaa321f9616c1a582a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: unidecode, torch, python-crfsuite, seqeval, sacremoses, sentencepiece, tokenizers, transformers, underthesea\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed python-crfsuite-0.9.7 sacremoses-0.0.45 sentencepiece-0.1.91 seqeval-1.2.2 tokenizers-0.9.3 torch-1.5.1 transformers-3.5.1 underthesea-1.3.1 unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjsyeDFRKpfF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc2c221-a1d8-45a6-99ca-18fa2cc55264"
      },
      "source": [
        "from underthesea import sent_tokenize\n",
        "text = 'Qung Bnh : Ct tc lng hnh, him ha rnh rp cu Long i v dng sng? Hin tng khai thc ct lu trn sng cch cu Long i vi trm mt v pha h lu, khin cu v sng Long i ang ng trc him ha kh lng? Va qua Php lut Plus nhn phn nh ca nhng ngi dn sng  x Xun Ninh , huyn Qung Ninh , tnh Qung Bnh v vic hin nay  x ny, c th l ti thn Xun Dc 1 khu vc ven sng Long i lu ny xut hin nhng bi tp kt ct tri php v hin tng khai thc ct lu trn sng c ngy ln m gy nh hng n n cuc sng thng nht ca ngi dn ni y. T nhng ngun tin nu trn sng ngy 21/9, PV  tip cn hin trng on sng Long i thuc thn Xun D 1 , x Xun Ninh , huyn Qung Ninh , tnh Qung Bnh ni ngi dn phn nh  cng c thng tin. Ti y, PV nhn thy nhiu bi tp kt ct gn khu vc dn c sinh sng, hng ngy nhiu tu ch ct vo y  tp kt ct, gy ra ting n kh chu nh hng khng nh n sinh hot ca ngi dn. Nhng bi tp kt ct tri php. Hn th na theo tm hiu ca PV c bit, nhng v tr c bi tp kt ct k trn khng  tiu chun  tu cp bn tp kt?. Tra cng ngy PV  i theo hng thng ngun sng Long i m theo phn nh l xy ra tnh trng khai thc ct tri php thng xuyn din ra. PV nhn thy mt chic thuyn ang neo u cch b chng vi chc mt v cch mng cu Long i chng vi trm mt theo hng h ngun ang ht ct ln thuyn. Chic thuyn ( ) ang khai thc ct tri php cch cu Long i khng xa. Tip tc ghi nhn v theo di v vic khong chng hn 30 pht, chic thuyn  y ct  c i chuyn i tp kt. Chic thuyn sau khi ht ct tri php di chuyn v bi tp kt. Qua tm hiu ca PV c bit, ct  khu vc gn cu Long i l ct nhim mn nu dng vo vic thi cng cng trnh s nh hng n cht lng ca cng trnh  V ct ny c ch thuyn bn li cho ngi s dng vi gi r hn so vi ct c khai thc  m c cp php gy nn s cnh tranh khng lnh mnh v gi ct. Tuy nhin nhiu ngi dn cha nhn thy n vic cht lng ca cng trnh sau ny khi s dng ct nhim mn ny. iu ng ni l vic khai thc ct tri php li din ra khu vc gn mng cu Long i (c ng st ln ng b) nguy c sc l t khu vc mng cu, khin cu Long i ng trc him ha kh lng?. Lin quan n vn  ny, trao i vi PV ng Nguyn Trng Tin  Ch tch x Xun Ninh cho bit v pha x cng  nhiu ln x l nhc nh ngi dn trong vn  tp kt ct ng ni quy nh. Ngoi ra, x ang hng dn v hon thnh cc th tc nhm a cc im tp kt tri php ny ng vo ni quy nh trong thi gian sm nht, ng Tin cho bit thm. ng Nguyn Trng Tin  Ch tch x Xun Ninh (bn phi) ti bui lm vic vi PV. Khi c PV cng cp bng chng v vic thuyn khai thc ct tri php ngay gia ban ngy gn khu vc mng cu Long i , ng Tin  ht sc bt ng ni Nh vy l khng c ri, khng c ri s cho x l ngay Tip  PV lin lc qua in thoi vi ng Phm Trung ng  Ch tch UBND huyn Qung Ninh  phn nh s vic th ng ng cho bit, ang bn v hng dn PV lin h vi ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh  lm vic. Ti bui l vic vi ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh PV  cung cp clip v vic nn khai thc tri php din ra ngay trn sng Long i on gn mng cu ng Giai cng  kin quyt v ha s u tranh x l, ng thi phi hp vi cc c quan chc nng khc thng xuyn kim tra  chm dt tnh trng ny. ng Nguyn Vit Giai cho bit s u tranh x l Cn v vic cc bi tp kt tri php, ng Giai cho bit s x l dt im trong thi gian sm nht  khng nh hng ti cuc sng ngi dn xung quanh. Khi c PV hi thi gian sm nht l bao lu ng Giai cho bit:  y ang cn vng mt khu th tc. thi gian gii quyt sm nht cng phi mt chng 7 n 10 ngy. Vic khai thc ct tri php gn cu Long i (c ng st ln ng b) nguy c st l t khu vc mng cu, khin cu Long i ng trc him ha kh lng? Tuy l vy nhng trong sng 22/9, PV mt ln na n ti hin trng chic thuyn khai thc tri php th nhn thy tnh hnh khai thc ct tri php vn khng h thay i. Mt ln na PV  gi in thoi cho ng Nguyn Trng Tin  Ch tch x Xun Ninh v ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh  phn nh th li c 2 v ha s x l. Trong sng 22/9 vic khai thc ct tri php vn din ra m khng c s can thip ca c quyan chc nng? T nhng vic nu trn, d lun khng th khng t ra cu hi liu nhng vic xy ra  y c phi l c s bo k hoc c s tip tay ca lc lng chc nng c thm thm quyn hay khng? Php lut Plus s tip tc thng tin v vic n bn c.'\n",
        "sent_tokenize(text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Qung Bnh : Ct tc lng hnh, him ha rnh rp cu Long i v dng sng?',\n",
              " 'Hin tng khai thc ct lu trn sng cch cu Long i vi trm mt v pha h lu, khin cu v sng Long i ang ng trc him ha kh lng?',\n",
              " 'Va qua Php lut Plus nhn phn nh ca nhng ngi dn sng  x Xun Ninh , huyn Qung Ninh , tnh Qung Bnh v vic hin nay  x ny, c th l ti thn Xun Dc 1 khu vc ven sng Long i lu ny xut hin nhng bi tp kt ct tri php v hin tng khai thc ct lu trn sng c ngy ln m gy nh hng n n cuc sng thng nht ca ngi dn ni y.',\n",
              " 'T nhng ngun tin nu trn sng ngy 21/9, PV  tip cn hin trng on sng Long i thuc thn Xun D 1 , x Xun Ninh , huyn Qung Ninh , tnh Qung Bnh ni ngi dn phn nh  cng c thng tin.',\n",
              " 'Ti y, PV nhn thy nhiu bi tp kt ct gn khu vc dn c sinh sng, hng ngy nhiu tu ch ct vo y  tp kt ct, gy ra ting n kh chu nh hng khng nh n sinh hot ca ngi dn.',\n",
              " 'Nhng bi tp kt ct tri php.',\n",
              " 'Hn th na theo tm hiu ca PV c bit, nhng v tr c bi tp kt ct k trn khng  tiu chun  tu cp bn tp kt?.',\n",
              " 'Tra cng ngy PV  i theo hng thng ngun sng Long i m theo phn nh l xy ra tnh trng khai thc ct tri php thng xuyn din ra.',\n",
              " 'PV nhn thy mt chic thuyn ang neo u cch b chng vi chc mt v cch mng cu Long i chng vi trm mt theo hng h ngun ang ht ct ln thuyn.',\n",
              " 'Chic thuyn ( ) ang khai thc ct tri php cch cu Long i khng xa.',\n",
              " 'Tip tc ghi nhn v theo di v vic khong chng hn 30 pht, chic thuyn  y ct  c i chuyn i tp kt.',\n",
              " 'Chic thuyn sau khi ht ct tri php di chuyn v bi tp kt.',\n",
              " 'Qua tm hiu ca PV c bit, ct  khu vc gn cu Long i l ct nhim mn nu dng vo vic thi cng cng trnh s nh hng n cht lng ca cng trnh  V ct ny c ch thuyn bn li cho ngi s dng vi gi r hn so vi ct c khai thc  m c cp php gy nn s cnh tranh khng lnh mnh v gi ct.',\n",
              " 'Tuy nhin nhiu ngi dn cha nhn thy n vic cht lng ca cng trnh sau ny khi s dng ct nhim mn ny.',\n",
              " 'iu ng ni l vic khai thc ct tri php li din ra khu vc gn mng cu Long i (c ng st ln ng b) nguy c sc l t khu vc mng cu, khin cu Long i ng trc him ha kh lng?.',\n",
              " 'Lin quan n vn  ny, trao i vi PV ng Nguyn Trng Tin  Ch tch x Xun Ninh cho bit v pha x cng  nhiu ln x l nhc nh ngi dn trong vn  tp kt ct ng ni quy nh.',\n",
              " 'Ngoi ra, x ang hng dn v hon thnh cc th tc nhm a cc im tp kt tri php ny ng vo ni quy nh trong thi gian sm nht, ng Tin cho bit thm.',\n",
              " 'ng Nguyn Trng Tin  Ch tch x Xun Ninh (bn phi) ti bui lm vic vi PV.',\n",
              " 'Khi c PV cng cp bng chng v vic thuyn khai thc ct tri php ngay gia ban ngy gn khu vc mng cu Long i , ng Tin  ht sc bt ng ni Nh vy l khng c ri, khng c ri s cho x l ngay Tip  PV lin lc qua in thoi vi ng Phm Trung ng  Ch tch UBND huyn Qung Ninh  phn nh s vic th ng ng cho bit, ang bn v hng dn PV lin h vi ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh  lm vic.',\n",
              " 'Ti bui l vic vi ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh PV  cung cp clip v vic nn khai thc tri php din ra ngay trn sng Long i on gn mng cu ng Giai cng  kin quyt v ha s u tranh x l, ng thi phi hp vi cc c quan chc nng khc thng xuyn kim tra  chm dt tnh trng ny.',\n",
              " 'ng Nguyn Vit Giai cho bit s u tranh x l Cn v vic cc bi tp kt tri php, ng Giai cho bit s x l dt im trong thi gian sm nht  khng nh hng ti cuc sng ngi dn xung quanh.',\n",
              " 'Khi c PV hi thi gian sm nht l bao lu ng Giai cho bit:  y ang cn vng mt khu th tc.',\n",
              " 'thi gian gii quyt sm nht cng phi mt chng 7 n 10 ngy. Vic khai thc ct tri php gn cu Long i (c ng st ln ng b) nguy c st l t khu vc mng cu, khin cu Long i ng trc him ha kh lng?',\n",
              " 'Tuy l vy nhng trong sng 22/9, PV mt ln na n ti hin trng chic thuyn khai thc tri php th nhn thy tnh hnh khai thc ct tri php vn khng h thay i.',\n",
              " 'Mt ln na PV  gi in thoi cho ng Nguyn Trng Tin  Ch tch x Xun Ninh v ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh  phn nh th li c 2 v ha s x l.',\n",
              " 'Trong sng 22/9 vic khai thc ct tri php vn din ra m khng c s can thip ca c quyan chc nng?',\n",
              " 'T nhng vic nu trn, d lun khng th khng t ra cu hi liu nhng vic xy ra  y c phi l c s bo k hoc c s tip tay ca lc lng chc nng c thm thm quyn hay khng?',\n",
              " 'Php lut Plus s tip tc thng tin v vic n bn c.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chNxSZ0mx5wS"
      },
      "source": [
        "# Extract raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phjF0edvNGpc"
      },
      "source": [
        "import os\n",
        "import re"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pTphYOiMYho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa98a8e-4253-4918-f81b-ecb8ec5fe006"
      },
      "source": [
        "# get all subfolers and files in subfolers\n",
        "# [(\"top_subfolders\", [subfolders_in_top_subfolders], [files_in_top_subfolders])]\n",
        "sub_folders = [f for f in os.walk(\"VLSP2020_RE_test\")][1:]\n",
        "sub_folders = sorted(sub_folders, key=lambda x: x[0])   # sort by top_subfolder name\n",
        "\n",
        "## top subfolder contain only 1 single file.\n",
        "check = False\n",
        "for i in sub_folders:\n",
        "    if i[1] or len(i[-1])!= 1:\n",
        "        print(\"ALERT!!!\")\n",
        "        check = True\n",
        "\n",
        "if not check:\n",
        "    print(\"There is \", len(sub_folders), \" subfolders. All subfolders contain only 1 file.\",\n",
        "          \" So that we have \", len(sub_folders), \" files.\")\n",
        "\n",
        "# generate data files name\n",
        "files_path = [os.path.join(i[0], i[-1][0]) for i in sub_folders]\n",
        "\n",
        "# print(*files_path, sep=\"\\n\")\n",
        "\n",
        "# print(files_path)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is  300  subfolders. All subfolders contain only 1 file.  So that we have  300  files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LJk2k0vD0dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619446d3-4af1-4b0a-ed86-65029740d26e"
      },
      "source": [
        "# Xem trong b d liu c nhng character g\n",
        "\n",
        "character_lst = []\n",
        "for file in files_path:\n",
        "    with open(file, mode='r') as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "        # find line start with \"#Text=\"\n",
        "        textline_id = []\n",
        "        for i, text in enumerate(lines):\n",
        "            if (\"#Text=\" == text[0:6]):\n",
        "                textline_id.append(i)\n",
        "\n",
        "        # every data file has only one line that start with \"#Text=\"\"\n",
        "        assert (len(textline_id) == 1), str(\"1 is not number of line start with #Text=. \\nDoc: \" + file)\n",
        "\n",
        "        for c in lines[textline_id[0]][6:]:\n",
        "            if c not in character_lst:\n",
        "                character_lst.append(c)\n",
        "\n",
        "\n",
        "# Print all of the single characters, 30 per row.\n",
        "# For every batch of 30 tokens...\n",
        "for i in range(0, len(character_lst), 30):\n",
        "    \n",
        "    # Limit the end index so we don't go past the end of the list.\n",
        "    end = min(i + 30, len(character_lst) + 1)\n",
        "    \n",
        "    # Print out the tokens, separated by a space.\n",
        "    print(repr(' '.join(character_lst[i:end])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"  y   l   d o k h i  n Y A q u t   c  m   '   M\"\n",
            "'  a v  S N D b   g        .      1 0  , s'\n",
            "' p    r  f        x T : \"         e  '\n",
            "'  G 4 / 9 H    K L  2  3 ? F  - C V B 6 5 P ( )  '\n",
            "'    \\xa0     7  8 w   Q R O J  I % U E X  Z ;  '\n",
            "' W z +    j & |  !    \\ufeff @             '\n",
            "'= # [ ]  <           *     _        '\n",
            "'     '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugs-iTcldECs"
      },
      "source": [
        "constant = {\"entity_name\": [\"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"MISCELLANEOUS\"],\n",
        "          \"relation_name\": [\"LOCATED\", \"PART  WHOLE\", \"AFFILIATION\", \"PERSONAL - SOCIAL\"]\n",
        "           }"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rqndrEtimOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4769c3a3-16d7-40dd-d6a5-9d86a54ba3f9"
      },
      "source": [
        "\"\"\"\n",
        "            dict = {\"doc_id\": id of folder contain doc, \n",
        "                      \"text\": doc, \n",
        "                 \"token_ids\": [                                        tokens_id, ...], \n",
        "              \"subtoken_ids\": [                                   None or sub-id, ...],\n",
        "                       \"pos\": [                             [start pos, end pos], ...],\n",
        "                    \"tokens\": [                                      tokens_text, ...],\n",
        "                    \"entity\": [                        [entity_ids, entity_name], ...],\n",
        "                  \"relation\": [    [relation, stoken_id, ssubtoken_id direction], ...]\n",
        "                   }\n",
        "\n",
        "\n",
        "                      doc_id: id of folder contain doc                                            (str)\n",
        "                         doc: doc. line start with \"#Text=\"                                       (str)\n",
        "                   token_ids: ids of tokens.                                    first column       (list int)\n",
        "                subtoken_ids: int if crr token is a subtoken, otherwise None    first column      (list int, None)\n",
        "                         pos: posittion of tokens.                              second column     (list list int) \n",
        "                              [\n",
        "                                  [start pos, end pos],\n",
        "                                  ...\n",
        "                              ]\n",
        "                       token: tokens.                                           third column      (list str)\n",
        "                      entity: entity infor if token is entity, else None.       4th, 5th column   (list list, None)\n",
        "                              [\n",
        "                                  [entity_id, entity_name],\n",
        "                                  ...\n",
        "                              ]\n",
        "                    relation: relation if token is in a relation, else None.    other column      (list list, None)\n",
        "                              [\n",
        "                                  [[relation1_name, relation1_start_tokenID, relation1_start_subtokenID, [start_entity_id, end_entity_id]], ...],  \n",
        "                                                                         --> relation1_start_subtokenID may be None if start token is not a sub token\n",
        "                                  [[relation1_name, relation1_start_tokenID, relation1_start_subtokenID,                             None], ...],  <-- dataset has mistake. Don't have direction.\n",
        "                                                                                 \n",
        "                                  \n",
        "                                  ....\n",
        "                              ]\n",
        "                    \n",
        "\"\"\"\n",
        "\n",
        "import copy\n",
        "\n",
        "raw_tdata = []\n",
        "\n",
        "for file in files_path:\n",
        "    docif = {}\n",
        "    with open(file, mode='r') as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "        # example: VLSP2020_RE_training/23351113.conll/CURATION_USER.tsv -> 23351113\n",
        "        docif[\"doc_id\"] = copy.deepcopy(file[(file.find(\"/\") + 1): file.find(\".\")])\n",
        "\n",
        "        # find line start with \"#Text=\"\n",
        "        textline_id = []\n",
        "        for i, text in enumerate(lines):\n",
        "            if (\"#Text=\" == text[0:6]):\n",
        "                textline_id.append(i)\n",
        "\n",
        "        # every data file has only one line that start with \"#Text=\"\"\n",
        "        assert (len(textline_id) == 1), str(\"1 is not number of line start with #Text=. \\nDoc: \" + file)\n",
        "\n",
        "        docif[\"text\"] = copy.deepcopy(lines[textline_id[0]][6:])\n",
        "\n",
        "        first_cline = lines[(textline_id[0] + 1)].rstrip(\"\\t\").split(\"\\t\")   # first column_line\n",
        "        assert (len(first_cline) in [3, 5, 7, 8]), str(\"Doc has problem. doc: \" + file)\n",
        "\n",
        "\n",
        "        token_ids, subtoken_ids, pos, tokens = [], [], [], []\n",
        "        entity = []\n",
        "        relation = []\n",
        "\n",
        "        pretk_id = 0\n",
        "\n",
        "        for tk_id, line in enumerate(lines[(textline_id[0] + 1):]):\n",
        "            lineif = line.rstrip(\"\\t\").split(\"\\t\")   # seperate by one \\t between columns: [abc\\txyz\\t]\n",
        "\n",
        "            # check if columns is seperated by only one single Tab character '\\t'\n",
        "            lineif1 = re.split(r'\\t+', line.rstrip('\\t'))   # seperate by all \\t between column: [abc\\t\\t\\txyz\\t]\n",
        "            assert (lineif == lineif1), str(\"Columns is not seperated by only one single TAB '\\\\t'. doc: \" + file + \" line: \" + line)\n",
        "\n",
        "            # check if inside a doc, only exist one number of (no) columns\n",
        "            # above we check if len(lineif) in [3, 5, 7, 8], too. so we can make sure that\n",
        "            # in a doc, number of columns only in [3, 5, 7, 8]\n",
        "            # and all line in a doc has same no columns\n",
        "            assert len(lineif) == len(first_cline), str(\"Number of columns in doc is not consistent. \\nDoc: \" + file + \" line: \" + line)\n",
        "\n",
        "\n",
        "            # remove all \"_\" in lineif because we don't need it\n",
        "            # [3, 5, 7, 8] -> [3, 4, 5, 7]\n",
        "            # and all data has first three column. (4th and 5th) is a pair, (6th and 7th) is a pair\n",
        "            # after removing all \"_\", if:\n",
        "            # len(lineif) = 3 -> token_ids, pos, no entity, no relation\n",
        "            # len(lineif) = 5 -> token_ids, pos, entity, no relation\n",
        "            # len(lineif) = 7 -> token_ids, pos, entity, relation\n",
        "\n",
        "            # len(lineif) = 4 --> token_ids, pos, no entity, no relation (this is a mistake in dataset, in data file has 8 columns)\n",
        "\n",
        "            lineif = [col for col in lineif if col != \"_\"]\n",
        "\n",
        "            assert (len(lineif) in [3, 4, 5, 7]), str(\"Problem with number of columns after remove \\'_\\'.\\nIn doc: \" + file + \" line \" + line)\n",
        "\n",
        "            # match first column format\n",
        "            # startwith (\"1-\") then (number) end:   1-id\n",
        "            pattern_token_ids = re.compile(\"^(1-)([\\d]+)$\")\n",
        "\n",
        "            # a token may has many subtokens\n",
        "            # startwith (\"1-\") then (number) then (. char) then (number) end:   1-id.subid \n",
        "            # pattern_subtoken_ids = re.compile(\"^(1-)([\\d]+)(\\.)([\\d]+)$\")\n",
        "\n",
        "            # Currently in train dataset, number of subtoken of a token is 0 or 1\n",
        "            # startwith (\"1-\") then (number) then (.1) end:   1-id.1 \n",
        "            pattern_subtoken_ids = re.compile(\"^(1-)([\\d]+)(\\.1)$\")\n",
        "\n",
        "            assert (pattern_token_ids.match(lineif[0]) or pattern_subtoken_ids.match(lineif[0])), \\\n",
        "            str(\"Unexpected first column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "            if pattern_token_ids.match(lineif[0]):\n",
        "                # 1-id\n",
        "                # Check if token id is increased by one in each line or not.\n",
        "                assert (int(lineif[0][2:]) == (pretk_id + 1)), str(\"First column, Token_ID is not increased by one in each line. \\nIn doc: \" + file + \" \\nline: \" + line)\n",
        "\n",
        "                token_ids.append(int(lineif[0][2:]))\n",
        "                subtoken_ids.append(None)   # Not a subtoken\n",
        "\n",
        "                pretk_id += 1\n",
        "            \n",
        "            else:\n",
        "                # 1-id.1\n",
        "                # 1-id.subid\n",
        "                tmp = lineif[0].find(\".\")\n",
        "                tokenID = int(lineif[0][2:tmp])\n",
        "                subtokenID = int(lineif[0][(tmp+1):])\n",
        "                \n",
        "                assert (tokenID == token_ids[-1]), str(\"Exist subtoken without a token before it. \\nIn doc: \" + file + \" \\nline\" + line)\n",
        "\n",
        "                token_ids.append(tokenID)\n",
        "                subtoken_ids.append(subtokenID)\n",
        "\n",
        "                print(\"\\nTHERE IS A SUBTOKEN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "\n",
        "            # match second column format\n",
        "            # startwith (number) then (\"-\" char) then (number) end\n",
        "            pattern_pos = re.compile(\"^([\\d]+)(\\-)([\\d]+)$\")\n",
        "            assert (pattern_pos.match(lineif[1])), str(\"Unexpected second column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "            pos.append([int(ele) for ele in lineif[1].split(\"-\")])    # example: \"3-6\" -> [3, 6]\n",
        "            \n",
        "            # if current token is a subtoken, check if pos subtoken is inside father token or not.\n",
        "            if pattern_subtoken_ids.match(lineif[0]):\n",
        "                father_token = token_ids.index(token_ids[-1])\n",
        "\n",
        "                assert (pos[father_token][0] <= pos[-1][0]) and (pos[-1][1] <= pos[father_token][1]), \\\n",
        "                str(\"Subtoken\\'s position is not inside father token\\'s position. \\nIndoc: \" + file + \"\\Line: \" + line)\n",
        "\n",
        "\n",
        "            # third column\n",
        "            #check if token is matched with pos (second column) or not\n",
        "            crr_token_pos = [int(ele) for ele in lineif[1].split(\"-\")]\n",
        "            if lineif[2] == lines[textline_id[0]][6:][crr_token_pos[0]:crr_token_pos[1]]:\n",
        "                tokens.append(lineif[2])\n",
        "            else:\n",
        "                assert False, str(\"Token in 3th column not match with position at 2th column. \\nIn doc: \" + file + \" \\nline: \" + line)\n",
        "            \n",
        "\n",
        "            if (len(lineif) == 3) or (len(lineif) == 4):\n",
        "                entity.append(None)\n",
        "                relation.append(None)\n",
        "\n",
        "                if len(lineif) == 4:\n",
        "                    print(\"\\n4 COLUMNS.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "\n",
        "            # because we removed all \"_\", \n",
        "            # so when len(lineif) = 5 or len(lineif) = 7, this line must has: token_ids, pos, tokens and entity.\n",
        "            # (we don't have to check if 4th, 5th column is \"_\" anymore, since we removed all \"_\")\n",
        "            if (len(lineif) == 5) or (len(lineif) == 7):\n",
        "                # 4th column now only have two posibilities: \"*\" or \"*[number]\"\n",
        "                pattern_entity_id = re.compile(\"^(\\*)(\\[)([\\d]+)(\\])$\")\n",
        "                assert ((lineif[3] == \"*\") or pattern_entity_id.match(lineif[3])), str(\"Unexpected fourth column's format. In doc: \" + file + \" line \" + line)\n",
        "\n",
        "                # in doc: 23352816\n",
        "                # line: 1-23\t126-136\t</ENAMEX>)\t*\t*\t_\t_\t_\t\n",
        "                # there is a mistake in 5th column. Unknow enity name\n",
        "                # I will let this token entity is None.\n",
        "\n",
        "                # We can just let all token entity is None\n",
        "                # if 5th column is not in constant[\"entity_name\"]\n",
        "                # but below, I just code for this specific case\n",
        "                # because I want to know more about dataset\n",
        "\n",
        "                if (lineif[3] == \"*\"):\n",
        "\n",
        "                    if (lineif[4] == \"*\"):   # specific mistake case\n",
        "                        entity.append(None)\n",
        "                        print(\"\\nENTITY NAME MISTAKE IN 5TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                    else:\n",
        "                        assert (lineif[4] in constant[\"entity_name\"]), str(\"Unknown entity name. \\nDoc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "                        entity_id = 0\n",
        "                        entity_n = lineif[4]\n",
        "\n",
        "                        entity.append([entity_id, entity_n])\n",
        "                \n",
        "                elif pattern_entity_id.match(lineif[3]):\n",
        "                    # *[number]: *[26] -> 26\n",
        "                    entity_id = int(lineif[3][2:-1])\n",
        "                    \n",
        "                    # PERSON[26]\n",
        "                    tmp = lineif[4].find(\"[\")\n",
        "                    \n",
        "                    assert (entity_id == int(lineif[4][(tmp+1):-1])), str(\"Entity ID in 4th and 5th column are not the same. In doc: \" + file + \" line \" + line)\n",
        "                    \n",
        "                    assert (lineif[4][:tmp] in constant[\"entity_name\"]), str(\"Unknown entity name in doc: \" + file + \" line \" + line)\n",
        "                    \n",
        "                    entity_n = lineif[4][:tmp]\n",
        "\n",
        "                    entity.append([entity_id, entity_n])\n",
        "\n",
        "                # may be we dont need this last else because we use regex above\n",
        "                else:\n",
        "                    assert False, str(\"4th, 5th column has UNKNOWN MISTAKE. In Doc: \" + file + \"\\nline: \" + line)\n",
        "\n",
        "\n",
        "\n",
        "            if len(lineif) == 5:\n",
        "                relation.append(None)\n",
        "            \n",
        "\n",
        "            if len(lineif) == 7:\n",
        "                # example:\n",
        "                # AFFILIATION\t1-593[13_14]\n",
        "                # PART  WHOLE\t1-42[1_2]\n",
        "                # PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-80[3_8]|1-105[7_8]\n",
        "\n",
        "                # PART  WHOLE\t1-42    (an error in dataset that need to be handled)\n",
        "\n",
        "                rel_names = lineif[5].split(\"|\")    # PERSONAL - SOCIAL|PERSONAL - SOCIAL --> [\"PERSONAL - SOCIAL\", \"PERSONAL - SOCIAL\"]\n",
        "                rel_oifs = lineif[6].split(\"|\")     # 1-80[3_8]|1-105[7_8] --> [\"1-80[3_8]\", \"1-105[7_8]\"]\n",
        "\n",
        "                # in doc: 23351515\n",
        "                # line: 1-318\n",
        "                # 6th column: PART  WHOLE|LOCATED|PART  WHOLE|*\n",
        "                # last relation name is: *  -> mistake\n",
        "                # We can read data and change it to right one \n",
        "                # but I will remove this \"*\" relation in 6th and 7th column\n",
        "\n",
        "                if '*' in rel_names:\n",
        "                    tmp = rel_names.index('*')\n",
        "\n",
        "                    del rel_names[tmp]\n",
        "                    del rel_oifs[tmp]\n",
        "\n",
        "                    print(\"\\nRELATION MISTAKE IN 6TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                # MISTAKE In doc: 23351856\n",
        "                # line: 1-185\t807-812\tTriu\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
        "                # assert ((len(rel_names) == len(rel_oifs)) and (len(rel_names) >= 1)), str(\"Number of relations in 6th and 7th columns is different to each other. In doc: \" + file + \" line \" + line )\n",
        "                # handle later\n",
        "\n",
        "                assert (len(rel_names) == len(rel_oifs)), str(\"Number of relations in 6th and 7th columns is different to each other. \\nIn doc: \" + file + \" \\nline \" + line )\n",
        "\n",
        "\n",
        "                rels = []\n",
        "                for i in range(len(rel_names)):\n",
        "                    assert (rel_names[i] in constant[\"relation_name\"]), \\\n",
        "                    str(\"Unknown relation_name in doc: \" + file + \" \\nline \" + line)\n",
        "                    \n",
        "                    relation_n = rel_names[i]\n",
        "\n",
        "                    # (startwith \"1-\") then (number) then ([ char) then (number) then (_ char) then (number) then (] char) end\n",
        "                    #             1-         26            [             3             _             0             ]   \n",
        "                    pattern_relation_oif = re.compile(\"^(1-)([\\d]+)(\\[)([\\d]+)(\\_)([\\d]+)(\\])$\")\n",
        "\n",
        "                    # (startwith \"1-\") then (number) then (.1) then ([ char) then (number) then (_ char) then (number) then (] char) end\n",
        "                    #             1-         26            .1        [             3             _             0             ]   \n",
        "                    pattern_relation_oif_1 = re.compile(\"^(1-)([\\d]+)(\\.1)(\\[)([\\d]+)(\\_)([\\d]+)(\\])$\")\n",
        "                    \n",
        "                    # below is a mistake in dataset\n",
        "                    # but currently, this type mistake has only below form (only has token id).    <--- TRAIN DOES NOT HAVE, BUT DEV HAS\n",
        "                    # (don't have subtoken id mistake type, yet)   <--- TRAIN DOES NOT HAVE, BUT DEV HAS\n",
        "                    # (startwith \"1-\") then (number)  end\n",
        "                    #             1-         26          \n",
        "                    pattern_relation_oif_mistake_1 = re.compile(\"^(1-)([\\d]+)$\")\n",
        "\n",
        "\n",
        "                    # below is a mistake in dataset <--- subtoken id only\n",
        "                    # (startwith \"1-\") then (number) then (.1) end\n",
        "                    #             1-         26            .1\n",
        "                    pattern_relation_oif_mistake_2 = re.compile(\"^(1-)([\\d]+)(\\.1)$\")\n",
        "\n",
        "\n",
        "\n",
        "                    assert (pattern_relation_oif.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_1.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_mistake_1.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_mistake_2.match(rel_oifs[i])), \\\n",
        "                            str(\"Unexpected seventh column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "                    \n",
        "\n",
        "                    # NOTICE:\n",
        "                    # IN BELOW CODE, I DONT CHECK IF ONE OF TWO ENTITIES OF A RELATION\n",
        "                    # IS \"MISCELLANEOUS\" OR NOT. \n",
        "                    # MISCELLANEOUS IS A LEGIT ENTITY NAME, BUT IT IS NOT USED IN ANY RELATION TYPE.\n",
        "                    # I WONDER IF DATASET HAS THIS MISTAKE OR NOT.\n",
        "                    # I WILL CHECK IT WHEN I CREATE SENTENCES AS INPUT OF BERT.\n",
        "\n",
        "\n",
        "                    if pattern_relation_oif.match(rel_oifs[i]):\n",
        "                        # 1-id[id_id]\n",
        "\n",
        "                        tmp_stkid = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_etkid = rel_oifs[i].find(\"[\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp_stkid:tmp_etkid]\n",
        "\n",
        "                        # start subtoken id\n",
        "                        sstoken_id = None\n",
        "\n",
        "                        direction = rel_oifs[i][(tmp_etkid+1):-1]\n",
        "\n",
        "                        direction = direction.split(\"_\")   # [sentity_id, eentity_id]\n",
        "\n",
        "                        rels.append([relation_n, int(stoken_id), sstoken_id, [int(direction[0]), int(direction[1])]])\n",
        "\n",
        "                    elif pattern_relation_oif_1.match(rel_oifs[i]):\n",
        "                        # 1-id.subid[id_id]\n",
        "\n",
        "                        tmp_sid = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_eid = rel_oifs[i].find(\".\")\n",
        "\n",
        "                        tmp_ssid = rel_oifs[i].find(\".\") + 1\n",
        "                        tmp_esid = rel_oifs[i].find(\"[\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp_sid:tmp_eid]\n",
        "\n",
        "                        sstoken_id = rel_oifs[i][tmp_ssid:tmp_esid]\n",
        "\n",
        "\n",
        "                        direction = rel_oifs[i][(tmp_esid+1):-1]\n",
        "                        direction = direction.split(\"_\")\n",
        "\n",
        "                        rels.append([relation_n, int(stoken_id), int(sstoken_id), [int(direction[0]), int(direction[1])]])\n",
        "\n",
        "                        print(\"\\nSPECIAL SUBTOKEN IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "                    \n",
        "                    elif pattern_relation_oif_mistake_1.match(rel_oifs[i]):\n",
        "                        # 1-id\n",
        "                        tmp = rel_oifs[i].find(\"-\") + 1\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp:]\n",
        "                        \n",
        "                        sstoken_id = None\n",
        "                        direction = None\n",
        "\n",
        "                        # rels.append([relation_n, stoken_id, sstoken_id, direction])\n",
        "                        rels.append([relation_n, int(stoken_id), None, None])\n",
        "\n",
        "                        print(\"\\nMISTAKE IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "                    \n",
        "                    elif pattern_relation_oif_mistake_2.match(rel_oifs[i]):\n",
        "                        # 1-id.1\n",
        "                        tmp = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_1 = rel_oifs[i].find(\".1\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp:tmp_1]\n",
        "                        \n",
        "                        sstoken_id = 1\n",
        "                        direction = None\n",
        "\n",
        "                        # rels.append([relation_n, stoken_id, sstoken_id, direction])\n",
        "                        rels.append([relation_n, int(stoken_id), sstoken_id, None])\n",
        "\n",
        "                        print(\"\\nMISTAKE IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "                        print(stoken_id, ' - ', sstoken_id)\n",
        "                    \n",
        "                    else:\n",
        "                        assert False, str(\"Unexpected seventh column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "\n",
        "                \n",
        "                if len(rels) == 0:\n",
        "                    # MISTAKE In doc: 23351856\n",
        "                    # line: 1-185\t807-812\tTriu\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
        "                    relation.append(None)\n",
        "                    print(\"\\nREALTION NAME MISTAKE IN 6TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                else:\n",
        "                    relation.append(rels)\n",
        "\n",
        "\n",
        "        docif[\"token_ids\"] = copy.deepcopy(token_ids)\n",
        "        docif[\"subtoken_ids\"] = copy.deepcopy(subtoken_ids)\n",
        "        docif[\"pos\"] = copy.deepcopy(pos)\n",
        "        docif[\"tokens\"] = copy.deepcopy(tokens)\n",
        "        docif[\"entity\"] = copy.deepcopy(entity)\n",
        "        docif[\"relation\"] = copy.deepcopy(relation)\n",
        "\n",
        "    raw_tdata.append(copy.deepcopy(docif))\n",
        "\n",
        "\n",
        "print(len(raw_tdata))           \n",
        "                \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvsWfSzerWz5"
      },
      "source": [
        "# CHECK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABXxWzmtrb6c",
        "outputId": "90a9a797-cbe8-46f1-b03b-a0932426847b"
      },
      "source": [
        "for docif in raw_tdata:\n",
        "\n",
        "    for iline in range(len(docif['token_ids'])):\n",
        "        if docif['subtoken_ids'][iline] != None:\n",
        "            print('FOUND SUBTOKEN')\n",
        "            assert Fasle, 'FOUND SUBTOKEN'\n",
        "\n",
        "        if docif['relation'][iline] != None:\n",
        "            print('FOUND RELATION')\n",
        "            assert Fasle, 'FOUND RELATION'\n",
        "        \n",
        "\n",
        "\n",
        "print('SEEM THAT TEST DOES NOT HAS SUBTOKENS AND RELATIONS')\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SEEM THAT TEST DOES NOT HAS SUBTOKENS AND RELATIONS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wzBGIIgKYPD"
      },
      "source": [
        "# Fix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruagt4UGbqL5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8bb639ca-2200-4336-c661-c2b4277dd2d3"
      },
      "source": [
        "\"\"\"\n",
        "raw_data: - is a list (len = 506)\n",
        "          - each row is a dict {}, information from a single doc (506 doc)\n",
        "            - doc_id: number id in name of folder that contain doc\n",
        "            - text: text in line start with \"#Text=\"\n",
        "            - token_ids: list of int. (1st column)\n",
        "            - subtoken_ids: list. (1st column)\n",
        "                            an element can be None if token is not a subtoken: 1-id -> 1.26,\n",
        "                                           or int (subtoken_id) if token is a subtoken: 1-id.subid -> 1.26.1.\n",
        "                                           currently in train data, only exist subtoken id 1.\n",
        "                            (in extract raw data code, my code can get any subid, not just specify subid = 1.\n",
        "                             but i check if in data has other subid, it will return error -> to know more about data)\n",
        "            - pos: list of child list. (2st column)\n",
        "                   each child list has two int elements.\n",
        "                   [start_position, end_position]\n",
        "            - tokens: list of strings. (3th column)\n",
        "            - entity: list.\n",
        "                      an element is: None if crr token is not entity\n",
        "                                     a list with: 2 element if crr token is an entity.\n",
        "                                                  [entity_id, entity_name]\n",
        "                                                  entity_id: int, from 4th column\n",
        "                                                  entity_name: string, from 5th column\n",
        "            - relation: list\n",
        "                        an element is: None if there is no relation in 6ht, 7th column.\n",
        "                                       a list of child list. number of child list is number of relation in 6th, 7th column.\n",
        "                                                 each child list has: 4 elemnt\n",
        "                                                 [relation_name, stoken_id, sstoken_id, direction[sentity_id, eentity_id]]\n",
        "                                                 relation_name: string, from 6h column\n",
        "                                                 stoken_id: int, tokenid from 7th column\n",
        "                                                 sstoken_id: from 7th column\n",
        "                                                             None, if entity_1 is a token\n",
        "                                                             else: int, subid if entity_1 is subtoken\n",
        "                                                 direction: from 7th column\n",
        "                                                            None, if there is a mistake in dataset\n",
        "                                                            else: [entity_1_id, entity_2_id]\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nraw_data: - is a list (len = 506)\\n          - each row is a dict {}, information from a single doc (506 doc)\\n            - doc_id: number id in name of folder that contain doc\\n            - text: text in line start with \"#Text=\"\\n            - token_ids: list of int. (1st column)\\n            - subtoken_ids: list. (1st column)\\n                            an element can be None if token is not a subtoken: 1-id -> 1.26,\\n                                           or int (subtoken_id) if token is a subtoken: 1-id.subid -> 1.26.1.\\n                                           currently in train data, only exist subtoken id 1.\\n                            (in extract raw data code, my code can get any subid, not just specify subid = 1.\\n                             but i check if in data has other subid, it will return error -> to know more about data)\\n            - pos: list of child list. (2st column)\\n                   each child list has two int elements.\\n                   [start_position, end_position]\\n            - tokens: list of strings. (3th column)\\n            - entity: list.\\n                      an element is: None if crr token is not entity\\n                                     a list with: 2 element if crr token is an entity.\\n                                                  [entity_id, entity_name]\\n                                                  entity_id: int, from 4th column\\n                                                  entity_name: string, from 5th column\\n            - relation: list\\n                        an element is: None if there is no relation in 6ht, 7th column.\\n                                       a list of child list. number of child list is number of relation in 6th, 7th column.\\n                                                 each child list has: 4 elemnt\\n                                                 [relation_name, stoken_id, sstoken_id, direction[sentity_id, eentity_id]]\\n                                                 relation_name: string, from 6h column\\n                                                 stoken_id: int, tokenid from 7th column\\n                                                 sstoken_id: from 7th column\\n                                                             None, if entity_1 is a token\\n                                                             else: int, subid if entity_1 is subtoken\\n                                                 direction: from 7th column\\n                                                            None, if there is a mistake in dataset\\n                                                            else: [entity_1_id, entity_2_id]\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQKOe59wfEcS"
      },
      "source": [
        "## Fix2: Others fix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpazvkcLtgm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844404bb-ec82-4929-84fa-99f02cf7d25d"
      },
      "source": [
        "\n",
        "# doc: 23352816\n",
        "# hai token 152, 153 mc d cng c entity_id = 0 v ng cnh nhau\n",
        "# nhng hai token ny khng phi l 1 entity m l 2 entity v chng c entity_name khc nhau v chng thuc 2 cu khc nhau\n",
        "# ngoi ra token 153: \". VMISS\" dnh du chm  cu trc, v y cng l 1 entity\n",
        "# ta cn sa li trong doc ny: tch \". VMISS\" thnh 2 token \".\" v \"VMISS\"\n",
        "# c ngha l ta s thm 1 dng na cho \".\" v sa li \". VMISS\" thnh \"VMISS\"\n",
        "\n",
        "raw_tdata_new = copy.deepcopy(raw_tdata)\n",
        "\n",
        "for docif in raw_tdata_new:\n",
        "    if docif['doc_id'] == '23352918':\n",
        "\n",
        "        for i in range(len(docif['token_ids'])):\n",
        "\n",
        "            if (docif['tokens'][i] == \"b.Hassan\") and (docif['tokens'][i-1] == \"khng\"):\n",
        "                \n",
        "                # them 1 dong cho dau '.'\n",
        "                docif['token_ids'].insert(i, docif['token_ids'][i])\n",
        "                docif['pos'].insert(i, [docif['pos'][i][0], (docif['pos'][i][0]+3)])\n",
        "                docif['tokens'].insert(i, 'b.')   # copy k t '.' trong doc ri paste vo  trnh li unicode\n",
        "                docif['subtoken_ids'].insert(i, None)\n",
        "                docif['entity'].insert(i, None)\n",
        "                docif['relation'].insert(i, None)\n",
        "\n",
        "                # sua lai dong '. VMISS' (do thm '.' vo trc nn dng '. VMISS' thnh i+1)\n",
        "                docif['tokens'][i+1] = 'Hassan'\n",
        "                docif['pos'][i+1] = [docif['pos'][i+1][0] + 4, docif['pos'][i+1][1]]\n",
        "\n",
        "                # do them 1 dong nen phai sua lai token_ids phia sau va relation link toi token_ids phia sau\n",
        "                # thay i token_ids ca ton b phn di, nu c subid trong relation  u th thay i, thay i stoken_id trong relation\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if j > i:\n",
        "                        docif['token_ids'][j] += 1\n",
        "\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            # do  trn, ton b token_ids pha sau (> i) s b thay i (cng thm 1)\n",
        "                            # nn nhng relation c stoken_id nm  phn pha sau ny cng s cn thay i theo (cng thm 1)\n",
        "                            if (docif['relation'][j][k][1] > docif['token_ids'][i]):\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1\n",
        "\n",
        "\n",
        "\n",
        "print('\\n\\nCHECKING')\n",
        "for idoc, docif in enumerate(raw_tdata_new):\n",
        "\n",
        "    if docif['doc_id'] == '23352918':\n",
        "\n",
        "        relation_lst = []\n",
        "        for i in range(len(raw_tdata[idoc]['relation'])):\n",
        "            if raw_tdata[idoc]['relation'][i] != None:\n",
        "                relation_lst.append(raw_tdata[idoc]['relation'][i])\n",
        "\n",
        "\n",
        "        relation_ith = 0\n",
        "        for i in range(len(docif['token_ids'])):\n",
        "\n",
        "            assert (docif['tokens'][i] == docif['text'][docif['pos'][i][0]:docif['pos'][i][1]]), \\\n",
        "            str('Wrong position')\n",
        "\n",
        "            if i < (len(docif['token_ids']) - 1):\n",
        "                if (docif['token_ids'][i] + 1) != docif['token_ids'][i+1]:\n",
        "                    assert False, str('Tokens_ids has problem')\n",
        "            \n",
        "            if docif['relation'][i] != None:\n",
        "                for j in range(len(docif['relation'][i])):\n",
        "   \n",
        "                    # so snh xem thay i stoken_id c ng khng\n",
        "                    # string ca token v pos ca token s khng i so vi raw_tdata\n",
        "                    stoken_new = docif['relation'][i][j][1]\n",
        "                    stoken_new_ele_id = docif['token_ids'].index(stoken_new)\n",
        "                    \n",
        "                    stoken = relation_lst[relation_ith][j][1]\n",
        "                    if relation_lst[relation_ith][j][2] == 1:\n",
        "                        stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken) + 1\n",
        "                    else:\n",
        "                        stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken)\n",
        "\n",
        "                    if docif['pos'][stoken_new_ele_id] != raw_tdata[idoc]['pos'][stoken_ele_id]:\n",
        "                        '''\n",
        "                        print('\\n-----Doc: ', docif['doc_id'])\n",
        "                        print(relation_ith)\n",
        "                        print(docif['relation'][i])\n",
        "                        print(relation_lst[relation_ith])\n",
        "                        print(docif['pos'][stoken_new_ele_id])\n",
        "                        print(raw_tdata[idoc]['pos'][stoken_ele_id])\n",
        "                        '''\n",
        "                        assert False, str('ERROR CODE 3')\n",
        "                    \n",
        "                relation_ith += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('DONE. EVERYTHINGS SEEM TO BE CORRECTED :D')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "CHECKING\n",
            "DONE. EVERYTHINGS SEEM TO BE CORRECTED :D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QeZVNtl9rxT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a27a7fa8-90e1-4d7f-ad55-4eb558f9c10e"
      },
      "source": [
        "'''\n",
        "for idoc, docif in enumerate(raw_tdata_new_v2):\n",
        "\n",
        "    if docif['doc_id'] == '23352816':\n",
        "        for i in range(len(docif['token_ids'])):\n",
        "            for key in docif:\n",
        "                if key not in ['doc_id', 'text']:\n",
        "                    print(docif[key][i], end='\\t')\n",
        "            print('\\n')\n",
        "'''"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfor idoc, docif in enumerate(raw_tdata_new_v2):\\n\\n    if docif['doc_id'] == '23352816':\\n        for i in range(len(docif['token_ids'])):\\n            for key in docif:\\n                if key not in ['doc_id', 'text']:\\n                    print(docif[key][i], end='\\t')\\n            print('\\n')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0VsUK91_VNX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8be83d51-3734-460a-d0a7-d990a65fb216"
      },
      "source": [
        "'''\n",
        "### doc: 23352687\n",
        "# sent: Vn Mnh hng thng tt T37  thnh cng  ni dung chy 200m.\n",
        "# trong doc ny, trc cu trn c 2 du cch, khi dng Underthesea th Underthesea tch cu chnh xc l b 2 du cch i.\n",
        "# nhng trong doc th tokenize b li, t \"Vn\" trong doc l \" Vn\" tc l dnh du 1 cch\n",
        "# dn ti start pos ca entity khc vi start pos ca cu (chnh nhau 1 n v cho chnh nhau 1 du cch)\n",
        "# phng n: sa li pos v token ca entity trong trng hp ny, b i du cch tha. \n",
        "\n",
        "raw_tdata_new_v3 = copy.deepcopy(raw_tdata_new_v2)\n",
        "\n",
        "for docif in raw_tdata_new_v3:\n",
        "    \n",
        "    if docif['doc_id'] == '23352687':\n",
        "        for i in range(len(docif['tokens'])):\n",
        "            if docif['tokens'][i] == \"Vn\":\n",
        "                print(docif[\"pos\"][i][0])\n",
        "                print(repr(docif[\"tokens\"][i]))\n",
        "\n",
        "                docif[\"pos\"][i][0] = docif[\"pos\"][i][0] + 1   # b 1 du cch  u th pos s dch ln 1 n v\n",
        "                docif[\"tokens\"][i] = docif[\"tokens\"][i].lstrip()   # b du cch i\n",
        "\n",
        "                assert (docif['text'][docif[\"pos\"][i][0]:docif[\"pos\"][i][1]] == docif[\"tokens\"][i]), \\\n",
        "                str(\"Change Failed!\")\n",
        "\n",
        "                print(docif[\"pos\"][i][0])\n",
        "                print(repr(docif[\"tokens\"][i]))\n",
        "\n",
        "                break\n",
        "        break\n",
        "\n",
        "\n",
        "'''\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n### doc: 23352687\\n# sent: Vn Mnh hng thng tt T37  thnh cng  ni dung chy 200m.\\n# trong doc ny, trc cu trn c 2 du cch, khi dng Underthesea th Underthesea tch cu chnh xc l b 2 du cch i.\\n# nhng trong doc th tokenize b li, t \"Vn\" trong doc l \" Vn\" tc l dnh du 1 cch\\n# dn ti start pos ca entity khc vi start pos ca cu (chnh nhau 1 n v cho chnh nhau 1 du cch)\\n# phng n: sa li pos v token ca entity trong trng hp ny, b i du cch tha. \\n\\nraw_tdata_new_v3 = copy.deepcopy(raw_tdata_new_v2)\\n\\nfor docif in raw_tdata_new_v3:\\n    \\n    if docif[\\'doc_id\\'] == \\'23352687\\':\\n        for i in range(len(docif[\\'tokens\\'])):\\n            if docif[\\'tokens\\'][i] == \"\\xa0Vn\":\\n                print(docif[\"pos\"][i][0])\\n                print(repr(docif[\"tokens\"][i]))\\n\\n                docif[\"pos\"][i][0] = docif[\"pos\"][i][0] + 1   # b 1 du cch  u th pos s dch ln 1 n v\\n                docif[\"tokens\"][i] = docif[\"tokens\"][i].lstrip()   # b du cch i\\n\\n                assert (docif[\\'text\\'][docif[\"pos\"][i][0]:docif[\"pos\"][i][1]] == docif[\"tokens\"][i]),                 str(\"Change Failed!\")\\n\\n                print(docif[\"pos\"][i][0])\\n                print(repr(docif[\"tokens\"][i]))\\n\\n                break\\n        break\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBr6Gx-Y2oj2"
      },
      "source": [
        "## Find all entity in a doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVl3FKeND9dk"
      },
      "source": [
        "# Tm tt c cc entity trong mt doc:\n",
        "\n",
        "def find_all_entity_in_doc(raw_data, doc_id):\n",
        "    \n",
        "    for docif in raw_data:\n",
        "\n",
        "        ######## Tm tt c cc entity trong doc hin ti:\n",
        "\n",
        "        #                      -                                          entity_1                                              -  ...\n",
        "        #                      | -                 token_1                  -  -                   token_2                -  ...|\n",
        "        ##### doc_entity_lst = [ [ [ele_id, token_id, entity_id, entity_name], [ele_id, token_id, entity_id, , entity_name], ...], ...]\n",
        "\n",
        "        ### Lu : ele_id  y l element index ca token y trong docif[\"entity\"]\n",
        "        ### ch khng phi l token_ids\n",
        "        ### lu ci ele_id thay v token_ids  truy cp token bng index nhanh hn\n",
        "        ### nu lu token_ids th phi t t token_ids tm xem token ny n nm v tr no th mi ra index (ele_id)  truy\n",
        "        ### thng th, v token_ids bt u = 1, element index bt u = 0 nn: token_ids tng ng s c: token_ids = ele_id + 1\n",
        "        ### nhng nu trong doc c subtoken th iu trn s khng c m bo\n",
        "\n",
        "        # token c entity_id ging nhau m khng ng cnh nhau (cc dng cha token khng lin tip nhau) th thuc 2 entity khc nhau. \n",
        "        # Trng hp ny l do dataset li, b lp entity_id (bn trn c entity_id = 0 ri xung di (token khng cnh nhau) li thy entity_id = 0)\n",
        "        # Hin ti mi thy c entity_id = 0 l b lp li cho nhiu entity khc nhau\n",
        "\n",
        "        \n",
        "\n",
        "        if docif['doc_id'] == doc_id:\n",
        "\n",
        "            doc_entity_lst = []\n",
        "            tmplst = []\n",
        "\n",
        "            for i in range(len(docif[\"entity\"])):\n",
        "                if docif[\"entity\"][i] != None:\n",
        "                    tmplst.append([i, docif[\"token_ids\"][i], docif[\"entity\"][i][0], docif[\"entity\"][i][1]])\n",
        "\n",
        "                    if (i < (len(docif[\"entity\"]) - 1)):\n",
        "\n",
        "                        if docif[\"entity\"][i+1] == None:\n",
        "                            doc_entity_lst.append(tmplst)\n",
        "                            tmplst = []\n",
        "\n",
        "                        elif (docif[\"entity\"][i][0] != docif[\"entity\"][i+1][0]) or (docif[\"entity\"][i][1] != docif[\"entity\"][i+1][1]):\n",
        "                            doc_entity_lst.append(tmplst)\n",
        "                            tmplst = []\n",
        "\n",
        "                    if i == (len(docif[\"entity\"]) - 1):\n",
        "                        doc_entity_lst.append(tmplst)\n",
        "\n",
        "\n",
        "            ###################### Fix li hai entity khc nhau nhng ng cnh nhau v b trng entity_id\n",
        "            # tuy nhin, ta ch tm cc entity_id = 0 thi, v chng d li nht, d b trng id nht\n",
        "            # mi cp entity_id = 0 ng cnh nhau trong cc doc trong list bn di u s b tch ra thnh cc entity ring\n",
        "\n",
        "            ##### comment of V1 <-- trong notebook extract_train th code c hot ng n do khng c cm entity li no > 2 entity\n",
        "            ##### tuy nhin trong dev th xut hin mt s cm 3 entity id 0 b li\n",
        "            # tuy nhin ta s x l tng cp mt trong tng ln x l, run_times l s cp trng trong doc\n",
        "            # nu c doc no va c cp entity_0 li va c cp khng li th phi s khng c thm list di vo m phi x l ring\n",
        "            # tr khi cp b li l cp u tin th run_times t l 1\n",
        "            # data cng khng c qu nhiu nhng cp nh ny\n",
        "            #####\n",
        "            \n",
        "\n",
        "            # chy 2 cell code bn di trc  tm cc cm entity_id = 0 c t 2 token tr ln\n",
        "            #  xem nhng cm no b li, cm no khng b, cm no li m cn sa\n",
        "            # ta s cn ly doc_id v ith ca cm b li d sa\n",
        "            # sau khi sa trong ny, nhng cm c sa s khng cn xut hin khi chy 2 cell bn di na\n",
        "            # nhng cm khng c sa (khng li hoc li m chn khng sa) vn s c in ra\n",
        "\n",
        "            # ngoi ra, do ch ch danh entity cn sa nn d doc c c entity li ln entity khng li th vn sa c\n",
        "\n",
        "            # lu : mi cp entity li c bao nhiu token th s c tch ht ra thnh tng y entity.\n",
        "            # tc l v d entity id 0 li c 3 token th c tch ra thnh 3 entity id 0 ring bit, mi entity ch l 1 token\n",
        "            # code bn di ch x l trng hp ny.\n",
        "            # khng x l trng hp kiu entity id 0 li c 3 token, \n",
        "            # nhng li cn tch ra 2 entity (thay v 3), 1 entity gm 2 token u, 1 entity l 1 token cui\n",
        "            # l do: li qu, v s cm li rt t, trng hp nh kia th cng rt t hn v c khi k xy ra\n",
        "\n",
        "\n",
        "            \n",
        "            doc_error_lst = [{'doc_error_id': '23352918', 'ith_er_lst': [6]},\n",
        "                             {'doc_error_id': '23352926', 'ith_er_lst': [3]},\n",
        "                             {'doc_error_id': '23352941', 'ith_er_lst': [0, 2, 26, 30]},\n",
        "                             {'doc_error_id': '23352961', 'ith_er_lst': [1]},\n",
        "                             {'doc_error_id': '23352963', 'ith_er_lst': [1, 38]},\n",
        "                             {'doc_error_id': '23352975', 'ith_er_lst': [10]},\n",
        "                             {'doc_error_id': '23353032', 'ith_er_lst': [7]},\n",
        "                             {'doc_error_id': '24503940', 'ith_er_lst': [37]},\n",
        "                             {'doc_error_id': '24568398', 'ith_er_lst': [41, 43, 57, 99, 101, 103]},\n",
        "                             {'doc_error_id': '24657217', 'ith_er_lst': [17]},\n",
        "                             {'doc_error_id': '24681732', 'ith_er_lst': [0, 8]},\n",
        "                             {'doc_error_id': '24796868', 'ith_er_lst': [2, 4, 5]},\n",
        "                             {'doc_error_id': '24839422', 'ith_er_lst': [12]},\n",
        "                             {'doc_error_id': '24847781', 'ith_er_lst': [9]},\n",
        "                             {'doc_error_id': '24854363', 'ith_er_lst': [0]},\n",
        "                             {'doc_error_id': '24891267', 'ith_er_lst': [209]}\n",
        "                            ]\n",
        "\n",
        "            doc_error_id_lst = [doc_error['doc_error_id'] for doc_error in doc_error_lst]\n",
        "\n",
        "            increase_ids = 0\n",
        "\n",
        "            if doc_id in doc_error_id_lst:\n",
        "                ith_doc_id = doc_error_id_lst.index(doc_id)\n",
        "\n",
        "                assert (doc_error_lst[ith_doc_id]['doc_error_id'] == doc_id), str('PRBOLEM')\n",
        "\n",
        "                #print('\\n\\n------', doc_id)\n",
        "\n",
        "                ith_er_list = sorted(doc_error_lst[ith_doc_id]['ith_er_lst'])\n",
        "\n",
        "                for irun, ith_er_id in enumerate(ith_er_list):\n",
        "                    #print('--', irun)\n",
        "                    doc_entity_lst_copy = None\n",
        "                    doc_entity_lst_copy = copy.deepcopy(doc_entity_lst)\n",
        "\n",
        "                    for ient, ent in enumerate(doc_entity_lst):\n",
        "                        if ient == (ith_er_id + increase_ids): # do b dch nn cn cng vi s id b dch\n",
        "\n",
        "                            assert ((ent[0][2] == 0) and (len(ent) > 1)), \\\n",
        "                            str('\\nWrong ith_er_lst. \\nDoc: ' + str(doc_id) + '\\nith_er_id: ' + str(ith_er_id))\n",
        "\n",
        "                            #print(doc_entity_lst_copy)\n",
        "\n",
        "                            # v d: 3 token th ch cn chy 3 - 1 = 2 ln\n",
        "                            # ln u (itk = 0) th ly token cui entity (token th 3: ent[-1]) chn vo sau v tr hin ti ca entity\n",
        "                            # ln hai (itk = 1) th ly token ngay trc token cui (token th 2 t cui ln: ent[-2]) chn vo sau v tr hin ti ca entity\n",
        "                            for itk in range(len(ent) - 1):\n",
        "                                doc_entity_lst_copy.insert((ient+1), copy.deepcopy([ent[(-1 - itk)]]))\n",
        "\n",
        "                            # cui cng th bin entity li hin ti thnh token u ca entity li hin ti l xong\n",
        "                            doc_entity_lst_copy[ient] = copy.deepcopy([ent[0]])\n",
        "\n",
        "                            #print(doc_entity_lst_copy)\n",
        "\n",
        "                            # do bn trn ta chn thm (len(ent) - 1) entity mi vo entity list\n",
        "                            # nn id ca entity b li pha sau s b tng ln lng tng ng\n",
        "                            # l tng (len(ent)-1) ca mi ent b sa trc n\n",
        "                            increase_ids += (len(ent) - 1)\n",
        "\n",
        "                            break\n",
        "                            \n",
        "                    doc_entity_lst = copy.deepcopy(doc_entity_lst_copy)\n",
        "                    #print(doc_entity_lst)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "            \n",
        "\n",
        "\n",
        "            '''\n",
        "            # Doc: 23351965\n",
        "            # hai token cnh nhau (320, 321), cng entity_id 0, cng entity_name location nhng khng phi l 1 entity <-- li data\n",
        "            # m l hai entity, v entity ny link ti entity kia.\n",
        "\n",
        "            if docif['doc_id'] == '23351965':\n",
        "                doc_entity_lst_copy = copy.deepcopy(doc_entity_lst)\n",
        "                for ient, ent in enumerate(doc_entity_lst):\n",
        "                    if (len(ent) == 2) and (ent[0][2] == 0) and (ent[1][2] == 0):\n",
        "                        #print(doc_entity_lst_copy)\n",
        "                        doc_entity_lst_copy.insert((ient+1), [doc_entity_lst_copy[ient][1]])\n",
        "                        doc_entity_lst_copy[ient] = [doc_entity_lst_copy[ient][0]]\n",
        "                        #print(doc_entity_lst_copy)\n",
        "                        \n",
        "                doc_entity_lst = copy.deepcopy(doc_entity_lst_copy)\n",
        "                #print(doc_entity_lst)\n",
        "\n",
        "\n",
        "            # Doc: 23352753  b ging bn trn\n",
        "            # hai token cnh nhau (884, 885), cng entity_id 0, cng entity_name location nhng khng phi l 1 entity <-- li data\n",
        "            # m l hai entity, v entity ny link ti entity kia.\n",
        "\n",
        "            if docif['doc_id'] == '23352753':\n",
        "                doc_entity_lst_copy_2 = copy.deepcopy(doc_entity_lst)\n",
        "                for ient, ent in enumerate(doc_entity_lst):\n",
        "                    if (len(ent) == 2) and (ent[0][2] == 0) and (ent[1][2] == 0):\n",
        "                        #print(doc_entity_lst_copy_2)\n",
        "                        doc_entity_lst_copy_2.insert((ient+1), [doc_entity_lst_copy_2[ient][1]])\n",
        "                        doc_entity_lst_copy_2[ient] = [doc_entity_lst_copy_2[ient][0]]\n",
        "                        #print(doc_entity_lst_copy_2)\n",
        "                        \n",
        "                doc_entity_lst = copy.deepcopy(doc_entity_lst_copy_2)\n",
        "                #print(doc_entity_lst)\n",
        "            \n",
        "            '''\n",
        "\n",
        "\n",
        "            \n",
        "            return doc_entity_lst\n",
        "\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIA5LQry20nQ"
      },
      "source": [
        "# thng th nhng entity cnh nhau v trng id khin ta d lm thnh 1 entity\n",
        "# th nhng entity ny thng c entity_id = 0\n",
        "# nn ta s tm cc cp entity_id = 0 ny v xem cp no l li cp no khng li\n",
        "# sau  c xem trong doc ny y l li hay khng phi li (c cp khng phi li)\n",
        "# nu l cp li th s phi thm trng hp  cell code bn trn\n",
        "#  v sau dng hm tm entity bn trn s khng b li\n",
        "\n",
        "# MT LU  L: \n",
        "# V D: \n",
        "# 1-209\t935-944\tFrankfurt\t*\tLOCATION\t\n",
        "# 1-210\t945-949\t(c\t*\tLOCATION\t\n",
        "\n",
        "# 1-161\t739-743\tBali\t*\tLOCATION\t_\t_\t\n",
        "# 1-162\t744-754\t(Indonesia\t*\tLOCATION\tPART  WHOLE\t1-161\t\n",
        "               \n",
        "# gn khng chnh xc, bn trn khng c relation nhng bn di li c\n",
        "# nn nu c relation nh bn di th tch ra lm 2 entity\n",
        "# cn khng c relation nh bn trn th  n l mt entity\n",
        "# v nu khng c relation m vn tch ra lm 2 entity th label gia chng s l others, khng chnh xc\n",
        "# nu  l cng 1 entity th s hp l hn. min l  cng entity khng nh hng g\n",
        "# v trong data c rt nhiu label other gia cc location (nh M vi Anh c th l others)\n",
        "# nn nu ta chia ra th sau trong test set cng b  lm others, tc l khng hp l\n",
        "# th  thnh 1, th train data cng th m test data cng th\n",
        "# hoc ta c th tch nhng phi thm nhn part-whole vo\n",
        "\n",
        "# cng c th ton b cc entity_id = 0 cnh nhau u l cc entity khc nha, nhng nu khng phi th s khng hon ho\n",
        "# nn c th xt cc trng hp ring thay v t ng tch cc entity_0 cnh nhau thnh cc entity khc nhau\n",
        "\n",
        "def find_all_fault_entity_id_0(raw_data):\n",
        "\n",
        "    for docif in raw_data:\n",
        "        ent_lst = find_all_entity_in_doc(raw_data, docif['doc_id'])\n",
        "        \n",
        "        for i in range(len(ent_lst)):\n",
        "\n",
        "            if (len(ent_lst[i]) > 1) and (ent_lst[i][0][2] == 0):\n",
        "                print('\\n\\n------', docif['doc_id'], ' -ith: ', i)\n",
        "                for j in range(len(ent_lst[i])):\n",
        "                    first_tk_eleid = ent_lst[i][j][0]\n",
        "                    for key in docif:\n",
        "                        if key not in ['doc_id', 'text']:\n",
        "                            print(docif[key][first_tk_eleid], end='\\t')\n",
        "                    \n",
        "                    print('\\n')\n",
        "        \n",
        "\n",
        "\n",
        "                \n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POdyqoDS5dA1"
      },
      "source": [
        "# tn hm hi gy nhm\n",
        "# cc entity c in ra c th l li hoc l khng li, a phn l li\n",
        "# v cc enity li bn di l quyt nh khng sa\n",
        "#find_all_fault_entity_id_0(raw_tdata_new_v4)\n",
        "find_all_fault_entity_id_0(raw_tdata)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfXerQf-fQSv"
      },
      "source": [
        "# Create train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qce_9RiBsIZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "4d7166b3-dd1e-4779-bc72-b7260722da8d"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "                    label: \n",
        "                    Du x l k hiu relation label (docif[\"relation\"]) xut hin  ch ca entity no\n",
        "\n",
        "                              Entity:                 entity_1    -    entity_2\n",
        "                    Th t trong cu:                 trc            sau\n",
        "                      \n",
        "                      Relation label:     LOCATED                         x     (per/org - loc)\n",
        "                                       IS_LOCATED         x                     (loc     - per/org)\n",
        "                                       PARTWHOLE\t                      x     (part    - whole)\n",
        "                                       WHOLE-PART         x                     (whole   - part)\n",
        "                                  PERSONALSOCIAL                               (Undirected)\n",
        "                                      AFFILIATION\t                      x     \n",
        "                                   AFFILIATION_TO         x\n",
        "                                           OTHERS                               (l nhn gi 2 entity cng 1 cu m khng c relation trong data)\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n                    label: \\n                    Du x l k hiu relation label (docif[\"relation\"]) xut hin  ch ca entity no\\n\\n                              Entity:                 entity_1    -    entity_2\\n                    Th t trong cu:                 trc            sau\\n                      \\n                      Relation label:     LOCATED                         x     (per/org - loc)\\n                                       IS_LOCATED         x                     (loc     - per/org)\\n                                       PARTWHOLE\\t                      x     (part    - whole)\\n                                       WHOLE-PART         x                     (whole   - part)\\n                                  PERSONALSOCIAL                               (Undirected)\\n                                      AFFILIATION\\t                      x     \\n                                   AFFILIATION_TO         x\\n                                           OTHERS                               (l nhn gi 2 entity cng 1 cu m khng c relation trong data)\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqEMUOpfqHZp"
      },
      "source": [
        "original_labels = ['LOCATED', 'PART  WHOLE', 'PERSONAL - SOCIAL', 'AFFILIATION']\n",
        "\n",
        "# entity cha relation nm  pha sau th l label gc\n",
        "labels = {'LOCATED': 'LOCATED', 'IS_LOCATED': 'IS_LOCATED', \n",
        "         'PART_WHOLE': 'PART_WHOLE', 'WHOLE_PART': 'WHOLE_PART', \n",
        "         'PERSONAL_SOCIAL': 'PERSONAL_SOCIAL', \n",
        "         'AFFILIATION': 'AFFILIATION', 'AFFILIATION_TO': 'AFFILIATION_TO', \n",
        "         'OTHERS': 'OTHERS'\n",
        "         }"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8crlbk4MJTlx"
      },
      "source": [
        "### Func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBWuvOzUJUCP"
      },
      "source": [
        "def split_by_colon_punc(doc_sent_tokenize):\n",
        "\n",
        "    new_doc_sent_tokenize = []\n",
        "    for isent, sent in enumerate(doc_sent_tokenize):\n",
        "        if ':' not in sent:\n",
        "            new_doc_sent_tokenize.append(sent)\n",
        "        \n",
        "        else:\n",
        "            new_sents = sent.split(\":\")\n",
        "                \n",
        "            for inew_sent, new_sent in enumerate(new_sents):\n",
        "                if inew_sent != (len(new_sents) - 1):\n",
        "                    new_doc_sent_tokenize.append(str(new_sent.lstrip() + ':'))\n",
        "                else:\n",
        "                    new_doc_sent_tokenize.append(str(new_sent.strip()))\n",
        "\n",
        "    return new_doc_sent_tokenize\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt1PwbNGJG_W"
      },
      "source": [
        "from underthesea import sent_tokenize, word_tokenize\n",
        "\n",
        "def my_sentences_tokenize(doc_id, text):\n",
        "    ######## split sentence from docif[\"text\"] using Underthesea library\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # check if sum length of all sentence < len(text)\n",
        "    len_sentences = [len(s) for s in sentences]\n",
        "    assert (sum(len_sentences) <= len(text)), str(\"\\nSentence tokenize has problem. \\nDoc: \" + docif[\"doc_id\"])\n",
        "\n",
        "\n",
        "\n",
        "    ######\n",
        "    ### trong doc ny vic chia sentence bng Underthesea b li dn ti vic mt entity nm  2 cu.\n",
        "    new_sentences = []\n",
        "    \n",
        "    if doc_id == \"24419297\":  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (sent[-3:] != \"St.\") and (sent[:4] != \"Jude\"):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (sent[-3:] == \"St.\") and (sentences[isent + 1][:4] == \"Jude\"):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "    elif doc_id == \"24530851\":  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (sent[-3:] != \"St.\") and (sent[:4] != \"Luke\") and (sent[:6] != \"Damien\"):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (sent[-3:] == \"St.\") and (sentences[isent + 1][:4] == \"Luke\"):\n",
        "                new_sentences.append(str(sent + sentences[isent + 1]))\n",
        "            \n",
        "            elif (sent[-3:] == \"St.\") and (sentences[isent + 1][:6] == \"Damien\"):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    \n",
        "    elif doc_id == \"24854400\":  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (sent[-3:] != \"JR.\") and (sent[:5] != \"Smith\"):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (sent[-3:] == \"JR.\") and (sentences[isent + 1][:5] == \"Smith\"):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "\n",
        "\n",
        "    elif doc_id == \"24413768\":  # test important\n",
        "        print(sentences)\n",
        "        new_sentences_2 = []\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            \n",
        "\n",
        "            if (\"Danh sch 18 tp th nhn gii thng V lng vng: Cng ty Trch nhim hu hn Vn Minh\" not in sent) and\\\n",
        "            (\"Danh sch 46 c nhn nhn gii thng V lng vng: Phm Quc Thnh (Cng ty Vinasun nh Dng Vit Nam ),\" not in sent):\n",
        "                new_sentences_2.append(sent)\n",
        "            \n",
        "            elif (\"Danh sch 18 tp th nhn gii thng V lng vng: Cng ty Trch nhim hu hn Vn Minh\" in sent):\n",
        "                new_sents = sent.split(\";\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    new_sentences_2.append(str(new_sent.strip()))\n",
        "\n",
        "            elif (\"Danh sch 46 c nhn nhn gii thng V lng vng: Phm Quc Thnh (Cng ty Vinasun nh Dng Vit Nam ),\" in sent):\n",
        "                new_sents = sent.split(\",\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    new_sentences_2.append(str(new_sent.strip()))\n",
        "        \n",
        "        print(new_sentences_2)\n",
        "        sentences = copy.deepcopy(new_sentences_2)\n",
        "        print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id in [\"23352299\", \"23352417\"]:  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"How Do I Look?\" not in sent) and ('Asia' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"How Do I Look?\" in sent) and ('Asia' in sentences[isent + 1]):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "        \n",
        "    ### trong cc doc bn di vic chia sentence bng Underthesea b li dn ti vic mt relation link ti mt entity thuc cu khc.\n",
        "    ### thng do sau tn ngi vit tt c du chm\n",
        "    \n",
        "    elif doc_id == \"23352323\":  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('L Qu D' not in sent) and ('Ha Thi' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif ('L Qu D' in sent) and ('Ha Thi' in sentences[isent + 1]):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "\n",
        "    \n",
        "    elif doc_id == \"23352491\":  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('Reuters' not in sent) and ('Tuyt Mai' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif ('Reuters' in sent) and ('Tuyt Mai' in sentences[isent + 1]):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == \"23352572\":  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"William\" not in sent) and (\"B. Rosen\" not in sent) and ('E. Cashman' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"William\" in sent) and (\"B. Rosen\" in sentences[isent + 1]) and ('E. Cashman' in sentences[isent + 2]):\n",
        "                new_sentences.append(str('Ch huy lc lng M  y l Thiu tng William. B. Rosen v Thiu tng Thy qun lc chin Robert. E. Cashman   xut k hoch rt qun khi Khe Sanh .'))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "        new_sentences_2 = []\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            assert ('' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            if (\"...\" not in sent):\n",
        "                new_sentences_2.append(sent)\n",
        "            \n",
        "            elif (\"...\" in sent):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_2.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences_2.append(str(new_sent.lstrip()))\n",
        "        \n",
        "        #print(new_sentences_2)\n",
        "        sentences = copy.deepcopy(new_sentences_2)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    ######\n",
        "    # c mt s doc c ...\n",
        "\n",
        "    elif doc_id == '23352499':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"gii phng... Hm nay\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"gii phng... Hm nay\" in sent):\n",
        "                tmppp_1 = sent.find(\". Hm nay\")\n",
        "                tmppp_2 = sent.find(\". Mt\")\n",
        "                new_sentences.append(str(sent[:(tmppp_1+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp_1+2):(tmppp_2+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp_2+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "        new_sentences_1 = []\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            \n",
        "\n",
        "            if (\"\" not in sent) or ('UAVnhng' in sent):\n",
        "                new_sentences_1.append(sent)\n",
        "            \n",
        "            elif (\"\" in sent) and ('UAVnhng' not in sent):\n",
        "                new_sents = sent.split(\"\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_1.append(str(new_sent.lstrip() + ''))\n",
        "                    else:\n",
        "                        new_sentences_1.append(str(new_sent.lstrip()))\n",
        "        \n",
        "        #print(new_sentences_1)\n",
        "        sentences = copy.deepcopy(new_sentences_1)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == '23352989':  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"Shinji Kagawa  Nh vo\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"Shinji Kagawa  Nh vo\" in sent):\n",
        "                tmppp = sent.find(\" Nh vo\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    ### nhng doc m ta ch tch theo  test\n",
        "    # cc doc cc cu ch c  hoc mt s doc c hai loi nhng ta ch chia cu c \n",
        "    elif doc_id in ['23352963', '23352995', '24386181', '24419138', '24492634', '24599825', '24600969', \\\n",
        "                    '24630076', '24665411', '24677601', '24689923', '24691721', '24710822', \\\n",
        "                    '24715735', '24743776', '24755376', '24794285', '24799096', '24805251', '24827011', \\\n",
        "                    '24829505', '24854363', '24856833', '24882485', '24894691', '24580249']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "\n",
        "            if (doc_id != '24691721') and (doc_id != '24710822') and (doc_id != '24743776') and (doc_id != '24755376') and \\\n",
        "            (doc_id != '24827011') and (doc_id != '24854363') and (doc_id != '24894691') and (doc_id != '24580249'):\n",
        "                assert ('...' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "\n",
        "            if (\"\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"\" in sent):\n",
        "                new_sents = sent.split(\"\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + ''))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "    ### nhng doc m ta ch tch theo ... test\n",
        "    # cc doc cc cu ch c ... hoc mt s doc c hai loi nhng ta ch chia cu c ...\n",
        "    elif doc_id in ['23352940', '23352949', '23352967', '24410798', '24434007', '24454904', '24477819', \\\n",
        "                    '24569400', '24578615', '24585044', '24587840', '24597278', '24615775', '24624726', \\\n",
        "                    '24645143', '24657217', '24660008', '24681368', '24701718', '24738794', '24753325', \\\n",
        "                    '24797533', '24844657', '24867954', '24881113', '24457046', '24457804']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            \n",
        "            if (doc_id != '24569400') and (doc_id != '24597278') and (doc_id != '24615775') and (doc_id != '24701718') and \\\n",
        "            (doc_id != '24738794'):   # doc nay co ca 2 nhung ta chi chia theo ...\n",
        "                assert ('' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "            \n",
        "\n",
        "            if (\"...\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"...\" in sent):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    \n",
        "\n",
        "    ### nhng doc m ta tch theo c 2  test\n",
        "    elif doc_id in ['23352911', '24651289', '24832737', '24663382']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            \n",
        "\n",
        "            if ((\"...\" not in sent) and ('' not in sent)):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif ((\"...\" in sent) and ('' not in sent)):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ((\"...\" not in sent) and ('' in sent)):\n",
        "                new_sents = sent.split(\"\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + ''))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "\n",
        "            elif ((\"...\" in sent) and ('' in sent)):\n",
        "                new_sents = sent.replace('...', '')\n",
        "                new_sents = new_sents.split('')\n",
        "\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ###### test\n",
        "    # trong doc ny vic chia cu b li, 2 cu b gp thnh 1 cu\n",
        "    # nhng cu ny thng c k t: .\n",
        "    \n",
        "    elif doc_id in ['24546530', '24570211', '24578615', '24686546', '24752878']:   # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('.' in sent) and ('.' != sent[-2:]):\n",
        "                new_sents = sent.split('.')\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '.'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ('.' not in sent) or ('.' == sent[-2:]):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == '24891267':  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"Vin Thiu ... T Hong lun hon\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"Vin Thiu ... T Hong lun hon\" in sent):\n",
        "                tmppp = sent.find(\". T Hong lun hon\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    elif doc_id == '24778288':  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"ca ch.... Ngoi t hp mi\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"ca ch.... Ngoi t hp mi\" in sent):\n",
        "                tmppp = sent.find(\". Ngoi t hp mi\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    \n",
        "    elif doc_id == '24663125':  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"lm vic  gii quyt.. Trng Trung hc c\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"lm vic  gii quyt.. Trng Trung hc c\" in sent):\n",
        "                tmppp = sent.find(\". Trng Trung hc c\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "    elif doc_id == '24825034':  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"ch an ton.. Nm 2017 Cng\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"ch an ton.. Nm 2017 Cng\" in sent):\n",
        "                tmppp = sent.find(\". Nm 2017 Cng\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    # tach dau .  test\n",
        "    \n",
        "    if doc_id in ['23352948', '23353014', '24503940', '24506379', '24557491', '24578615', '24684290', '24688913', \\\n",
        "                  '24711089', '24823386', '24881311']:\n",
        "        \n",
        "        # ith_sent l th t cc cu cn tch du . trong doc (th t cu bt u t 0)\n",
        "        # ith_dot l s th t ca cc du . trong cu m ti cc du . ny ta s tch cu thnh cc phn khc nhau\n",
        "        # ith_dot cn m th t cn thn bng tay (th t du . bt u t 1)\n",
        "        # ith_dot l mt list ca list. list th n ca ith_dot l danh sch v tr nhng du chm m ta s dng  tch cu th n tng n\n",
        "        # trong ith_sent   <- cn lu   ng th t, v ith_sent cn xp theo th t tng dn\n",
        "        # cn lm vy v c th 1 doc c nhiu cn cn tch, ri trong cc cu ny li c cu c nhiu du . cn tch\n",
        "\n",
        "        #  tng: ta duyt cc cu trong doc, da vo ith_sent_lst  bit cu no cn tch du .\n",
        "        # cu no khng cn tch th ta thm lun vo danh sch cc cu trong doc\n",
        "        # cu no cn tch th: ta da tip vo ith_dot_lst  bit ta s tch ti nhng du . no trong cu\n",
        "        # v d cu cn tch ti 2 du .: th 2 v th 3 trong cu (-> t 1 cu tch thnh 3 cu)\n",
        "        # ta s tm v tr index ca cc du . ny trong cu cn tch\n",
        "        # ri da vo index   ct cu thnh cc phn cn chia\n",
        "\n",
        "        doc_nfix_lst = [{'doc_id': '23352948', 'ith_sent_lst': [13], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '23353014', 'ith_sent_lst': [1, 8], 'ith_dot_lst': [[1], [1]]},\n",
        "                        {'doc_id': '24503940', 'ith_sent_lst': [29], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '24506379', 'ith_sent_lst': [14, 24], 'ith_dot_lst': [[1], [1]]},\n",
        "                        {'doc_id': '24557491', 'ith_sent_lst': [0], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '24578615', 'ith_sent_lst': [13], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '24684290', 'ith_sent_lst': [72], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '24688913', 'ith_sent_lst': [34], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '24711089', 'ith_sent_lst': [35], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '24823386', 'ith_sent_lst': [15, 19, 22], 'ith_dot_lst': [[1], [1], [1]]},\n",
        "                        \n",
        "                        {'doc_id': '24881311', 'ith_sent_lst': [42], 'ith_dot_lst': [[1]]}\n",
        "                        ]\n",
        "        \n",
        "        ##### to find pos of ith dot\n",
        "        def find_ith_dot_pos(haystack, needle, n):\n",
        "            start = haystack.find(needle)\n",
        "            while start >= 0 and n > 1:\n",
        "                start = haystack.find(needle, start+len(needle))\n",
        "                n -= 1\n",
        "            return start\n",
        "        #####\n",
        "\n",
        "\n",
        "        crr_doc_nfix = None\n",
        "        for doc_nfix in doc_nfix_lst:\n",
        "            if doc_nfix['doc_id'] == doc_id:\n",
        "                crr_doc_nfix = doc_nfix\n",
        "\n",
        "        new_sentences_6 = []\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            '''\n",
        "            if doc_id not in ['23352948', '24506379', '24578615']:\n",
        "                assert (('...' not in sent) and (\"\" not in sent)), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "            '''\n",
        "\n",
        "\n",
        "            assert (len(crr_doc_nfix['ith_sent_lst']) == len(crr_doc_nfix['ith_dot_lst'])), \\\n",
        "            str('\\nLEN ith_sent_lst not equal to LEN ith_dot_lst. \\nDoc: ' + doc_id)\n",
        "\n",
        "            if isent not in crr_doc_nfix['ith_sent_lst']:\n",
        "                new_sentences_6.append(sent)\n",
        "            \n",
        "            else:\n",
        "                \n",
        "                assert (('...' not in sent) and (\"\" not in sent)), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "                crr_ith_sent = crr_doc_nfix['ith_sent_lst'].index(isent)\n",
        "\n",
        "                ith_dot_pos_lst = [-1, (len(sent)-1)]\n",
        "\n",
        "                for ith_dot in crr_doc_nfix['ith_dot_lst'][crr_ith_sent]:\n",
        "                    dot_pos = find_ith_dot_pos(sent, '.', ith_dot)\n",
        "\n",
        "                    assert (dot_pos >= 0), str('\\nNot found ith dot. \\nDoc: ' + doc_id + '\\nSent: ' + sent)\n",
        "\n",
        "                    ith_dot_pos_lst.insert(-1, dot_pos)\n",
        "                \n",
        "                for iith in range(len(ith_dot_pos_lst) - 1):\n",
        "                    correct_sent = sent[(ith_dot_pos_lst[iith] + 1):(ith_dot_pos_lst[iith+1] + 1)]\n",
        "                    new_sentences_6.append(str(correct_sent).strip())\n",
        "\n",
        "\n",
        "        #print(new_sentences_6)\n",
        "        sentences = copy.deepcopy(new_sentences_6)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    return sentences\n",
        "\n",
        "    ###### \n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_riTWvT_OHxy"
      },
      "source": [
        "def get_sentence_entities(docif, doc_entity_lst, sent, sspos, espos):\n",
        "    \n",
        "    sen_entity_lst = []\n",
        "\n",
        "    for i in range(len(doc_entity_lst)):   # each entity\n",
        "        first_token_eid = doc_entity_lst[i][0][0]\n",
        "        last_token_eid = doc_entity_lst[i][-1][0]\n",
        "            \n",
        "        # nu im u ca cu < im u ca token u v im cui ca token cui < im cui ca cu\n",
        "        # th entity ny l entity ca cu ny\n",
        "        if (sspos <= docif[\"pos\"][first_token_eid][0]) and (docif[\"pos\"][last_token_eid][1] <= espos):\n",
        "            sen_entity_lst.append(doc_entity_lst[i])\n",
        "\n",
        "        # im u ca entity < im u ca cu nhng im cui ca entity li ln hn im u ca cu\n",
        "        # c li: 1 entity nhng thuc 2 cu, tc l 1 phn ca entity thuc cu trc, phn cn li li thuc cu ang xt.\n",
        "        # iu ny c th do chia cu bng Underthesea c vn  hoc dataset c vn \n",
        "        elif (docif[\"pos\"][first_token_eid][0] < sspos) and (sspos < docif[\"pos\"][last_token_eid][1]):\n",
        "                \n",
        "        # trong dataset c li ny. \n",
        "        # tuy nhin khng c nhiu, nn  hiu thm v dataset, ti ch sa chnh xc cc li ny\n",
        "        # v  sa bn trn\n",
        "\n",
        "            assert False, str(\"\\n--- An entity belongs to two sentences instead of just one. (Error Code 1) \\nIn doc: \" + docif[\"doc_id\"] + \"\\nSentence: \" + repr(sent))\n",
        "                \n",
        "        # im u ca entity < im cui ca cu nhng im cui ca entity li ln hn im cui ca cu\n",
        "        # c li: 1 entity nhng thuc 2 cu, tc l 1 phn ca entity thuc cu ang xt, phn cn li li thuc cu sau.\n",
        "        # iu ny c th do chia cu bng Underthesea c vn  hoc dataset c vn \n",
        "        elif (docif[\"pos\"][first_token_eid][0] < espos) and (espos < docif[\"pos\"][last_token_eid][1]):\n",
        "                \n",
        "            assert False, str(\"\\nAn entity belongs to two sentences instead of just one. (Error Code 2) \\nIn doc: \" + docif[\"doc_id\"] + \"\\nSentence: \" + repr(sent))\n",
        "\n",
        "\n",
        "    # c th xy ra trng hp chia cu b li, mt cu to b chia thnh hai cu nh\n",
        "    # mi cu nh li cha cc entity\n",
        "    # nhng entity cu nh ny link ti cu nh kia -> li\n",
        "    # hoc trong data c li, entity cu ny link ti cu khc.\n",
        "    # nn cn xem xem cc relation trong cu c link ti cc entity tm thy trong cu khng\n",
        "        \n",
        "    # hay relation gia 2 entity l ng, nhng stoken_id trong relation khng tr vo token u tin ca entity id kia\n",
        "    # m li tr vo token gia hoc cui entity kia ( fix li ny bn trn)\n",
        "\n",
        "    # v relation ch link ti token_ids ca token u tin trong entity khc\n",
        "    # nn ta s thu thp danh sch token_ids ca cc token u tin cc entity trong cu\n",
        "    first_tkids_lst = []\n",
        "    for i in range(len(sen_entity_lst)):\n",
        "        first_tkids_lst.append(sen_entity_lst[i][0][1])\n",
        "\n",
        "\n",
        "    for i in range(len(sen_entity_lst)):\n",
        "        first_tkeid = sen_entity_lst[i][0][0]\n",
        "\n",
        "        if docif['relation'][first_tkeid] != None:   # tng relation trong cu\n",
        "            for j in range(len(docif['relation'][first_tkeid])):\n",
        "                if docif['relation'][first_tkeid][j][1] not in first_tkids_lst:\n",
        "                        \n",
        "                    '''\n",
        "                    print(str('\\nSentence tokenize has problem. \\nDoc: ' + docif['doc_id'] + '\\nRelation stoken ID: ' + str(docif['relation'][first_tkeid][j][1])  + ' \\nprvSent: ' + sentences[isent-1] + '\\nSent: ' + sent))\n",
        "\n",
        "                    print(docif['relation'][first_tkeid])\n",
        "                    '''\n",
        "\n",
        "                    assert False, \\\n",
        "                    str('\\nSentence tokenize has problem. \\nDoc: ' + docif['doc_id'] + '\\nRelation stoken ID: ' + str(docif['relation'][first_tkeid][j][1])  + ' \\nprvSent: ' + sentences[isent-1] + '\\nSent: ' + sent)\n",
        "\n",
        "\n",
        "\n",
        "    return sen_entity_lst\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZremDvk8tP7"
      },
      "source": [
        "def relation_name_to_sentence_label(relation_name, relation_entity):\n",
        "\n",
        "    sentence_label = None\n",
        "    # nu entity cha relation l entity 1 th label ngc li\n",
        "    if relation_entity == 1:\n",
        "        if relation_name == 'LOCATED':\n",
        "            sentence_label = 'IS_LOCATED'\n",
        "\n",
        "        elif relation_name == 'PART  WHOLE':\n",
        "            sentence_label = 'WHOLE_PART'\n",
        "\n",
        "        elif relation_name == 'PERSONAL - SOCIAL':\n",
        "            sentence_label = 'PERSONAL_SOCIAL'\n",
        "\n",
        "        elif relation_name == 'AFFILIATION':\n",
        "            sentence_label = 'AFFILIATION_TO'\n",
        "\n",
        "        else:\n",
        "            assert False, str('UNKNOW RELATION NAME: ' + relation_name)\n",
        "    \n",
        "    # nu entity cha relation l entity 2 th label gi nguyn\n",
        "    elif relation_entity == 2:\n",
        "        if relation_name == 'LOCATED':\n",
        "            sentence_label = 'LOCATED'\n",
        "\n",
        "        elif relation_name == 'PART  WHOLE':\n",
        "            sentence_label = 'PART_WHOLE'\n",
        "\n",
        "        elif relation_name == 'PERSONAL - SOCIAL':\n",
        "            sentence_label = 'PERSONAL_SOCIAL'\n",
        "\n",
        "        elif relation_name == 'AFFILIATION':\n",
        "            sentence_label = 'AFFILIATION'\n",
        "\n",
        "        else:\n",
        "            assert False, str('UNKNOW RELATION NAME: ' + relation_name)\n",
        "\n",
        "    else:\n",
        "        assert False, (\"Unexpect relation_entity. Expect 1 or 2 but got: \" + relation_entity + \".\")\n",
        "\n",
        "    \n",
        "    return sentence_label"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyIgy3tfLDTs"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKFuhu7TYFB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c1239a-5d27-446a-d2c8-922d2fc52c02"
      },
      "source": [
        "# To train data (cu cha cp entity v label)\n",
        "\n",
        "tdata = []\n",
        "\n",
        "#raw_data = raw_tdata_new_v6\n",
        "raw_data = raw_tdata_new\n",
        "\n",
        "sent_id = 0\n",
        "\n",
        "for docif in raw_data:\n",
        "\n",
        "    # find all entity in current doc    \n",
        "    doc_entity_lst = find_all_entity_in_doc(raw_data, docif['doc_id'])\n",
        "\n",
        "    # if whole doc has 0 or 1 entity -->  no relation in this doc --> skip\n",
        "    # there is many doc like this (like doc has only 3 columns,...)\n",
        "    if len(doc_entity_lst) <= 1:\n",
        "        continue\n",
        "\n",
        "\n",
        "    text = docif[\"text\"]\n",
        "    sentences = my_sentences_tokenize(docif['doc_id'], text)\n",
        "\n",
        "    \n",
        "    ######## extract training sentence\n",
        " \n",
        "    pre_espos = 0   # end of pre sentence\n",
        "\n",
        "    for isent, sent in enumerate(sentences):\n",
        "\n",
        "        sentif = {}\n",
        "        '''\n",
        "        sentif[\"doc_id\"] = docif[\"doc_id\"]\n",
        "        sentif[\"sentence\"] = sent\n",
        "        '''\n",
        "        ###### sentence position\n",
        "        # tm v tr ca cu  da vo  bit entity (cc tokens) thuc cu no\n",
        "\n",
        "        # text may have two indentical sentences\n",
        "        # so we have to find start position of current sentence in the rest of the text that not contain previous sentences.\n",
        "\n",
        "        assert (text[pre_espos:].find(sent) >= 0), str(\"Position has problem. \\nDoc: \" + docif[\"doc_id\"] + \"\\nCurrent sentence: \" + sent)\n",
        "\n",
        "        sspos = text[pre_espos:].find(sent) + pre_espos\n",
        "        espos = sspos + len(sent)\n",
        "\n",
        "        # update pre_espos\n",
        "        pre_espos = espos\n",
        "        \n",
        "        assert (sent == text[sspos:espos]), str(\"Position founded is not matched in text. \\nDoc: \" + docif[\"doc_id\"] + \"\\nCurrent sentence: \" + sent)\n",
        "\n",
        "        '''\n",
        "        sentif[\"spos\"] = [sspos, espos]\n",
        "        '''\n",
        "\n",
        "        ###### get all entity in current sentence\n",
        "        \n",
        "        sen_entity_lst = get_sentence_entities(docif, doc_entity_lst, sent, sspos, espos)\n",
        "\n",
        "        # if current sentence has 0 or 1 entity -> no relation availabel to classify -> skip\n",
        "        if len(sen_entity_lst) <= 1: \n",
        "            continue\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "        ###### create n*(n-1)/2 sentence\n",
        "        \n",
        "        for ient, ent_1 in enumerate(sen_entity_lst):\n",
        "            for jent, ent_2 in enumerate(sen_entity_lst[(ient+1):]):\n",
        "                \n",
        "                first_tkeid_ent1 = ent_1[0][0]   # dng cha token u trong entity\n",
        "                first_tkeid_ent2 = ent_2[0][0]\n",
        "                \n",
        "                last_tkeid_ent1 = ent_1[-1][0]   # dng cha token cui trong entity\n",
        "                last_tkeid_ent2 = ent_2[-1][0]\n",
        "\n",
        "                if (docif['entity'][first_tkeid_ent1][1] != \"MISCELLANEOUS\") and (docif['entity'][first_tkeid_ent2][1] != \"MISCELLANEOUS\"):\n",
        "                    \n",
        "                    # pos ca entity trong doc: start ca token u v end ca token cui trong entity\n",
        "                    ent1_pos_doc = [docif['pos'][first_tkeid_ent1][0], docif['pos'][last_tkeid_ent1][1]]\n",
        "                    ent2_pos_doc = [docif['pos'][first_tkeid_ent2][0], docif['pos'][last_tkeid_ent2][1]]\n",
        "\n",
        "                    # pos ca entity trong cu cha entity\n",
        "                    ent1_pos_sent = [(ent1_pos_doc[0] - sspos), (ent1_pos_doc[1] - sspos)]\n",
        "                    ent2_pos_sent = [(ent2_pos_doc[0] - sspos), (ent2_pos_doc[1] - sspos)]\n",
        "\n",
        "                    \n",
        "                    # kim tra xem pos trong doc v sent c khp, tr v cng entity khng\n",
        "                    assert (sent[ent1_pos_sent[0]:ent1_pos_sent[1]] == text[ent1_pos_doc[0]:ent1_pos_doc[1]]), \\\n",
        "                    str('Entity 1: pos_doc and pos_sent not matched. \\nDoc: ' + docif['doc_id'] + '\\nSent: ' + sent)\n",
        "\n",
        "                    assert (sent[ent2_pos_sent[0]:ent2_pos_sent[1]] == text[ent2_pos_doc[0]:ent2_pos_doc[1]]), \\\n",
        "                    str('Entity 1: pos_doc and pos_sent not matched. \\nDoc: ' + docif['doc_id'] + '\\nSent: ' + sent)\n",
        "\n",
        "\n",
        "                    ent1_text = sent[ent1_pos_sent[0]:ent1_pos_sent[1]]\n",
        "                    ent2_text = sent[ent2_pos_sent[0]:ent2_pos_sent[1]]\n",
        "\n",
        "                    ###############\n",
        "                    # kim tra xem mi token trong entity  c mt trong entity ly t pos hay cha\n",
        "                    for itk, tk in enumerate(ent_1):\n",
        "                        eid_tk = tk[0]\n",
        "\n",
        "                        if itk < (len(ent_1) - 1):\n",
        "                            eid_n_tk = ent_1[itk+1][0]\n",
        "\n",
        "                            assert (docif['pos'][eid_tk][1] < docif['pos'][eid_n_tk][0]), \\\n",
        "                            str(\"Position not increase. Doc: \" + docif['doc_id'] + \"\\nSent: \" + sent + \"\\ncrr-pos: \" + str(docif['pos'][eid_tk][1]) + \"\\nnpos: \" + str(docif['pos'][eid_ntk][0]))\n",
        "\n",
        "\n",
        "                        assert (ent1_pos_doc[0] <= docif['pos'][eid_tk][0]) and (docif['pos'][eid_tk][1] <= ent1_pos_doc[1]), \\\n",
        "                        str('Entity\\'s token pos not inside entity pos')\n",
        "                    \n",
        "                    ###############\n",
        "\n",
        "                    ##########\n",
        "\n",
        "                    assert (ent_1[0][1] == docif['token_ids'][first_tkeid_ent1]), str('NOT MATCHED entity first token id')\n",
        "                    assert (ent_2[0][1] == docif['token_ids'][first_tkeid_ent2]), str('NOT MATCHED entity first token id')\n",
        "\n",
        "                    sentence_label = None\n",
        "\n",
        "                    # nu c 2 entity khng c relation\n",
        "                    if (docif['relation'][first_tkeid_ent1] == None) and (docif['relation'][first_tkeid_ent2] == None):\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                    # nu c 2 entity c relation\n",
        "                    elif (docif['relation'][first_tkeid_ent1] != None) and (docif['relation'][first_tkeid_ent2] != None):\n",
        "                        assert False, 'this is test'\n",
        "                        # mc nh l OTHERS, nu bn di tm thy relation link ti th s c thay i\n",
        "                        # cn nu bn di tm khng thy (tc l khng c relation) th s khng b thay i v vn l OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                        for rel_1 in docif['relation'][first_tkeid_ent1]:\n",
        "                            if rel_1[1] == ent_2[0][1]:   # relation  entity 1 link ti token u entity 2\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_1[0], 1)\n",
        "\n",
        "                                # kiem tra relation direction\n",
        "                                # do relation nam o ent_1 nen direction la: [ent_2, ent_1]\n",
        "                                if rel_1[3] != None:\n",
        "                                    assert (rel_1[3] == [ent_2[0][2], ent_1[0][2]]), str('CODE 1: Not match direction')\n",
        "                                \n",
        "\n",
        "                        for rel_2 in docif['relation'][first_tkeid_ent2]:\n",
        "                            if rel_2[1] == ent_1[0][1]:   # relation  entity 2 link ti token u entity 1\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_2[0], 2)\n",
        "\n",
        "                                # do relation nam o ent_2 nen direction la: [ent_1, ent_2]\n",
        "                                if rel_2[3] != None:\n",
        "                                    assert (rel_2[3] == [ent_1[0][2], ent_2[0][2]]), str('CODE 2: Not match direction')\n",
        "        \n",
        "\n",
        "                    # nu entity 1 c relation, entity 2 khng c\n",
        "                    elif (docif['relation'][first_tkeid_ent1] != None) and (docif['relation'][first_tkeid_ent2] == None):\n",
        "                        assert False, 'this is test'\n",
        "                        # mc nh l OTHERS, nu bn di tm thy relation link ti th s c thay i\n",
        "                        # cn nu bn di tm khng thy (tc l khng c relation) th s khng b thay i v vn l OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                        for rel_1 in docif['relation'][first_tkeid_ent1]:\n",
        "                            if rel_1[1] == ent_2[0][1]:   # relation  entity 1 link ti token u entity 2\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_1[0], 1)\n",
        "\n",
        "                                # do relation nam o ent_1 nen direction la: [ent_2, ent_1]\n",
        "                                if rel_1[3] != None:\n",
        "                                    assert (rel_1[3] == [ent_2[0][2], ent_1[0][2]]), str('CODE 3: Not match direction')\n",
        "\n",
        "                                \n",
        "\n",
        "                    # nu entity 2 c relation, entity 1 khng c\n",
        "                    elif (docif['relation'][first_tkeid_ent1] == None) and (docif['relation'][first_tkeid_ent2] != None):\n",
        "                        assert False, 'this is test'\n",
        "                        # mc nh l OTHERS, nu bn di tm thy relation link ti th s c thay i\n",
        "                        # cn nu bn di tm khng thy (tc l khng c relation) th s khng b thay i v vn l OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "                        \n",
        "                        for rel_2 in docif['relation'][first_tkeid_ent2]:\n",
        "                            if rel_2[1] == ent_1[0][1]:   # relation  entity 2 link ti token u entity 1\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_2[0], 2)\n",
        "\n",
        "                                # do relation nam o ent_2 nen direction la: [ent_1, ent_2]\n",
        "                                if rel_2[3] != None:\n",
        "                                    assert (rel_2[3] == [ent_1[0][2], ent_2[0][2]]), str('CODE 4: Not match direction')\n",
        "\n",
        "                        \n",
        "                    ##########\n",
        "\n",
        "                    sentif[\"doc_id\"] = docif[\"doc_id\"]\n",
        "\n",
        "                    sent_id += 1\n",
        "                    sentif['sent_id'] = sent_id\n",
        "\n",
        "                    sentif[\"sentence\"] = sent\n",
        "                    sentif[\"spos\"] = [sspos, espos]\n",
        "\n",
        "                    entity_1 = {'text': copy.deepcopy(ent1_text), 'pos': copy.deepcopy(ent1_pos_sent)}\n",
        "                    entity_2 = {'text': copy.deepcopy(ent2_text), 'pos': copy.deepcopy(ent2_pos_sent)}\n",
        "\n",
        "                    ent_1_first = copy.deepcopy(ent_1[0])\n",
        "                    ent_1_last = copy.deepcopy(ent_1[-1])\n",
        "\n",
        "                    ent_2_first = copy.deepcopy(ent_2[0])\n",
        "                    ent_2_last = copy.deepcopy(ent_2[-1])\n",
        "\n",
        "                    entity_1_info = {'first_token': copy.deepcopy(ent_1_first), 'last_token': copy.deepcopy(ent_1_last),}\n",
        "                    entity_2_info = {'first_token': copy.deepcopy(ent_2_first), 'last_token': copy.deepcopy(ent_2_last),}\n",
        "\n",
        "\n",
        "                    sentif['entity_1'] = copy.deepcopy(entity_1)\n",
        "                    sentif['entity_2'] = copy.deepcopy(entity_2)\n",
        "\n",
        "                    sentif['entity_1_info'] = copy.deepcopy(entity_1_info)\n",
        "                    sentif['entity_2_info'] = copy.deepcopy(entity_2_info)\n",
        "\n",
        "                    sentif['label'] = copy.deepcopy(sentence_label)\n",
        "\n",
        "                    \n",
        "                    \n",
        "                    # may dong tren co the khong co copy.deepcopy nhung dong ben duoi khong co la bi loi\n",
        "                    tdata.append(copy.deepcopy(sentif))\n",
        "\n",
        "print(\"DONE\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"\\ufeffGii thng 'V lng vng' l s ghi nhn v tn vinh i ng li xe Ti 25/12, y ban An ton Giao thng Quc gia  t chc trao gii thng 'V lng vng' nm 2017 cho cc tp th v c nhn xng ng nht.\", \"Trong ln th 5 t chc, gii thng 'V lng vng' ghi nhn thm nhiu im mi so vi 4 ln trao thng trc y.\", 'ng 8h ti 25/12, L trao gii thng V lng vng 2017 chnh thc din ra ti Nh ht i Ting ni Vit Nam (s 58 Qun S , qun Hai B Trng , TP H Ni ) vi nhng mn biu din vn ngh c sc.', 'Tham d L trao gii c i din Lnh o y ban An ton giao thng Quc gia , B GTVT , B Cng an , Hip hi vn ti  t Vit Nam v cc i biu l y vin y ban, y vin Ban Thng trc y ban An ton giao thng Quc gia , cng 18 tp th v 46 li xe t gii thng V lng vng ln th 5 nm 2017.', ' c 64 tp th v c nhn c trao gii Ph Ch tch chuyn trch y ban An ton Giao thng Quc gia Khut Vit Hng khng nh, sau 5 nm t chc, gii thng V lng vng  nhn c s quan tm rng ri ca cc doanh nhip kinh doanh trong lnh vc vn ti  t.', 'y l gii thng duy nht c quy m quc gia  tn vinh nhng doanh nghip, li xe an ton.', 'nh gi v  ngha ca gii thng V lng vng, ng Hng cho hay, gii thng ny  to ra phong tro li xe an ton i vi i ng li xe, qua  gp phn chuyn bin tch cc trong m bo trt t an ton giao thng.', 'Qu trnh bnh chn, trao gii c Ban T chc nh gi mt cch khch quan, v t, da trn nhng tiu ch ca cuc thi.', 'Th trng B GTVT Nguyn Ngc ng khng nh i ng li xe c nhng ng gp ln cho cng tc m bo trt t, an ton giao thng Cn c Th l v tiu ch ca cuc thi, Hi ng bnh xt ca y ban An ton Giao thng Quc gia  la chn ra 18 doanh nghip, hp tc x vn ti i din cho gn 100 n v kinh doanh vn ti ca cc vng, min trn phm vi c nc, ng k tham d gii.', 'Bn cnh , c tt c 46 c nhn l nhng li xe xe i din cho hng nghn li xe kinh doanh vn ti c trao gii V lng vng.', 'y l nhng c nhn  c thnh tch li xe an ton cng cc hnh ng th hin phm cht o c trong sng v cch ng x vn minh, lch s ca ngi li xe.', 'Ngoi ra, mt im mi trong gii thng V lng vng 2017 l Ban T chc  trao gii thng Vn ha giao thng cho 4 c nhn gm: Trn Vn Quang v Trn Vn Quyt (Cng ty CP xe khch s I Sn La , Sn La ), Nguyn Hng Phi (Doanh nghip t nhn Hi Anh , Sn La ), Chu M Hunh (Cng ty TNHH Kim Cng , ng Thp ).', 'y l gii thng dnh cho nhng c nhn c nhng hnh ng p,  thc v nhn vn trong qu trnh lm vic ca mnh.', 'Pht biu ti l trao gii, Th trng B GTVT Nguyn Ngc ng khng nh, tm quan trng ca i ng li xe i vi t nc ta l v cng ln.', 'Khi t nc cn chin tranh, chnh nhng chin s li xe vn ti  c nhng ng gp c bit quan trng trong hai cuc khng chin chng Php v M .', 'Cn trong thi bnh, c bit l hin nay trnh  qun tr kinh doanh vn ti ca doanh nghip c nng cao, cht lng dch v ngy cng bo m th i ng li xe  gp phn rt quan trng  gip tnh hnh trt t an ton giao thng c chuyn bin tch cc, tai nn giao thng tip tc gim c 3 tiu ch v s v, s ngi cht, s ngi b thng.', 'S ng gp ca nhng doanh nghip vn ti v li xe ang ngy m gi xe tt, li xe an ton, thc hin mc tiu Tnh mng con ngi l trn ht lp nn nhng cung ng bnh yn chuyn i hnh phc cho mi nh, cho cng ng v ton x hi.', 'Chng ta trn trng cm n i vi nhng ng gp ca h cho s pht trin ca t nc ni chung v cho s nghip bo m trt t an ton giao thng ni ring\", Th trng ng ni.', 'Ti L trao gii thng V Lng Vng ln th 5, y ban An ton giao thng Quc gia tip tc pht ng Phong tro thi ua xy dng Doanh nghip vn ti an ton v Li xe an ton nm 2018 v bnh chn gii thng V Lng Vng ln th 6.', 'Cng ti bui l, Ban t chc gii thng V lng vng chn thnh cm n Cng ty c phn  t Trng Hi  ng hnh, h tr gii thng trong 3 nm qua.', 'Danh sch 18 tp th nhn gii thng V lng vng: Cng ty Trch nhim hu hn Vn Minh , Ngh An ; Chi nhnh cng ty C phn Sun taxi ti Gia Lai , Gia Lai ; Cng ty C phn Logistics Portserco ,  Nng ; Hp tc x C kh Vn ti v Dch v Din Hng , Gia Lai ; Cng ty Trch nhim hu hn  t i Duy , Nam nh ; Cng ty C phn Thng mi v Du lch H Vinh , Ngh An ; Cng ty C phn Vn ti v Dch v Lin Ninh , H Ni ; Cng ty C phn Vn ti giao nhn v Thng mi Quang Chu , Tp. H Ch Minh ; Cng ty Trch nhim hu hn Mt thnh vin Mai Linh Kontum , Kontum ; Cng ty Giao nhn kho vn Hi Dng , Hi Dng ; Hp tc x Vn ti Thng Nht , Qung Ngi ; Cng ty C phn Du lch Xun Long , in Bin ; Cng ty C phn xe khch s 1 Sn La , Sn La ; Cng ty C phn Vn ti  t Qung Ninh , Qung Ninh ; Cng ty C phn thng mi du lch v vn chuyn khch Tnh Ngha , Qung Ninh ; Cng ty Trch nhim hu hn vn ti Thnh Hng , Hi Phng ; Cng ty C phn Vn ti  t An Giang , An Giang ; Cng ty C phn Vn ti b Tn Cng thnh vin Tng cng ty Tn Cng Si Gn  Trc thuc qun chng Hi qun , Tp. H Ch Minh .', 'Danh sch 46 c nhn nhn gii thng V lng vng: Phm Quc Thnh (Cng ty Vinasun nh Dng Vit Nam ), Nguyn Vit Trung (Cng ty CP Vn ti v Dch v Lin Ninh ),  Hi ng (X nghip xe khch Nam H Ni ), Nguyn Hu Hon (Cng ty CP Vn ti Newway ), Nguyn Vn Tin (Trung tm Tn t ), Nguyn c Nam (Cng ty CP Xe in H Ni ), Nguyn Anh Phng (Cng ty TNHH Thng mi Thin Phong ), L Vn Trung (Cng ty CP Tp on Mai Linh ), ng nh Cng (Cng ty CP Thng mi du lch v Vn chuyn khch Tnh Ngha ), Nguyn Hu Chu (Cng ty CP xe khch s I Sn La ), Lu Ngc Long (Cng ty CP Vn ti b Tn Cng ), Ng Vn Bnh (Cng ty TNHH Vn Minh ), on Hu Bng (Cng ty TNHH Phc Xuyn ), Nguyn Vn Quyn (Cng ty CP Thng mi v Du lch H Vinh ), Phan Mu Cng (Cng ty CP vn ti b Tn Cng ), Phan Xun L (Chi nhnh Cng ty CP Sun taxi ti Gia Lai ), Phm Khnh (Cng ty CP Logistics Portserco ), Nguyn Anh Hong (Cng ty CP C kh Sn La ), Trn Xun Trng v Bi Vn Nhn (Cng ty CP xe khch Si Gn ), Nguyn Hu Trung (Cng ty CP Giao nhn v Thng mi Quang Chu ), Trn Can (HTX Vn ti Thng Nht Qung Ngi ), Nguyn H Vinh (Doanh nghip t nhn Trng Hi ), Trng Quang Hiu (HTX C kh vn ti v Dch v Din Hng ), Lng Vn Li (Cng ty CP Vn ti An Giang ), Nguyn Thnh Khng (Cng ty TNHH Thng mi v Dch v Hi Phng ), Nguyn Vn Bo (Cng ty TNHH MTV Mai Linh Kon Tum ), L nh Dng (Cng ty CP Du lch Xun Long ), Nguyn Vn Quc (Cng ty TNHH Minh Quc ), Ng Ch Tun (HTX Vn ti  t Thnh Tuyn ), Trnh ng An (Cng ty CP vn ti  t Thanh Ha ), Trn Vn i (Cng ty TNHH Mai Linh Thanh Ha ), L c Hiu (Cng ty TNHH Dch v Taxi min Bc ), V Tin Quang (Cng ty CP xe khch Qung Ninh ), inh Vit Th (Cng ty CP Vn ti  t in Bin ), L Anh Thng (X nghip Xe but Cu Bu ), Nguyn Ngc Tng (X nghip Xe but H Ni ), Trn ng Thanh (X nghip Xe but 10/10 H Ni ), Ng Vn Quyt (Cng ty TNHH MTV Mai Linh Vnh Phc ), L Minh Hong (Cng ty TNHH MTV Mai Linh Gia Lai ), Trn Vn Dnh (Cng ty CP Thng mi du lch H Lan ), Nguyn Ngc Chun (Cng ty CP Giao nhn kho vn Hi Dng ), inh Th Giang Nam (Cng ty CP Vn ti  t Qung Ninh ), Nguyn Danh Li (Cng ty TNHH Vn ti Thnh Hng ),  Thanh Tn (X nghip Xe but Yn Vin ), V Duy Thun (HTX Vn ti hng ha hnh khch v Du lch Tn Vit ).', 'Qu Nguyn']\n",
            "[\"\\ufeffGii thng 'V lng vng' l s ghi nhn v tn vinh i ng li xe Ti 25/12, y ban An ton Giao thng Quc gia  t chc trao gii thng 'V lng vng' nm 2017 cho cc tp th v c nhn xng ng nht.\", \"Trong ln th 5 t chc, gii thng 'V lng vng' ghi nhn thm nhiu im mi so vi 4 ln trao thng trc y.\", 'ng 8h ti 25/12, L trao gii thng V lng vng 2017 chnh thc din ra ti Nh ht i Ting ni Vit Nam (s 58 Qun S , qun Hai B Trng , TP H Ni ) vi nhng mn biu din vn ngh c sc.', 'Tham d L trao gii c i din Lnh o y ban An ton giao thng Quc gia , B GTVT , B Cng an , Hip hi vn ti  t Vit Nam v cc i biu l y vin y ban, y vin Ban Thng trc y ban An ton giao thng Quc gia , cng 18 tp th v 46 li xe t gii thng V lng vng ln th 5 nm 2017.', ' c 64 tp th v c nhn c trao gii Ph Ch tch chuyn trch y ban An ton Giao thng Quc gia Khut Vit Hng khng nh, sau 5 nm t chc, gii thng V lng vng  nhn c s quan tm rng ri ca cc doanh nhip kinh doanh trong lnh vc vn ti  t.', 'y l gii thng duy nht c quy m quc gia  tn vinh nhng doanh nghip, li xe an ton.', 'nh gi v  ngha ca gii thng V lng vng, ng Hng cho hay, gii thng ny  to ra phong tro li xe an ton i vi i ng li xe, qua  gp phn chuyn bin tch cc trong m bo trt t an ton giao thng.', 'Qu trnh bnh chn, trao gii c Ban T chc nh gi mt cch khch quan, v t, da trn nhng tiu ch ca cuc thi.', 'Th trng B GTVT Nguyn Ngc ng khng nh i ng li xe c nhng ng gp ln cho cng tc m bo trt t, an ton giao thng Cn c Th l v tiu ch ca cuc thi, Hi ng bnh xt ca y ban An ton Giao thng Quc gia  la chn ra 18 doanh nghip, hp tc x vn ti i din cho gn 100 n v kinh doanh vn ti ca cc vng, min trn phm vi c nc, ng k tham d gii.', 'Bn cnh , c tt c 46 c nhn l nhng li xe xe i din cho hng nghn li xe kinh doanh vn ti c trao gii V lng vng.', 'y l nhng c nhn  c thnh tch li xe an ton cng cc hnh ng th hin phm cht o c trong sng v cch ng x vn minh, lch s ca ngi li xe.', 'Ngoi ra, mt im mi trong gii thng V lng vng 2017 l Ban T chc  trao gii thng Vn ha giao thng cho 4 c nhn gm: Trn Vn Quang v Trn Vn Quyt (Cng ty CP xe khch s I Sn La , Sn La ), Nguyn Hng Phi (Doanh nghip t nhn Hi Anh , Sn La ), Chu M Hunh (Cng ty TNHH Kim Cng , ng Thp ).', 'y l gii thng dnh cho nhng c nhn c nhng hnh ng p,  thc v nhn vn trong qu trnh lm vic ca mnh.', 'Pht biu ti l trao gii, Th trng B GTVT Nguyn Ngc ng khng nh, tm quan trng ca i ng li xe i vi t nc ta l v cng ln.', 'Khi t nc cn chin tranh, chnh nhng chin s li xe vn ti  c nhng ng gp c bit quan trng trong hai cuc khng chin chng Php v M .', 'Cn trong thi bnh, c bit l hin nay trnh  qun tr kinh doanh vn ti ca doanh nghip c nng cao, cht lng dch v ngy cng bo m th i ng li xe  gp phn rt quan trng  gip tnh hnh trt t an ton giao thng c chuyn bin tch cc, tai nn giao thng tip tc gim c 3 tiu ch v s v, s ngi cht, s ngi b thng.', 'S ng gp ca nhng doanh nghip vn ti v li xe ang ngy m gi xe tt, li xe an ton, thc hin mc tiu Tnh mng con ngi l trn ht lp nn nhng cung ng bnh yn chuyn i hnh phc cho mi nh, cho cng ng v ton x hi.', 'Chng ta trn trng cm n i vi nhng ng gp ca h cho s pht trin ca t nc ni chung v cho s nghip bo m trt t an ton giao thng ni ring\", Th trng ng ni.', 'Ti L trao gii thng V Lng Vng ln th 5, y ban An ton giao thng Quc gia tip tc pht ng Phong tro thi ua xy dng Doanh nghip vn ti an ton v Li xe an ton nm 2018 v bnh chn gii thng V Lng Vng ln th 6.', 'Cng ti bui l, Ban t chc gii thng V lng vng chn thnh cm n Cng ty c phn  t Trng Hi  ng hnh, h tr gii thng trong 3 nm qua.', 'Danh sch 18 tp th nhn gii thng V lng vng: Cng ty Trch nhim hu hn Vn Minh , Ngh An', 'Chi nhnh cng ty C phn Sun taxi ti Gia Lai , Gia Lai', 'Cng ty C phn Logistics Portserco ,  Nng', 'Hp tc x C kh Vn ti v Dch v Din Hng , Gia Lai', 'Cng ty Trch nhim hu hn  t i Duy , Nam nh', 'Cng ty C phn Thng mi v Du lch H Vinh , Ngh An', 'Cng ty C phn Vn ti v Dch v Lin Ninh , H Ni', 'Cng ty C phn Vn ti giao nhn v Thng mi Quang Chu , Tp. H Ch Minh', 'Cng ty Trch nhim hu hn Mt thnh vin Mai Linh Kontum , Kontum', 'Cng ty Giao nhn kho vn Hi Dng , Hi Dng', 'Hp tc x Vn ti Thng Nht , Qung Ngi', 'Cng ty C phn Du lch Xun Long , in Bin', 'Cng ty C phn xe khch s 1 Sn La , Sn La', 'Cng ty C phn Vn ti  t Qung Ninh , Qung Ninh', 'Cng ty C phn thng mi du lch v vn chuyn khch Tnh Ngha , Qung Ninh', 'Cng ty Trch nhim hu hn vn ti Thnh Hng , Hi Phng', 'Cng ty C phn Vn ti  t An Giang , An Giang', 'Cng ty C phn Vn ti b Tn Cng thnh vin Tng cng ty Tn Cng Si Gn  Trc thuc qun chng Hi qun , Tp. H Ch Minh .', 'Danh sch 46 c nhn nhn gii thng V lng vng: Phm Quc Thnh (Cng ty Vinasun nh Dng Vit Nam )', 'Nguyn Vit Trung (Cng ty CP Vn ti v Dch v Lin Ninh )', ' Hi ng (X nghip xe khch Nam H Ni )', 'Nguyn Hu Hon (Cng ty CP Vn ti Newway )', 'Nguyn Vn Tin (Trung tm Tn t )', 'Nguyn c Nam (Cng ty CP Xe in H Ni )', 'Nguyn Anh Phng (Cng ty TNHH Thng mi Thin Phong )', 'L Vn Trung (Cng ty CP Tp on Mai Linh )', 'ng nh Cng (Cng ty CP Thng mi du lch v Vn chuyn khch Tnh Ngha )', 'Nguyn Hu Chu (Cng ty CP xe khch s I Sn La )', 'Lu Ngc Long (Cng ty CP Vn ti b Tn Cng )', 'Ng Vn Bnh (Cng ty TNHH Vn Minh )', 'on Hu Bng (Cng ty TNHH Phc Xuyn )', 'Nguyn Vn Quyn (Cng ty CP Thng mi v Du lch H Vinh )', 'Phan Mu Cng (Cng ty CP vn ti b Tn Cng )', 'Phan Xun L (Chi nhnh Cng ty CP Sun taxi ti Gia Lai )', 'Phm Khnh (Cng ty CP Logistics Portserco )', 'Nguyn Anh Hong (Cng ty CP C kh Sn La )', 'Trn Xun Trng v Bi Vn Nhn (Cng ty CP xe khch Si Gn )', 'Nguyn Hu Trung (Cng ty CP Giao nhn v Thng mi Quang Chu )', 'Trn Can (HTX Vn ti Thng Nht Qung Ngi )', 'Nguyn H Vinh (Doanh nghip t nhn Trng Hi )', 'Trng Quang Hiu (HTX C kh vn ti v Dch v Din Hng )', 'Lng Vn Li (Cng ty CP Vn ti An Giang )', 'Nguyn Thnh Khng (Cng ty TNHH Thng mi v Dch v Hi Phng )', 'Nguyn Vn Bo (Cng ty TNHH MTV Mai Linh Kon Tum )', 'L nh Dng (Cng ty CP Du lch Xun Long )', 'Nguyn Vn Quc (Cng ty TNHH Minh Quc )', 'Ng Ch Tun (HTX Vn ti  t Thnh Tuyn )', 'Trnh ng An (Cng ty CP vn ti  t Thanh Ha )', 'Trn Vn i (Cng ty TNHH Mai Linh Thanh Ha )', 'L c Hiu (Cng ty TNHH Dch v Taxi min Bc )', 'V Tin Quang (Cng ty CP xe khch Qung Ninh )', 'inh Vit Th (Cng ty CP Vn ti  t in Bin )', 'L Anh Thng (X nghip Xe but Cu Bu )', 'Nguyn Ngc Tng (X nghip Xe but H Ni )', 'Trn ng Thanh (X nghip Xe but 10/10 H Ni )', 'Ng Vn Quyt (Cng ty TNHH MTV Mai Linh Vnh Phc )', 'L Minh Hong (Cng ty TNHH MTV Mai Linh Gia Lai )', 'Trn Vn Dnh (Cng ty CP Thng mi du lch H Lan )', 'Nguyn Ngc Chun (Cng ty CP Giao nhn kho vn Hi Dng )', 'inh Th Giang Nam (Cng ty CP Vn ti  t Qung Ninh )', 'Nguyn Danh Li (Cng ty TNHH Vn ti Thnh Hng )', ' Thanh Tn (X nghip Xe but Yn Vin )', 'V Duy Thun (HTX Vn ti hng ha hnh khch v Du lch Tn Vit ).', 'Qu Nguyn']\n",
            "[\"\\ufeffGii thng 'V lng vng' l s ghi nhn v tn vinh i ng li xe Ti 25/12, y ban An ton Giao thng Quc gia  t chc trao gii thng 'V lng vng' nm 2017 cho cc tp th v c nhn xng ng nht.\", \"Trong ln th 5 t chc, gii thng 'V lng vng' ghi nhn thm nhiu im mi so vi 4 ln trao thng trc y.\", 'ng 8h ti 25/12, L trao gii thng V lng vng 2017 chnh thc din ra ti Nh ht i Ting ni Vit Nam (s 58 Qun S , qun Hai B Trng , TP H Ni ) vi nhng mn biu din vn ngh c sc.', 'Tham d L trao gii c i din Lnh o y ban An ton giao thng Quc gia , B GTVT , B Cng an , Hip hi vn ti  t Vit Nam v cc i biu l y vin y ban, y vin Ban Thng trc y ban An ton giao thng Quc gia , cng 18 tp th v 46 li xe t gii thng V lng vng ln th 5 nm 2017.', ' c 64 tp th v c nhn c trao gii Ph Ch tch chuyn trch y ban An ton Giao thng Quc gia Khut Vit Hng khng nh, sau 5 nm t chc, gii thng V lng vng  nhn c s quan tm rng ri ca cc doanh nhip kinh doanh trong lnh vc vn ti  t.', 'y l gii thng duy nht c quy m quc gia  tn vinh nhng doanh nghip, li xe an ton.', 'nh gi v  ngha ca gii thng V lng vng, ng Hng cho hay, gii thng ny  to ra phong tro li xe an ton i vi i ng li xe, qua  gp phn chuyn bin tch cc trong m bo trt t an ton giao thng.', 'Qu trnh bnh chn, trao gii c Ban T chc nh gi mt cch khch quan, v t, da trn nhng tiu ch ca cuc thi.', 'Th trng B GTVT Nguyn Ngc ng khng nh i ng li xe c nhng ng gp ln cho cng tc m bo trt t, an ton giao thng Cn c Th l v tiu ch ca cuc thi, Hi ng bnh xt ca y ban An ton Giao thng Quc gia  la chn ra 18 doanh nghip, hp tc x vn ti i din cho gn 100 n v kinh doanh vn ti ca cc vng, min trn phm vi c nc, ng k tham d gii.', 'Bn cnh , c tt c 46 c nhn l nhng li xe xe i din cho hng nghn li xe kinh doanh vn ti c trao gii V lng vng.', 'y l nhng c nhn  c thnh tch li xe an ton cng cc hnh ng th hin phm cht o c trong sng v cch ng x vn minh, lch s ca ngi li xe.', 'Ngoi ra, mt im mi trong gii thng V lng vng 2017 l Ban T chc  trao gii thng Vn ha giao thng cho 4 c nhn gm: Trn Vn Quang v Trn Vn Quyt (Cng ty CP xe khch s I Sn La , Sn La ), Nguyn Hng Phi (Doanh nghip t nhn Hi Anh , Sn La ), Chu M Hunh (Cng ty TNHH Kim Cng , ng Thp ).', 'y l gii thng dnh cho nhng c nhn c nhng hnh ng p,  thc v nhn vn trong qu trnh lm vic ca mnh.', 'Pht biu ti l trao gii, Th trng B GTVT Nguyn Ngc ng khng nh, tm quan trng ca i ng li xe i vi t nc ta l v cng ln.', 'Khi t nc cn chin tranh, chnh nhng chin s li xe vn ti  c nhng ng gp c bit quan trng trong hai cuc khng chin chng Php v M .', 'Cn trong thi bnh, c bit l hin nay trnh  qun tr kinh doanh vn ti ca doanh nghip c nng cao, cht lng dch v ngy cng bo m th i ng li xe  gp phn rt quan trng  gip tnh hnh trt t an ton giao thng c chuyn bin tch cc, tai nn giao thng tip tc gim c 3 tiu ch v s v, s ngi cht, s ngi b thng.', 'S ng gp ca nhng doanh nghip vn ti v li xe ang ngy m gi xe tt, li xe an ton, thc hin mc tiu Tnh mng con ngi l trn ht lp nn nhng cung ng bnh yn chuyn i hnh phc cho mi nh, cho cng ng v ton x hi.', 'Chng ta trn trng cm n i vi nhng ng gp ca h cho s pht trin ca t nc ni chung v cho s nghip bo m trt t an ton giao thng ni ring\", Th trng ng ni.', 'Ti L trao gii thng V Lng Vng ln th 5, y ban An ton giao thng Quc gia tip tc pht ng Phong tro thi ua xy dng Doanh nghip vn ti an ton v Li xe an ton nm 2018 v bnh chn gii thng V Lng Vng ln th 6.', 'Cng ti bui l, Ban t chc gii thng V lng vng chn thnh cm n Cng ty c phn  t Trng Hi  ng hnh, h tr gii thng trong 3 nm qua.', 'Danh sch 18 tp th nhn gii thng V lng vng: Cng ty Trch nhim hu hn Vn Minh , Ngh An', 'Chi nhnh cng ty C phn Sun taxi ti Gia Lai , Gia Lai', 'Cng ty C phn Logistics Portserco ,  Nng', 'Hp tc x C kh Vn ti v Dch v Din Hng , Gia Lai', 'Cng ty Trch nhim hu hn  t i Duy , Nam nh', 'Cng ty C phn Thng mi v Du lch H Vinh , Ngh An', 'Cng ty C phn Vn ti v Dch v Lin Ninh , H Ni', 'Cng ty C phn Vn ti giao nhn v Thng mi Quang Chu , Tp. H Ch Minh', 'Cng ty Trch nhim hu hn Mt thnh vin Mai Linh Kontum , Kontum', 'Cng ty Giao nhn kho vn Hi Dng , Hi Dng', 'Hp tc x Vn ti Thng Nht , Qung Ngi', 'Cng ty C phn Du lch Xun Long , in Bin', 'Cng ty C phn xe khch s 1 Sn La , Sn La', 'Cng ty C phn Vn ti  t Qung Ninh , Qung Ninh', 'Cng ty C phn thng mi du lch v vn chuyn khch Tnh Ngha , Qung Ninh', 'Cng ty Trch nhim hu hn vn ti Thnh Hng , Hi Phng', 'Cng ty C phn Vn ti  t An Giang , An Giang', 'Cng ty C phn Vn ti b Tn Cng thnh vin Tng cng ty Tn Cng Si Gn  Trc thuc qun chng Hi qun , Tp. H Ch Minh .', 'Danh sch 46 c nhn nhn gii thng V lng vng: Phm Quc Thnh (Cng ty Vinasun nh Dng Vit Nam )', 'Nguyn Vit Trung (Cng ty CP Vn ti v Dch v Lin Ninh )', ' Hi ng (X nghip xe khch Nam H Ni )', 'Nguyn Hu Hon (Cng ty CP Vn ti Newway )', 'Nguyn Vn Tin (Trung tm Tn t )', 'Nguyn c Nam (Cng ty CP Xe in H Ni )', 'Nguyn Anh Phng (Cng ty TNHH Thng mi Thin Phong )', 'L Vn Trung (Cng ty CP Tp on Mai Linh )', 'ng nh Cng (Cng ty CP Thng mi du lch v Vn chuyn khch Tnh Ngha )', 'Nguyn Hu Chu (Cng ty CP xe khch s I Sn La )', 'Lu Ngc Long (Cng ty CP Vn ti b Tn Cng )', 'Ng Vn Bnh (Cng ty TNHH Vn Minh )', 'on Hu Bng (Cng ty TNHH Phc Xuyn )', 'Nguyn Vn Quyn (Cng ty CP Thng mi v Du lch H Vinh )', 'Phan Mu Cng (Cng ty CP vn ti b Tn Cng )', 'Phan Xun L (Chi nhnh Cng ty CP Sun taxi ti Gia Lai )', 'Phm Khnh (Cng ty CP Logistics Portserco )', 'Nguyn Anh Hong (Cng ty CP C kh Sn La )', 'Trn Xun Trng v Bi Vn Nhn (Cng ty CP xe khch Si Gn )', 'Nguyn Hu Trung (Cng ty CP Giao nhn v Thng mi Quang Chu )', 'Trn Can (HTX Vn ti Thng Nht Qung Ngi )', 'Nguyn H Vinh (Doanh nghip t nhn Trng Hi )', 'Trng Quang Hiu (HTX C kh vn ti v Dch v Din Hng )', 'Lng Vn Li (Cng ty CP Vn ti An Giang )', 'Nguyn Thnh Khng (Cng ty TNHH Thng mi v Dch v Hi Phng )', 'Nguyn Vn Bo (Cng ty TNHH MTV Mai Linh Kon Tum )', 'L nh Dng (Cng ty CP Du lch Xun Long )', 'Nguyn Vn Quc (Cng ty TNHH Minh Quc )', 'Ng Ch Tun (HTX Vn ti  t Thnh Tuyn )', 'Trnh ng An (Cng ty CP vn ti  t Thanh Ha )', 'Trn Vn i (Cng ty TNHH Mai Linh Thanh Ha )', 'L c Hiu (Cng ty TNHH Dch v Taxi min Bc )', 'V Tin Quang (Cng ty CP xe khch Qung Ninh )', 'inh Vit Th (Cng ty CP Vn ti  t in Bin )', 'L Anh Thng (X nghip Xe but Cu Bu )', 'Nguyn Ngc Tng (X nghip Xe but H Ni )', 'Trn ng Thanh (X nghip Xe but 10/10 H Ni )', 'Ng Vn Quyt (Cng ty TNHH MTV Mai Linh Vnh Phc )', 'L Minh Hong (Cng ty TNHH MTV Mai Linh Gia Lai )', 'Trn Vn Dnh (Cng ty CP Thng mi du lch H Lan )', 'Nguyn Ngc Chun (Cng ty CP Giao nhn kho vn Hi Dng )', 'inh Th Giang Nam (Cng ty CP Vn ti  t Qung Ninh )', 'Nguyn Danh Li (Cng ty TNHH Vn ti Thnh Hng )', ' Thanh Tn (X nghip Xe but Yn Vin )', 'V Duy Thun (HTX Vn ti hng ha hnh khch v Du lch Tn Vit ).', 'Qu Nguyn']\n",
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu7DbslJN1i4",
        "outputId": "6e82277b-3e01-4329-bb1a-f721503d198c"
      },
      "source": [
        "\"\"\"\n",
        "         'LOCATED': 'LOCATED', 'IS_LOCATED': 'IS_LOCATED', \n",
        "         'PART_WHOLE': 'PART_WHOLE', 'WHOLE_PART': 'WHOLE_PART', \n",
        "         'PERSONAL_SOCIAL': 'PERSONAL_SOCIAL', \n",
        "         'AFFILIATION': 'AFFILIATION', 'AFFILIATION_TO': 'AFFILIATION_TO', \n",
        "         'OTHERS': 'OTHERS'\n",
        "\"\"\"\n",
        "\n",
        "count_label_LOCATED = 0\n",
        "count_label_PART_WHOLE = 0\n",
        "count_label_PERSONAL_SOCIAL = 0\n",
        "count_label_AFFILIATION = 0\n",
        "count_label_OTHERS = 0\n",
        "\n",
        "\n",
        "for tdata_point in tdata:\n",
        "    if tdata_point['label'] == 'OTHERS':\n",
        "        count_label_OTHERS += 1\n",
        "\n",
        "    elif tdata_point['label'] == 'PERSONAL_SOCIAL':\n",
        "        count_label_PERSONAL_SOCIAL += 1\n",
        "    \n",
        "    elif tdata_point['label'] in ['LOCATED', 'IS_LOCATED']:\n",
        "        count_label_LOCATED += 1\n",
        "    \n",
        "    elif tdata_point['label'] in ['PART_WHOLE', 'WHOLE_PART']:\n",
        "        count_label_PART_WHOLE += 1\n",
        "    \n",
        "    elif tdata_point['label'] in ['AFFILIATION', 'AFFILIATION_TO']:\n",
        "        count_label_AFFILIATION += 1\n",
        "    \n",
        "    \n",
        "\n",
        "print('LOCATED: ', count_label_LOCATED)\n",
        "print('PART_WHOLE: ', count_label_PART_WHOLE)\n",
        "print('PERSONAL_SOCIAL: ', count_label_PERSONAL_SOCIAL)\n",
        "print('AFFILIATION: ', count_label_AFFILIATION)\n",
        "print('OTHERS: ', count_label_OTHERS)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LOCATED:  0\n",
            "PART_WHOLE:  0\n",
            "PERSONAL_SOCIAL:  0\n",
            "AFFILIATION:  0\n",
            "OTHERS:  9567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SfkCqCFN35t",
        "outputId": "77d45f38-ebe2-485d-d406-37143df9cde0"
      },
      "source": [
        "print('Count of labels that is not OTHERS: ', (len(tdata) - count_label_OTHERS))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of labels that is not OTHERS:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3O1aoGj796m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0662e352-b946-4e09-fc74-da2a4c6b539b"
      },
      "source": [
        "print(len(tdata))\n",
        "print(*tdata[0:15], sep='\\n')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9567\n",
            "{'doc_id': '23352901', 'sent_id': 1, 'sentence': \"y l l do khin Yoon Ah quyt nh ct mi tc di 'n thn' Mi tc c ca thnh vin SNSD b h hng nng n v Yoon Ah thm ch khng mun nui tc li.\", 'spos': [0, 158], 'entity_1': {'text': 'Yoon Ah', 'pos': [19, 26]}, 'entity_2': {'text': 'SNSD', 'pos': [90, 94]}, 'entity_1_info': {'first_token': [5, 6, 1, 'PERSON'], 'last_token': [6, 7, 1, 'PERSON']}, 'entity_2_info': {'first_token': [21, 22, 0, 'ORGANIZATION'], 'last_token': [21, 22, 0, 'ORGANIZATION']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 2, 'sentence': \"y l l do khin Yoon Ah quyt nh ct mi tc di 'n thn' Mi tc c ca thnh vin SNSD b h hng nng n v Yoon Ah thm ch khng mun nui tc li.\", 'spos': [0, 158], 'entity_1': {'text': 'Yoon Ah', 'pos': [19, 26]}, 'entity_2': {'text': 'Yoon Ah', 'pos': [117, 124]}, 'entity_1_info': {'first_token': [5, 6, 1, 'PERSON'], 'last_token': [6, 7, 1, 'PERSON']}, 'entity_2_info': {'first_token': [28, 29, 2, 'PERSON'], 'last_token': [29, 30, 2, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 3, 'sentence': \"y l l do khin Yoon Ah quyt nh ct mi tc di 'n thn' Mi tc c ca thnh vin SNSD b h hng nng n v Yoon Ah thm ch khng mun nui tc li.\", 'spos': [0, 158], 'entity_1': {'text': 'SNSD', 'pos': [90, 94]}, 'entity_2': {'text': 'Yoon Ah', 'pos': [117, 124]}, 'entity_1_info': {'first_token': [21, 22, 0, 'ORGANIZATION'], 'last_token': [21, 22, 0, 'ORGANIZATION']}, 'entity_2_info': {'first_token': [28, 29, 2, 'PERSON'], 'last_token': [29, 30, 2, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 4, 'sentence': 'Soo Young v Seo Hyun ct  ng phim, Yoon Ah v Sunny mun thay i bn thn v Yuri cng mi ta thnh kiu tc ngang vai tr trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Soo Young', 'pos': [0, 9]}, 'entity_2': {'text': 'Seo Hyun', 'pos': [13, 21]}, 'entity_1_info': {'first_token': [258, 259, 8, 'PERSON'], 'last_token': [259, 260, 8, 'PERSON']}, 'entity_2_info': {'first_token': [261, 262, 9, 'PERSON'], 'last_token': [262, 263, 9, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 5, 'sentence': 'Soo Young v Seo Hyun ct  ng phim, Yoon Ah v Sunny mun thay i bn thn v Yuri cng mi ta thnh kiu tc ngang vai tr trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Soo Young', 'pos': [0, 9]}, 'entity_2': {'text': 'Yoon Ah', 'pos': [40, 47]}, 'entity_1_info': {'first_token': [258, 259, 8, 'PERSON'], 'last_token': [259, 260, 8, 'PERSON']}, 'entity_2_info': {'first_token': [267, 268, 10, 'PERSON'], 'last_token': [268, 269, 10, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 6, 'sentence': 'Soo Young v Seo Hyun ct  ng phim, Yoon Ah v Sunny mun thay i bn thn v Yuri cng mi ta thnh kiu tc ngang vai tr trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Soo Young', 'pos': [0, 9]}, 'entity_2': {'text': 'Sunny', 'pos': [51, 56]}, 'entity_1_info': {'first_token': [258, 259, 8, 'PERSON'], 'last_token': [259, 260, 8, 'PERSON']}, 'entity_2_info': {'first_token': [270, 271, 0, 'PERSON'], 'last_token': [270, 271, 0, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 7, 'sentence': 'Soo Young v Seo Hyun ct  ng phim, Yoon Ah v Sunny mun thay i bn thn v Yuri cng mi ta thnh kiu tc ngang vai tr trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Seo Hyun', 'pos': [13, 21]}, 'entity_2': {'text': 'Yoon Ah', 'pos': [40, 47]}, 'entity_1_info': {'first_token': [261, 262, 9, 'PERSON'], 'last_token': [262, 263, 9, 'PERSON']}, 'entity_2_info': {'first_token': [267, 268, 10, 'PERSON'], 'last_token': [268, 269, 10, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 8, 'sentence': 'Soo Young v Seo Hyun ct  ng phim, Yoon Ah v Sunny mun thay i bn thn v Yuri cng mi ta thnh kiu tc ngang vai tr trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Seo Hyun', 'pos': [13, 21]}, 'entity_2': {'text': 'Sunny', 'pos': [51, 56]}, 'entity_1_info': {'first_token': [261, 262, 9, 'PERSON'], 'last_token': [262, 263, 9, 'PERSON']}, 'entity_2_info': {'first_token': [270, 271, 0, 'PERSON'], 'last_token': [270, 271, 0, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 9, 'sentence': 'Soo Young v Seo Hyun ct  ng phim, Yoon Ah v Sunny mun thay i bn thn v Yuri cng mi ta thnh kiu tc ngang vai tr trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Yoon Ah', 'pos': [40, 47]}, 'entity_2': {'text': 'Sunny', 'pos': [51, 56]}, 'entity_1_info': {'first_token': [267, 268, 10, 'PERSON'], 'last_token': [268, 269, 10, 'PERSON']}, 'entity_2_info': {'first_token': [270, 271, 0, 'PERSON'], 'last_token': [270, 271, 0, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352906', 'sent_id': 10, 'sentence': 'By t quan im cng bc tc nh Tu Dinh Huong , thnh vin Pymini tip li:\"Nh mnh m phn cho ngi sau th cn chn ton ci ngon  phn, ch khng c chuyn n ung by ba, mc k ngi n sau nh th ny.', 'spos': [1110, 1325], 'entity_1': {'text': 'Tu Dinh Huong', 'pos': [34, 47]}, 'entity_2': {'text': 'Pymini', 'pos': [61, 67]}, 'entity_1_info': {'first_token': [261, 262, 2, 'PERSON'], 'last_token': [263, 264, 2, 'PERSON']}, 'entity_2_info': {'first_token': [267, 268, 0, 'PERSON'], 'last_token': [267, 268, 0, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352911', 'sent_id': 11, 'sentence': \"Ca s ng Dng chi n bu trong liveshow 'Mt tri ca ti' Nhn k nim 20 nm ca ht, ca s ng Dng s t chc liveshow c ta  Mt tri ca ti c din ra trong 2 ngy 14, 15/10 ti H Ni .\", 'spos': [0, 206], 'entity_1': {'text': 'ng Dng', 'pos': [6, 16]}, 'entity_2': {'text': 'ng Dng', 'pos': [98, 108]}, 'entity_1_info': {'first_token': [2, 3, 1, 'PERSON'], 'last_token': [3, 4, 1, 'PERSON']}, 'entity_2_info': {'first_token': [22, 23, 2, 'PERSON'], 'last_token': [23, 24, 2, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352911', 'sent_id': 12, 'sentence': \"Ca s ng Dng chi n bu trong liveshow 'Mt tri ca ti' Nhn k nim 20 nm ca ht, ca s ng Dng s t chc liveshow c ta  Mt tri ca ti c din ra trong 2 ngy 14, 15/10 ti H Ni .\", 'spos': [0, 206], 'entity_1': {'text': 'ng Dng', 'pos': [6, 16]}, 'entity_2': {'text': 'H Ni', 'pos': [198, 204]}, 'entity_1_info': {'first_token': [2, 3, 1, 'PERSON'], 'last_token': [3, 4, 1, 'PERSON']}, 'entity_2_info': {'first_token': [44, 45, 3, 'LOCATION'], 'last_token': [45, 46, 3, 'LOCATION']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352911', 'sent_id': 13, 'sentence': \"Ca s ng Dng chi n bu trong liveshow 'Mt tri ca ti' Nhn k nim 20 nm ca ht, ca s ng Dng s t chc liveshow c ta  Mt tri ca ti c din ra trong 2 ngy 14, 15/10 ti H Ni .\", 'spos': [0, 206], 'entity_1': {'text': 'ng Dng', 'pos': [98, 108]}, 'entity_2': {'text': 'H Ni', 'pos': [198, 204]}, 'entity_1_info': {'first_token': [22, 23, 2, 'PERSON'], 'last_token': [23, 24, 2, 'PERSON']}, 'entity_2_info': {'first_token': [44, 45, 3, 'LOCATION'], 'last_token': [45, 46, 3, 'LOCATION']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352911', 'sent_id': 14, 'sentence': 'Trc khi theo hc thanh nhc v tr thnh ging ca chnh thng hng u Vit Nam , ca s ng Dng tng c gn 10 nm theo hc n bu ca ngh s Thanh Tm (m nhc s H Hoi Anh ).', 'spos': [207, 392], 'entity_1': {'text': 'Vit Nam', 'pos': [73, 81]}, 'entity_2': {'text': 'ng Dng', 'pos': [90, 100]}, 'entity_1_info': {'first_token': [62, 63, 4, 'LOCATION'], 'last_token': [63, 64, 4, 'LOCATION']}, 'entity_2_info': {'first_token': [67, 68, 5, 'PERSON'], 'last_token': [68, 69, 5, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352911', 'sent_id': 15, 'sentence': 'Trc khi theo hc thanh nhc v tr thnh ging ca chnh thng hng u Vit Nam , ca s ng Dng tng c gn 10 nm theo hc n bu ca ngh s Thanh Tm (m nhc s H Hoi Anh ).', 'spos': [207, 392], 'entity_1': {'text': 'Vit Nam', 'pos': [73, 81]}, 'entity_2': {'text': 'Thanh Tm', 'pos': [149, 158]}, 'entity_1_info': {'first_token': [62, 63, 4, 'LOCATION'], 'last_token': [63, 64, 4, 'LOCATION']}, 'entity_2_info': {'first_token': [81, 82, 6, 'PERSON'], 'last_token': [82, 83, 6, 'PERSON']}, 'label': 'OTHERS'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0mBtqdZthaa"
      },
      "source": [
        "# Write to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTM1DITfqJtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc26eb3-6b47-4034-e668-ea5013cb11b8"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xLBzf43i3NC"
      },
      "source": [
        "# write to file\n",
        "import codecs\n",
        "import json\n",
        "\n",
        "with codecs.open('test_data_v2.json', 'w', encoding='utf-8') as fout:\n",
        "    json.dump(tdata, fout, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72boeFzXgq-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b071d885-6cdd-4a19-a75d-7c67b3ea8635"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B8JEv6kDzuV"
      },
      "source": [
        "!mkdir \"/gdrive/MyDrive/VLSP2020_RE\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ng7tJuLD4_V"
      },
      "source": [
        "!mkdir \"/gdrive/MyDrive/VLSP2020_RE/json_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU0dYmmshRFZ"
      },
      "source": [
        "!cp -i test_data_v2.json \"/gdrive/MyDrive/VLSP2020_RE/json_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eWnRTZYSwm2",
        "outputId": "ff989506-6fd2-450e-b9a4-92a3e450d78c"
      },
      "source": [
        "import filecmp\n",
        "filecmp.cmp('test_data_v2.json', '/gdrive/MyDrive/VLSP2020_RE/json_data/test_data_v2.json')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}
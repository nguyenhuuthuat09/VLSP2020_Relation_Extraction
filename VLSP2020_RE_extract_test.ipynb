{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VLSP2020_RE_extract_test.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "H3ICier0JyzI",
        "PcS3CiYZxv8a",
        "jhOuiCNgaVqk",
        "chNxSZ0mx5wS",
        "jvsWfSzerWz5",
        "-wzBGIIgKYPD",
        "RQKOe59wfEcS",
        "kBr6Gx-Y2oj2",
        "8crlbk4MJTlx"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQDbz_q5irs0"
      },
      "source": [
        "# Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxh7foXeJtrf"
      },
      "source": [
        "## Unrar dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etKZzzHSF6_i"
      },
      "source": [
        "Please upload VLSP2020_RE_test.rar to Colab then */content* folder then unrar it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXjtAzVJHNI_",
        "outputId": "108b2f62-ff97-40ce-df63-264f53757142"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6x3Nl_tfBf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f378ca7d-3cc0-4102-b9c6-cd903c11a851"
      },
      "source": [
        "!pip install unrar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unrar\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/0b/53130ccd483e3db8c8a460cb579bdb21b458d5494d67a261e1a5b273fbb9/unrar-0.4-py3-none-any.whl\n",
            "Installing collected packages: unrar\n",
            "Successfully installed unrar-0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnlpfnYxiSiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7a2b0f-3a52-4340-be45-f2d5b73591ee"
      },
      "source": [
        "!unrar x VLSP2020_RE_test.rar"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from VLSP2020_RE_test.rar\n",
            "\n",
            "Creating    VLSP2020_RE_test                                          OK\n",
            "Creating    VLSP2020_RE_test/23352901.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352901.conll/CURATION_USER (1).tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352905.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352905.conll/CURATION_USER (1).tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352906.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352906.conll/CURATION_USER (1).tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352907.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352907.conll/CURATION_USER (1).tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352911.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352911.conll/CURATION_USER (1).tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352914.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352914.conll/CURATION_USER (1).tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352918.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352918.conll/CURATION_USER (1).tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352923.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352923.conll/CURATION_USER (1).tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352924.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352924.conll/CURATION_USER (1).tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352926.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352926.conll/CURATION_USER (1).tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352929.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352929.conll/CURATION_USER (1).tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352933.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352933.conll/CURATION_USER (1).tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352934.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352934.conll/CURATION_USER (1).tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352936.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352936.conll/CURATION_USER (1).tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352937.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352937.conll/CURATION_USER (1).tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352938.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352938.conll/CURATION_USER (1).tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352939.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352939.conll/CURATION_USER (1).tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352940.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352940.conll/CURATION_USER (1).tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352941.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352941.conll/CURATION_USER (1).tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352944.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352944.conll/CURATION_USER (1).tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352947.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352947.conll/CURATION_USER (1).tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352948.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352948.conll/CURATION_USER (1).tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352949.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352949.conll/CURATION_USER (1).tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352951.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352951.conll/CURATION_USER (1).tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352954.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352954.conll/CURATION_USER (1).tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352959.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352959.conll/CURATION_USER (1).tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352961.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352961.conll/CURATION_USER (1).tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352963.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352963.conll/CURATION_USER (1).tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352964.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352964.conll/CURATION_USER (1).tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352965.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352965.conll/CURATION_USER (1).tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352966.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352966.conll/CURATION_USER (1).tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352967.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352967.conll/CURATION_USER (1).tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352972.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352972.conll/CURATION_USER (1).tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352975.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352975.conll/CURATION_USER (1).tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352977.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352977.conll/CURATION_USER (1).tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352978.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352978.conll/CURATION_USER (1).tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352979.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352979.conll/CURATION_USER (1).tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352989.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352989.conll/CURATION_USER (1).tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352992.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352992.conll/CURATION_USER (1).tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352994.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352994.conll/CURATION_USER (1).tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352995.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352995.conll/CURATION_USER (1).tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352997.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352997.conll/CURATION_USER (1).tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23352999.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23352999.conll/CURATION_USER (1).tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353000.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353000.conll/CURATION_USER (1).tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353001.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353001.conll/CURATION_USER (1).tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353004.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353004.conll/CURATION_USER (1).tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353007.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353007.conll/CURATION_USER (1).tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353014.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353014.conll/CURATION_USER (1).tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353015.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353015.conll/CURATION_USER (1).tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353017.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353017.conll/CURATION_USER (1).tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353018.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353018.conll/CURATION_USER (1).tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353022.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353022.conll/CURATION_USER (1).tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353026.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353026.conll/CURATION_USER (1).tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353030.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353030.conll/CURATION_USER (1).tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353031.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353031.conll/CURATION_USER (1).tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353032.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353032.conll/CURATION_USER (1).tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353034.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353034.conll/CURATION_USER (1).tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353036.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353036.conll/CURATION_USER (1).tsv        \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/23353037.conll                           OK\n",
            "Extracting  VLSP2020_RE_test/23353037.conll/CURATION_USER (1).tsv        \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24382548.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24382548.txt/CURATION_USER (1).tsv          \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24386181.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24386181.txt/CURATION_USER (1).tsv          \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24386853.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24386853.txt/CURATION_USER (1).tsv          \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24396110.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24396110.txt/CURATION_USER (1).tsv          \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24396579.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24396579.txt/CURATION_USER (1).tsv          \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24396882.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24396882.txt/CURATION_USER (1).tsv          \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24398354.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24398354.txt/CURATION_USER (1).tsv          \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24398606.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24398606.txt/CURATION_USER (1).tsv          \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24402906.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24402906.txt/CURATION_USER (1).tsv          \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24403633.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24403633.txt/CURATION_USER (1).tsv          \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24410798.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24410798.txt/CURATION_USER (1).tsv          \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24413768.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24413768.txt/CURATION_USER (1).tsv          \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24415062.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24415062.txt/CURATION_USER (1).tsv          \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24416404.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24416404.txt/CURATION_USER (1).tsv          \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24419138.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24419138.txt/CURATION_USER (1).tsv          \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24419297.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24419297.txt/CURATION_USER (1).tsv          \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24423042.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24423042.txt/CURATION_USER (1).tsv          \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24425264.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24425264.txt/CURATION_USER (1).tsv          \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24427769.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24427769.txt/CURATION_USER (1).tsv          \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24430244.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24430244.txt/CURATION_USER (1).tsv          \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24430640.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24430640.txt/CURATION_USER (1).tsv          \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24430933.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24430933.txt/CURATION_USER (1).tsv          \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24434007.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24434007.txt/CURATION_USER (1).tsv          \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24434088.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24434088.txt/CURATION_USER (1).tsv          \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24437568.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24437568.txt/CURATION_USER (1).tsv          \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24438042.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24438042.txt/CURATION_USER (1).tsv          \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24440102.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24440102.txt/CURATION_USER (1).tsv          \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24446278.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24446278.txt/CURATION_USER (1).tsv          \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24449031.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24449031.txt/CURATION_USER (1).tsv          \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24451925.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24451925.txt/CURATION_USER (1).tsv          \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24452575.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24452575.txt/CURATION_USER (1).tsv          \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24454904.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24454904.txt/CURATION_USER (1).tsv          \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24457046.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24457046.txt/CURATION_USER (1).tsv          \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24457804.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24457804.txt/CURATION_USER (1).tsv          \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24460828.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24460828.txt/CURATION_USER (1).tsv          \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24466614.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24466614.txt/CURATION_USER (1).tsv          \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24467261.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24467261.txt/CURATION_USER (1).tsv          \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24467995.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24467995.txt/CURATION_USER (1).tsv          \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24468281.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24468281.txt/CURATION_USER (1).tsv          \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24471464.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24471464.txt/CURATION_USER (1).tsv          \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24473311.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24473311.txt/CURATION_USER (1).tsv          \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24476024.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24476024.txt/CURATION_USER (1).tsv          \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24477503.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24477503.txt/CURATION_USER (1).tsv          \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24477819.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24477819.txt/CURATION_USER (1).tsv          \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24483786.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24483786.txt/CURATION_USER (1).tsv          \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24484476.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24484476.txt/CURATION_USER (1).tsv          \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24487539.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24487539.txt/CURATION_USER (1).tsv          \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24492634.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24492634.txt/CURATION_USER (1).tsv          \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24495542.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24495542.txt/CURATION_USER (1).tsv          \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24497042.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24497042.txt/CURATION_USER (1).tsv          \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24503940.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24503940.txt/CURATION_USER (1).tsv          \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24506379.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24506379.txt/CURATION_USER (1).tsv          \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24506693.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24506693.txt/CURATION_USER (1).tsv          \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24509762.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24509762.txt/CURATION_USER (1).tsv          \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24518119.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24518119.txt/CURATION_USER (1).tsv          \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24518855.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24518855.txt/CURATION_USER (1).tsv          \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24520486.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24520486.txt/CURATION_USER (1).tsv          \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24521205.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24521205.txt/CURATION_USER (1).tsv          \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24527838.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24527838.txt/CURATION_USER (1).tsv          \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24530851.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24530851.txt/CURATION_USER (1).tsv          \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24546530.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24546530.txt/CURATION_USER (1).tsv          \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24550113.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24550113.txt/CURATION_USER (1).tsv          \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24557491.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24557491.txt/CURATION_USER (1).tsv          \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24561887.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24561887.txt/CURATION_USER (1).tsv          \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24566358.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24566358.txt/CURATION_USER (1).tsv          \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24568398.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24568398.txt/CURATION_USER (1).tsv          \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24569400.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24569400.txt/CURATION_USER (1).tsv          \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24570211.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24570211.txt/CURATION_USER (1).tsv          \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24572848.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24572848.txt/CURATION_USER (1).tsv          \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24578615.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24578615.txt/CURATION_USER (1).tsv          \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24579908.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24579908.txt/CURATION_USER (1).tsv          \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24580249.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24580249.txt/CURATION_USER (1).tsv          \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24583634.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24583634.txt/CURATION_USER (1).tsv          \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24585044.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24585044.txt/CURATION_USER (1).tsv          \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24587840.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24587840.txt/CURATION_USER (1).tsv          \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24589198.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24589198.txt/CURATION_USER (1).tsv          \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24589556.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24589556.txt/CURATION_USER (1).tsv          \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24596366.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24596366.txt/CURATION_USER (1).tsv          \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24597278.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24597278.txt/CURATION_USER (1).tsv          \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24599825.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24599825.txt/CURATION_USER (1).tsv          \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24600969.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24600969.txt/CURATION_USER (1).tsv          \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24601755.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24601755.txt/CURATION_USER (1).tsv          \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24603220.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24603220.txt/CURATION_USER (1).tsv          \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24607573.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24607573.txt/CURATION_USER (1).tsv          \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24608560.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24608560.txt/CURATION_USER (1).tsv          \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24609160.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24609160.txt/CURATION_USER (1).tsv          \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24609184.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24609184.txt/CURATION_USER (1).tsv          \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24610918.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24610918.txt/CURATION_USER (1).tsv          \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24612134.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24612134.txt/CURATION_USER (1).tsv          \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24615775.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24615775.txt/CURATION_USER (1).tsv          \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24616349.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24616349.txt/CURATION_USER (1).tsv          \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24620897.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24620897.txt/CURATION_USER (1).tsv          \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24623163.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24623163.txt/CURATION_USER (1).tsv          \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24624726.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24624726.txt/CURATION_USER (1).tsv          \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24629498.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24629498.txt/CURATION_USER (1).tsv          \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24630076.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24630076.txt/CURATION_USER (1).tsv          \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24630208.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24630208.txt/CURATION_USER (1).tsv          \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24631914.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24631914.txt/CURATION_USER (1).tsv          \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24632539.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24632539.txt/CURATION_USER (1).tsv          \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24636919.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24636919.txt/CURATION_USER (1).tsv          \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24640923.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24640923.txt/CURATION_USER (1).tsv          \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24642763.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24642763.txt/CURATION_USER (1).tsv          \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24644081.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24644081.txt/CURATION_USER (1).tsv          \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24645143.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24645143.txt/CURATION_USER (1).tsv          \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24645829.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24645829.txt/CURATION_USER (1).tsv          \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24647189.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24647189.txt/CURATION_USER (1).tsv          \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24648109.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24648109.txt/CURATION_USER (1).tsv          \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24651289.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24651289.txt/CURATION_USER (1).tsv          \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24657217.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24657217.txt/CURATION_USER (1).tsv          \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24659167.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24659167.txt/CURATION_USER (1).tsv          \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24660008.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24660008.txt/CURATION_USER (1).tsv          \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24660307.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24660307.txt/CURATION_USER (1).tsv          \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24661858.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24661858.txt/CURATION_USER (1).tsv          \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24663125.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24663125.txt/CURATION_USER (1).tsv          \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24663382.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24663382.txt/CURATION_USER (1).tsv          \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24665411.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24665411.txt/CURATION_USER (1).tsv          \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24667794.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24667794.txt/CURATION_USER (1).tsv          \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24672816.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24672816.txt/CURATION_USER (1).tsv          \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24673482.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24673482.txt/CURATION_USER (1).tsv          \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24676707.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24676707.txt/CURATION_USER (1).tsv          \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24677601.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24677601.txt/CURATION_USER (1).tsv          \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24678234.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24678234.txt/CURATION_USER (1).tsv          \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24681341.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24681341.txt/CURATION_USER (1).tsv          \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24681368.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24681368.txt/CURATION_USER (1).tsv          \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24681732.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24681732.txt/CURATION_USER (1).tsv          \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24683119.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24683119.txt/CURATION_USER (1).tsv          \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24684290.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24684290.txt/CURATION_USER (1).tsv          \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24685628.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24685628.txt/CURATION_USER (1).tsv          \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24686546.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24686546.txt/CURATION_USER (1).tsv          \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24688913.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24688913.txt/CURATION_USER (1).tsv          \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24689923.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24689923.txt/CURATION_USER (1).tsv          \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24691721.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24691721.txt/CURATION_USER (1).tsv          \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24696981.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24696981.txt/CURATION_USER (1).tsv          \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24697334.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24697334.txt/CURATION_USER (1).tsv          \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24701718.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24701718.txt/CURATION_USER (1).tsv          \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24703481.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24703481.txt/CURATION_USER (1).tsv          \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24705236.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24705236.txt/CURATION_USER (1).tsv          \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24705340.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24705340.txt/CURATION_USER (1).tsv          \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24710465.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24710465.txt/CURATION_USER (1).tsv          \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24710822.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24710822.txt/CURATION_USER (1).tsv          \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24711089.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24711089.txt/CURATION_USER (1).tsv          \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24711109.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24711109.txt/CURATION_USER (1).tsv          \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24712774.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24712774.txt/CURATION_USER (1).tsv          \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24714805.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24714805.txt/CURATION_USER (1).tsv          \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24715735.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24715735.txt/CURATION_USER (1).tsv          \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24717794.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24717794.txt/CURATION_USER (1).tsv          \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24718610.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24718610.txt/CURATION_USER (1).tsv          \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24721608.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24721608.txt/CURATION_USER (1).tsv          \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24731470.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24731470.txt/CURATION_USER (1).tsv          \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24735645.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24735645.txt/CURATION_USER (1).tsv          \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24737869.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24737869.txt/CURATION_USER (1).tsv          \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24738161.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24738161.txt/CURATION_USER (1).tsv          \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24738794.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24738794.txt/CURATION_USER (1).tsv          \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24739126.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24739126.txt/CURATION_USER (1).tsv          \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24742457.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24742457.txt/CURATION_USER (1).tsv          \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24743184.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24743184.txt/CURATION_USER (1).tsv          \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24743776.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24743776.txt/CURATION_USER (1).tsv          \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24744439.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24744439.txt/CURATION_USER (1).tsv          \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24749898.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24749898.txt/CURATION_USER (1).tsv          \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24752878.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24752878.txt/CURATION_USER (1).tsv          \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24752981.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24752981.txt/CURATION_USER (1).tsv          \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24753325.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24753325.txt/CURATION_USER (1).tsv          \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24754804.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24754804.txt/CURATION_USER (1).tsv          \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24755376.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24755376.txt/CURATION_USER (1).tsv          \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24760038.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24760038.txt/CURATION_USER (1).tsv          \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24765248.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24765248.txt/CURATION_USER (1).tsv          \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24765250.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24765250.txt/CURATION_USER (1).tsv          \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24767906.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24767906.txt/CURATION_USER (1).tsv          \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24768650.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24768650.txt/CURATION_USER (1).tsv          \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24768997.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24768997.txt/CURATION_USER (1).tsv          \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24769741.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24769741.txt/CURATION_USER (1).tsv          \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24777727.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24777727.txt/CURATION_USER (1).tsv          \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24778288.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24778288.txt/CURATION_USER (1).tsv          \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24778760.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24778760.txt/CURATION_USER (1).tsv          \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24782798.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24782798.txt/CURATION_USER (1).tsv          \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24789938.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24789938.txt/CURATION_USER (1).tsv          \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24793543.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24793543.txt/CURATION_USER (1).tsv          \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24794285.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24794285.txt/CURATION_USER (1).tsv          \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24796218.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24796218.txt/CURATION_USER (1).tsv          \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24796868.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24796868.txt/CURATION_USER (1).tsv          \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24797448.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24797448.txt/CURATION_USER (1).tsv          \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24797533.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24797533.txt/CURATION_USER (1).tsv          \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24798725.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24798725.txt/CURATION_USER (1).tsv          \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24799096.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24799096.txt/CURATION_USER (1).tsv          \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24800851.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24800851.txt/CURATION_USER (1).tsv          \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24802191.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24802191.txt/CURATION_USER (1).tsv          \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24804667.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24804667.txt/CURATION_USER (1).tsv          \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24805251.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24805251.txt/CURATION_USER (1).tsv          \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24808362.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24808362.txt/CURATION_USER (1).tsv          \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24808463.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24808463.txt/CURATION_USER (1).tsv          \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24808720.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24808720.txt/CURATION_USER (1).tsv          \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24809032.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24809032.txt/CURATION_USER (1).tsv          \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24817845.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24817845.txt/CURATION_USER (1).tsv          \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24823386.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24823386.txt/CURATION_USER (1).tsv          \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24823817.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24823817.txt/CURATION_USER (1).tsv          \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24825034.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24825034.txt/CURATION_USER (1).tsv          \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24826498.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24826498.txt/CURATION_USER (1).tsv          \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24827011.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24827011.txt/CURATION_USER (1).tsv          \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24827089.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24827089.txt/CURATION_USER (1).tsv          \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24829505.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24829505.txt/CURATION_USER (1).tsv          \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24830846.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24830846.txt/CURATION_USER (1).tsv          \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24831053.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24831053.txt/CURATION_USER (1).tsv          \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24832737.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24832737.txt/CURATION_USER (1).tsv          \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24834287.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24834287.txt/CURATION_USER (1).tsv          \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24839422.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24839422.txt/CURATION_USER (1).tsv          \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24843696.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24843696.txt/CURATION_USER (1).tsv          \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24844657.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24844657.txt/CURATION_USER (1).tsv          \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24847781.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24847781.txt/CURATION_USER (1).tsv          \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24848294.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24848294.txt/CURATION_USER (1).tsv          \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24848490.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24848490.txt/CURATION_USER (1).tsv          \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24848521.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24848521.txt/CURATION_USER (1).tsv          \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24849193.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24849193.txt/CURATION_USER (1).tsv          \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24849538.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24849538.txt/CURATION_USER (1).tsv          \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24851327.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24851327.txt/CURATION_USER (1).tsv          \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24854363.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24854363.txt/CURATION_USER (1).tsv          \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24854400.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24854400.txt/CURATION_USER (1).tsv          \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24856273.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24856273.txt/CURATION_USER (1).tsv          \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24856833.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24856833.txt/CURATION_USER (1).tsv          \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24857007.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24857007.txt/CURATION_USER (1).tsv          \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24857924.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24857924.txt/CURATION_USER (1).tsv          \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24867415.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24867415.txt/CURATION_USER (1).tsv          \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24867954.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24867954.txt/CURATION_USER (1).tsv          \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24870101.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24870101.txt/CURATION_USER (1).tsv          \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24870792.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24870792.txt/CURATION_USER (1).tsv          \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24870899.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24870899.txt/CURATION_USER (1).tsv          \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24872304.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24872304.txt/CURATION_USER (1).tsv          \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24874547.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24874547.txt/CURATION_USER (1).tsv          \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24875238.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24875238.txt/CURATION_USER (1).tsv          \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24881113.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24881113.txt/CURATION_USER (1).tsv          \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24881311.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24881311.txt/CURATION_USER (1).tsv          \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24882485.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24882485.txt/CURATION_USER (1).tsv          \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24888805.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24888805.txt/CURATION_USER (1).tsv          \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24891267.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24891267.txt/CURATION_USER (1).tsv          \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24894135.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24894135.txt/CURATION_USER (1).tsv          \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24894691.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24894691.txt/CURATION_USER (1).tsv          \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24900537.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24900537.txt/CURATION_USER (1).tsv          \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24900662.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24900662.txt/CURATION_USER (1).tsv          \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24900741.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24900741.txt/CURATION_USER (1).tsv          \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24900743.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24900743.txt/CURATION_USER (1).tsv          \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24901248.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24901248.txt/CURATION_USER (1).tsv          \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_test/24903191.txt                             OK\n",
            "Extracting  VLSP2020_RE_test/24903191.txt/CURATION_USER (1).tsv          \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3ICier0JyzI"
      },
      "source": [
        "## Install Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcS3CiYZxv8a"
      },
      "source": [
        "### Install VNCoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzsgsAb4uET6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f9348b4-0999-4b69-ba92-78c92034d7b2"
      },
      "source": [
        "# Install the vncorenlp python wrapper\n",
        "!pip install vncorenlp"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vncorenlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2020.12.5)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp37-none-any.whl size=2645936 sha256=c9476ca0c370ad73ac2983a1a7708b9ab853855d16b80d640a15505f95937c28\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxVE9cR6yZ3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef7c35c-5e51-4e49-9f7c-fcc54befe593"
      },
      "source": [
        "# Download VnCoreNLP-1.1.1.jar & all of its  component (i.e. RDRSegmenter, pos, ner, deprel) \n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!mkdir -p vncorenlp/models/dep\n",
        "!mkdir -p vncorenlp/models/ner\n",
        "!mkdir -p vncorenlp/models/postagger\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/dep/vi-dep.xz\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger\n",
        "\n",
        "\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "\n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/\n",
        "\n",
        "!mv vi-dep.xz vncorenlp/models/dep/\n",
        "\n",
        "!mv vi-500brownclusters.xz vncorenlp/models/ner/\n",
        "!mv vi-ner.xz vncorenlp/models/ner/\n",
        "!mv vi-pretrainedembeddings.xz vncorenlp/models/ner/\n",
        "\n",
        "!mv vi-tagger vncorenlp/models/postagger/\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-09 12:16:06--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M   125MB/s    in 0.2s    \n",
            "\n",
            "2021-05-09 12:16:08 (125 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2021-05-09 12:16:08--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-05-09 12:16:08 (22.8 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2021-05-09 12:16:08--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2021-05-09 12:16:09 (13.1 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n",
            "--2021-05-09 12:16:09--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/dep/vi-dep.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16048864 (15M) [application/octet-stream]\n",
            "Saving to: ‘vi-dep.xz’\n",
            "\n",
            "vi-dep.xz           100%[===================>]  15.30M  75.6MB/s    in 0.2s    \n",
            "\n",
            "2021-05-09 12:16:10 (75.6 MB/s) - ‘vi-dep.xz’ saved [16048864/16048864]\n",
            "\n",
            "--2021-05-09 12:16:10--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5599844 (5.3M) [application/octet-stream]\n",
            "Saving to: ‘vi-500brownclusters.xz’\n",
            "\n",
            "vi-500brownclusters 100%[===================>]   5.34M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-09 12:16:10 (47.2 MB/s) - ‘vi-500brownclusters.xz’ saved [5599844/5599844]\n",
            "\n",
            "--2021-05-09 12:16:10--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9956876 (9.5M) [application/octet-stream]\n",
            "Saving to: ‘vi-ner.xz’\n",
            "\n",
            "vi-ner.xz           100%[===================>]   9.50M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-09 12:16:11 (81.1 MB/s) - ‘vi-ner.xz’ saved [9956876/9956876]\n",
            "\n",
            "--2021-05-09 12:16:11--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 57313672 (55M) [application/octet-stream]\n",
            "Saving to: ‘vi-pretrainedembeddings.xz’\n",
            "\n",
            "vi-pretrainedembedd 100%[===================>]  54.66M   137MB/s    in 0.4s    \n",
            "\n",
            "2021-05-09 12:16:14 (137 MB/s) - ‘vi-pretrainedembeddings.xz’ saved [57313672/57313672]\n",
            "\n",
            "--2021-05-09 12:16:14--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29709468 (28M) [application/octet-stream]\n",
            "Saving to: ‘vi-tagger’\n",
            "\n",
            "vi-tagger           100%[===================>]  28.33M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-09 12:16:15 (195 MB/s) - ‘vi-tagger’ saved [29709468/29709468]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXfVgT46BB-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5699ccd-73fb-4e67-cf99-e441d8ff5c9c"
      },
      "source": [
        "import unicodedata\n",
        "from vncorenlp import VnCoreNLP\n",
        "\n",
        "# To perform word segmentation, POS tagging, NER and then dependency parsing\n",
        "annotator1 = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx2g') \n",
        "\n",
        "# To perform word segmentation, POS tagging and then NER\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg,pos,ner\", max_heap_size='-Xmx2g') \n",
        "# To perform word segmentation and then POS tagging\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg,pos\", max_heap_size='-Xmx2g') \n",
        "# To perform word segmentation only\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
        "# Input \n",
        "text = unicodedata.normalize(\"NFD\", \"Thanh Thủy\")\n",
        "\n",
        "\n",
        "# To perform word segmentation only\n",
        "word_segmented_text = annotator1.tokenize(text) \n",
        "\n",
        "print(*word_segmented_text, sep=\"\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Thanh', 'Thủy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhOuiCNgaVqk"
      },
      "source": [
        "### Install Underthesea"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IujpzlPaKfo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0f2930-4ec3-4d8d-8533-a9f5f9990df0"
      },
      "source": [
        "!pip install underthesea"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting underthesea\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/5f/03ab9091b88e7851aa92da33f8eea6f111423cc1194cf1636c63c1fff3d0/underthesea-1.3.1-py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.41.1)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 38.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from underthesea) (0.22.2.post1)\n",
            "Collecting torch<=1.5.1,>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/cf/007b6de316c9f3d4cb315a60c308342cc299e464167f5ebc369e93b5e23a/torch-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (753.2MB)\n",
            "\u001b[K     |████████████████████████████████| 753.2MB 20kB/s \n",
            "\u001b[?25hCollecting python-crfsuite>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.13)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.0.1)\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
            "\u001b[?25hCollecting transformers<=3.5.1,>=3.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch<=1.5.1,>=1.1.0->underthesea) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2020.12.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 47.8MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (2019.12.20)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/ac/f5ba028f0f097d855e1541301e946d4672eb0f30b6e25cb2369075f916d2/tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 56.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (20.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers<=3.5.1,>=3.5.0->underthesea) (56.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<=3.5.1,>=3.5.0->underthesea) (2.4.7)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=d2bd02bfafa8f545e973fd67348d007972969793bf8efcaa321f9616c1a582a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: unidecode, torch, python-crfsuite, seqeval, sacremoses, sentencepiece, tokenizers, transformers, underthesea\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed python-crfsuite-0.9.7 sacremoses-0.0.45 sentencepiece-0.1.91 seqeval-1.2.2 tokenizers-0.9.3 torch-1.5.1 transformers-3.5.1 underthesea-1.3.1 unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjsyeDFRKpfF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc2c221-a1d8-45a6-99ca-18fa2cc55264"
      },
      "source": [
        "from underthesea import sent_tokenize\n",
        "text = 'Quảng Bình : Cát tặc lộng hành, hiểm họa rình rập cầu Long Đại và dòng sông? Hiện tượng khai thác cát “lậu” trên sông cách cầu Long Đại vài trăm mét về phía hạ lưu, khiến cầu và sông Long Đại đang đứng trước hiểm họa khó lường? Vừa qua Pháp luật Plus nhận phản ánh của những người dân sống ở xã Xuân Ninh , huyện Quảng Ninh , tỉnh Quảng Bình về việc hiện nay ở xã này, cụ thể là tại thôn Xuân Dục 1 khu vực ven sông Long Đại lâu này xuất hiện những bãi tập kết cát trái phép và hiện tượng khai thác cát “lậu” trên sông cả ngày lẫn đêm gây ảnh hưởng đến đến cuộc sống thường nhật của người dân nơi đây. Từ những nguồn tin nêu trên sáng ngày 21/9, PV đã tiếp cận hiện trường đoạn sông Long Đại thuộc thôn Xuân Dụ 1 , xã Xuân Ninh , huyện Quảng Ninh , tỉnh Quảng Bình nơi người dân phản ánh để củng cố thông tin. Tại đây, PV nhận thấy nhiều bãi tập kết cát gần khu vực dân cư sinh sống, hàng ngày nhiều tàu chở cát vào đây để tập kết cát, gây ra tiếng ồn khó chịu ảnh hưởng không nhỏ đến sinh hoạt của người dân. Những bãi tập kết cát trái phép. Hơn thế nữa theo tìm hiểu của PV được biết, những vị trí có bãi tập kết cát kể trên không đủ tiêu chuẩn để tàu cập bến tập kết?. Trưa cùng ngày PV đã đi theo hướng thượng nguồn sông Long Đại mà theo phản ánh là xảy ra tình trạng khai thác cát trái phép thường xuyên diễn ra. PV nhận thấy một chiếc thuyền đang neo đậu cách bờ chừng vài chục mét và cách móng cầu Long Đại chừng vài trăm mét theo hướng hạ nguồn đang hút cát lên thuyền. Chiếc thuyền (ô đỏ) đang khai thác cát trái phép cách cầu Long Đại không xa. Tiếp tục ghi nhận và theo dõi vụ việc khoảng chừng hơn 30 phút, chiếc thuyền đã đầy cát đã được đi chuyển đi tập kết. Chiếc thuyền sau khi hút cát trái phép di chuyển về bãi tập kết. Qua tìm hiểu của PV được biết, cát ở khu vực gần cầu Long Đại là cát nhiễm mặn nếu dùng vào việc thi công công trình sẽ ảnh hưởng đến chất lượng của công trình đó… Và cát này được chủ thuyền bán lại cho người sử dụng với giá rẻ hơn so với cát được khai thác ở mỏ được cấp phép gây nên sự cạnh tranh không lành mạnh về giá cát. Tuy nhiên nhiều người dân chưa nhận thấy đến việc chất lượng của công trình sau này khi sử dụng cát nhiễm mặn này. Điều đáng nói là việc khai thác cát trái phép lại diễn ra khu vực gần móng cầu Long Đại (cả đường sắt lẫn đường bộ) nguy cơ sạc lở đất khu vực móng cầu, khiến cầu Long Đại đứng trước hiểm họa khó lường?. Liên quan đến vấn đề này, trao đổi với PV ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh cho biết về phía xã cũng đã nhiều lần xử lý nhắc nhở người dân trong vấn đề tập kết cát đúng nơi quy định. “Ngoài ra, xã đang hướng dẫn và hoàn thành các thủ tục nhằm đưa các điểm tập kết trái phép này đúng vào nơi quy định trong thời gian sớm nhất”, ông Tiến cho biết thêm. Ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh (bên phải) tại buổi làm việc với PV. Khi được PV cũng cấp bằng chứng về việc thuyền khai thác cát trái phép ngay giữa ban ngày gần khu vực móng cầu Long Đại , ông Tiến đã hết sức bất ngờ nói “Như vậy là không được rồi, không được rồi… sẽ cho xử lý ngay” Tiếp đó PV liên lạc qua điện thoại với ông Phạm Trung Đông – Chủ tịch UBND huyện Quảng Ninh để phản ánh sự việc thì ông Đông cho biết, đang bận và hướng dẫn PV liên hệ với ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh để làm việc. Tại buổi là việc với ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh PV đã cung cấp clip về việc nạn khai thác trái phép diễn ra ngay trên sông Long Đại đoạn gần móng cầu ông Giai cũng đã kiên quyết và hứa sẽ đấu tranh xử lý, đồng thời phối hợp với các cơ quan chức năng khác thường xuyên kiểm tra để chấm dứt tình trạng này. Ông Nguyễn Viết Giai cho biết sẽ đấu tranh xử lý Còn về việc các bãi tập kết trái phép, ông Giai cho biết sẽ xử lý dứt điểm trong thời gian sớm nhất để không ảnh hưởng tới cuộc sống người dân xung quanh. Khi được PV hỏi thời gian sớm nhất là bao lâu ông Giai cho biết: “ở đây đang còn vướng một khâu thủ tục. thời gian giải quyết sớm nhất cũng phải mất chừng 7 đến 10 ngày.” Việc khai thác cát trái phép gần cầu Long Đại (cả đường sắt lẫn đường bộ) nguy cơ sạt lở đất khu vực móng cầu, khiến cầu Long Đại đứng trước hiểm họa khó lường? Tuy là vậy nhưng trong sáng 22/9, PV một lần nữa đến tại hiện trường chiếc thuyền khai thác trái phép thì nhận thấy tình hình khai thác cát trái phép vẫn không hề thay đổi. Một lần nữa PV đã gọi điện thoại cho ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh và ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh để phản ánh thì lại được 2 vị hứa “sẽ xử lý”. Trong sáng 22/9 việc khai thác cát trái phép vẫn diễn ra mà không có sự can thiệp của cơ quyan chức năng? Từ những việc nêu trên, dư luận không thể không đặt ra câu hỏi liệu những việc xảy ra ở đây có phải là có sự “bảo kê” hoặc có sự tiếp tay của lực lượng chức năng có thẩm thẩm quyền hay không? Pháp luật Plus sẽ tiếp tục thông tin vụ việc đến bạn đọc.'\n",
        "sent_tokenize(text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Quảng Bình : Cát tặc lộng hành, hiểm họa rình rập cầu Long Đại và dòng sông?',\n",
              " 'Hiện tượng khai thác cát “lậu” trên sông cách cầu Long Đại vài trăm mét về phía hạ lưu, khiến cầu và sông Long Đại đang đứng trước hiểm họa khó lường?',\n",
              " 'Vừa qua Pháp luật Plus nhận phản ánh của những người dân sống ở xã Xuân Ninh , huyện Quảng Ninh , tỉnh Quảng Bình về việc hiện nay ở xã này, cụ thể là tại thôn Xuân Dục 1 khu vực ven sông Long Đại lâu này xuất hiện những bãi tập kết cát trái phép và hiện tượng khai thác cát “lậu” trên sông cả ngày lẫn đêm gây ảnh hưởng đến đến cuộc sống thường nhật của người dân nơi đây.',\n",
              " 'Từ những nguồn tin nêu trên sáng ngày 21/9, PV đã tiếp cận hiện trường đoạn sông Long Đại thuộc thôn Xuân Dụ 1 , xã Xuân Ninh , huyện Quảng Ninh , tỉnh Quảng Bình nơi người dân phản ánh để củng cố thông tin.',\n",
              " 'Tại đây, PV nhận thấy nhiều bãi tập kết cát gần khu vực dân cư sinh sống, hàng ngày nhiều tàu chở cát vào đây để tập kết cát, gây ra tiếng ồn khó chịu ảnh hưởng không nhỏ đến sinh hoạt của người dân.',\n",
              " 'Những bãi tập kết cát trái phép.',\n",
              " 'Hơn thế nữa theo tìm hiểu của PV được biết, những vị trí có bãi tập kết cát kể trên không đủ tiêu chuẩn để tàu cập bến tập kết?.',\n",
              " 'Trưa cùng ngày PV đã đi theo hướng thượng nguồn sông Long Đại mà theo phản ánh là xảy ra tình trạng khai thác cát trái phép thường xuyên diễn ra.',\n",
              " 'PV nhận thấy một chiếc thuyền đang neo đậu cách bờ chừng vài chục mét và cách móng cầu Long Đại chừng vài trăm mét theo hướng hạ nguồn đang hút cát lên thuyền.',\n",
              " 'Chiếc thuyền (ô đỏ) đang khai thác cát trái phép cách cầu Long Đại không xa.',\n",
              " 'Tiếp tục ghi nhận và theo dõi vụ việc khoảng chừng hơn 30 phút, chiếc thuyền đã đầy cát đã được đi chuyển đi tập kết.',\n",
              " 'Chiếc thuyền sau khi hút cát trái phép di chuyển về bãi tập kết.',\n",
              " 'Qua tìm hiểu của PV được biết, cát ở khu vực gần cầu Long Đại là cát nhiễm mặn nếu dùng vào việc thi công công trình sẽ ảnh hưởng đến chất lượng của công trình đó… Và cát này được chủ thuyền bán lại cho người sử dụng với giá rẻ hơn so với cát được khai thác ở mỏ được cấp phép gây nên sự cạnh tranh không lành mạnh về giá cát.',\n",
              " 'Tuy nhiên nhiều người dân chưa nhận thấy đến việc chất lượng của công trình sau này khi sử dụng cát nhiễm mặn này.',\n",
              " 'Điều đáng nói là việc khai thác cát trái phép lại diễn ra khu vực gần móng cầu Long Đại (cả đường sắt lẫn đường bộ) nguy cơ sạc lở đất khu vực móng cầu, khiến cầu Long Đại đứng trước hiểm họa khó lường?.',\n",
              " 'Liên quan đến vấn đề này, trao đổi với PV ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh cho biết về phía xã cũng đã nhiều lần xử lý nhắc nhở người dân trong vấn đề tập kết cát đúng nơi quy định.',\n",
              " '“Ngoài ra, xã đang hướng dẫn và hoàn thành các thủ tục nhằm đưa các điểm tập kết trái phép này đúng vào nơi quy định trong thời gian sớm nhất”, ông Tiến cho biết thêm.',\n",
              " 'Ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh (bên phải) tại buổi làm việc với PV.',\n",
              " 'Khi được PV cũng cấp bằng chứng về việc thuyền khai thác cát trái phép ngay giữa ban ngày gần khu vực móng cầu Long Đại , ông Tiến đã hết sức bất ngờ nói “Như vậy là không được rồi, không được rồi… sẽ cho xử lý ngay” Tiếp đó PV liên lạc qua điện thoại với ông Phạm Trung Đông – Chủ tịch UBND huyện Quảng Ninh để phản ánh sự việc thì ông Đông cho biết, đang bận và hướng dẫn PV liên hệ với ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh để làm việc.',\n",
              " 'Tại buổi là việc với ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh PV đã cung cấp clip về việc nạn khai thác trái phép diễn ra ngay trên sông Long Đại đoạn gần móng cầu ông Giai cũng đã kiên quyết và hứa sẽ đấu tranh xử lý, đồng thời phối hợp với các cơ quan chức năng khác thường xuyên kiểm tra để chấm dứt tình trạng này.',\n",
              " 'Ông Nguyễn Viết Giai cho biết sẽ đấu tranh xử lý Còn về việc các bãi tập kết trái phép, ông Giai cho biết sẽ xử lý dứt điểm trong thời gian sớm nhất để không ảnh hưởng tới cuộc sống người dân xung quanh.',\n",
              " 'Khi được PV hỏi thời gian sớm nhất là bao lâu ông Giai cho biết: “ở đây đang còn vướng một khâu thủ tục.',\n",
              " 'thời gian giải quyết sớm nhất cũng phải mất chừng 7 đến 10 ngày.” Việc khai thác cát trái phép gần cầu Long Đại (cả đường sắt lẫn đường bộ) nguy cơ sạt lở đất khu vực móng cầu, khiến cầu Long Đại đứng trước hiểm họa khó lường?',\n",
              " 'Tuy là vậy nhưng trong sáng 22/9, PV một lần nữa đến tại hiện trường chiếc thuyền khai thác trái phép thì nhận thấy tình hình khai thác cát trái phép vẫn không hề thay đổi.',\n",
              " 'Một lần nữa PV đã gọi điện thoại cho ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh và ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh để phản ánh thì lại được 2 vị hứa “sẽ xử lý”.',\n",
              " 'Trong sáng 22/9 việc khai thác cát trái phép vẫn diễn ra mà không có sự can thiệp của cơ quyan chức năng?',\n",
              " 'Từ những việc nêu trên, dư luận không thể không đặt ra câu hỏi liệu những việc xảy ra ở đây có phải là có sự “bảo kê” hoặc có sự tiếp tay của lực lượng chức năng có thẩm thẩm quyền hay không?',\n",
              " 'Pháp luật Plus sẽ tiếp tục thông tin vụ việc đến bạn đọc.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chNxSZ0mx5wS"
      },
      "source": [
        "# Extract raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phjF0edvNGpc"
      },
      "source": [
        "import os\n",
        "import re"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pTphYOiMYho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa98a8e-4253-4918-f81b-ecb8ec5fe006"
      },
      "source": [
        "# get all subfolers and files in subfolers\n",
        "# [(\"top_subfolders\", [subfolders_in_top_subfolders], [files_in_top_subfolders])]\n",
        "sub_folders = [f for f in os.walk(\"VLSP2020_RE_test\")][1:]\n",
        "sub_folders = sorted(sub_folders, key=lambda x: x[0])   # sort by top_subfolder name\n",
        "\n",
        "## top subfolder contain only 1 single file.\n",
        "check = False\n",
        "for i in sub_folders:\n",
        "    if i[1] or len(i[-1])!= 1:\n",
        "        print(\"ALERT!!!\")\n",
        "        check = True\n",
        "\n",
        "if not check:\n",
        "    print(\"There is \", len(sub_folders), \" subfolders. All subfolders contain only 1 file.\",\n",
        "          \" So that we have \", len(sub_folders), \" files.\")\n",
        "\n",
        "# generate data files name\n",
        "files_path = [os.path.join(i[0], i[-1][0]) for i in sub_folders]\n",
        "\n",
        "# print(*files_path, sep=\"\\n\")\n",
        "\n",
        "# print(files_path)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is  300  subfolders. All subfolders contain only 1 file.  So that we have  300  files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LJk2k0vD0dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619446d3-4af1-4b0a-ed86-65029740d26e"
      },
      "source": [
        "# Xem trong bộ dữ liệu có những character gì\n",
        "\n",
        "character_lst = []\n",
        "for file in files_path:\n",
        "    with open(file, mode='r') as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "        # find line start with \"#Text=\"\n",
        "        textline_id = []\n",
        "        for i, text in enumerate(lines):\n",
        "            if (\"#Text=\" == text[0:6]):\n",
        "                textline_id.append(i)\n",
        "\n",
        "        # every data file has only one line that start with \"#Text=\"\"\n",
        "        assert (len(textline_id) == 1), str(\"1 is not number of line start with #Text=. \\nDoc: \" + file)\n",
        "\n",
        "        for c in lines[textline_id[0]][6:]:\n",
        "            if c not in character_lst:\n",
        "                character_lst.append(c)\n",
        "\n",
        "\n",
        "# Print all of the single characters, 30 per row.\n",
        "# For every batch of 30 tokens...\n",
        "for i in range(0, len(character_lst), 30):\n",
        "    \n",
        "    # Limit the end index so we don't go past the end of the list.\n",
        "    end = min(i + 30, len(character_lst) + 1)\n",
        "    \n",
        "    # Print out the tokens, separated by a space.\n",
        "    print(repr(' '.join(character_lst[i:end])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Đ â y   l à ý d o k h i ế n Y A q u t đ ị c ắ m á ó ' ữ ầ M\"\n",
            "'ũ ủ a v ê S N D b ư ỏ g ặ ề ậ í ô ố ạ . ú ỳ ả ỷ ệ 1 0 ă , s'\n",
            "'ì p ể ẫ ơ r ự f ã ấ ớ ổ ĩ ễ ộ x T : \" ọ ờ ằ ứ ợ ù ử ỡ e é ẹ'\n",
            "'ỗ ẳ G 4 / 9 H ỉ ẻ ừ K L ở 2 ồ 3 ? F ụ - C V B 6 5 P ( ) è ò'\n",
            "'“ ” ẩ Ả \\xa0 … ‘ ’ ẵ 7 ẽ 8 w ỹ Ư Q R O J õ I % U E X Ấ Z ; Ở Á'\n",
            "'– W z + Ô Â Ý j & | ç ! Ủ Ă Ì \\ufeff @ ½ Ú Ứ ō ä Б И У С Ố ° ø ë'\n",
            "'= # [ ] ỵ < Í ÷ ž č ě Ế Ộ À ü ′ * Ễ Ạ Ê Ð _ Ơ Ổ Ẩ ² Ầ Ợ Ự Ã'\n",
            "'Ị Ớ 徐 晃 公 明'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugs-iTcldECs"
      },
      "source": [
        "constant = {\"entity_name\": [\"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"MISCELLANEOUS\"],\n",
        "          \"relation_name\": [\"LOCATED\", \"PART – WHOLE\", \"AFFILIATION\", \"PERSONAL - SOCIAL\"]\n",
        "           }"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rqndrEtimOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4769c3a3-16d7-40dd-d6a5-9d86a54ba3f9"
      },
      "source": [
        "\"\"\"\n",
        "            dict = {\"doc_id\": id of folder contain doc, \n",
        "                      \"text\": doc, \n",
        "                 \"token_ids\": [                                        tokens_id, ...], \n",
        "              \"subtoken_ids\": [                                   None or sub-id, ...],\n",
        "                       \"pos\": [                             [start pos, end pos], ...],\n",
        "                    \"tokens\": [                                      tokens_text, ...],\n",
        "                    \"entity\": [                        [entity_ids, entity_name], ...],\n",
        "                  \"relation\": [    [relation, stoken_id, ssubtoken_id direction], ...]\n",
        "                   }\n",
        "\n",
        "\n",
        "                      doc_id: id of folder contain doc                                            (str)\n",
        "                         doc: doc. line start with \"#Text=\"                                       (str)\n",
        "                   token_ids: ids of tokens.                                    first column       (list int)\n",
        "                subtoken_ids: int if crr token is a subtoken, otherwise None    first column      (list int, None)\n",
        "                         pos: posittion of tokens.                              second column     (list list int) \n",
        "                              [\n",
        "                                  [start pos, end pos],\n",
        "                                  ...\n",
        "                              ]\n",
        "                       token: tokens.                                           third column      (list str)\n",
        "                      entity: entity infor if token is entity, else None.       4th, 5th column   (list list, None)\n",
        "                              [\n",
        "                                  [entity_id, entity_name],\n",
        "                                  ...\n",
        "                              ]\n",
        "                    relation: relation if token is in a relation, else None.    other column      (list list, None)\n",
        "                              [\n",
        "                                  [[relation1_name, relation1_start_tokenID, relation1_start_subtokenID, [start_entity_id, end_entity_id]], ...],  \n",
        "                                                                         --> relation1_start_subtokenID may be None if start token is not a sub token\n",
        "                                  [[relation1_name, relation1_start_tokenID, relation1_start_subtokenID,                             None], ...],  <-- dataset has mistake. Don't have direction.\n",
        "                                                                                 \n",
        "                                  \n",
        "                                  ....\n",
        "                              ]\n",
        "                    \n",
        "\"\"\"\n",
        "\n",
        "import copy\n",
        "\n",
        "raw_tdata = []\n",
        "\n",
        "for file in files_path:\n",
        "    docif = {}\n",
        "    with open(file, mode='r') as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "        # example: VLSP2020_RE_training/23351113.conll/CURATION_USER.tsv -> 23351113\n",
        "        docif[\"doc_id\"] = copy.deepcopy(file[(file.find(\"/\") + 1): file.find(\".\")])\n",
        "\n",
        "        # find line start with \"#Text=\"\n",
        "        textline_id = []\n",
        "        for i, text in enumerate(lines):\n",
        "            if (\"#Text=\" == text[0:6]):\n",
        "                textline_id.append(i)\n",
        "\n",
        "        # every data file has only one line that start with \"#Text=\"\"\n",
        "        assert (len(textline_id) == 1), str(\"1 is not number of line start with #Text=. \\nDoc: \" + file)\n",
        "\n",
        "        docif[\"text\"] = copy.deepcopy(lines[textline_id[0]][6:])\n",
        "\n",
        "        first_cline = lines[(textline_id[0] + 1)].rstrip(\"\\t\").split(\"\\t\")   # first column_line\n",
        "        assert (len(first_cline) in [3, 5, 7, 8]), str(\"Doc has problem. doc: \" + file)\n",
        "\n",
        "\n",
        "        token_ids, subtoken_ids, pos, tokens = [], [], [], []\n",
        "        entity = []\n",
        "        relation = []\n",
        "\n",
        "        pretk_id = 0\n",
        "\n",
        "        for tk_id, line in enumerate(lines[(textline_id[0] + 1):]):\n",
        "            lineif = line.rstrip(\"\\t\").split(\"\\t\")   # seperate by one \\t between columns: [abc\\txyz\\t]\n",
        "\n",
        "            # check if columns is seperated by only one single Tab character '\\t'\n",
        "            lineif1 = re.split(r'\\t+', line.rstrip('\\t'))   # seperate by all \\t between column: [abc\\t\\t\\txyz\\t]\n",
        "            assert (lineif == lineif1), str(\"Columns is not seperated by only one single TAB '\\\\t'. doc: \" + file + \" line: \" + line)\n",
        "\n",
        "            # check if inside a doc, only exist one number of (no) columns\n",
        "            # above we check if len(lineif) in [3, 5, 7, 8], too. so we can make sure that\n",
        "            # in a doc, number of columns only in [3, 5, 7, 8]\n",
        "            # and all line in a doc has same no columns\n",
        "            assert len(lineif) == len(first_cline), str(\"Number of columns in doc is not consistent. \\nDoc: \" + file + \" line: \" + line)\n",
        "\n",
        "\n",
        "            # remove all \"_\" in lineif because we don't need it\n",
        "            # [3, 5, 7, 8] -> [3, 4, 5, 7]\n",
        "            # and all data has first three column. (4th and 5th) is a pair, (6th and 7th) is a pair\n",
        "            # after removing all \"_\", if:\n",
        "            # len(lineif) = 3 -> token_ids, pos, no entity, no relation\n",
        "            # len(lineif) = 5 -> token_ids, pos, entity, no relation\n",
        "            # len(lineif) = 7 -> token_ids, pos, entity, relation\n",
        "\n",
        "            # len(lineif) = 4 --> token_ids, pos, no entity, no relation (this is a mistake in dataset, in data file has 8 columns)\n",
        "\n",
        "            lineif = [col for col in lineif if col != \"_\"]\n",
        "\n",
        "            assert (len(lineif) in [3, 4, 5, 7]), str(\"Problem with number of columns after remove \\'_\\'.\\nIn doc: \" + file + \" line \" + line)\n",
        "\n",
        "            # match first column format\n",
        "            # startwith (\"1-\") then (number) end:   1-id\n",
        "            pattern_token_ids = re.compile(\"^(1-)([\\d]+)$\")\n",
        "\n",
        "            # a token may has many subtokens\n",
        "            # startwith (\"1-\") then (number) then (. char) then (number) end:   1-id.subid \n",
        "            # pattern_subtoken_ids = re.compile(\"^(1-)([\\d]+)(\\.)([\\d]+)$\")\n",
        "\n",
        "            # Currently in train dataset, number of subtoken of a token is 0 or 1\n",
        "            # startwith (\"1-\") then (number) then (.1) end:   1-id.1 \n",
        "            pattern_subtoken_ids = re.compile(\"^(1-)([\\d]+)(\\.1)$\")\n",
        "\n",
        "            assert (pattern_token_ids.match(lineif[0]) or pattern_subtoken_ids.match(lineif[0])), \\\n",
        "            str(\"Unexpected first column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "            if pattern_token_ids.match(lineif[0]):\n",
        "                # 1-id\n",
        "                # Check if token id is increased by one in each line or not.\n",
        "                assert (int(lineif[0][2:]) == (pretk_id + 1)), str(\"First column, Token_ID is not increased by one in each line. \\nIn doc: \" + file + \" \\nline: \" + line)\n",
        "\n",
        "                token_ids.append(int(lineif[0][2:]))\n",
        "                subtoken_ids.append(None)   # Not a subtoken\n",
        "\n",
        "                pretk_id += 1\n",
        "            \n",
        "            else:\n",
        "                # 1-id.1\n",
        "                # 1-id.subid\n",
        "                tmp = lineif[0].find(\".\")\n",
        "                tokenID = int(lineif[0][2:tmp])\n",
        "                subtokenID = int(lineif[0][(tmp+1):])\n",
        "                \n",
        "                assert (tokenID == token_ids[-1]), str(\"Exist subtoken without a token before it. \\nIn doc: \" + file + \" \\nline\" + line)\n",
        "\n",
        "                token_ids.append(tokenID)\n",
        "                subtoken_ids.append(subtokenID)\n",
        "\n",
        "                print(\"\\nTHERE IS A SUBTOKEN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "\n",
        "            # match second column format\n",
        "            # startwith (number) then (\"-\" char) then (number) end\n",
        "            pattern_pos = re.compile(\"^([\\d]+)(\\-)([\\d]+)$\")\n",
        "            assert (pattern_pos.match(lineif[1])), str(\"Unexpected second column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "            pos.append([int(ele) for ele in lineif[1].split(\"-\")])    # example: \"3-6\" -> [3, 6]\n",
        "            \n",
        "            # if current token is a subtoken, check if pos subtoken is inside father token or not.\n",
        "            if pattern_subtoken_ids.match(lineif[0]):\n",
        "                father_token = token_ids.index(token_ids[-1])\n",
        "\n",
        "                assert (pos[father_token][0] <= pos[-1][0]) and (pos[-1][1] <= pos[father_token][1]), \\\n",
        "                str(\"Subtoken\\'s position is not inside father token\\'s position. \\nIndoc: \" + file + \"\\Line: \" + line)\n",
        "\n",
        "\n",
        "            # third column\n",
        "            #check if token is matched with pos (second column) or not\n",
        "            crr_token_pos = [int(ele) for ele in lineif[1].split(\"-\")]\n",
        "            if lineif[2] == lines[textline_id[0]][6:][crr_token_pos[0]:crr_token_pos[1]]:\n",
        "                tokens.append(lineif[2])\n",
        "            else:\n",
        "                assert False, str(\"Token in 3th column not match with position at 2th column. \\nIn doc: \" + file + \" \\nline: \" + line)\n",
        "            \n",
        "\n",
        "            if (len(lineif) == 3) or (len(lineif) == 4):\n",
        "                entity.append(None)\n",
        "                relation.append(None)\n",
        "\n",
        "                if len(lineif) == 4:\n",
        "                    print(\"\\n4 COLUMNS.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "\n",
        "            # because we removed all \"_\", \n",
        "            # so when len(lineif) = 5 or len(lineif) = 7, this line must has: token_ids, pos, tokens and entity.\n",
        "            # (we don't have to check if 4th, 5th column is \"_\" anymore, since we removed all \"_\")\n",
        "            if (len(lineif) == 5) or (len(lineif) == 7):\n",
        "                # 4th column now only have two posibilities: \"*\" or \"*[number]\"\n",
        "                pattern_entity_id = re.compile(\"^(\\*)(\\[)([\\d]+)(\\])$\")\n",
        "                assert ((lineif[3] == \"*\") or pattern_entity_id.match(lineif[3])), str(\"Unexpected fourth column's format. In doc: \" + file + \" line \" + line)\n",
        "\n",
        "                # in doc: 23352816\n",
        "                # line: 1-23\t126-136\t</ENAMEX>)\t*\t*\t_\t_\t_\t\n",
        "                # there is a mistake in 5th column. Unknow enity name\n",
        "                # I will let this token entity is None.\n",
        "\n",
        "                # We can just let all token entity is None\n",
        "                # if 5th column is not in constant[\"entity_name\"]\n",
        "                # but below, I just code for this specific case\n",
        "                # because I want to know more about dataset\n",
        "\n",
        "                if (lineif[3] == \"*\"):\n",
        "\n",
        "                    if (lineif[4] == \"*\"):   # specific mistake case\n",
        "                        entity.append(None)\n",
        "                        print(\"\\nENTITY NAME MISTAKE IN 5TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                    else:\n",
        "                        assert (lineif[4] in constant[\"entity_name\"]), str(\"Unknown entity name. \\nDoc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "                        entity_id = 0\n",
        "                        entity_n = lineif[4]\n",
        "\n",
        "                        entity.append([entity_id, entity_n])\n",
        "                \n",
        "                elif pattern_entity_id.match(lineif[3]):\n",
        "                    # *[number]: *[26] -> 26\n",
        "                    entity_id = int(lineif[3][2:-1])\n",
        "                    \n",
        "                    # PERSON[26]\n",
        "                    tmp = lineif[4].find(\"[\")\n",
        "                    \n",
        "                    assert (entity_id == int(lineif[4][(tmp+1):-1])), str(\"Entity ID in 4th and 5th column are not the same. In doc: \" + file + \" line \" + line)\n",
        "                    \n",
        "                    assert (lineif[4][:tmp] in constant[\"entity_name\"]), str(\"Unknown entity name in doc: \" + file + \" line \" + line)\n",
        "                    \n",
        "                    entity_n = lineif[4][:tmp]\n",
        "\n",
        "                    entity.append([entity_id, entity_n])\n",
        "\n",
        "                # may be we dont need this last else because we use regex above\n",
        "                else:\n",
        "                    assert False, str(\"4th, 5th column has UNKNOWN MISTAKE. In Doc: \" + file + \"\\nline: \" + line)\n",
        "\n",
        "\n",
        "\n",
        "            if len(lineif) == 5:\n",
        "                relation.append(None)\n",
        "            \n",
        "\n",
        "            if len(lineif) == 7:\n",
        "                # example:\n",
        "                # AFFILIATION\t1-593[13_14]\n",
        "                # PART – WHOLE\t1-42[1_2]\n",
        "                # PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-80[3_8]|1-105[7_8]\n",
        "\n",
        "                # PART – WHOLE\t1-42    (an error in dataset that need to be handled)\n",
        "\n",
        "                rel_names = lineif[5].split(\"|\")    # PERSONAL - SOCIAL|PERSONAL - SOCIAL --> [\"PERSONAL - SOCIAL\", \"PERSONAL - SOCIAL\"]\n",
        "                rel_oifs = lineif[6].split(\"|\")     # 1-80[3_8]|1-105[7_8] --> [\"1-80[3_8]\", \"1-105[7_8]\"]\n",
        "\n",
        "                # in doc: 23351515\n",
        "                # line: 1-318\n",
        "                # 6th column: PART – WHOLE|LOCATED|PART – WHOLE|*\n",
        "                # last relation name is: *  -> mistake\n",
        "                # We can read data and change it to right one \n",
        "                # but I will remove this \"*\" relation in 6th and 7th column\n",
        "\n",
        "                if '*' in rel_names:\n",
        "                    tmp = rel_names.index('*')\n",
        "\n",
        "                    del rel_names[tmp]\n",
        "                    del rel_oifs[tmp]\n",
        "\n",
        "                    print(\"\\nRELATION MISTAKE IN 6TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                # MISTAKE In doc: 23351856\n",
        "                # line: 1-185\t807-812\tTriều\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
        "                # assert ((len(rel_names) == len(rel_oifs)) and (len(rel_names) >= 1)), str(\"Number of relations in 6th and 7th columns is different to each other. In doc: \" + file + \" line \" + line )\n",
        "                # handle later\n",
        "\n",
        "                assert (len(rel_names) == len(rel_oifs)), str(\"Number of relations in 6th and 7th columns is different to each other. \\nIn doc: \" + file + \" \\nline \" + line )\n",
        "\n",
        "\n",
        "                rels = []\n",
        "                for i in range(len(rel_names)):\n",
        "                    assert (rel_names[i] in constant[\"relation_name\"]), \\\n",
        "                    str(\"Unknown relation_name in doc: \" + file + \" \\nline \" + line)\n",
        "                    \n",
        "                    relation_n = rel_names[i]\n",
        "\n",
        "                    # (startwith \"1-\") then (number) then ([ char) then (number) then (_ char) then (number) then (] char) end\n",
        "                    #             1-         26            [             3             _             0             ]   \n",
        "                    pattern_relation_oif = re.compile(\"^(1-)([\\d]+)(\\[)([\\d]+)(\\_)([\\d]+)(\\])$\")\n",
        "\n",
        "                    # (startwith \"1-\") then (number) then (.1) then ([ char) then (number) then (_ char) then (number) then (] char) end\n",
        "                    #             1-         26            .1        [             3             _             0             ]   \n",
        "                    pattern_relation_oif_1 = re.compile(\"^(1-)([\\d]+)(\\.1)(\\[)([\\d]+)(\\_)([\\d]+)(\\])$\")\n",
        "                    \n",
        "                    # below is a mistake in dataset\n",
        "                    # but currently, this type mistake has only below form (only has token id).    <--- TRAIN DOES NOT HAVE, BUT DEV HAS\n",
        "                    # (don't have subtoken id mistake type, yet)   <--- TRAIN DOES NOT HAVE, BUT DEV HAS\n",
        "                    # (startwith \"1-\") then (number)  end\n",
        "                    #             1-         26          \n",
        "                    pattern_relation_oif_mistake_1 = re.compile(\"^(1-)([\\d]+)$\")\n",
        "\n",
        "\n",
        "                    # below is a mistake in dataset <--- subtoken id only\n",
        "                    # (startwith \"1-\") then (number) then (.1) end\n",
        "                    #             1-         26            .1\n",
        "                    pattern_relation_oif_mistake_2 = re.compile(\"^(1-)([\\d]+)(\\.1)$\")\n",
        "\n",
        "\n",
        "\n",
        "                    assert (pattern_relation_oif.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_1.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_mistake_1.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_mistake_2.match(rel_oifs[i])), \\\n",
        "                            str(\"Unexpected seventh column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "                    \n",
        "\n",
        "                    # NOTICE:\n",
        "                    # IN BELOW CODE, I DONT CHECK IF ONE OF TWO ENTITIES OF A RELATION\n",
        "                    # IS \"MISCELLANEOUS\" OR NOT. \n",
        "                    # MISCELLANEOUS IS A LEGIT ENTITY NAME, BUT IT IS NOT USED IN ANY RELATION TYPE.\n",
        "                    # I WONDER IF DATASET HAS THIS MISTAKE OR NOT.\n",
        "                    # I WILL CHECK IT WHEN I CREATE SENTENCES AS INPUT OF BERT.\n",
        "\n",
        "\n",
        "                    if pattern_relation_oif.match(rel_oifs[i]):\n",
        "                        # 1-id[id_id]\n",
        "\n",
        "                        tmp_stkid = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_etkid = rel_oifs[i].find(\"[\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp_stkid:tmp_etkid]\n",
        "\n",
        "                        # start subtoken id\n",
        "                        sstoken_id = None\n",
        "\n",
        "                        direction = rel_oifs[i][(tmp_etkid+1):-1]\n",
        "\n",
        "                        direction = direction.split(\"_\")   # [sentity_id, eentity_id]\n",
        "\n",
        "                        rels.append([relation_n, int(stoken_id), sstoken_id, [int(direction[0]), int(direction[1])]])\n",
        "\n",
        "                    elif pattern_relation_oif_1.match(rel_oifs[i]):\n",
        "                        # 1-id.subid[id_id]\n",
        "\n",
        "                        tmp_sid = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_eid = rel_oifs[i].find(\".\")\n",
        "\n",
        "                        tmp_ssid = rel_oifs[i].find(\".\") + 1\n",
        "                        tmp_esid = rel_oifs[i].find(\"[\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp_sid:tmp_eid]\n",
        "\n",
        "                        sstoken_id = rel_oifs[i][tmp_ssid:tmp_esid]\n",
        "\n",
        "\n",
        "                        direction = rel_oifs[i][(tmp_esid+1):-1]\n",
        "                        direction = direction.split(\"_\")\n",
        "\n",
        "                        rels.append([relation_n, int(stoken_id), int(sstoken_id), [int(direction[0]), int(direction[1])]])\n",
        "\n",
        "                        print(\"\\nSPECIAL SUBTOKEN IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "                    \n",
        "                    elif pattern_relation_oif_mistake_1.match(rel_oifs[i]):\n",
        "                        # 1-id\n",
        "                        tmp = rel_oifs[i].find(\"-\") + 1\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp:]\n",
        "                        \n",
        "                        sstoken_id = None\n",
        "                        direction = None\n",
        "\n",
        "                        # rels.append([relation_n, stoken_id, sstoken_id, direction])\n",
        "                        rels.append([relation_n, int(stoken_id), None, None])\n",
        "\n",
        "                        print(\"\\nMISTAKE IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "                    \n",
        "                    elif pattern_relation_oif_mistake_2.match(rel_oifs[i]):\n",
        "                        # 1-id.1\n",
        "                        tmp = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_1 = rel_oifs[i].find(\".1\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp:tmp_1]\n",
        "                        \n",
        "                        sstoken_id = 1\n",
        "                        direction = None\n",
        "\n",
        "                        # rels.append([relation_n, stoken_id, sstoken_id, direction])\n",
        "                        rels.append([relation_n, int(stoken_id), sstoken_id, None])\n",
        "\n",
        "                        print(\"\\nMISTAKE IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "                        print(stoken_id, ' - ', sstoken_id)\n",
        "                    \n",
        "                    else:\n",
        "                        assert False, str(\"Unexpected seventh column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "\n",
        "                \n",
        "                if len(rels) == 0:\n",
        "                    # MISTAKE In doc: 23351856\n",
        "                    # line: 1-185\t807-812\tTriều\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
        "                    relation.append(None)\n",
        "                    print(\"\\nREALTION NAME MISTAKE IN 6TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                else:\n",
        "                    relation.append(rels)\n",
        "\n",
        "\n",
        "        docif[\"token_ids\"] = copy.deepcopy(token_ids)\n",
        "        docif[\"subtoken_ids\"] = copy.deepcopy(subtoken_ids)\n",
        "        docif[\"pos\"] = copy.deepcopy(pos)\n",
        "        docif[\"tokens\"] = copy.deepcopy(tokens)\n",
        "        docif[\"entity\"] = copy.deepcopy(entity)\n",
        "        docif[\"relation\"] = copy.deepcopy(relation)\n",
        "\n",
        "    raw_tdata.append(copy.deepcopy(docif))\n",
        "\n",
        "\n",
        "print(len(raw_tdata))           \n",
        "                \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvsWfSzerWz5"
      },
      "source": [
        "# CHECK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABXxWzmtrb6c",
        "outputId": "90a9a797-cbe8-46f1-b03b-a0932426847b"
      },
      "source": [
        "for docif in raw_tdata:\n",
        "\n",
        "    for iline in range(len(docif['token_ids'])):\n",
        "        if docif['subtoken_ids'][iline] != None:\n",
        "            print('FOUND SUBTOKEN')\n",
        "            assert Fasle, 'FOUND SUBTOKEN'\n",
        "\n",
        "        if docif['relation'][iline] != None:\n",
        "            print('FOUND RELATION')\n",
        "            assert Fasle, 'FOUND RELATION'\n",
        "        \n",
        "\n",
        "\n",
        "print('SEEM THAT TEST DOES NOT HAS SUBTOKENS AND RELATIONS')\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SEEM THAT TEST DOES NOT HAS SUBTOKENS AND RELATIONS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wzBGIIgKYPD"
      },
      "source": [
        "# Fix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruagt4UGbqL5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8bb639ca-2200-4336-c661-c2b4277dd2d3"
      },
      "source": [
        "\"\"\"\n",
        "raw_data: - is a list (len = 506)\n",
        "          - each row is a dict {}, information from a single doc (506 doc)\n",
        "            - doc_id: number id in name of folder that contain doc\n",
        "            - text: text in line start with \"#Text=\"\n",
        "            - token_ids: list of int. (1st column)\n",
        "            - subtoken_ids: list. (1st column)\n",
        "                            an element can be None if token is not a subtoken: 1-id -> 1.26,\n",
        "                                           or int (subtoken_id) if token is a subtoken: 1-id.subid -> 1.26.1.\n",
        "                                           currently in train data, only exist subtoken id 1.\n",
        "                            (in extract raw data code, my code can get any subid, not just specify subid = 1.\n",
        "                             but i check if in data has other subid, it will return error -> to know more about data)\n",
        "            - pos: list of child list. (2st column)\n",
        "                   each child list has two int elements.\n",
        "                   [start_position, end_position]\n",
        "            - tokens: list of strings. (3th column)\n",
        "            - entity: list.\n",
        "                      an element is: None if crr token is not entity\n",
        "                                     a list with: 2 element if crr token is an entity.\n",
        "                                                  [entity_id, entity_name]\n",
        "                                                  entity_id: int, from 4th column\n",
        "                                                  entity_name: string, from 5th column\n",
        "            - relation: list\n",
        "                        an element is: None if there is no relation in 6ht, 7th column.\n",
        "                                       a list of child list. number of child list is number of relation in 6th, 7th column.\n",
        "                                                 each child list has: 4 elemnt\n",
        "                                                 [relation_name, stoken_id, sstoken_id, direction[sentity_id, eentity_id]]\n",
        "                                                 relation_name: string, from 6h column\n",
        "                                                 stoken_id: int, tokenid from 7th column\n",
        "                                                 sstoken_id: from 7th column\n",
        "                                                             None, if entity_1 is a token\n",
        "                                                             else: int, subid if entity_1 is subtoken\n",
        "                                                 direction: from 7th column\n",
        "                                                            None, if there is a mistake in dataset\n",
        "                                                            else: [entity_1_id, entity_2_id]\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nraw_data: - is a list (len = 506)\\n          - each row is a dict {}, information from a single doc (506 doc)\\n            - doc_id: number id in name of folder that contain doc\\n            - text: text in line start with \"#Text=\"\\n            - token_ids: list of int. (1st column)\\n            - subtoken_ids: list. (1st column)\\n                            an element can be None if token is not a subtoken: 1-id -> 1.26,\\n                                           or int (subtoken_id) if token is a subtoken: 1-id.subid -> 1.26.1.\\n                                           currently in train data, only exist subtoken id 1.\\n                            (in extract raw data code, my code can get any subid, not just specify subid = 1.\\n                             but i check if in data has other subid, it will return error -> to know more about data)\\n            - pos: list of child list. (2st column)\\n                   each child list has two int elements.\\n                   [start_position, end_position]\\n            - tokens: list of strings. (3th column)\\n            - entity: list.\\n                      an element is: None if crr token is not entity\\n                                     a list with: 2 element if crr token is an entity.\\n                                                  [entity_id, entity_name]\\n                                                  entity_id: int, from 4th column\\n                                                  entity_name: string, from 5th column\\n            - relation: list\\n                        an element is: None if there is no relation in 6ht, 7th column.\\n                                       a list of child list. number of child list is number of relation in 6th, 7th column.\\n                                                 each child list has: 4 elemnt\\n                                                 [relation_name, stoken_id, sstoken_id, direction[sentity_id, eentity_id]]\\n                                                 relation_name: string, from 6h column\\n                                                 stoken_id: int, tokenid from 7th column\\n                                                 sstoken_id: from 7th column\\n                                                             None, if entity_1 is a token\\n                                                             else: int, subid if entity_1 is subtoken\\n                                                 direction: from 7th column\\n                                                            None, if there is a mistake in dataset\\n                                                            else: [entity_1_id, entity_2_id]\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQKOe59wfEcS"
      },
      "source": [
        "## Fix2: Others fix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpazvkcLtgm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844404bb-ec82-4929-84fa-99f02cf7d25d"
      },
      "source": [
        "\n",
        "# doc: 23352816\n",
        "# hai token 152, 153 mặc dù cùng có entity_id = 0 và đứng cạnh nhau\n",
        "# nhưng hai token này không phải là 1 entity mà là 2 entity vì chúng có entity_name khác nhau và chúng thuộc 2 câu khác nhau\n",
        "# ngoài ra token 153: \". VMISS\" dính dấu chấm ở câu trước, và đây cũng là 1 entity\n",
        "# ta cần sửa lỗi trong doc này: tách \". VMISS\" thành 2 token \".\" và \"VMISS\"\n",
        "# có nghĩa là ta sẽ thêm 1 dòng nữa cho \".\" và sửa lại \". VMISS\" thành \"VMISS\"\n",
        "\n",
        "raw_tdata_new = copy.deepcopy(raw_tdata)\n",
        "\n",
        "for docif in raw_tdata_new:\n",
        "    if docif['doc_id'] == '23352918':\n",
        "\n",
        "        for i in range(len(docif['token_ids'])):\n",
        "\n",
        "            if (docif['tokens'][i] == \"bố. Hassan\") and (docif['tokens'][i-1] == \"khủng\"):\n",
        "                \n",
        "                # them 1 dong cho dau '.'\n",
        "                docif['token_ids'].insert(i, docif['token_ids'][i])\n",
        "                docif['pos'].insert(i, [docif['pos'][i][0], (docif['pos'][i][0]+3)])\n",
        "                docif['tokens'].insert(i, 'bố.')   # copy kí tự '.' trong doc rồi paste vào để tránh lỗi unicode\n",
        "                docif['subtoken_ids'].insert(i, None)\n",
        "                docif['entity'].insert(i, None)\n",
        "                docif['relation'].insert(i, None)\n",
        "\n",
        "                # sua lai dong '. VMISS' (do thêm '.' vào trước nên dòng '. VMISS' thành i+1)\n",
        "                docif['tokens'][i+1] = 'Hassan'\n",
        "                docif['pos'][i+1] = [docif['pos'][i+1][0] + 4, docif['pos'][i+1][1]]\n",
        "\n",
        "                # do them 1 dong nen phai sua lai token_ids phia sau va relation link toi token_ids phia sau\n",
        "                # thay đổi token_ids của toàn bộ phần dưới, nếu có subid trong relation ở đâu thì thay đổi, thay đổi stoken_id trong relation\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if j > i:\n",
        "                        docif['token_ids'][j] += 1\n",
        "\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            # do ở trên, toàn bộ token_ids phía sau (> i) sẽ bị thay đổi (cộng thêm 1)\n",
        "                            # nên những relation có stoken_id nằm ở phần phía sau này cũng sẽ cần thay đổi theo (cộng thêm 1)\n",
        "                            if (docif['relation'][j][k][1] > docif['token_ids'][i]):\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1\n",
        "\n",
        "\n",
        "\n",
        "print('\\n\\nCHECKING')\n",
        "for idoc, docif in enumerate(raw_tdata_new):\n",
        "\n",
        "    if docif['doc_id'] == '23352918':\n",
        "\n",
        "        relation_lst = []\n",
        "        for i in range(len(raw_tdata[idoc]['relation'])):\n",
        "            if raw_tdata[idoc]['relation'][i] != None:\n",
        "                relation_lst.append(raw_tdata[idoc]['relation'][i])\n",
        "\n",
        "\n",
        "        relation_ith = 0\n",
        "        for i in range(len(docif['token_ids'])):\n",
        "\n",
        "            assert (docif['tokens'][i] == docif['text'][docif['pos'][i][0]:docif['pos'][i][1]]), \\\n",
        "            str('Wrong position')\n",
        "\n",
        "            if i < (len(docif['token_ids']) - 1):\n",
        "                if (docif['token_ids'][i] + 1) != docif['token_ids'][i+1]:\n",
        "                    assert False, str('Tokens_ids has problem')\n",
        "            \n",
        "            if docif['relation'][i] != None:\n",
        "                for j in range(len(docif['relation'][i])):\n",
        "   \n",
        "                    # so sánh xem thay đổi stoken_id có đúng không\n",
        "                    # string của token và pos của token sẽ không đổi so với raw_tdata\n",
        "                    stoken_new = docif['relation'][i][j][1]\n",
        "                    stoken_new_ele_id = docif['token_ids'].index(stoken_new)\n",
        "                    \n",
        "                    stoken = relation_lst[relation_ith][j][1]\n",
        "                    if relation_lst[relation_ith][j][2] == 1:\n",
        "                        stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken) + 1\n",
        "                    else:\n",
        "                        stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken)\n",
        "\n",
        "                    if docif['pos'][stoken_new_ele_id] != raw_tdata[idoc]['pos'][stoken_ele_id]:\n",
        "                        '''\n",
        "                        print('\\n-----Doc: ', docif['doc_id'])\n",
        "                        print(relation_ith)\n",
        "                        print(docif['relation'][i])\n",
        "                        print(relation_lst[relation_ith])\n",
        "                        print(docif['pos'][stoken_new_ele_id])\n",
        "                        print(raw_tdata[idoc]['pos'][stoken_ele_id])\n",
        "                        '''\n",
        "                        assert False, str('ERROR CODE 3')\n",
        "                    \n",
        "                relation_ith += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('DONE. EVERYTHINGS SEEM TO BE CORRECTED :D')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "CHECKING\n",
            "DONE. EVERYTHINGS SEEM TO BE CORRECTED :D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QeZVNtl9rxT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a27a7fa8-90e1-4d7f-ad55-4eb558f9c10e"
      },
      "source": [
        "'''\n",
        "for idoc, docif in enumerate(raw_tdata_new_v2):\n",
        "\n",
        "    if docif['doc_id'] == '23352816':\n",
        "        for i in range(len(docif['token_ids'])):\n",
        "            for key in docif:\n",
        "                if key not in ['doc_id', 'text']:\n",
        "                    print(docif[key][i], end='\\t')\n",
        "            print('\\n')\n",
        "'''"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfor idoc, docif in enumerate(raw_tdata_new_v2):\\n\\n    if docif['doc_id'] == '23352816':\\n        for i in range(len(docif['token_ids'])):\\n            for key in docif:\\n                if key not in ['doc_id', 'text']:\\n                    print(docif[key][i], end='\\t')\\n            print('\\n')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0VsUK91_VNX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8be83d51-3734-460a-d0a7-d990a65fb216"
      },
      "source": [
        "'''\n",
        "### doc: 23352687\n",
        "# sent: Văn Mạnh hạng thương tật T37 đã thành công ở nội dung chạy 200m.\n",
        "# trong doc này, trước câu trên có 2 dấu cách, khi dùng Underthesea thì Underthesea tách câu chính xác là bỏ 2 dấu cách đi.\n",
        "# nhưng trong doc thì tokenize bị lỗi, từ \"Văn\" trong doc là \" Văn\" tức là dính dấu 1 cách\n",
        "# dẫn tới start pos của entity khác với start pos của câu (chênh nhau 1 đơn vị cho chênh nhau 1 dấu cách)\n",
        "# phương án: sửa lại pos và token của entity trong trường hợp này, bỏ đi dấu cách thừa. \n",
        "\n",
        "raw_tdata_new_v3 = copy.deepcopy(raw_tdata_new_v2)\n",
        "\n",
        "for docif in raw_tdata_new_v3:\n",
        "    \n",
        "    if docif['doc_id'] == '23352687':\n",
        "        for i in range(len(docif['tokens'])):\n",
        "            if docif['tokens'][i] == \" Văn\":\n",
        "                print(docif[\"pos\"][i][0])\n",
        "                print(repr(docif[\"tokens\"][i]))\n",
        "\n",
        "                docif[\"pos\"][i][0] = docif[\"pos\"][i][0] + 1   # bỏ 1 dấu cách ở đầu thì pos sẽ dịch lên 1 đơn vị\n",
        "                docif[\"tokens\"][i] = docif[\"tokens\"][i].lstrip()   # bỏ dấu cách đi\n",
        "\n",
        "                assert (docif['text'][docif[\"pos\"][i][0]:docif[\"pos\"][i][1]] == docif[\"tokens\"][i]), \\\n",
        "                str(\"Change Failed!\")\n",
        "\n",
        "                print(docif[\"pos\"][i][0])\n",
        "                print(repr(docif[\"tokens\"][i]))\n",
        "\n",
        "                break\n",
        "        break\n",
        "\n",
        "\n",
        "'''\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n### doc: 23352687\\n# sent: Văn Mạnh hạng thương tật T37 đã thành công ở nội dung chạy 200m.\\n# trong doc này, trước câu trên có 2 dấu cách, khi dùng Underthesea thì Underthesea tách câu chính xác là bỏ 2 dấu cách đi.\\n# nhưng trong doc thì tokenize bị lỗi, từ \"Văn\" trong doc là \" Văn\" tức là dính dấu 1 cách\\n# dẫn tới start pos của entity khác với start pos của câu (chênh nhau 1 đơn vị cho chênh nhau 1 dấu cách)\\n# phương án: sửa lại pos và token của entity trong trường hợp này, bỏ đi dấu cách thừa. \\n\\nraw_tdata_new_v3 = copy.deepcopy(raw_tdata_new_v2)\\n\\nfor docif in raw_tdata_new_v3:\\n    \\n    if docif[\\'doc_id\\'] == \\'23352687\\':\\n        for i in range(len(docif[\\'tokens\\'])):\\n            if docif[\\'tokens\\'][i] == \"\\xa0Văn\":\\n                print(docif[\"pos\"][i][0])\\n                print(repr(docif[\"tokens\"][i]))\\n\\n                docif[\"pos\"][i][0] = docif[\"pos\"][i][0] + 1   # bỏ 1 dấu cách ở đầu thì pos sẽ dịch lên 1 đơn vị\\n                docif[\"tokens\"][i] = docif[\"tokens\"][i].lstrip()   # bỏ dấu cách đi\\n\\n                assert (docif[\\'text\\'][docif[\"pos\"][i][0]:docif[\"pos\"][i][1]] == docif[\"tokens\"][i]),                 str(\"Change Failed!\")\\n\\n                print(docif[\"pos\"][i][0])\\n                print(repr(docif[\"tokens\"][i]))\\n\\n                break\\n        break\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBr6Gx-Y2oj2"
      },
      "source": [
        "## Find all entity in a doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVl3FKeND9dk"
      },
      "source": [
        "# Tìm tất cả các entity trong một doc:\n",
        "\n",
        "def find_all_entity_in_doc(raw_data, doc_id):\n",
        "    \n",
        "    for docif in raw_data:\n",
        "\n",
        "        ######## Tìm tất cả các entity trong doc hiện tại:\n",
        "\n",
        "        #                      -                                          entity_1                                              -  ...\n",
        "        #                      | -                 token_1                  -  -                   token_2                -  ...|\n",
        "        ##### doc_entity_lst = [ [ [ele_id, token_id, entity_id, entity_name], [ele_id, token_id, entity_id, , entity_name], ...], ...]\n",
        "\n",
        "        ### Lưu ý: ele_id ở đây là element index của token đấy trong docif[\"entity\"]\n",
        "        ### chứ không phải là token_ids\n",
        "        ### lưu cái ele_id thay vì token_ids để truy cập token bằng index nhanh hơn\n",
        "        ### nếu lưu token_ids thì phải từ từ token_ids tìm xem token này nó nằm vị trí nào thì mới ra index (ele_id) để truy\n",
        "        ### thường thì, vì token_ids bắt đầu = 1, element index bắt đầu = 0 nên: token_ids tương ứng sẽ có: token_ids = ele_id + 1\n",
        "        ### nhưng nếu trong doc có subtoken thì điều trên sẽ không được đảm bảo\n",
        "\n",
        "        # token có entity_id giống nhau mà không đứng cạnh nhau (các dòng chứa token không liên tiếp nhau) thì thuộc 2 entity khác nhau. \n",
        "        # Trường hợp này là do dataset lỗi, bị lặp entity_id (bên trên có entity_id = 0 rồi xuống dưới (token không cạnh nhau) lại thấy entity_id = 0)\n",
        "        # Hiện tại mới thấy có entity_id = 0 là bị lặp lại cho nhiều entity khác nhau\n",
        "\n",
        "        \n",
        "\n",
        "        if docif['doc_id'] == doc_id:\n",
        "\n",
        "            doc_entity_lst = []\n",
        "            tmplst = []\n",
        "\n",
        "            for i in range(len(docif[\"entity\"])):\n",
        "                if docif[\"entity\"][i] != None:\n",
        "                    tmplst.append([i, docif[\"token_ids\"][i], docif[\"entity\"][i][0], docif[\"entity\"][i][1]])\n",
        "\n",
        "                    if (i < (len(docif[\"entity\"]) - 1)):\n",
        "\n",
        "                        if docif[\"entity\"][i+1] == None:\n",
        "                            doc_entity_lst.append(tmplst)\n",
        "                            tmplst = []\n",
        "\n",
        "                        elif (docif[\"entity\"][i][0] != docif[\"entity\"][i+1][0]) or (docif[\"entity\"][i][1] != docif[\"entity\"][i+1][1]):\n",
        "                            doc_entity_lst.append(tmplst)\n",
        "                            tmplst = []\n",
        "\n",
        "                    if i == (len(docif[\"entity\"]) - 1):\n",
        "                        doc_entity_lst.append(tmplst)\n",
        "\n",
        "\n",
        "            ###################### Fix lỗi hai entity khác nhau nhưng đứng cạnh nhau và bị trùng entity_id\n",
        "            # tuy nhiên, ta chỉ tìm các entity_id = 0 thôi, vì chúng dễ lỗi nhất, dễ bị trùng id nhất\n",
        "            # mọi cặp entity_id = 0 đứng cạnh nhau trong các doc trong list bên dưới đều sẽ bị tách ra thành các entity riêng\n",
        "\n",
        "            ##### comment of V1 <-- trong notebook extract_train thì code cũ hoạt động ổn do không có cụm entity lỗi nào > 2 entity\n",
        "            ##### tuy nhiên trong dev thì xuất hiện một số cụm 3 entity id 0 bị lỗi\n",
        "            # tuy nhiên ta sẽ xử lý từng cặp một trong từng lần xử lý, run_times là số cặp trùng trong doc\n",
        "            # nếu có doc nào vừa có cặp entity_0 lỗi vừa có cặp không lỗi thì phải sẽ không được thêm list dưới vào mà phải xử lý riêng\n",
        "            # trừ khi cặp bị lỗi là cặp đầu tiên thì run_times đặt là 1\n",
        "            # data cũng không có quá nhiều những cặp như này\n",
        "            #####\n",
        "            \n",
        "\n",
        "            # chạy 2 cell code bên dưới trước để tìm các cụm entity_id = 0 có từ 2 token trở lên\n",
        "            # để xem những cụm nào bị lỗi, cụm nào không bị, cụm nào lỗi mà cần sửa\n",
        "            # ta sẽ cần lấy doc_id và ith của cụm bị lỗi dể sửa\n",
        "            # sau khi sửa trong này, những cụm được sửa sẽ không còn xuất hiện khi chạy 2 cell bên dưới nữa\n",
        "            # những cụm không được sửa (không lỗi hoặc lỗi mà chọn không sửa) vẫn sẽ được in ra\n",
        "\n",
        "            # ngoài ra, do chỉ đích danh entity cần sửa nên dù doc có cả entity lỗi lẫn entity không lỗi thì vẫn sửa được\n",
        "\n",
        "            # lưu ý: mọi cặp entity lỗi có bao nhiêu token thì sẽ được tách hết ra thành từng đấy entity.\n",
        "            # tức là ví dụ entity id 0 lỗi có 3 token thì được tách ra thành 3 entity id 0 riêng biệt, mỗi entity chỉ là 1 token\n",
        "            # code bên dưới chỉ xử lý trường hợp này.\n",
        "            # không xử lý trường hợp kiểu entity id 0 lỗi có 3 token, \n",
        "            # nhưng lại cần tách ra 2 entity (thay vì 3), 1 entity gồm 2 token đầu, 1 entity là 1 token cuối\n",
        "            # lý do: lười quá, và số cụm lỗi rất ít, trường hợp như kia thì càng rất ít hơn và có khi k xảy ra\n",
        "\n",
        "\n",
        "            \n",
        "            doc_error_lst = [{'doc_error_id': '23352918', 'ith_er_lst': [6]},\n",
        "                             {'doc_error_id': '23352926', 'ith_er_lst': [3]},\n",
        "                             {'doc_error_id': '23352941', 'ith_er_lst': [0, 2, 26, 30]},\n",
        "                             {'doc_error_id': '23352961', 'ith_er_lst': [1]},\n",
        "                             {'doc_error_id': '23352963', 'ith_er_lst': [1, 38]},\n",
        "                             {'doc_error_id': '23352975', 'ith_er_lst': [10]},\n",
        "                             {'doc_error_id': '23353032', 'ith_er_lst': [7]},\n",
        "                             {'doc_error_id': '24503940', 'ith_er_lst': [37]},\n",
        "                             {'doc_error_id': '24568398', 'ith_er_lst': [41, 43, 57, 99, 101, 103]},\n",
        "                             {'doc_error_id': '24657217', 'ith_er_lst': [17]},\n",
        "                             {'doc_error_id': '24681732', 'ith_er_lst': [0, 8]},\n",
        "                             {'doc_error_id': '24796868', 'ith_er_lst': [2, 4, 5]},\n",
        "                             {'doc_error_id': '24839422', 'ith_er_lst': [12]},\n",
        "                             {'doc_error_id': '24847781', 'ith_er_lst': [9]},\n",
        "                             {'doc_error_id': '24854363', 'ith_er_lst': [0]},\n",
        "                             {'doc_error_id': '24891267', 'ith_er_lst': [209]}\n",
        "                            ]\n",
        "\n",
        "            doc_error_id_lst = [doc_error['doc_error_id'] for doc_error in doc_error_lst]\n",
        "\n",
        "            increase_ids = 0\n",
        "\n",
        "            if doc_id in doc_error_id_lst:\n",
        "                ith_doc_id = doc_error_id_lst.index(doc_id)\n",
        "\n",
        "                assert (doc_error_lst[ith_doc_id]['doc_error_id'] == doc_id), str('PRBOLEM')\n",
        "\n",
        "                #print('\\n\\n------', doc_id)\n",
        "\n",
        "                ith_er_list = sorted(doc_error_lst[ith_doc_id]['ith_er_lst'])\n",
        "\n",
        "                for irun, ith_er_id in enumerate(ith_er_list):\n",
        "                    #print('--', irun)\n",
        "                    doc_entity_lst_copy = None\n",
        "                    doc_entity_lst_copy = copy.deepcopy(doc_entity_lst)\n",
        "\n",
        "                    for ient, ent in enumerate(doc_entity_lst):\n",
        "                        if ient == (ith_er_id + increase_ids): # do bị dịch nên cần cộng với số id bị dịch\n",
        "\n",
        "                            assert ((ent[0][2] == 0) and (len(ent) > 1)), \\\n",
        "                            str('\\nWrong ith_er_lst. \\nDoc: ' + str(doc_id) + '\\nith_er_id: ' + str(ith_er_id))\n",
        "\n",
        "                            #print(doc_entity_lst_copy)\n",
        "\n",
        "                            # ví dụ: 3 token thì chỉ cần chạy 3 - 1 = 2 lần\n",
        "                            # lần đầu (itk = 0) thì lấy token cuối entity (token thứ 3: ent[-1]) chèn vào sau vị trí hiện tại của entity\n",
        "                            # lần hai (itk = 1) thì lấy token ngay trước token cuối (token thứ 2 từ cuối lên: ent[-2]) chèn vào sau vị trí hiện tại của entity\n",
        "                            for itk in range(len(ent) - 1):\n",
        "                                doc_entity_lst_copy.insert((ient+1), copy.deepcopy([ent[(-1 - itk)]]))\n",
        "\n",
        "                            # cuối cùng thì biến entity lỗi hiện tại thành token đầu của entity lỗi hiện tại là xong\n",
        "                            doc_entity_lst_copy[ient] = copy.deepcopy([ent[0]])\n",
        "\n",
        "                            #print(doc_entity_lst_copy)\n",
        "\n",
        "                            # do bên trên ta chèn thêm (len(ent) - 1) entity mới vào entity list\n",
        "                            # nên id của entity bị lỗi phía sau sẽ bị tăng lên lượng tương ứng\n",
        "                            # là tổng (len(ent)-1) của mọi ent bị sửa trước nó\n",
        "                            increase_ids += (len(ent) - 1)\n",
        "\n",
        "                            break\n",
        "                            \n",
        "                    doc_entity_lst = copy.deepcopy(doc_entity_lst_copy)\n",
        "                    #print(doc_entity_lst)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "            \n",
        "\n",
        "\n",
        "            '''\n",
        "            # Doc: 23351965\n",
        "            # hai token cạnh nhau (320, 321), cùng entity_id 0, cùng entity_name location nhưng không phải là 1 entity <-- lỗi data\n",
        "            # mà là hai entity, vì entity này link tới entity kia.\n",
        "\n",
        "            if docif['doc_id'] == '23351965':\n",
        "                doc_entity_lst_copy = copy.deepcopy(doc_entity_lst)\n",
        "                for ient, ent in enumerate(doc_entity_lst):\n",
        "                    if (len(ent) == 2) and (ent[0][2] == 0) and (ent[1][2] == 0):\n",
        "                        #print(doc_entity_lst_copy)\n",
        "                        doc_entity_lst_copy.insert((ient+1), [doc_entity_lst_copy[ient][1]])\n",
        "                        doc_entity_lst_copy[ient] = [doc_entity_lst_copy[ient][0]]\n",
        "                        #print(doc_entity_lst_copy)\n",
        "                        \n",
        "                doc_entity_lst = copy.deepcopy(doc_entity_lst_copy)\n",
        "                #print(doc_entity_lst)\n",
        "\n",
        "\n",
        "            # Doc: 23352753  bị giống bên trên\n",
        "            # hai token cạnh nhau (884, 885), cùng entity_id 0, cùng entity_name location nhưng không phải là 1 entity <-- lỗi data\n",
        "            # mà là hai entity, vì entity này link tới entity kia.\n",
        "\n",
        "            if docif['doc_id'] == '23352753':\n",
        "                doc_entity_lst_copy_2 = copy.deepcopy(doc_entity_lst)\n",
        "                for ient, ent in enumerate(doc_entity_lst):\n",
        "                    if (len(ent) == 2) and (ent[0][2] == 0) and (ent[1][2] == 0):\n",
        "                        #print(doc_entity_lst_copy_2)\n",
        "                        doc_entity_lst_copy_2.insert((ient+1), [doc_entity_lst_copy_2[ient][1]])\n",
        "                        doc_entity_lst_copy_2[ient] = [doc_entity_lst_copy_2[ient][0]]\n",
        "                        #print(doc_entity_lst_copy_2)\n",
        "                        \n",
        "                doc_entity_lst = copy.deepcopy(doc_entity_lst_copy_2)\n",
        "                #print(doc_entity_lst)\n",
        "            \n",
        "            '''\n",
        "\n",
        "\n",
        "            \n",
        "            return doc_entity_lst\n",
        "\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIA5LQry20nQ"
      },
      "source": [
        "# thường thì những entity cạnh nhau và trùng id khiến ta dễ lầm thành 1 entity\n",
        "# thì những entity này thường có entity_id = 0\n",
        "# nên ta sẽ tìm các cặp entity_id = 0 này và xem cặp nào là lỗi cặp nào không lỗi\n",
        "# sau đó đọc xem trong doc này đây là lỗi hay không phải lỗi (có cặp không phải lỗi)\n",
        "# nếu là cặp lỗi thì sẽ phải thêm trường hợp ở cell code bên trên\n",
        "# để về sau dùng hàm tìm entity bên trên sẽ không bị lỗi\n",
        "\n",
        "# MỘT LƯU Ý LÀ: \n",
        "# VÍ DỤ: \n",
        "# 1-209\t935-944\tFrankfurt\t*\tLOCATION\t\n",
        "# 1-210\t945-949\t(Đức\t*\tLOCATION\t\n",
        "\n",
        "# 1-161\t739-743\tBali\t*\tLOCATION\t_\t_\t\n",
        "# 1-162\t744-754\t(Indonesia\t*\tLOCATION\tPART – WHOLE\t1-161\t\n",
        "               \n",
        "# gán không chính xác, bên trên không có relation nhưng bên dưới lại có\n",
        "# nên nếu có relation như bên dưới thì tách ra làm 2 entity\n",
        "# còn không có relation như bên trên thì để nó là một entity\n",
        "# vì nếu không có relation mà vẫn tách ra làm 2 entity thì label giữa chúng sẽ là others, không chính xác\n",
        "# nếu để là cùng 1 entity thì sẽ hợp lý hơn. miễn là để cùng entity không ảnh hưởng gì\n",
        "# và trong data có rất nhiều label other giữa các location (như Mỹ với Anh có thể là others)\n",
        "# nên nếu ta chia ra thì sau trong test set cũng bị để làm others, tức là không hợp lý\n",
        "# thà để thành 1, thì train data cũng thế mà test data cũng thế\n",
        "# hoặc ta có thể tách nhưng phải thêm nhãn part-whole vào\n",
        "\n",
        "# cũng có thể toàn bộ các entity_id = 0 cạnh nhau đều là các entity khác nha, nhưng nếu không phải thì sẽ không hoàn hảo\n",
        "# nên có thể xét các trường hợp riêng thay vì tự động tách các entity_0 cạnh nhau thành các entity khác nhau\n",
        "\n",
        "def find_all_fault_entity_id_0(raw_data):\n",
        "\n",
        "    for docif in raw_data:\n",
        "        ent_lst = find_all_entity_in_doc(raw_data, docif['doc_id'])\n",
        "        \n",
        "        for i in range(len(ent_lst)):\n",
        "\n",
        "            if (len(ent_lst[i]) > 1) and (ent_lst[i][0][2] == 0):\n",
        "                print('\\n\\n------', docif['doc_id'], ' -ith: ', i)\n",
        "                for j in range(len(ent_lst[i])):\n",
        "                    first_tk_eleid = ent_lst[i][j][0]\n",
        "                    for key in docif:\n",
        "                        if key not in ['doc_id', 'text']:\n",
        "                            print(docif[key][first_tk_eleid], end='\\t')\n",
        "                    \n",
        "                    print('\\n')\n",
        "        \n",
        "\n",
        "\n",
        "                \n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POdyqoDS5dA1"
      },
      "source": [
        "# tên hàm hơi gây nhầm\n",
        "# các entity được in ra có thể là lỗi hoặc là không lỗi, đa phần là lỗi\n",
        "# và các enity lỗi bên dưới là quyết định không sửa\n",
        "#find_all_fault_entity_id_0(raw_tdata_new_v4)\n",
        "find_all_fault_entity_id_0(raw_tdata)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfXerQf-fQSv"
      },
      "source": [
        "# Create train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qce_9RiBsIZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "4d7166b3-dd1e-4779-bc72-b7260722da8d"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "                    label: \n",
        "                    Dấu x là ký hiệu relation label (docif[\"relation\"]) xuất hiện ở chỗ của entity nào\n",
        "\n",
        "                              Entity:                 entity_1    -    entity_2\n",
        "                    Thứ tự trong câu:                 trước            sau\n",
        "                      \n",
        "                      Relation label:     LOCATED                         x     (per/org - loc)\n",
        "                                       IS_LOCATED         x                     (loc     - per/org)\n",
        "                                       PART–WHOLE\t                      x     (part    - whole)\n",
        "                                       WHOLE-PART         x                     (whole   - part)\n",
        "                                  PERSONAL–SOCIAL                               (Undirected)\n",
        "                                      AFFILIATION\t                      x     \n",
        "                                   AFFILIATION_TO         x\n",
        "                                           OTHERS                               (là nhãn giữ 2 entity cùng 1 câu mà không có relation trong data)\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n                    label: \\n                    Dấu x là ký hiệu relation label (docif[\"relation\"]) xuất hiện ở chỗ của entity nào\\n\\n                              Entity:                 entity_1    -    entity_2\\n                    Thứ tự trong câu:                 trước            sau\\n                      \\n                      Relation label:     LOCATED                         x     (per/org - loc)\\n                                       IS_LOCATED         x                     (loc     - per/org)\\n                                       PART–WHOLE\\t                      x     (part    - whole)\\n                                       WHOLE-PART         x                     (whole   - part)\\n                                  PERSONAL–SOCIAL                               (Undirected)\\n                                      AFFILIATION\\t                      x     \\n                                   AFFILIATION_TO         x\\n                                           OTHERS                               (là nhãn giữ 2 entity cùng 1 câu mà không có relation trong data)\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqEMUOpfqHZp"
      },
      "source": [
        "original_labels = ['LOCATED', 'PART – WHOLE', 'PERSONAL - SOCIAL', 'AFFILIATION']\n",
        "\n",
        "# entity chứa relation nằm ở phía sau thì là label gốc\n",
        "labels = {'LOCATED': 'LOCATED', 'IS_LOCATED': 'IS_LOCATED', \n",
        "         'PART_WHOLE': 'PART_WHOLE', 'WHOLE_PART': 'WHOLE_PART', \n",
        "         'PERSONAL_SOCIAL': 'PERSONAL_SOCIAL', \n",
        "         'AFFILIATION': 'AFFILIATION', 'AFFILIATION_TO': 'AFFILIATION_TO', \n",
        "         'OTHERS': 'OTHERS'\n",
        "         }"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8crlbk4MJTlx"
      },
      "source": [
        "### Func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBWuvOzUJUCP"
      },
      "source": [
        "def split_by_colon_punc(doc_sent_tokenize):\n",
        "\n",
        "    new_doc_sent_tokenize = []\n",
        "    for isent, sent in enumerate(doc_sent_tokenize):\n",
        "        if ':' not in sent:\n",
        "            new_doc_sent_tokenize.append(sent)\n",
        "        \n",
        "        else:\n",
        "            new_sents = sent.split(\":\")\n",
        "                \n",
        "            for inew_sent, new_sent in enumerate(new_sents):\n",
        "                if inew_sent != (len(new_sents) - 1):\n",
        "                    new_doc_sent_tokenize.append(str(new_sent.lstrip() + ':'))\n",
        "                else:\n",
        "                    new_doc_sent_tokenize.append(str(new_sent.strip()))\n",
        "\n",
        "    return new_doc_sent_tokenize\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt1PwbNGJG_W"
      },
      "source": [
        "from underthesea import sent_tokenize, word_tokenize\n",
        "\n",
        "def my_sentences_tokenize(doc_id, text):\n",
        "    ######## split sentence from docif[\"text\"] using Underthesea library\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # check if sum length of all sentence < len(text)\n",
        "    len_sentences = [len(s) for s in sentences]\n",
        "    assert (sum(len_sentences) <= len(text)), str(\"\\nSentence tokenize has problem. \\nDoc: \" + docif[\"doc_id\"])\n",
        "\n",
        "\n",
        "\n",
        "    ######\n",
        "    ### trong doc này việc chia sentence bằng Underthesea bị lỗi dẫn tới việc một entity nằm ở 2 câu.\n",
        "    new_sentences = []\n",
        "    \n",
        "    if doc_id == \"24419297\":  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (sent[-3:] != \"St.\") and (sent[:4] != \"Jude\"):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (sent[-3:] == \"St.\") and (sentences[isent + 1][:4] == \"Jude\"):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "    elif doc_id == \"24530851\":  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (sent[-3:] != \"St.\") and (sent[:4] != \"Luke\") and (sent[:6] != \"Damien\"):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (sent[-3:] == \"St.\") and (sentences[isent + 1][:4] == \"Luke\"):\n",
        "                new_sentences.append(str(sent + sentences[isent + 1]))\n",
        "            \n",
        "            elif (sent[-3:] == \"St.\") and (sentences[isent + 1][:6] == \"Damien\"):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    \n",
        "    elif doc_id == \"24854400\":  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (sent[-3:] != \"JR.\") and (sent[:5] != \"Smith\"):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (sent[-3:] == \"JR.\") and (sentences[isent + 1][:5] == \"Smith\"):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "\n",
        "\n",
        "    elif doc_id == \"24413768\":  # test important\n",
        "        print(sentences)\n",
        "        new_sentences_2 = []\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            \n",
        "\n",
        "            if (\"Danh sách 18 tập thể nhận giải thưởng “Vô lăng vàng”: Công ty Trách nhiệm hữu hạn Văn Minh\" not in sent) and\\\n",
        "            (\"Danh sách 46 cá nhân nhận giải thưởng “Vô lăng vàng”: Phạm Quốc Thịnh (Công ty Vinasun Ánh Dương Việt Nam ),\" not in sent):\n",
        "                new_sentences_2.append(sent)\n",
        "            \n",
        "            elif (\"Danh sách 18 tập thể nhận giải thưởng “Vô lăng vàng”: Công ty Trách nhiệm hữu hạn Văn Minh\" in sent):\n",
        "                new_sents = sent.split(\";\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    new_sentences_2.append(str(new_sent.strip()))\n",
        "\n",
        "            elif (\"Danh sách 46 cá nhân nhận giải thưởng “Vô lăng vàng”: Phạm Quốc Thịnh (Công ty Vinasun Ánh Dương Việt Nam ),\" in sent):\n",
        "                new_sents = sent.split(\",\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    new_sentences_2.append(str(new_sent.strip()))\n",
        "        \n",
        "        print(new_sentences_2)\n",
        "        sentences = copy.deepcopy(new_sentences_2)\n",
        "        print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id in [\"23352299\", \"23352417\"]:  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"How Do I Look?\" not in sent) and ('Asia' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"How Do I Look?\" in sent) and ('Asia' in sentences[isent + 1]):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "        \n",
        "    ### trong các doc bên dưới việc chia sentence bằng Underthesea bị lỗi dẫn tới việc một relation link tới một entity thuộc câu khác.\n",
        "    ### thường do sau tên người viết tắt có dấu chấm\n",
        "    \n",
        "    elif doc_id == \"23352323\":  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('Lê Quý D' not in sent) and ('Hòa Thái' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif ('Lê Quý D' in sent) and ('Hòa Thái' in sentences[isent + 1]):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "\n",
        "    \n",
        "    elif doc_id == \"23352491\":  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('Reuters' not in sent) and ('Tuyết Mai' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif ('Reuters' in sent) and ('Tuyết Mai' in sentences[isent + 1]):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == \"23352572\":  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"William\" not in sent) and (\"B. Rosen\" not in sent) and ('E. Cashman' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"William\" in sent) and (\"B. Rosen\" in sentences[isent + 1]) and ('E. Cashman' in sentences[isent + 2]):\n",
        "                new_sentences.append(str('Chỉ huy lực lượng Mỹ ở đây là Thiếu tướng William. B. Rosen và Thiếu tướng Thủy quân lục chiến Robert. E. Cashman đã đề xuất kế hoạch rút quân khỏi Khe Sanh .'))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "        new_sentences_2 = []\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            assert ('…' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            if (\"...\" not in sent):\n",
        "                new_sentences_2.append(sent)\n",
        "            \n",
        "            elif (\"...\" in sent):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_2.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences_2.append(str(new_sent.lstrip()))\n",
        "        \n",
        "        #print(new_sentences_2)\n",
        "        sentences = copy.deepcopy(new_sentences_2)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    ######\n",
        "    # có một số doc có ...\n",
        "\n",
        "    elif doc_id == '23352499':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"giải phóng... Hôm nay\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"giải phóng... Hôm nay\" in sent):\n",
        "                tmppp_1 = sent.find(\". Hôm nay\")\n",
        "                tmppp_2 = sent.find(\". Một\")\n",
        "                new_sentences.append(str(sent[:(tmppp_1+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp_1+2):(tmppp_2+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp_2+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "        new_sentences_1 = []\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            \n",
        "\n",
        "            if (\"…\" not in sent) or ('UAV…nhưng' in sent):\n",
        "                new_sentences_1.append(sent)\n",
        "            \n",
        "            elif (\"…\" in sent) and ('UAV…nhưng' not in sent):\n",
        "                new_sents = sent.split(\"…\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_1.append(str(new_sent.lstrip() + '…'))\n",
        "                    else:\n",
        "                        new_sentences_1.append(str(new_sent.lstrip()))\n",
        "        \n",
        "        #print(new_sentences_1)\n",
        "        sentences = copy.deepcopy(new_sentences_1)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == '23352989':  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"Shinji Kagawa … Nhờ vào\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"Shinji Kagawa … Nhờ vào\" in sent):\n",
        "                tmppp = sent.find(\"… Nhờ vào\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    ### những doc mà ta chỉ tách theo … test\n",
        "    # các doc các câu chỉ có … hoặc một số doc có hai loại nhưng ta chỉ chia câu có …\n",
        "    elif doc_id in ['23352963', '23352995', '24386181', '24419138', '24492634', '24599825', '24600969', \\\n",
        "                    '24630076', '24665411', '24677601', '24689923', '24691721', '24710822', \\\n",
        "                    '24715735', '24743776', '24755376', '24794285', '24799096', '24805251', '24827011', \\\n",
        "                    '24829505', '24854363', '24856833', '24882485', '24894691', '24580249']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "\n",
        "            if (doc_id != '24691721') and (doc_id != '24710822') and (doc_id != '24743776') and (doc_id != '24755376') and \\\n",
        "            (doc_id != '24827011') and (doc_id != '24854363') and (doc_id != '24894691') and (doc_id != '24580249'):\n",
        "                assert ('...' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "\n",
        "            if (\"…\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"…\" in sent):\n",
        "                new_sents = sent.split(\"…\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '…'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "    ### những doc mà ta chỉ tách theo ... test\n",
        "    # các doc các câu chỉ có ... hoặc một số doc có hai loại nhưng ta chỉ chia câu có ...\n",
        "    elif doc_id in ['23352940', '23352949', '23352967', '24410798', '24434007', '24454904', '24477819', \\\n",
        "                    '24569400', '24578615', '24585044', '24587840', '24597278', '24615775', '24624726', \\\n",
        "                    '24645143', '24657217', '24660008', '24681368', '24701718', '24738794', '24753325', \\\n",
        "                    '24797533', '24844657', '24867954', '24881113', '24457046', '24457804']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            \n",
        "            if (doc_id != '24569400') and (doc_id != '24597278') and (doc_id != '24615775') and (doc_id != '24701718') and \\\n",
        "            (doc_id != '24738794'):   # doc nay co ca 2 nhung ta chi chia theo ...\n",
        "                assert ('…' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "            \n",
        "\n",
        "            if (\"...\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"...\" in sent):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    \n",
        "\n",
        "    ### những doc mà ta tách theo cả 2  test\n",
        "    elif doc_id in ['23352911', '24651289', '24832737', '24663382']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            \n",
        "\n",
        "            if ((\"...\" not in sent) and ('…' not in sent)):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif ((\"...\" in sent) and ('…' not in sent)):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ((\"...\" not in sent) and ('…' in sent)):\n",
        "                new_sents = sent.split(\"…\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '…'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "\n",
        "            elif ((\"...\" in sent) and ('…' in sent)):\n",
        "                new_sents = sent.replace('...', '…')\n",
        "                new_sents = new_sents.split('…')\n",
        "\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ###### test\n",
        "    # trong doc này việc chia câu bị lỗi, 2 câu bị gộp thành 1 câu\n",
        "    # những câu này thường có kí tự: .”\n",
        "    \n",
        "    elif doc_id in ['24546530', '24570211', '24578615', '24686546', '24752878']:   # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('.”' in sent) and ('.”' != sent[-2:]):\n",
        "                new_sents = sent.split('.”')\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '.”'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ('.”' not in sent) or ('.”' == sent[-2:]):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == '24891267':  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"Viên Thiệu ... Từ Hoảng luôn hoàn\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"Viên Thiệu ... Từ Hoảng luôn hoàn\" in sent):\n",
        "                tmppp = sent.find(\". Từ Hoảng luôn hoàn\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    elif doc_id == '24778288':  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"của địch.... Ngoài tổ hợp mới\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"của địch.... Ngoài tổ hợp mới\" in sent):\n",
        "                tmppp = sent.find(\". Ngoài tổ hợp mới\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    \n",
        "    elif doc_id == '24663125':  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"làm việc để giải quyết.. Trường Trung học cơ\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"làm việc để giải quyết.. Trường Trung học cơ\" in sent):\n",
        "                tmppp = sent.find(\". Trường Trung học cơ\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "    elif doc_id == '24825034':  # test\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"chè an toàn.. Năm 2017 Công\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"chè an toàn.. Năm 2017 Công\" in sent):\n",
        "                tmppp = sent.find(\". Năm 2017 Công\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    # tach dau .  test\n",
        "    \n",
        "    if doc_id in ['23352948', '23353014', '24503940', '24506379', '24557491', '24578615', '24684290', '24688913', \\\n",
        "                  '24711089', '24823386', '24881311']:\n",
        "        \n",
        "        # ith_sent là thứ tự các câu cần tách dấu . trong doc (thứ tự câu bắt đầu từ 0)\n",
        "        # ith_dot là số thứ tự của các dấu . trong câu mà tại các dấu . này ta sẽ tách câu thành các phần khác nhau\n",
        "        # ith_dot cần đếm thứ tự cẩn thận bằng tay (thứ tự dấu . bắt đầu từ 1)\n",
        "        # ith_dot là một list của list. list thứ n của ith_dot là danh sách vị trí những dấu chấm mà ta sẽ dùng để tách câu thứ n tương ứn\n",
        "        # trong ith_sent   <- cần lưu ý để đúng thứ tự, và ith_sent cần xếp theo thứ tự tăng dần\n",
        "        # cần làm vậy vì có thể 1 doc có nhiều cần cần tách, rồi trong các câu này lại có câu có nhiều dấu . cần tách\n",
        "\n",
        "        # ý tưởng: ta duyệt các câu trong doc, dựa vào ith_sent_lst để biết câu nào cần tách dấu .\n",
        "        # câu nào không cần tách thì ta thêm luôn vào danh sách các câu trong doc\n",
        "        # câu nào cần tách thì: ta dựa tiếp vào ith_dot_lst để biết ta sẽ tách tại những dấu . nào trong câu\n",
        "        # ví dụ câu cần tách tại 2 dấu .: thứ 2 và thứ 3 trong câu (-> từ 1 câu tách thành 3 câu)\n",
        "        # ta sẽ tìm vị trí index của các dấu . này trong câu cần tách\n",
        "        # rồi dựa vào index đó để cắt câu thành các phần cần chia\n",
        "\n",
        "        doc_nfix_lst = [{'doc_id': '23352948', 'ith_sent_lst': [13], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '23353014', 'ith_sent_lst': [1, 8], 'ith_dot_lst': [[1], [1]]},\n",
        "                        {'doc_id': '24503940', 'ith_sent_lst': [29], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '24506379', 'ith_sent_lst': [14, 24], 'ith_dot_lst': [[1], [1]]},\n",
        "                        {'doc_id': '24557491', 'ith_sent_lst': [0], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '24578615', 'ith_sent_lst': [13], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '24684290', 'ith_sent_lst': [72], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '24688913', 'ith_sent_lst': [34], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '24711089', 'ith_sent_lst': [35], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '24823386', 'ith_sent_lst': [15, 19, 22], 'ith_dot_lst': [[1], [1], [1]]},\n",
        "                        \n",
        "                        {'doc_id': '24881311', 'ith_sent_lst': [42], 'ith_dot_lst': [[1]]}\n",
        "                        ]\n",
        "        \n",
        "        ##### to find pos of ith dot\n",
        "        def find_ith_dot_pos(haystack, needle, n):\n",
        "            start = haystack.find(needle)\n",
        "            while start >= 0 and n > 1:\n",
        "                start = haystack.find(needle, start+len(needle))\n",
        "                n -= 1\n",
        "            return start\n",
        "        #####\n",
        "\n",
        "\n",
        "        crr_doc_nfix = None\n",
        "        for doc_nfix in doc_nfix_lst:\n",
        "            if doc_nfix['doc_id'] == doc_id:\n",
        "                crr_doc_nfix = doc_nfix\n",
        "\n",
        "        new_sentences_6 = []\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            '''\n",
        "            if doc_id not in ['23352948', '24506379', '24578615']:\n",
        "                assert (('...' not in sent) and (\"…\" not in sent)), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "            '''\n",
        "\n",
        "\n",
        "            assert (len(crr_doc_nfix['ith_sent_lst']) == len(crr_doc_nfix['ith_dot_lst'])), \\\n",
        "            str('\\nLEN ith_sent_lst not equal to LEN ith_dot_lst. \\nDoc: ' + doc_id)\n",
        "\n",
        "            if isent not in crr_doc_nfix['ith_sent_lst']:\n",
        "                new_sentences_6.append(sent)\n",
        "            \n",
        "            else:\n",
        "                \n",
        "                assert (('...' not in sent) and (\"…\" not in sent)), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "                crr_ith_sent = crr_doc_nfix['ith_sent_lst'].index(isent)\n",
        "\n",
        "                ith_dot_pos_lst = [-1, (len(sent)-1)]\n",
        "\n",
        "                for ith_dot in crr_doc_nfix['ith_dot_lst'][crr_ith_sent]:\n",
        "                    dot_pos = find_ith_dot_pos(sent, '.', ith_dot)\n",
        "\n",
        "                    assert (dot_pos >= 0), str('\\nNot found ith dot. \\nDoc: ' + doc_id + '\\nSent: ' + sent)\n",
        "\n",
        "                    ith_dot_pos_lst.insert(-1, dot_pos)\n",
        "                \n",
        "                for iith in range(len(ith_dot_pos_lst) - 1):\n",
        "                    correct_sent = sent[(ith_dot_pos_lst[iith] + 1):(ith_dot_pos_lst[iith+1] + 1)]\n",
        "                    new_sentences_6.append(str(correct_sent).strip())\n",
        "\n",
        "\n",
        "        #print(new_sentences_6)\n",
        "        sentences = copy.deepcopy(new_sentences_6)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    return sentences\n",
        "\n",
        "    ###### \n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_riTWvT_OHxy"
      },
      "source": [
        "def get_sentence_entities(docif, doc_entity_lst, sent, sspos, espos):\n",
        "    \n",
        "    sen_entity_lst = []\n",
        "\n",
        "    for i in range(len(doc_entity_lst)):   # each entity\n",
        "        first_token_eid = doc_entity_lst[i][0][0]\n",
        "        last_token_eid = doc_entity_lst[i][-1][0]\n",
        "            \n",
        "        # nếu điểm đầu của câu < điểm đầu của token đầu và điểm cuối của token cuối < điểm cuối của câu\n",
        "        # thì entity này là entity của câu này\n",
        "        if (sspos <= docif[\"pos\"][first_token_eid][0]) and (docif[\"pos\"][last_token_eid][1] <= espos):\n",
        "            sen_entity_lst.append(doc_entity_lst[i])\n",
        "\n",
        "        # điểm đầu của entity < điểm đầu của câu nhưng điểm cuối của entity lại lớn hơn điểm đầu của câu\n",
        "        # có lỗi: 1 entity nhưng thuộc 2 câu, tức là 1 phần của entity thuộc câu trước, phần còn lại lại thuộc câu đang xét.\n",
        "        # điều này có thể do chia câu bằng Underthesea có vấn đề hoặc dataset có vấn đề\n",
        "        elif (docif[\"pos\"][first_token_eid][0] < sspos) and (sspos < docif[\"pos\"][last_token_eid][1]):\n",
        "                \n",
        "        # trong dataset có lỗi này. \n",
        "        # tuy nhiên không có nhiều, nên để hiểu thêm về dataset, tôi chỉ sửa chính xác các lỗi này\n",
        "        # và đã sửa bên trên\n",
        "\n",
        "            assert False, str(\"\\n--- An entity belongs to two sentences instead of just one. (Error Code 1) \\nIn doc: \" + docif[\"doc_id\"] + \"\\nSentence: \" + repr(sent))\n",
        "                \n",
        "        # điểm đầu của entity < điểm cuối của câu nhưng điểm cuối của entity lại lớn hơn điểm cuối của câu\n",
        "        # có lỗi: 1 entity nhưng thuộc 2 câu, tức là 1 phần của entity thuộc câu đang xét, phần còn lại lại thuộc câu sau.\n",
        "        # điều này có thể do chia câu bằng Underthesea có vấn đề hoặc dataset có vấn đề\n",
        "        elif (docif[\"pos\"][first_token_eid][0] < espos) and (espos < docif[\"pos\"][last_token_eid][1]):\n",
        "                \n",
        "            assert False, str(\"\\nAn entity belongs to two sentences instead of just one. (Error Code 2) \\nIn doc: \" + docif[\"doc_id\"] + \"\\nSentence: \" + repr(sent))\n",
        "\n",
        "\n",
        "    # có thể xảy ra trường hợp chia câu bị lỗi, một câu to bị chia thành hai câu nhỏ\n",
        "    # mỗi câu nhỏ lại chứa các entity\n",
        "    # nhưng entity câu nhỏ này link tới câu nhỏ kia -> lỗi\n",
        "    # hoặc trong data có lỗi, entity câu này link tới câu khác.\n",
        "    # nên cần xem xem các relation trong câu có link tới các entity tìm thấy trong câu không\n",
        "        \n",
        "    # hay relation giữa 2 entity là đúng, nhưng stoken_id trong relation không trỏ vào token đầu tiên của entity id kia\n",
        "    # mà lại trỏ vào token giữa hoặc cuối entity kia (đã fix lỗi này bên trên)\n",
        "\n",
        "    # vì relation chỉ link tới token_ids của token đầu tiên trong entity khác\n",
        "    # nên ta sẽ thu thập danh sách token_ids của các token đầu tiên các entity trong câu\n",
        "    first_tkids_lst = []\n",
        "    for i in range(len(sen_entity_lst)):\n",
        "        first_tkids_lst.append(sen_entity_lst[i][0][1])\n",
        "\n",
        "\n",
        "    for i in range(len(sen_entity_lst)):\n",
        "        first_tkeid = sen_entity_lst[i][0][0]\n",
        "\n",
        "        if docif['relation'][first_tkeid] != None:   # từng relation trong câu\n",
        "            for j in range(len(docif['relation'][first_tkeid])):\n",
        "                if docif['relation'][first_tkeid][j][1] not in first_tkids_lst:\n",
        "                        \n",
        "                    '''\n",
        "                    print(str('\\nSentence tokenize has problem. \\nDoc: ' + docif['doc_id'] + '\\nRelation stoken ID: ' + str(docif['relation'][first_tkeid][j][1])  + ' \\nprvSent: ' + sentences[isent-1] + '\\nSent: ' + sent))\n",
        "\n",
        "                    print(docif['relation'][first_tkeid])\n",
        "                    '''\n",
        "\n",
        "                    assert False, \\\n",
        "                    str('\\nSentence tokenize has problem. \\nDoc: ' + docif['doc_id'] + '\\nRelation stoken ID: ' + str(docif['relation'][first_tkeid][j][1])  + ' \\nprvSent: ' + sentences[isent-1] + '\\nSent: ' + sent)\n",
        "\n",
        "\n",
        "\n",
        "    return sen_entity_lst\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZremDvk8tP7"
      },
      "source": [
        "def relation_name_to_sentence_label(relation_name, relation_entity):\n",
        "\n",
        "    sentence_label = None\n",
        "    # nếu entity chứa relation là entity 1 thì label ngược lại\n",
        "    if relation_entity == 1:\n",
        "        if relation_name == 'LOCATED':\n",
        "            sentence_label = 'IS_LOCATED'\n",
        "\n",
        "        elif relation_name == 'PART – WHOLE':\n",
        "            sentence_label = 'WHOLE_PART'\n",
        "\n",
        "        elif relation_name == 'PERSONAL - SOCIAL':\n",
        "            sentence_label = 'PERSONAL_SOCIAL'\n",
        "\n",
        "        elif relation_name == 'AFFILIATION':\n",
        "            sentence_label = 'AFFILIATION_TO'\n",
        "\n",
        "        else:\n",
        "            assert False, str('UNKNOW RELATION NAME: ' + relation_name)\n",
        "    \n",
        "    # nếu entity chứa relation là entity 2 thì label giữ nguyên\n",
        "    elif relation_entity == 2:\n",
        "        if relation_name == 'LOCATED':\n",
        "            sentence_label = 'LOCATED'\n",
        "\n",
        "        elif relation_name == 'PART – WHOLE':\n",
        "            sentence_label = 'PART_WHOLE'\n",
        "\n",
        "        elif relation_name == 'PERSONAL - SOCIAL':\n",
        "            sentence_label = 'PERSONAL_SOCIAL'\n",
        "\n",
        "        elif relation_name == 'AFFILIATION':\n",
        "            sentence_label = 'AFFILIATION'\n",
        "\n",
        "        else:\n",
        "            assert False, str('UNKNOW RELATION NAME: ' + relation_name)\n",
        "\n",
        "    else:\n",
        "        assert False, (\"Unexpect relation_entity. Expect 1 or 2 but got: \" + relation_entity + \".\")\n",
        "\n",
        "    \n",
        "    return sentence_label"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyIgy3tfLDTs"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKFuhu7TYFB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c1239a-5d27-446a-d2c8-922d2fc52c02"
      },
      "source": [
        "# Tạo train data (câu chứa cặp entity và label)\n",
        "\n",
        "tdata = []\n",
        "\n",
        "#raw_data = raw_tdata_new_v6\n",
        "raw_data = raw_tdata_new\n",
        "\n",
        "sent_id = 0\n",
        "\n",
        "for docif in raw_data:\n",
        "\n",
        "    # find all entity in current doc    \n",
        "    doc_entity_lst = find_all_entity_in_doc(raw_data, docif['doc_id'])\n",
        "\n",
        "    # if whole doc has 0 or 1 entity -->  no relation in this doc --> skip\n",
        "    # there is many doc like this (like doc has only 3 columns,...)\n",
        "    if len(doc_entity_lst) <= 1:\n",
        "        continue\n",
        "\n",
        "\n",
        "    text = docif[\"text\"]\n",
        "    sentences = my_sentences_tokenize(docif['doc_id'], text)\n",
        "\n",
        "    \n",
        "    ######## extract training sentence\n",
        " \n",
        "    pre_espos = 0   # end of pre sentence\n",
        "\n",
        "    for isent, sent in enumerate(sentences):\n",
        "\n",
        "        sentif = {}\n",
        "        '''\n",
        "        sentif[\"doc_id\"] = docif[\"doc_id\"]\n",
        "        sentif[\"sentence\"] = sent\n",
        "        '''\n",
        "        ###### sentence position\n",
        "        # tìm vị trí của câu để dựa vào đó biết entity (các tokens) thuộc câu nào\n",
        "\n",
        "        # text may have two indentical sentences\n",
        "        # so we have to find start position of current sentence in the rest of the text that not contain previous sentences.\n",
        "\n",
        "        assert (text[pre_espos:].find(sent) >= 0), str(\"Position has problem. \\nDoc: \" + docif[\"doc_id\"] + \"\\nCurrent sentence: \" + sent)\n",
        "\n",
        "        sspos = text[pre_espos:].find(sent) + pre_espos\n",
        "        espos = sspos + len(sent)\n",
        "\n",
        "        # update pre_espos\n",
        "        pre_espos = espos\n",
        "        \n",
        "        assert (sent == text[sspos:espos]), str(\"Position founded is not matched in text. \\nDoc: \" + docif[\"doc_id\"] + \"\\nCurrent sentence: \" + sent)\n",
        "\n",
        "        '''\n",
        "        sentif[\"spos\"] = [sspos, espos]\n",
        "        '''\n",
        "\n",
        "        ###### get all entity in current sentence\n",
        "        \n",
        "        sen_entity_lst = get_sentence_entities(docif, doc_entity_lst, sent, sspos, espos)\n",
        "\n",
        "        # if current sentence has 0 or 1 entity -> no relation availabel to classify -> skip\n",
        "        if len(sen_entity_lst) <= 1: \n",
        "            continue\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "        ###### create n*(n-1)/2 sentence\n",
        "        \n",
        "        for ient, ent_1 in enumerate(sen_entity_lst):\n",
        "            for jent, ent_2 in enumerate(sen_entity_lst[(ient+1):]):\n",
        "                \n",
        "                first_tkeid_ent1 = ent_1[0][0]   # dòng chứa token đầu trong entity\n",
        "                first_tkeid_ent2 = ent_2[0][0]\n",
        "                \n",
        "                last_tkeid_ent1 = ent_1[-1][0]   # dòng chứa token cuối trong entity\n",
        "                last_tkeid_ent2 = ent_2[-1][0]\n",
        "\n",
        "                if (docif['entity'][first_tkeid_ent1][1] != \"MISCELLANEOUS\") and (docif['entity'][first_tkeid_ent2][1] != \"MISCELLANEOUS\"):\n",
        "                    \n",
        "                    # pos của entity trong doc: start của token đầu và end của token cuối trong entity\n",
        "                    ent1_pos_doc = [docif['pos'][first_tkeid_ent1][0], docif['pos'][last_tkeid_ent1][1]]\n",
        "                    ent2_pos_doc = [docif['pos'][first_tkeid_ent2][0], docif['pos'][last_tkeid_ent2][1]]\n",
        "\n",
        "                    # pos của entity trong câu chứa entity\n",
        "                    ent1_pos_sent = [(ent1_pos_doc[0] - sspos), (ent1_pos_doc[1] - sspos)]\n",
        "                    ent2_pos_sent = [(ent2_pos_doc[0] - sspos), (ent2_pos_doc[1] - sspos)]\n",
        "\n",
        "                    \n",
        "                    # kiểm tra xem pos trong doc và sent có khớp, trả về cùng entity không\n",
        "                    assert (sent[ent1_pos_sent[0]:ent1_pos_sent[1]] == text[ent1_pos_doc[0]:ent1_pos_doc[1]]), \\\n",
        "                    str('Entity 1: pos_doc and pos_sent not matched. \\nDoc: ' + docif['doc_id'] + '\\nSent: ' + sent)\n",
        "\n",
        "                    assert (sent[ent2_pos_sent[0]:ent2_pos_sent[1]] == text[ent2_pos_doc[0]:ent2_pos_doc[1]]), \\\n",
        "                    str('Entity 1: pos_doc and pos_sent not matched. \\nDoc: ' + docif['doc_id'] + '\\nSent: ' + sent)\n",
        "\n",
        "\n",
        "                    ent1_text = sent[ent1_pos_sent[0]:ent1_pos_sent[1]]\n",
        "                    ent2_text = sent[ent2_pos_sent[0]:ent2_pos_sent[1]]\n",
        "\n",
        "                    ###############\n",
        "                    # kiểm tra xem mọi token trong entity đã có mặt trong entity lấy từ pos hay chưa\n",
        "                    for itk, tk in enumerate(ent_1):\n",
        "                        eid_tk = tk[0]\n",
        "\n",
        "                        if itk < (len(ent_1) - 1):\n",
        "                            eid_n_tk = ent_1[itk+1][0]\n",
        "\n",
        "                            assert (docif['pos'][eid_tk][1] < docif['pos'][eid_n_tk][0]), \\\n",
        "                            str(\"Position not increase. Doc: \" + docif['doc_id'] + \"\\nSent: \" + sent + \"\\ncrr-pos: \" + str(docif['pos'][eid_tk][1]) + \"\\nnpos: \" + str(docif['pos'][eid_ntk][0]))\n",
        "\n",
        "\n",
        "                        assert (ent1_pos_doc[0] <= docif['pos'][eid_tk][0]) and (docif['pos'][eid_tk][1] <= ent1_pos_doc[1]), \\\n",
        "                        str('Entity\\'s token pos not inside entity pos')\n",
        "                    \n",
        "                    ###############\n",
        "\n",
        "                    ##########\n",
        "\n",
        "                    assert (ent_1[0][1] == docif['token_ids'][first_tkeid_ent1]), str('NOT MATCHED entity first token id')\n",
        "                    assert (ent_2[0][1] == docif['token_ids'][first_tkeid_ent2]), str('NOT MATCHED entity first token id')\n",
        "\n",
        "                    sentence_label = None\n",
        "\n",
        "                    # nếu cả 2 entity không có relation\n",
        "                    if (docif['relation'][first_tkeid_ent1] == None) and (docif['relation'][first_tkeid_ent2] == None):\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                    # nếu cả 2 entity có relation\n",
        "                    elif (docif['relation'][first_tkeid_ent1] != None) and (docif['relation'][first_tkeid_ent2] != None):\n",
        "                        assert False, 'this is test'\n",
        "                        # mặc định là OTHERS, nếu bên dưới tìm thấy relation link tới thì sẽ được thay đổi\n",
        "                        # còn nếu bên dưới tìm không thấy (tức là không có relation) thì sẽ không bị thay đổi và vẫn là OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                        for rel_1 in docif['relation'][first_tkeid_ent1]:\n",
        "                            if rel_1[1] == ent_2[0][1]:   # relation ở entity 1 link tới token đầu entity 2\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_1[0], 1)\n",
        "\n",
        "                                # kiem tra relation direction\n",
        "                                # do relation nam o ent_1 nen direction la: [ent_2, ent_1]\n",
        "                                if rel_1[3] != None:\n",
        "                                    assert (rel_1[3] == [ent_2[0][2], ent_1[0][2]]), str('CODE 1: Not match direction')\n",
        "                                \n",
        "\n",
        "                        for rel_2 in docif['relation'][first_tkeid_ent2]:\n",
        "                            if rel_2[1] == ent_1[0][1]:   # relation ở entity 2 link tới token đầu entity 1\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_2[0], 2)\n",
        "\n",
        "                                # do relation nam o ent_2 nen direction la: [ent_1, ent_2]\n",
        "                                if rel_2[3] != None:\n",
        "                                    assert (rel_2[3] == [ent_1[0][2], ent_2[0][2]]), str('CODE 2: Not match direction')\n",
        "        \n",
        "\n",
        "                    # nếu entity 1 có relation, entity 2 không có\n",
        "                    elif (docif['relation'][first_tkeid_ent1] != None) and (docif['relation'][first_tkeid_ent2] == None):\n",
        "                        assert False, 'this is test'\n",
        "                        # mặc định là OTHERS, nếu bên dưới tìm thấy relation link tới thì sẽ được thay đổi\n",
        "                        # còn nếu bên dưới tìm không thấy (tức là không có relation) thì sẽ không bị thay đổi và vẫn là OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                        for rel_1 in docif['relation'][first_tkeid_ent1]:\n",
        "                            if rel_1[1] == ent_2[0][1]:   # relation ở entity 1 link tới token đầu entity 2\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_1[0], 1)\n",
        "\n",
        "                                # do relation nam o ent_1 nen direction la: [ent_2, ent_1]\n",
        "                                if rel_1[3] != None:\n",
        "                                    assert (rel_1[3] == [ent_2[0][2], ent_1[0][2]]), str('CODE 3: Not match direction')\n",
        "\n",
        "                                \n",
        "\n",
        "                    # nếu entity 2 có relation, entity 1 không có\n",
        "                    elif (docif['relation'][first_tkeid_ent1] == None) and (docif['relation'][first_tkeid_ent2] != None):\n",
        "                        assert False, 'this is test'\n",
        "                        # mặc định là OTHERS, nếu bên dưới tìm thấy relation link tới thì sẽ được thay đổi\n",
        "                        # còn nếu bên dưới tìm không thấy (tức là không có relation) thì sẽ không bị thay đổi và vẫn là OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "                        \n",
        "                        for rel_2 in docif['relation'][first_tkeid_ent2]:\n",
        "                            if rel_2[1] == ent_1[0][1]:   # relation ở entity 2 link tới token đầu entity 1\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_2[0], 2)\n",
        "\n",
        "                                # do relation nam o ent_2 nen direction la: [ent_1, ent_2]\n",
        "                                if rel_2[3] != None:\n",
        "                                    assert (rel_2[3] == [ent_1[0][2], ent_2[0][2]]), str('CODE 4: Not match direction')\n",
        "\n",
        "                        \n",
        "                    ##########\n",
        "\n",
        "                    sentif[\"doc_id\"] = docif[\"doc_id\"]\n",
        "\n",
        "                    sent_id += 1\n",
        "                    sentif['sent_id'] = sent_id\n",
        "\n",
        "                    sentif[\"sentence\"] = sent\n",
        "                    sentif[\"spos\"] = [sspos, espos]\n",
        "\n",
        "                    entity_1 = {'text': copy.deepcopy(ent1_text), 'pos': copy.deepcopy(ent1_pos_sent)}\n",
        "                    entity_2 = {'text': copy.deepcopy(ent2_text), 'pos': copy.deepcopy(ent2_pos_sent)}\n",
        "\n",
        "                    ent_1_first = copy.deepcopy(ent_1[0])\n",
        "                    ent_1_last = copy.deepcopy(ent_1[-1])\n",
        "\n",
        "                    ent_2_first = copy.deepcopy(ent_2[0])\n",
        "                    ent_2_last = copy.deepcopy(ent_2[-1])\n",
        "\n",
        "                    entity_1_info = {'first_token': copy.deepcopy(ent_1_first), 'last_token': copy.deepcopy(ent_1_last),}\n",
        "                    entity_2_info = {'first_token': copy.deepcopy(ent_2_first), 'last_token': copy.deepcopy(ent_2_last),}\n",
        "\n",
        "\n",
        "                    sentif['entity_1'] = copy.deepcopy(entity_1)\n",
        "                    sentif['entity_2'] = copy.deepcopy(entity_2)\n",
        "\n",
        "                    sentif['entity_1_info'] = copy.deepcopy(entity_1_info)\n",
        "                    sentif['entity_2_info'] = copy.deepcopy(entity_2_info)\n",
        "\n",
        "                    sentif['label'] = copy.deepcopy(sentence_label)\n",
        "\n",
        "                    \n",
        "                    \n",
        "                    # may dong tren co the khong co copy.deepcopy nhung dong ben duoi khong co la bi loi\n",
        "                    tdata.append(copy.deepcopy(sentif))\n",
        "\n",
        "print(\"DONE\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"\\ufeffGiải thưởng 'Vô lăng vàng' là sự ghi nhận và tôn vinh đội ngũ lái xe Tối 25/12, Ủy ban An toàn Giao thông Quốc gia đã tổ chức trao giải thưởng 'Vô lăng vàng' năm 2017 cho các tập thể và cá nhân xứng đáng nhất.\", \"Trong lần thứ 5 tổ chức, giải thưởng 'Vô lăng vàng' ghi nhận thêm nhiều điểm mới so với 4 lần trao thưởng trước đây.\", 'Đúng 8h tối 25/12, Lễ trao giải thưởng “Vô lăng vàng” 2017 chính thức diễn ra tại Nhà hát Đài Tiếng nói Việt Nam (số 58 Quán Sứ , quận Hai Bà Trưng , TP Hà Nội ) với những màn biểu diễn văn nghệ đặc sắc.', 'Tham dự Lễ trao giải có đại diện Lãnh đạo Ủy ban An toàn giao thông Quốc gia , Bộ GTVT , Bộ Công an , Hiệp hội vận tải ô tô Việt Nam và các đại biểu là Ủy viên Ủy ban, Ủy viên Ban Thường trực Ủy ban An toàn giao thông Quốc gia , cùng 18 tập thể và 46 lái xe đạt giải thưởng “Vô lăng vàng” lần thứ 5 năm 2017.', 'Đã có 64 tập thể và cá nhân được trao giải Phó Chủ tịch chuyên trách Ủy ban An toàn Giao thông Quốc gia Khuất Việt Hùng khẳng định, sau 5 năm tổ chức, giải thưởng “Vô lăng vàng” đã nhận được sự quan tâm rộng rãi của các doanh nhiệp kinh doanh trong lĩnh vực vận tải ô tô.', 'Đây là giải thưởng duy nhất có quy mô quốc gia để tôn vinh những doanh nghiệp, lái xe an toàn.', 'Đánh giá về ý nghĩa của giải thưởng “Vô lăng vàng”, ông Hùng cho hay, giải thưởng này đã tạo ra phong trào lái xe an toàn đối với đội ngũ lái xe, qua đó góp phần chuyển biến tích cực trong đảm bảo trật tự an toàn giao thông.', 'Quá trình bình chọn, trao giải được Ban Tổ chức đánh giá một cách khách quan, vô tư, dựa trên những tiêu chí của cuộc thi.', 'Thứ trưởng Bộ GTVT Nguyễn Ngọc Đông khẳng định đội ngũ lái xe có những đóng góp lớn cho công tác đảm bảo trật tự, an toàn giao thông Căn cứ Thể lệ và tiêu chí của cuộc thi, Hội đồng bình xét của Ủy ban An toàn Giao thông Quốc gia đã lựa chọn ra 18 doanh nghiệp, hợp tác xã vận tải đại diện cho gần 100 đơn vị kinh doanh vận tải của các vùng, miền trên phạm vi cả nước, đăng ký tham dự giải.', 'Bên cạnh đó, có tất cả 46 cá nhân là những lái xe xe đại diện cho hàng nghìn lái xe kinh doanh vận tải được trao giải “Vô lăng vàng”.', 'Đây là những cá nhân đã có thành tích lái xe an toàn cùng các hành động thể hiện phẩm chất đạo đức trong sáng và cách ứng xử văn minh, lịch sự của người lái xe.', 'Ngoài ra, một điểm mới trong giải thưởng “Vô lăng vàng” 2017 là Ban Tổ chức đã trao giải thưởng “Văn hóa giao thông” cho 4 cá nhân gồm: Trần Văn Quang và Trần Văn Quyết (Công ty CP xe khách số I Sơn La , Sơn La ), Nguyễn Hồng Phi (Doanh nghiệp tư nhân Hải Anh , Sơn La ), Châu Mỹ Huỳnh (Công ty TNHH Kim Cương , Đồng Tháp ).', 'Đây là giải thưởng dành cho những cá nhân có những hành động đẹp, ý thức và nhân văn trong quá trình làm việc của mình.', 'Phát biểu tại lễ trao giải, Thứ trưởng Bộ GTVT Nguyễn Ngọc Đông khẳng định, tầm quan trọng của đội ngũ lái xe đối với đất nước ta là vô cùng lớn.', 'Khi đất nước còn chiến tranh, chính những chiến sĩ lái xe vận tải đã có những đóng góp đặc biệt quan trọng trong hai cuộc kháng chiến chống Pháp và Mỹ .', 'Còn trong thời bình, đặc biệt là hiện nay trình độ quản trị kinh doanh vận tải của doanh nghiệp được nâng cao, chất lượng dịch vụ ngày càng bảo đảm thì đội ngũ lái xe đã góp phần rất quan trọng để giúp tình hình trật tự an toàn giao thông có chuyển biến tích cực, tai nạn giao thông tiếp tục giảm cả 3 tiêu chí về số vụ, số người chết, số người bị thương.', 'Sự đóng góp của những doanh nghiệp vận tải và lái xe đang ngày đêm giữ xe tốt, lái xe an toàn, thực hiện mục tiêu “Tính mạng con người là trên hết” lập nên những “cung đường bình yên” “chuyến đi hạnh phúc” cho mọi nhà, cho cộng đồng và toàn xã hội.', '“Chúng ta trân trọng cảm ơn đối với những đóng góp của họ cho sự phát triển của đất nước nói chung và cho sự nghiệp bảo đảm trật tự an toàn giao thông nói riêng\", Thứ trưởng Đông nói.', 'Tại Lễ trao giải thưởng “Vô Lăng Vàng” lần thứ 5, Ủy ban An toàn giao thông Quốc gia tiếp tục phát động Phong trào thi đua xây dựng “Doanh nghiệp vận tải an toàn” và “Lái xe an toàn” năm 2018 và bình chọn giải thưởng “Vô Lăng Vàng” lần thứ 6.', 'Cũng tại buổi lễ, Ban tổ chức giải thưởng “Vô lăng vàng” chân thành cám ơn Công ty cổ phần ô tô Trường Hải đã đồng hành, hỗ trợ giải thưởng trong 3 năm qua.', 'Danh sách 18 tập thể nhận giải thưởng “Vô lăng vàng”: Công ty Trách nhiệm hữu hạn Văn Minh , Nghệ An ; Chi nhánh công ty Cổ phần Sun taxi tại Gia Lai , Gia Lai ; Công ty Cổ phần Logistics Portserco , Đà Nẵng ; Hợp tác xã Cơ khí Vận tải và Dịch vụ Diên Hồng , Gia Lai ; Công ty Trách nhiệm hữu hạn ô tô Đại Duy , Nam Định ; Công ty Cổ phần Thương mại và Du lịch Hạ Vinh , Nghệ An ; Công ty Cổ phần Vận tải và Dịch vụ Liên Ninh , Hà Nội ; Công ty Cổ phần Vận tải giao nhận và Thương mại Quang Châu , Tp. Hồ Chí Minh ; Công ty Trách nhiệm hữu hạn Một thành viên Mai Linh Kontum , Kontum ; Công ty Giao nhận kho vận Hải Dương , Hải Dương ; Hợp tác xã Vận tải Thống Nhất , Quảng Ngãi ; Công ty Cổ phần Du lịch Xuân Long , Điện Biên ; Công ty Cổ phần xe khách số 1 Sơn La , Sơn La ; Công ty Cổ phần Vận tải ô tô Quảng Ninh , Quảng Ninh ; Công ty Cổ phần thương mại du lịch và vận chuyển khách Tình Nghĩa , Quảng Ninh ; Công ty Trách nhiệm hữu hạn vận tải Thịnh Hưng , Hải Phòng ; Công ty Cổ phần Vận tải ô tô An Giang , An Giang ; Công ty Cổ phần Vận tải bộ Tân Cảng thành viên Tổng công ty Tân Cảng Sài Gòn – Trực thuộc quân chủng Hải quân , Tp. Hồ Chí Minh .', 'Danh sách 46 cá nhân nhận giải thưởng “Vô lăng vàng”: Phạm Quốc Thịnh (Công ty Vinasun Ánh Dương Việt Nam ), Nguyễn Viết Trung (Công ty CP Vận tải và Dịch vụ Liên Ninh ), Đỗ Hải Đặng (Xí nghiệp xe khách Nam Hà Nội ), Nguyễn Hữu Hoàn (Công ty CP Vận tải Newway ), Nguyễn Văn Tiến (Trung tâm Tân Đạt ), Nguyễn Đức Nam (Công ty CP Xe điện Hà Nội ), Nguyễn Anh Phương (Công ty TNHH Thương mại Thiên Phong ), Lê Văn Trung (Công ty CP Tập đoàn Mai Linh ), Đặng Đình Cường (Công ty CP Thương mại du lịch và Vận chuyển khách Tình Nghĩa ), Nguyễn Hữu Châu (Công ty CP xe khách số I Sơn La ), Lưu Ngọc Long (Công ty CP Vận tải bộ Tân Cảng ), Ngô Văn Bình (Công ty TNHH Văn Minh ), Đoàn Hữu Bồng (Công ty TNHH Phúc Xuyên ), Nguyễn Văn Quyền (Công ty CP Thương mại và Du lịch Hạ Vinh ), Phan Mậu Cường (Công ty CP vận tải bộ Tân Cảng ), Phan Xuân Lý (Chi nhánh Công ty CP Sun taxi tại Gia Lai ), Phạm Khánh (Công ty CP Logistics Portserco ), Nguyễn Anh Hoàng (Công ty CP Cơ khí Sơn La ), Trần Xuân Trọng và Bùi Văn Nhân (Công ty CP xe khách Sài Gòn ), Nguyễn Hữu Trung (Công ty CP Giao nhận và Thương mại Quang Châu ), Trần Can (HTX Vận tải Thống Nhất Quảng Ngãi ), Nguyễn Hà Vinh (Doanh nghiệp tư nhân Trường Hải ), Trương Quang Hiếu (HTX Cơ khí vận tải và Dịch vụ Diên Hồng ), Lăng Văn Lợi (Công ty CP Vận tải An Giang ), Nguyễn Thành Khương (Công ty TNHH Thương mại và Dịch vụ Hải Phượng ), Nguyễn Văn Bảo (Công ty TNHH MTV Mai Linh Kon Tum ), Lê Đình Dũng (Công ty CP Du lịch Xuân Long ), Nguyễn Văn Quốc (Công ty TNHH Minh Quốc ), Ngô Chí Tuấn (HTX Vận tải ô tô Thành Tuyên ), Trịnh Đăng An (Công ty CP vận tải ô tô Thanh Hóa ), Trần Văn Đội (Công ty TNHH Mai Linh Thanh Hóa ), Lê Đức Hiếu (Công ty TNHH Dịch vụ Taxi miền Bắc ), Vũ Tiến Quang (Công ty CP xe khách Quảng Ninh ), Đinh Viết Thể (Công ty CP Vận tải ô tô Điện Biên ), Lê Anh Thắng (Xí nghiệp Xe buýt Cầu Bươu ), Nguyễn Ngọc Tùng (Xí nghiệp Xe buýt Hà Nội ), Trần Đăng Thanh (Xí nghiệp Xe buýt 10/10 Hà Nội ), Ngô Văn Quyết (Công ty TNHH MTV Mai Linh Vĩnh Phúc ), Lê Minh Hoàng (Công ty TNHH MTV Mai Linh Gia Lai ), Trần Văn Dính (Công ty CP Thương mại du lịch Hà Lan ), Nguyễn Ngọc Chuân (Công ty CP Giao nhận kho vận Hải Dương ), Đinh Thế Giang Nam (Công ty CP Vận tải ô tô Quảng Ninh ), Nguyễn Danh Lợi (Công ty TNHH Vận tải Thịnh Hưng ), Đỗ Thanh Tân (Xí nghiệp Xe buýt Yên Viên ), Vũ Duy Thuận (HTX Vận tải hàng hóa hành khách và Du lịch Tân Việt ).', 'Quý Nguyễn']\n",
            "[\"\\ufeffGiải thưởng 'Vô lăng vàng' là sự ghi nhận và tôn vinh đội ngũ lái xe Tối 25/12, Ủy ban An toàn Giao thông Quốc gia đã tổ chức trao giải thưởng 'Vô lăng vàng' năm 2017 cho các tập thể và cá nhân xứng đáng nhất.\", \"Trong lần thứ 5 tổ chức, giải thưởng 'Vô lăng vàng' ghi nhận thêm nhiều điểm mới so với 4 lần trao thưởng trước đây.\", 'Đúng 8h tối 25/12, Lễ trao giải thưởng “Vô lăng vàng” 2017 chính thức diễn ra tại Nhà hát Đài Tiếng nói Việt Nam (số 58 Quán Sứ , quận Hai Bà Trưng , TP Hà Nội ) với những màn biểu diễn văn nghệ đặc sắc.', 'Tham dự Lễ trao giải có đại diện Lãnh đạo Ủy ban An toàn giao thông Quốc gia , Bộ GTVT , Bộ Công an , Hiệp hội vận tải ô tô Việt Nam và các đại biểu là Ủy viên Ủy ban, Ủy viên Ban Thường trực Ủy ban An toàn giao thông Quốc gia , cùng 18 tập thể và 46 lái xe đạt giải thưởng “Vô lăng vàng” lần thứ 5 năm 2017.', 'Đã có 64 tập thể và cá nhân được trao giải Phó Chủ tịch chuyên trách Ủy ban An toàn Giao thông Quốc gia Khuất Việt Hùng khẳng định, sau 5 năm tổ chức, giải thưởng “Vô lăng vàng” đã nhận được sự quan tâm rộng rãi của các doanh nhiệp kinh doanh trong lĩnh vực vận tải ô tô.', 'Đây là giải thưởng duy nhất có quy mô quốc gia để tôn vinh những doanh nghiệp, lái xe an toàn.', 'Đánh giá về ý nghĩa của giải thưởng “Vô lăng vàng”, ông Hùng cho hay, giải thưởng này đã tạo ra phong trào lái xe an toàn đối với đội ngũ lái xe, qua đó góp phần chuyển biến tích cực trong đảm bảo trật tự an toàn giao thông.', 'Quá trình bình chọn, trao giải được Ban Tổ chức đánh giá một cách khách quan, vô tư, dựa trên những tiêu chí của cuộc thi.', 'Thứ trưởng Bộ GTVT Nguyễn Ngọc Đông khẳng định đội ngũ lái xe có những đóng góp lớn cho công tác đảm bảo trật tự, an toàn giao thông Căn cứ Thể lệ và tiêu chí của cuộc thi, Hội đồng bình xét của Ủy ban An toàn Giao thông Quốc gia đã lựa chọn ra 18 doanh nghiệp, hợp tác xã vận tải đại diện cho gần 100 đơn vị kinh doanh vận tải của các vùng, miền trên phạm vi cả nước, đăng ký tham dự giải.', 'Bên cạnh đó, có tất cả 46 cá nhân là những lái xe xe đại diện cho hàng nghìn lái xe kinh doanh vận tải được trao giải “Vô lăng vàng”.', 'Đây là những cá nhân đã có thành tích lái xe an toàn cùng các hành động thể hiện phẩm chất đạo đức trong sáng và cách ứng xử văn minh, lịch sự của người lái xe.', 'Ngoài ra, một điểm mới trong giải thưởng “Vô lăng vàng” 2017 là Ban Tổ chức đã trao giải thưởng “Văn hóa giao thông” cho 4 cá nhân gồm: Trần Văn Quang và Trần Văn Quyết (Công ty CP xe khách số I Sơn La , Sơn La ), Nguyễn Hồng Phi (Doanh nghiệp tư nhân Hải Anh , Sơn La ), Châu Mỹ Huỳnh (Công ty TNHH Kim Cương , Đồng Tháp ).', 'Đây là giải thưởng dành cho những cá nhân có những hành động đẹp, ý thức và nhân văn trong quá trình làm việc của mình.', 'Phát biểu tại lễ trao giải, Thứ trưởng Bộ GTVT Nguyễn Ngọc Đông khẳng định, tầm quan trọng của đội ngũ lái xe đối với đất nước ta là vô cùng lớn.', 'Khi đất nước còn chiến tranh, chính những chiến sĩ lái xe vận tải đã có những đóng góp đặc biệt quan trọng trong hai cuộc kháng chiến chống Pháp và Mỹ .', 'Còn trong thời bình, đặc biệt là hiện nay trình độ quản trị kinh doanh vận tải của doanh nghiệp được nâng cao, chất lượng dịch vụ ngày càng bảo đảm thì đội ngũ lái xe đã góp phần rất quan trọng để giúp tình hình trật tự an toàn giao thông có chuyển biến tích cực, tai nạn giao thông tiếp tục giảm cả 3 tiêu chí về số vụ, số người chết, số người bị thương.', 'Sự đóng góp của những doanh nghiệp vận tải và lái xe đang ngày đêm giữ xe tốt, lái xe an toàn, thực hiện mục tiêu “Tính mạng con người là trên hết” lập nên những “cung đường bình yên” “chuyến đi hạnh phúc” cho mọi nhà, cho cộng đồng và toàn xã hội.', '“Chúng ta trân trọng cảm ơn đối với những đóng góp của họ cho sự phát triển của đất nước nói chung và cho sự nghiệp bảo đảm trật tự an toàn giao thông nói riêng\", Thứ trưởng Đông nói.', 'Tại Lễ trao giải thưởng “Vô Lăng Vàng” lần thứ 5, Ủy ban An toàn giao thông Quốc gia tiếp tục phát động Phong trào thi đua xây dựng “Doanh nghiệp vận tải an toàn” và “Lái xe an toàn” năm 2018 và bình chọn giải thưởng “Vô Lăng Vàng” lần thứ 6.', 'Cũng tại buổi lễ, Ban tổ chức giải thưởng “Vô lăng vàng” chân thành cám ơn Công ty cổ phần ô tô Trường Hải đã đồng hành, hỗ trợ giải thưởng trong 3 năm qua.', 'Danh sách 18 tập thể nhận giải thưởng “Vô lăng vàng”: Công ty Trách nhiệm hữu hạn Văn Minh , Nghệ An', 'Chi nhánh công ty Cổ phần Sun taxi tại Gia Lai , Gia Lai', 'Công ty Cổ phần Logistics Portserco , Đà Nẵng', 'Hợp tác xã Cơ khí Vận tải và Dịch vụ Diên Hồng , Gia Lai', 'Công ty Trách nhiệm hữu hạn ô tô Đại Duy , Nam Định', 'Công ty Cổ phần Thương mại và Du lịch Hạ Vinh , Nghệ An', 'Công ty Cổ phần Vận tải và Dịch vụ Liên Ninh , Hà Nội', 'Công ty Cổ phần Vận tải giao nhận và Thương mại Quang Châu , Tp. Hồ Chí Minh', 'Công ty Trách nhiệm hữu hạn Một thành viên Mai Linh Kontum , Kontum', 'Công ty Giao nhận kho vận Hải Dương , Hải Dương', 'Hợp tác xã Vận tải Thống Nhất , Quảng Ngãi', 'Công ty Cổ phần Du lịch Xuân Long , Điện Biên', 'Công ty Cổ phần xe khách số 1 Sơn La , Sơn La', 'Công ty Cổ phần Vận tải ô tô Quảng Ninh , Quảng Ninh', 'Công ty Cổ phần thương mại du lịch và vận chuyển khách Tình Nghĩa , Quảng Ninh', 'Công ty Trách nhiệm hữu hạn vận tải Thịnh Hưng , Hải Phòng', 'Công ty Cổ phần Vận tải ô tô An Giang , An Giang', 'Công ty Cổ phần Vận tải bộ Tân Cảng thành viên Tổng công ty Tân Cảng Sài Gòn – Trực thuộc quân chủng Hải quân , Tp. Hồ Chí Minh .', 'Danh sách 46 cá nhân nhận giải thưởng “Vô lăng vàng”: Phạm Quốc Thịnh (Công ty Vinasun Ánh Dương Việt Nam )', 'Nguyễn Viết Trung (Công ty CP Vận tải và Dịch vụ Liên Ninh )', 'Đỗ Hải Đặng (Xí nghiệp xe khách Nam Hà Nội )', 'Nguyễn Hữu Hoàn (Công ty CP Vận tải Newway )', 'Nguyễn Văn Tiến (Trung tâm Tân Đạt )', 'Nguyễn Đức Nam (Công ty CP Xe điện Hà Nội )', 'Nguyễn Anh Phương (Công ty TNHH Thương mại Thiên Phong )', 'Lê Văn Trung (Công ty CP Tập đoàn Mai Linh )', 'Đặng Đình Cường (Công ty CP Thương mại du lịch và Vận chuyển khách Tình Nghĩa )', 'Nguyễn Hữu Châu (Công ty CP xe khách số I Sơn La )', 'Lưu Ngọc Long (Công ty CP Vận tải bộ Tân Cảng )', 'Ngô Văn Bình (Công ty TNHH Văn Minh )', 'Đoàn Hữu Bồng (Công ty TNHH Phúc Xuyên )', 'Nguyễn Văn Quyền (Công ty CP Thương mại và Du lịch Hạ Vinh )', 'Phan Mậu Cường (Công ty CP vận tải bộ Tân Cảng )', 'Phan Xuân Lý (Chi nhánh Công ty CP Sun taxi tại Gia Lai )', 'Phạm Khánh (Công ty CP Logistics Portserco )', 'Nguyễn Anh Hoàng (Công ty CP Cơ khí Sơn La )', 'Trần Xuân Trọng và Bùi Văn Nhân (Công ty CP xe khách Sài Gòn )', 'Nguyễn Hữu Trung (Công ty CP Giao nhận và Thương mại Quang Châu )', 'Trần Can (HTX Vận tải Thống Nhất Quảng Ngãi )', 'Nguyễn Hà Vinh (Doanh nghiệp tư nhân Trường Hải )', 'Trương Quang Hiếu (HTX Cơ khí vận tải và Dịch vụ Diên Hồng )', 'Lăng Văn Lợi (Công ty CP Vận tải An Giang )', 'Nguyễn Thành Khương (Công ty TNHH Thương mại và Dịch vụ Hải Phượng )', 'Nguyễn Văn Bảo (Công ty TNHH MTV Mai Linh Kon Tum )', 'Lê Đình Dũng (Công ty CP Du lịch Xuân Long )', 'Nguyễn Văn Quốc (Công ty TNHH Minh Quốc )', 'Ngô Chí Tuấn (HTX Vận tải ô tô Thành Tuyên )', 'Trịnh Đăng An (Công ty CP vận tải ô tô Thanh Hóa )', 'Trần Văn Đội (Công ty TNHH Mai Linh Thanh Hóa )', 'Lê Đức Hiếu (Công ty TNHH Dịch vụ Taxi miền Bắc )', 'Vũ Tiến Quang (Công ty CP xe khách Quảng Ninh )', 'Đinh Viết Thể (Công ty CP Vận tải ô tô Điện Biên )', 'Lê Anh Thắng (Xí nghiệp Xe buýt Cầu Bươu )', 'Nguyễn Ngọc Tùng (Xí nghiệp Xe buýt Hà Nội )', 'Trần Đăng Thanh (Xí nghiệp Xe buýt 10/10 Hà Nội )', 'Ngô Văn Quyết (Công ty TNHH MTV Mai Linh Vĩnh Phúc )', 'Lê Minh Hoàng (Công ty TNHH MTV Mai Linh Gia Lai )', 'Trần Văn Dính (Công ty CP Thương mại du lịch Hà Lan )', 'Nguyễn Ngọc Chuân (Công ty CP Giao nhận kho vận Hải Dương )', 'Đinh Thế Giang Nam (Công ty CP Vận tải ô tô Quảng Ninh )', 'Nguyễn Danh Lợi (Công ty TNHH Vận tải Thịnh Hưng )', 'Đỗ Thanh Tân (Xí nghiệp Xe buýt Yên Viên )', 'Vũ Duy Thuận (HTX Vận tải hàng hóa hành khách và Du lịch Tân Việt ).', 'Quý Nguyễn']\n",
            "[\"\\ufeffGiải thưởng 'Vô lăng vàng' là sự ghi nhận và tôn vinh đội ngũ lái xe Tối 25/12, Ủy ban An toàn Giao thông Quốc gia đã tổ chức trao giải thưởng 'Vô lăng vàng' năm 2017 cho các tập thể và cá nhân xứng đáng nhất.\", \"Trong lần thứ 5 tổ chức, giải thưởng 'Vô lăng vàng' ghi nhận thêm nhiều điểm mới so với 4 lần trao thưởng trước đây.\", 'Đúng 8h tối 25/12, Lễ trao giải thưởng “Vô lăng vàng” 2017 chính thức diễn ra tại Nhà hát Đài Tiếng nói Việt Nam (số 58 Quán Sứ , quận Hai Bà Trưng , TP Hà Nội ) với những màn biểu diễn văn nghệ đặc sắc.', 'Tham dự Lễ trao giải có đại diện Lãnh đạo Ủy ban An toàn giao thông Quốc gia , Bộ GTVT , Bộ Công an , Hiệp hội vận tải ô tô Việt Nam và các đại biểu là Ủy viên Ủy ban, Ủy viên Ban Thường trực Ủy ban An toàn giao thông Quốc gia , cùng 18 tập thể và 46 lái xe đạt giải thưởng “Vô lăng vàng” lần thứ 5 năm 2017.', 'Đã có 64 tập thể và cá nhân được trao giải Phó Chủ tịch chuyên trách Ủy ban An toàn Giao thông Quốc gia Khuất Việt Hùng khẳng định, sau 5 năm tổ chức, giải thưởng “Vô lăng vàng” đã nhận được sự quan tâm rộng rãi của các doanh nhiệp kinh doanh trong lĩnh vực vận tải ô tô.', 'Đây là giải thưởng duy nhất có quy mô quốc gia để tôn vinh những doanh nghiệp, lái xe an toàn.', 'Đánh giá về ý nghĩa của giải thưởng “Vô lăng vàng”, ông Hùng cho hay, giải thưởng này đã tạo ra phong trào lái xe an toàn đối với đội ngũ lái xe, qua đó góp phần chuyển biến tích cực trong đảm bảo trật tự an toàn giao thông.', 'Quá trình bình chọn, trao giải được Ban Tổ chức đánh giá một cách khách quan, vô tư, dựa trên những tiêu chí của cuộc thi.', 'Thứ trưởng Bộ GTVT Nguyễn Ngọc Đông khẳng định đội ngũ lái xe có những đóng góp lớn cho công tác đảm bảo trật tự, an toàn giao thông Căn cứ Thể lệ và tiêu chí của cuộc thi, Hội đồng bình xét của Ủy ban An toàn Giao thông Quốc gia đã lựa chọn ra 18 doanh nghiệp, hợp tác xã vận tải đại diện cho gần 100 đơn vị kinh doanh vận tải của các vùng, miền trên phạm vi cả nước, đăng ký tham dự giải.', 'Bên cạnh đó, có tất cả 46 cá nhân là những lái xe xe đại diện cho hàng nghìn lái xe kinh doanh vận tải được trao giải “Vô lăng vàng”.', 'Đây là những cá nhân đã có thành tích lái xe an toàn cùng các hành động thể hiện phẩm chất đạo đức trong sáng và cách ứng xử văn minh, lịch sự của người lái xe.', 'Ngoài ra, một điểm mới trong giải thưởng “Vô lăng vàng” 2017 là Ban Tổ chức đã trao giải thưởng “Văn hóa giao thông” cho 4 cá nhân gồm: Trần Văn Quang và Trần Văn Quyết (Công ty CP xe khách số I Sơn La , Sơn La ), Nguyễn Hồng Phi (Doanh nghiệp tư nhân Hải Anh , Sơn La ), Châu Mỹ Huỳnh (Công ty TNHH Kim Cương , Đồng Tháp ).', 'Đây là giải thưởng dành cho những cá nhân có những hành động đẹp, ý thức và nhân văn trong quá trình làm việc của mình.', 'Phát biểu tại lễ trao giải, Thứ trưởng Bộ GTVT Nguyễn Ngọc Đông khẳng định, tầm quan trọng của đội ngũ lái xe đối với đất nước ta là vô cùng lớn.', 'Khi đất nước còn chiến tranh, chính những chiến sĩ lái xe vận tải đã có những đóng góp đặc biệt quan trọng trong hai cuộc kháng chiến chống Pháp và Mỹ .', 'Còn trong thời bình, đặc biệt là hiện nay trình độ quản trị kinh doanh vận tải của doanh nghiệp được nâng cao, chất lượng dịch vụ ngày càng bảo đảm thì đội ngũ lái xe đã góp phần rất quan trọng để giúp tình hình trật tự an toàn giao thông có chuyển biến tích cực, tai nạn giao thông tiếp tục giảm cả 3 tiêu chí về số vụ, số người chết, số người bị thương.', 'Sự đóng góp của những doanh nghiệp vận tải và lái xe đang ngày đêm giữ xe tốt, lái xe an toàn, thực hiện mục tiêu “Tính mạng con người là trên hết” lập nên những “cung đường bình yên” “chuyến đi hạnh phúc” cho mọi nhà, cho cộng đồng và toàn xã hội.', '“Chúng ta trân trọng cảm ơn đối với những đóng góp của họ cho sự phát triển của đất nước nói chung và cho sự nghiệp bảo đảm trật tự an toàn giao thông nói riêng\", Thứ trưởng Đông nói.', 'Tại Lễ trao giải thưởng “Vô Lăng Vàng” lần thứ 5, Ủy ban An toàn giao thông Quốc gia tiếp tục phát động Phong trào thi đua xây dựng “Doanh nghiệp vận tải an toàn” và “Lái xe an toàn” năm 2018 và bình chọn giải thưởng “Vô Lăng Vàng” lần thứ 6.', 'Cũng tại buổi lễ, Ban tổ chức giải thưởng “Vô lăng vàng” chân thành cám ơn Công ty cổ phần ô tô Trường Hải đã đồng hành, hỗ trợ giải thưởng trong 3 năm qua.', 'Danh sách 18 tập thể nhận giải thưởng “Vô lăng vàng”: Công ty Trách nhiệm hữu hạn Văn Minh , Nghệ An', 'Chi nhánh công ty Cổ phần Sun taxi tại Gia Lai , Gia Lai', 'Công ty Cổ phần Logistics Portserco , Đà Nẵng', 'Hợp tác xã Cơ khí Vận tải và Dịch vụ Diên Hồng , Gia Lai', 'Công ty Trách nhiệm hữu hạn ô tô Đại Duy , Nam Định', 'Công ty Cổ phần Thương mại và Du lịch Hạ Vinh , Nghệ An', 'Công ty Cổ phần Vận tải và Dịch vụ Liên Ninh , Hà Nội', 'Công ty Cổ phần Vận tải giao nhận và Thương mại Quang Châu , Tp. Hồ Chí Minh', 'Công ty Trách nhiệm hữu hạn Một thành viên Mai Linh Kontum , Kontum', 'Công ty Giao nhận kho vận Hải Dương , Hải Dương', 'Hợp tác xã Vận tải Thống Nhất , Quảng Ngãi', 'Công ty Cổ phần Du lịch Xuân Long , Điện Biên', 'Công ty Cổ phần xe khách số 1 Sơn La , Sơn La', 'Công ty Cổ phần Vận tải ô tô Quảng Ninh , Quảng Ninh', 'Công ty Cổ phần thương mại du lịch và vận chuyển khách Tình Nghĩa , Quảng Ninh', 'Công ty Trách nhiệm hữu hạn vận tải Thịnh Hưng , Hải Phòng', 'Công ty Cổ phần Vận tải ô tô An Giang , An Giang', 'Công ty Cổ phần Vận tải bộ Tân Cảng thành viên Tổng công ty Tân Cảng Sài Gòn – Trực thuộc quân chủng Hải quân , Tp. Hồ Chí Minh .', 'Danh sách 46 cá nhân nhận giải thưởng “Vô lăng vàng”: Phạm Quốc Thịnh (Công ty Vinasun Ánh Dương Việt Nam )', 'Nguyễn Viết Trung (Công ty CP Vận tải và Dịch vụ Liên Ninh )', 'Đỗ Hải Đặng (Xí nghiệp xe khách Nam Hà Nội )', 'Nguyễn Hữu Hoàn (Công ty CP Vận tải Newway )', 'Nguyễn Văn Tiến (Trung tâm Tân Đạt )', 'Nguyễn Đức Nam (Công ty CP Xe điện Hà Nội )', 'Nguyễn Anh Phương (Công ty TNHH Thương mại Thiên Phong )', 'Lê Văn Trung (Công ty CP Tập đoàn Mai Linh )', 'Đặng Đình Cường (Công ty CP Thương mại du lịch và Vận chuyển khách Tình Nghĩa )', 'Nguyễn Hữu Châu (Công ty CP xe khách số I Sơn La )', 'Lưu Ngọc Long (Công ty CP Vận tải bộ Tân Cảng )', 'Ngô Văn Bình (Công ty TNHH Văn Minh )', 'Đoàn Hữu Bồng (Công ty TNHH Phúc Xuyên )', 'Nguyễn Văn Quyền (Công ty CP Thương mại và Du lịch Hạ Vinh )', 'Phan Mậu Cường (Công ty CP vận tải bộ Tân Cảng )', 'Phan Xuân Lý (Chi nhánh Công ty CP Sun taxi tại Gia Lai )', 'Phạm Khánh (Công ty CP Logistics Portserco )', 'Nguyễn Anh Hoàng (Công ty CP Cơ khí Sơn La )', 'Trần Xuân Trọng và Bùi Văn Nhân (Công ty CP xe khách Sài Gòn )', 'Nguyễn Hữu Trung (Công ty CP Giao nhận và Thương mại Quang Châu )', 'Trần Can (HTX Vận tải Thống Nhất Quảng Ngãi )', 'Nguyễn Hà Vinh (Doanh nghiệp tư nhân Trường Hải )', 'Trương Quang Hiếu (HTX Cơ khí vận tải và Dịch vụ Diên Hồng )', 'Lăng Văn Lợi (Công ty CP Vận tải An Giang )', 'Nguyễn Thành Khương (Công ty TNHH Thương mại và Dịch vụ Hải Phượng )', 'Nguyễn Văn Bảo (Công ty TNHH MTV Mai Linh Kon Tum )', 'Lê Đình Dũng (Công ty CP Du lịch Xuân Long )', 'Nguyễn Văn Quốc (Công ty TNHH Minh Quốc )', 'Ngô Chí Tuấn (HTX Vận tải ô tô Thành Tuyên )', 'Trịnh Đăng An (Công ty CP vận tải ô tô Thanh Hóa )', 'Trần Văn Đội (Công ty TNHH Mai Linh Thanh Hóa )', 'Lê Đức Hiếu (Công ty TNHH Dịch vụ Taxi miền Bắc )', 'Vũ Tiến Quang (Công ty CP xe khách Quảng Ninh )', 'Đinh Viết Thể (Công ty CP Vận tải ô tô Điện Biên )', 'Lê Anh Thắng (Xí nghiệp Xe buýt Cầu Bươu )', 'Nguyễn Ngọc Tùng (Xí nghiệp Xe buýt Hà Nội )', 'Trần Đăng Thanh (Xí nghiệp Xe buýt 10/10 Hà Nội )', 'Ngô Văn Quyết (Công ty TNHH MTV Mai Linh Vĩnh Phúc )', 'Lê Minh Hoàng (Công ty TNHH MTV Mai Linh Gia Lai )', 'Trần Văn Dính (Công ty CP Thương mại du lịch Hà Lan )', 'Nguyễn Ngọc Chuân (Công ty CP Giao nhận kho vận Hải Dương )', 'Đinh Thế Giang Nam (Công ty CP Vận tải ô tô Quảng Ninh )', 'Nguyễn Danh Lợi (Công ty TNHH Vận tải Thịnh Hưng )', 'Đỗ Thanh Tân (Xí nghiệp Xe buýt Yên Viên )', 'Vũ Duy Thuận (HTX Vận tải hàng hóa hành khách và Du lịch Tân Việt ).', 'Quý Nguyễn']\n",
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu7DbslJN1i4",
        "outputId": "6e82277b-3e01-4329-bb1a-f721503d198c"
      },
      "source": [
        "\"\"\"\n",
        "         'LOCATED': 'LOCATED', 'IS_LOCATED': 'IS_LOCATED', \n",
        "         'PART_WHOLE': 'PART_WHOLE', 'WHOLE_PART': 'WHOLE_PART', \n",
        "         'PERSONAL_SOCIAL': 'PERSONAL_SOCIAL', \n",
        "         'AFFILIATION': 'AFFILIATION', 'AFFILIATION_TO': 'AFFILIATION_TO', \n",
        "         'OTHERS': 'OTHERS'\n",
        "\"\"\"\n",
        "\n",
        "count_label_LOCATED = 0\n",
        "count_label_PART_WHOLE = 0\n",
        "count_label_PERSONAL_SOCIAL = 0\n",
        "count_label_AFFILIATION = 0\n",
        "count_label_OTHERS = 0\n",
        "\n",
        "\n",
        "for tdata_point in tdata:\n",
        "    if tdata_point['label'] == 'OTHERS':\n",
        "        count_label_OTHERS += 1\n",
        "\n",
        "    elif tdata_point['label'] == 'PERSONAL_SOCIAL':\n",
        "        count_label_PERSONAL_SOCIAL += 1\n",
        "    \n",
        "    elif tdata_point['label'] in ['LOCATED', 'IS_LOCATED']:\n",
        "        count_label_LOCATED += 1\n",
        "    \n",
        "    elif tdata_point['label'] in ['PART_WHOLE', 'WHOLE_PART']:\n",
        "        count_label_PART_WHOLE += 1\n",
        "    \n",
        "    elif tdata_point['label'] in ['AFFILIATION', 'AFFILIATION_TO']:\n",
        "        count_label_AFFILIATION += 1\n",
        "    \n",
        "    \n",
        "\n",
        "print('LOCATED: ', count_label_LOCATED)\n",
        "print('PART_WHOLE: ', count_label_PART_WHOLE)\n",
        "print('PERSONAL_SOCIAL: ', count_label_PERSONAL_SOCIAL)\n",
        "print('AFFILIATION: ', count_label_AFFILIATION)\n",
        "print('OTHERS: ', count_label_OTHERS)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LOCATED:  0\n",
            "PART_WHOLE:  0\n",
            "PERSONAL_SOCIAL:  0\n",
            "AFFILIATION:  0\n",
            "OTHERS:  9567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SfkCqCFN35t",
        "outputId": "77d45f38-ebe2-485d-d406-37143df9cde0"
      },
      "source": [
        "print('Count of labels that is not OTHERS: ', (len(tdata) - count_label_OTHERS))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of labels that is not OTHERS:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3O1aoGj796m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0662e352-b946-4e09-fc74-da2a4c6b539b"
      },
      "source": [
        "print(len(tdata))\n",
        "print(*tdata[0:15], sep='\\n')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9567\n",
            "{'doc_id': '23352901', 'sent_id': 1, 'sentence': \"Đây là lý do khiến Yoon Ah quyết định cắt mái tóc dài 'nữ thần' Mái tóc cũ của thành viên SNSD bị hư hỏng nặng nề và Yoon Ah thậm chí không muốn nuôi tóc lại.\", 'spos': [0, 158], 'entity_1': {'text': 'Yoon Ah', 'pos': [19, 26]}, 'entity_2': {'text': 'SNSD', 'pos': [90, 94]}, 'entity_1_info': {'first_token': [5, 6, 1, 'PERSON'], 'last_token': [6, 7, 1, 'PERSON']}, 'entity_2_info': {'first_token': [21, 22, 0, 'ORGANIZATION'], 'last_token': [21, 22, 0, 'ORGANIZATION']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 2, 'sentence': \"Đây là lý do khiến Yoon Ah quyết định cắt mái tóc dài 'nữ thần' Mái tóc cũ của thành viên SNSD bị hư hỏng nặng nề và Yoon Ah thậm chí không muốn nuôi tóc lại.\", 'spos': [0, 158], 'entity_1': {'text': 'Yoon Ah', 'pos': [19, 26]}, 'entity_2': {'text': 'Yoon Ah', 'pos': [117, 124]}, 'entity_1_info': {'first_token': [5, 6, 1, 'PERSON'], 'last_token': [6, 7, 1, 'PERSON']}, 'entity_2_info': {'first_token': [28, 29, 2, 'PERSON'], 'last_token': [29, 30, 2, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 3, 'sentence': \"Đây là lý do khiến Yoon Ah quyết định cắt mái tóc dài 'nữ thần' Mái tóc cũ của thành viên SNSD bị hư hỏng nặng nề và Yoon Ah thậm chí không muốn nuôi tóc lại.\", 'spos': [0, 158], 'entity_1': {'text': 'SNSD', 'pos': [90, 94]}, 'entity_2': {'text': 'Yoon Ah', 'pos': [117, 124]}, 'entity_1_info': {'first_token': [21, 22, 0, 'ORGANIZATION'], 'last_token': [21, 22, 0, 'ORGANIZATION']}, 'entity_2_info': {'first_token': [28, 29, 2, 'PERSON'], 'last_token': [29, 30, 2, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 4, 'sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Soo Young', 'pos': [0, 9]}, 'entity_2': {'text': 'Seo Hyun', 'pos': [13, 21]}, 'entity_1_info': {'first_token': [258, 259, 8, 'PERSON'], 'last_token': [259, 260, 8, 'PERSON']}, 'entity_2_info': {'first_token': [261, 262, 9, 'PERSON'], 'last_token': [262, 263, 9, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 5, 'sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Soo Young', 'pos': [0, 9]}, 'entity_2': {'text': 'Yoon Ah', 'pos': [40, 47]}, 'entity_1_info': {'first_token': [258, 259, 8, 'PERSON'], 'last_token': [259, 260, 8, 'PERSON']}, 'entity_2_info': {'first_token': [267, 268, 10, 'PERSON'], 'last_token': [268, 269, 10, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 6, 'sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Soo Young', 'pos': [0, 9]}, 'entity_2': {'text': 'Sunny', 'pos': [51, 56]}, 'entity_1_info': {'first_token': [258, 259, 8, 'PERSON'], 'last_token': [259, 260, 8, 'PERSON']}, 'entity_2_info': {'first_token': [270, 271, 0, 'PERSON'], 'last_token': [270, 271, 0, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 7, 'sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Seo Hyun', 'pos': [13, 21]}, 'entity_2': {'text': 'Yoon Ah', 'pos': [40, 47]}, 'entity_1_info': {'first_token': [261, 262, 9, 'PERSON'], 'last_token': [262, 263, 9, 'PERSON']}, 'entity_2_info': {'first_token': [267, 268, 10, 'PERSON'], 'last_token': [268, 269, 10, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 8, 'sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Seo Hyun', 'pos': [13, 21]}, 'entity_2': {'text': 'Sunny', 'pos': [51, 56]}, 'entity_1_info': {'first_token': [261, 262, 9, 'PERSON'], 'last_token': [262, 263, 9, 'PERSON']}, 'entity_2_info': {'first_token': [270, 271, 0, 'PERSON'], 'last_token': [270, 271, 0, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 9, 'sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Yoon Ah', 'pos': [40, 47]}, 'entity_2': {'text': 'Sunny', 'pos': [51, 56]}, 'entity_1_info': {'first_token': [267, 268, 10, 'PERSON'], 'last_token': [268, 269, 10, 'PERSON']}, 'entity_2_info': {'first_token': [270, 271, 0, 'PERSON'], 'last_token': [270, 271, 0, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352906', 'sent_id': 10, 'sentence': 'Bày tỏ quan điểm cũng bực tức như Tu Dinh Huong , thành viên Pymini tiếp lời:\"Nhà mình mà phần cho người sau thì còn chọn toàn cái ngon để phần, chứ không có chuyện ăn uống bày bừa, mặc kệ người ăn sau như thế này”.', 'spos': [1110, 1325], 'entity_1': {'text': 'Tu Dinh Huong', 'pos': [34, 47]}, 'entity_2': {'text': 'Pymini', 'pos': [61, 67]}, 'entity_1_info': {'first_token': [261, 262, 2, 'PERSON'], 'last_token': [263, 264, 2, 'PERSON']}, 'entity_2_info': {'first_token': [267, 268, 0, 'PERSON'], 'last_token': [267, 268, 0, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352911', 'sent_id': 11, 'sentence': \"Ca sĩ Đăng Dương chơi đàn bầu trong liveshow 'Mặt trời của tôi' Nhân kỷ niệm 20 năm ca hát, ca sĩ Đăng Dương sẽ tổ chức liveshow có tựa đề “Mặt trời của tôi” được diễn ra trong 2 ngày 14, 15/10 tại Hà Nội .\", 'spos': [0, 206], 'entity_1': {'text': 'Đăng Dương', 'pos': [6, 16]}, 'entity_2': {'text': 'Đăng Dương', 'pos': [98, 108]}, 'entity_1_info': {'first_token': [2, 3, 1, 'PERSON'], 'last_token': [3, 4, 1, 'PERSON']}, 'entity_2_info': {'first_token': [22, 23, 2, 'PERSON'], 'last_token': [23, 24, 2, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352911', 'sent_id': 12, 'sentence': \"Ca sĩ Đăng Dương chơi đàn bầu trong liveshow 'Mặt trời của tôi' Nhân kỷ niệm 20 năm ca hát, ca sĩ Đăng Dương sẽ tổ chức liveshow có tựa đề “Mặt trời của tôi” được diễn ra trong 2 ngày 14, 15/10 tại Hà Nội .\", 'spos': [0, 206], 'entity_1': {'text': 'Đăng Dương', 'pos': [6, 16]}, 'entity_2': {'text': 'Hà Nội', 'pos': [198, 204]}, 'entity_1_info': {'first_token': [2, 3, 1, 'PERSON'], 'last_token': [3, 4, 1, 'PERSON']}, 'entity_2_info': {'first_token': [44, 45, 3, 'LOCATION'], 'last_token': [45, 46, 3, 'LOCATION']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352911', 'sent_id': 13, 'sentence': \"Ca sĩ Đăng Dương chơi đàn bầu trong liveshow 'Mặt trời của tôi' Nhân kỷ niệm 20 năm ca hát, ca sĩ Đăng Dương sẽ tổ chức liveshow có tựa đề “Mặt trời của tôi” được diễn ra trong 2 ngày 14, 15/10 tại Hà Nội .\", 'spos': [0, 206], 'entity_1': {'text': 'Đăng Dương', 'pos': [98, 108]}, 'entity_2': {'text': 'Hà Nội', 'pos': [198, 204]}, 'entity_1_info': {'first_token': [22, 23, 2, 'PERSON'], 'last_token': [23, 24, 2, 'PERSON']}, 'entity_2_info': {'first_token': [44, 45, 3, 'LOCATION'], 'last_token': [45, 46, 3, 'LOCATION']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352911', 'sent_id': 14, 'sentence': 'Trước khi theo học thanh nhạc và trở thành giọng ca chính thống hàng đầu Việt Nam , ca sĩ Đăng Dương từng có gần 10 năm theo học đàn bầu của nghệ sĩ Thanh Tâm (mẹ nhạc sĩ Hồ Hoài Anh ).', 'spos': [207, 392], 'entity_1': {'text': 'Việt Nam', 'pos': [73, 81]}, 'entity_2': {'text': 'Đăng Dương', 'pos': [90, 100]}, 'entity_1_info': {'first_token': [62, 63, 4, 'LOCATION'], 'last_token': [63, 64, 4, 'LOCATION']}, 'entity_2_info': {'first_token': [67, 68, 5, 'PERSON'], 'last_token': [68, 69, 5, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352911', 'sent_id': 15, 'sentence': 'Trước khi theo học thanh nhạc và trở thành giọng ca chính thống hàng đầu Việt Nam , ca sĩ Đăng Dương từng có gần 10 năm theo học đàn bầu của nghệ sĩ Thanh Tâm (mẹ nhạc sĩ Hồ Hoài Anh ).', 'spos': [207, 392], 'entity_1': {'text': 'Việt Nam', 'pos': [73, 81]}, 'entity_2': {'text': 'Thanh Tâm', 'pos': [149, 158]}, 'entity_1_info': {'first_token': [62, 63, 4, 'LOCATION'], 'last_token': [63, 64, 4, 'LOCATION']}, 'entity_2_info': {'first_token': [81, 82, 6, 'PERSON'], 'last_token': [82, 83, 6, 'PERSON']}, 'label': 'OTHERS'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0mBtqdZthaa"
      },
      "source": [
        "# Write to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTM1DITfqJtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc26eb3-6b47-4034-e668-ea5013cb11b8"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xLBzf43i3NC"
      },
      "source": [
        "# write to file\n",
        "import codecs\n",
        "import json\n",
        "\n",
        "with codecs.open('test_data_v2.json', 'w', encoding='utf-8') as fout:\n",
        "    json.dump(tdata, fout, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72boeFzXgq-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b071d885-6cdd-4a19-a75d-7c67b3ea8635"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B8JEv6kDzuV"
      },
      "source": [
        "!mkdir \"/gdrive/MyDrive/VLSP2020_RE\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ng7tJuLD4_V"
      },
      "source": [
        "!mkdir \"/gdrive/MyDrive/VLSP2020_RE/json_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU0dYmmshRFZ"
      },
      "source": [
        "!cp -i test_data_v2.json \"/gdrive/MyDrive/VLSP2020_RE/json_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eWnRTZYSwm2",
        "outputId": "ff989506-6fd2-450e-b9a4-92a3e450d78c"
      },
      "source": [
        "import filecmp\n",
        "filecmp.cmp('test_data_v2.json', '/gdrive/MyDrive/VLSP2020_RE/json_data/test_data_v2.json')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}
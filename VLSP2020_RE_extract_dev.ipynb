{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VLSP2020_RE_extract_dev_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "H3ICier0JyzI",
        "PcS3CiYZxv8a",
        "jhOuiCNgaVqk",
        "chNxSZ0mx5wS",
        "-wzBGIIgKYPD",
        "EYMqFdP5eTPL",
        "kBr6Gx-Y2oj2",
        "dfXerQf-fQSv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQDbz_q5irs0"
      },
      "source": [
        "# Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxh7foXeJtrf"
      },
      "source": [
        "## Unrar dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etKZzzHSF6_i"
      },
      "source": [
        "Please upload VLSP2020_RE_dev.rar to Colab then */content* folder then unrar it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXjtAzVJHNI_",
        "outputId": "108b2f62-ff97-40ce-df63-264f53757142"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6x3Nl_tfBf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cecfe03-7595-4be0-c476-afce6e3fed8c"
      },
      "source": [
        "!pip install unrar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unrar\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/0b/53130ccd483e3db8c8a460cb579bdb21b458d5494d67a261e1a5b273fbb9/unrar-0.4-py3-none-any.whl\n",
            "Installing collected packages: unrar\n",
            "Successfully installed unrar-0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnlpfnYxiSiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c473b773-ec98-4102-c459-7cf1279eaae5"
      },
      "source": [
        "!unrar x VLSP2020_RE_dev.rar"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from VLSP2020_RE_dev.rar\n",
            "\n",
            "Creating    VLSP2020_RE_dev                                           OK\n",
            "Creating    VLSP2020_RE_dev/23351996.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23351996.conll/CURATION_USER.tsv             \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23351997.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23351997.conll/CURATION_USER.tsv             \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23351998.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23351998.conll/CURATION_USER.tsv             \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352000.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352000.conll/CURATION_USER.tsv             \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352001.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352001.conll/CURATION_USER.tsv             \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352002.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352002.conll/CURATION_USER.tsv             \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352003.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352003.conll/CURATION_USER.tsv             \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352006.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352006.conll/CURATION_USER.tsv             \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352009.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352009.conll/CURATION_USER.tsv             \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352013.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352013.conll/CURATION_USER.tsv             \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352014.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv             \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352016.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352016.conll/CURATION_USER.tsv             \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352018.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352018.conll/CURATION_USER.tsv             \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352019.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352019.conll/CURATION_USER.tsv             \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352020.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352020.conll/CURATION_USER.tsv             \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352021.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352021.conll/CURATION_USER.tsv             \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352024.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352024.conll/CURATION_USER.tsv             \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352026.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352026.conll/CURATION_USER.tsv             \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352027.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352027.conll/CURATION_USER.tsv             \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352028.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352028.conll/CURATION_USER.tsv             \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352030.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352030.conll/CURATION_USER.tsv             \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352033.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352033.conll/CURATION_USER.tsv             \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352065.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352065.conll/CURATION_USER.tsv             \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352066.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352066.conll/CURATION_USER.tsv             \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352070.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352070.conll/CURATION_USER.tsv             \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352071.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352071.conll/CURATION_USER.tsv             \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352073.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352073.conll/CURATION_USER.tsv             \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352075.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352075.conll/CURATION_USER.tsv             \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352078.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352078.conll/CURATION_USER.tsv             \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352079.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352079.conll/CURATION_USER.tsv             \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352081.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352081.conll/CURATION_USER.tsv             \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352084.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352084.conll/CURATION_USER.tsv             \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352085.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352085.conll/CURATION_USER.tsv             \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352087.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352087.conll/CURATION_USER.tsv             \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352089.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352089.conll/CURATION_USER.tsv             \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352090.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352090.conll/CURATION_USER.tsv             \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352099.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352099.conll/CURATION_USER.tsv             \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352107.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352107.conll/CURATION_USER.tsv             \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352110.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352110.conll/CURATION_USER.tsv             \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352117.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352117.conll/CURATION_USER.tsv             \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352122.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352122.conll/CURATION_USER.tsv             \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352125.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352125.conll/CURATION_USER.tsv             \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352126.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352126.conll/CURATION_USER.tsv             \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352143.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352143.conll/CURATION_USER.tsv             \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352161.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352161.conll/CURATION_USER.tsv             \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352163.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352163.conll/CURATION_USER.tsv             \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352173.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352173.conll/CURATION_USER.tsv             \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352190.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352190.conll/CURATION_USER.tsv             \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352200.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352200.conll/CURATION_USER.tsv             \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352232.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352232.conll/CURATION_USER.tsv             \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352236.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352236.conll/CURATION_USER.tsv             \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352239.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352239.conll/CURATION_USER.tsv             \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352240.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352240.conll/CURATION_USER.tsv             \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352260.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352260.conll/CURATION_USER.tsv             \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352265.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352265.conll/CURATION_USER.tsv             \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352283.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352283.conll/CURATION_USER.tsv             \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352292.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352292.conll/CURATION_USER.tsv             \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352296.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352296.conll/CURATION_USER.tsv             \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352297.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352297.conll/CURATION_USER.tsv             \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352298.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352298.conll/CURATION_USER.tsv             \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352299.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352299.conll/CURATION_USER.tsv             \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352300.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352300.conll/CURATION_USER.tsv             \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352301.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352301.conll/CURATION_USER.tsv             \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352303.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352303.conll/CURATION_USER.tsv             \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352305.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352305.conll/CURATION_USER.tsv             \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352309.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352309.conll/CURATION_USER.tsv             \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352314.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352314.conll/CURATION_USER.tsv             \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352316.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352316.conll/CURATION_USER.tsv             \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352317.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352317.conll/CURATION_USER.tsv             \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352319.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352319.conll/CURATION_USER.tsv             \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352320.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352320.conll/CURATION_USER.tsv             \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352321.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352321.conll/CURATION_USER.tsv             \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352322.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352322.conll/CURATION_USER.tsv             \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352323.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352323.conll/CURATION_USER.tsv             \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352326.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352326.conll/CURATION_USER.tsv             \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352327.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352327.conll/CURATION_USER.tsv             \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352328.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352328.conll/CURATION_USER.tsv             \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352329.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352329.conll/CURATION_USER.tsv             \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352331.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352331.conll/CURATION_USER.tsv             \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352332.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352332.conll/CURATION_USER.tsv             \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352334.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352334.conll/CURATION_USER.tsv             \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352335.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352335.conll/CURATION_USER.tsv             \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352337.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352337.conll/CURATION_USER.tsv             \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352343.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352343.conll/CURATION_USER.tsv             \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352345.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352345.conll/CURATION_USER.tsv             \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352346.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352346.conll/CURATION_USER.tsv             \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352347.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352347.conll/CURATION_USER.tsv             \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352348.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352348.conll/CURATION_USER.tsv             \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352352.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352352.conll/CURATION_USER.tsv             \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352357.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352357.conll/CURATION_USER.tsv             \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352359.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352359.conll/CURATION_USER.tsv             \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352361.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352361.conll/CURATION_USER.tsv             \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352363.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352363.conll/CURATION_USER.tsv             \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352366.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352366.conll/CURATION_USER.tsv             \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352370.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352370.conll/CURATION_USER.tsv             \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352372.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352372.conll/CURATION_USER.tsv             \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352373.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352373.conll/CURATION_USER.tsv             \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352376.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352376.conll/CURATION_USER.tsv             \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352378.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352378.conll/CURATION_USER.tsv             \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352381.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352381.conll/CURATION_USER.tsv             \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352382.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352382.conll/CURATION_USER.tsv             \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352385.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352385.conll/CURATION_USER.tsv             \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352390.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352390.conll/CURATION_USER.tsv             \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352393.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352393.conll/CURATION_USER.tsv             \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352396.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352396.conll/CURATION_USER.tsv             \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352397.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352397.conll/CURATION_USER.tsv             \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352401.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352401.conll/CURATION_USER.tsv             \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352402.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352402.conll/CURATION_USER.tsv             \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352405.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352405.conll/CURATION_USER.tsv             \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352408.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352408.conll/CURATION_USER.tsv             \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352409.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352409.conll/CURATION_USER.tsv             \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352410.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352410.conll/CURATION_USER.tsv             \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352411.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352411.conll/CURATION_USER.tsv             \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352412.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352412.conll/CURATION_USER.tsv             \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352413.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352413.conll/CURATION_USER.tsv             \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352414.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352414.conll/CURATION_USER.tsv             \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352415.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352415.conll/CURATION_USER.tsv             \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352416.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352416.conll/CURATION_USER.tsv             \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352417.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352417.conll/CURATION_USER.tsv             \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352419.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352419.conll/CURATION_USER.tsv             \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352421.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352421.conll/CURATION_USER.tsv             \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352425.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352425.conll/CURATION_USER.tsv             \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352427.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352427.conll/CURATION_USER.tsv             \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352428.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352428.conll/CURATION_USER.tsv             \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352429.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352429.conll/CURATION_USER.tsv             \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352432.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352432.conll/CURATION_USER.tsv             \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352433.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352433.conll/CURATION_USER.tsv             \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352434.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352434.conll/CURATION_USER.tsv             \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352436.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352436.conll/CURATION_USER.tsv             \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352437.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352437.conll/CURATION_USER.tsv             \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352438.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352438.conll/CURATION_USER.tsv             \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352440.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352440.conll/CURATION_USER.tsv             \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352442.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352442.conll/CURATION_USER.tsv             \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352443.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352443.conll/CURATION_USER.tsv             \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352444.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352444.conll/CURATION_USER.tsv             \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352445.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352445.conll/CURATION_USER.tsv             \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352448.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352448.conll/CURATION_USER.tsv             \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352449.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352449.conll/CURATION_USER.tsv             \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352450.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352450.conll/CURATION_USER.tsv             \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352451.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352451.conll/CURATION_USER.tsv             \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352453.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352453.conll/CURATION_USER.tsv             \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352456.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352456.conll/CURATION_USER.tsv             \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352457.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352457.conll/CURATION_USER.tsv             \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352460.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352460.conll/CURATION_USER.tsv             \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352461.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352461.conll/CURATION_USER.tsv             \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352462.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352462.conll/CURATION_USER.tsv             \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352467.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352467.conll/CURATION_USER.tsv             \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352468.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352468.conll/CURATION_USER.tsv             \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352470.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352470.conll/CURATION_USER.tsv             \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352471.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352471.conll/CURATION_USER.tsv             \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352473.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352473.conll/CURATION_USER.tsv             \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352476.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352476.conll/CURATION_USER.tsv             \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352477.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352477.conll/CURATION_USER.tsv             \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352480.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352480.conll/CURATION_USER.tsv             \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352482.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352482.conll/CURATION_USER.tsv             \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352486.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352486.conll/CURATION_USER.tsv             \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352487.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352487.conll/CURATION_USER.tsv             \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352488.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352488.conll/CURATION_USER.tsv             \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352489.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352489.conll/CURATION_USER.tsv             \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352491.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv             \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352492.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352492.conll/CURATION_USER.tsv             \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352494.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352494.conll/CURATION_USER.tsv             \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352495.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352495.conll/CURATION_USER.tsv             \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352497.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352497.conll/CURATION_USER.tsv             \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352498.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352498.conll/CURATION_USER.tsv             \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352499.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv             \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352507.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352507.conll/CURATION_USER.tsv             \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352508.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352508.conll/CURATION_USER.tsv             \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352517.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv             \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352518.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352518.conll/CURATION_USER.tsv             \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352522.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352522.conll/CURATION_USER.tsv             \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352523.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352523.conll/CURATION_USER.tsv             \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352524.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352524.conll/CURATION_USER.tsv             \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352526.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352526.conll/CURATION_USER.tsv             \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352530.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352530.conll/CURATION_USER.tsv             \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352532.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352532.conll/CURATION_USER.tsv             \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352535.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352535.conll/CURATION_USER.tsv             \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352536.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352536.conll/CURATION_USER.tsv             \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352538.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352538.conll/CURATION_USER.tsv             \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352539.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352539.conll/CURATION_USER.tsv             \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352540.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352540.conll/CURATION_USER.tsv             \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352543.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352543.conll/CURATION_USER.tsv             \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352545.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352545.conll/CURATION_USER.tsv             \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352546.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352546.conll/CURATION_USER.tsv             \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352548.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352548.conll/CURATION_USER.tsv             \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352550.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352550.conll/CURATION_USER.tsv             \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352551.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352551.conll/CURATION_USER.tsv             \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352552.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352552.conll/CURATION_USER.tsv             \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352560.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352560.conll/CURATION_USER.tsv             \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352561.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352561.conll/CURATION_USER.tsv             \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352562.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352562.conll/CURATION_USER.tsv             \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352563.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352563.conll/CURATION_USER.tsv             \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352568.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352568.conll/CURATION_USER.tsv             \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352571.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352571.conll/CURATION_USER.tsv             \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352572.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352572.conll/CURATION_USER.tsv             \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352573.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352573.conll/CURATION_USER.tsv             \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352575.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352575.conll/CURATION_USER.tsv             \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352577.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352577.conll/CURATION_USER.tsv             \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352578.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352578.conll/CURATION_USER.tsv             \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352580.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352580.conll/CURATION_USER.tsv             \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352581.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352581.conll/CURATION_USER.tsv             \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352583.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352583.conll/CURATION_USER.tsv             \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352585.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352585.conll/CURATION_USER.tsv             \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352586.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352586.conll/CURATION_USER.tsv             \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352588.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352588.conll/CURATION_USER.tsv             \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352589.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352589.conll/CURATION_USER.tsv             \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352590.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352590.conll/CURATION_USER.tsv             \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352591.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352591.conll/CURATION_USER.tsv             \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352592.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352592.conll/CURATION_USER.tsv             \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352593.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352593.conll/CURATION_USER.tsv             \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352594.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352594.conll/CURATION_USER.tsv             \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352596.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352596.conll/CURATION_USER.tsv             \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352597.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352597.conll/CURATION_USER.tsv             \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352600.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352600.conll/CURATION_USER.tsv             \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352601.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352601.conll/CURATION_USER.tsv             \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352602.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352602.conll/CURATION_USER.tsv             \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352603.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352603.conll/CURATION_USER.tsv             \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352605.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv             \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352609.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352609.conll/CURATION_USER.tsv             \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352610.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352610.conll/CURATION_USER.tsv             \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352612.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352612.conll/CURATION_USER.tsv             \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352614.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352614.conll/CURATION_USER.tsv             \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352615.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352615.conll/CURATION_USER.tsv             \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352617.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352617.conll/CURATION_USER.tsv             \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352618.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352618.conll/CURATION_USER.tsv             \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352619.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352619.conll/CURATION_USER.tsv             \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352620.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352620.conll/CURATION_USER.tsv             \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352623.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352623.conll/CURATION_USER.tsv             \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352624.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352624.conll/CURATION_USER.tsv             \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352625.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352625.conll/CURATION_USER.tsv             \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352626.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352626.conll/CURATION_USER.tsv             \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352628.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352628.conll/CURATION_USER.tsv             \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352629.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352629.conll/CURATION_USER.tsv             \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352631.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352631.conll/CURATION_USER.tsv             \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352634.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352634.conll/CURATION_USER.tsv             \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352635.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352635.conll/CURATION_USER.tsv             \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352637.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352637.conll/CURATION_USER.tsv             \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352640.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352640.conll/CURATION_USER.tsv             \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352641.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352641.conll/CURATION_USER.tsv             \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352642.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352642.conll/CURATION_USER.tsv             \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352644.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352644.conll/CURATION_USER.tsv             \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352648.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352648.conll/CURATION_USER.tsv             \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352649.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352649.conll/CURATION_USER.tsv             \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352650.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352650.conll/CURATION_USER.tsv             \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352651.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352651.conll/CURATION_USER.tsv             \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352653.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352653.conll/CURATION_USER.tsv             \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352654.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv             \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352733.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352733.conll/CURATION_USER.tsv             \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352734.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352734.conll/CURATION_USER.tsv             \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352736.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352736.conll/CURATION_USER.tsv             \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3ICier0JyzI"
      },
      "source": [
        "## Install Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcS3CiYZxv8a"
      },
      "source": [
        "### Install VNCoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzsgsAb4uET6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ea2ac7-a9b8-4cc1-ef77-ae648aa2fcf5"
      },
      "source": [
        "# Install the vncorenlp python wrapper\n",
        "!pip install vncorenlp==1.0.3"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vncorenlp==1.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp==1.0.3) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (2.10)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp37-none-any.whl size=2645936 sha256=193f48b0db8b29cfead3cd048f0529f53579bfae5360f0ac5975ecded0805f11\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxVE9cR6yZ3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe410fa3-9ab6-499d-8632-5d7e8800e4e2"
      },
      "source": [
        "# Download VnCoreNLP-1.1.1.jar & all of its  component (i.e. RDRSegmenter, pos, ner, deprel) \n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!mkdir -p vncorenlp/models/dep\n",
        "!mkdir -p vncorenlp/models/ner\n",
        "!mkdir -p vncorenlp/models/postagger\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/dep/vi-dep.xz\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger\n",
        "\n",
        "\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "\n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/\n",
        "\n",
        "!mv vi-dep.xz vncorenlp/models/dep/\n",
        "\n",
        "!mv vi-500brownclusters.xz vncorenlp/models/ner/\n",
        "!mv vi-ner.xz vncorenlp/models/ner/\n",
        "!mv vi-pretrainedembeddings.xz vncorenlp/models/ner/\n",
        "\n",
        "!mv vi-tagger vncorenlp/models/postagger/\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-09 12:15:46--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M  59.5MB/s    in 0.4s    \n",
            "\n",
            "2021-05-09 12:15:47 (59.5 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2021-05-09 12:15:47--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-05-09 12:15:48 (12.2 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2021-05-09 12:15:48--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-05-09 12:15:48 (5.96 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n",
            "--2021-05-09 12:15:48--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/dep/vi-dep.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16048864 (15M) [application/octet-stream]\n",
            "Saving to: ‘vi-dep.xz’\n",
            "\n",
            "vi-dep.xz           100%[===================>]  15.30M  46.6MB/s    in 0.3s    \n",
            "\n",
            "2021-05-09 12:15:49 (46.6 MB/s) - ‘vi-dep.xz’ saved [16048864/16048864]\n",
            "\n",
            "--2021-05-09 12:15:49--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5599844 (5.3M) [application/octet-stream]\n",
            "Saving to: ‘vi-500brownclusters.xz’\n",
            "\n",
            "vi-500brownclusters 100%[===================>]   5.34M  28.6MB/s    in 0.2s    \n",
            "\n",
            "2021-05-09 12:15:49 (28.6 MB/s) - ‘vi-500brownclusters.xz’ saved [5599844/5599844]\n",
            "\n",
            "--2021-05-09 12:15:49--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9956876 (9.5M) [application/octet-stream]\n",
            "Saving to: ‘vi-ner.xz’\n",
            "\n",
            "vi-ner.xz           100%[===================>]   9.50M  21.1MB/s    in 0.5s    \n",
            "\n",
            "2021-05-09 12:15:50 (21.1 MB/s) - ‘vi-ner.xz’ saved [9956876/9956876]\n",
            "\n",
            "--2021-05-09 12:15:50--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 57313672 (55M) [application/octet-stream]\n",
            "Saving to: ‘vi-pretrainedembeddings.xz’\n",
            "\n",
            "vi-pretrainedembedd 100%[===================>]  54.66M  93.5MB/s    in 0.6s    \n",
            "\n",
            "2021-05-09 12:15:52 (93.5 MB/s) - ‘vi-pretrainedembeddings.xz’ saved [57313672/57313672]\n",
            "\n",
            "--2021-05-09 12:15:52--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29709468 (28M) [application/octet-stream]\n",
            "Saving to: ‘vi-tagger’\n",
            "\n",
            "vi-tagger           100%[===================>]  28.33M  79.6MB/s    in 0.4s    \n",
            "\n",
            "2021-05-09 12:15:52 (79.6 MB/s) - ‘vi-tagger’ saved [29709468/29709468]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXfVgT46BB-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7782ede-9618-4f02-c815-e11f8ecaf00f"
      },
      "source": [
        "import unicodedata\n",
        "from vncorenlp import VnCoreNLP\n",
        "\n",
        "# To perform word segmentation, POS tagging, NER and then dependency parsing\n",
        "annotator1 = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx2g') \n",
        "\n",
        "# To perform word segmentation, POS tagging and then NER\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg,pos,ner\", max_heap_size='-Xmx2g') \n",
        "# To perform word segmentation and then POS tagging\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg,pos\", max_heap_size='-Xmx2g') \n",
        "# To perform word segmentation only\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
        "# Input \n",
        "text = unicodedata.normalize(\"NFD\", \"Thanh Thủy\")\n",
        "\n",
        "\n",
        "# To perform word segmentation only\n",
        "word_segmented_text = annotator1.tokenize(text) \n",
        "\n",
        "print(*word_segmented_text, sep=\"\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Thanh', 'Thủy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhOuiCNgaVqk"
      },
      "source": [
        "### Install Underthesea"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IujpzlPaKfo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f25268-5d38-470e-c8a5-676a29e93b3f"
      },
      "source": [
        "!pip install underthesea==1.2.3"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting underthesea==1.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/46/1acb7e83092bbcbc9082afe3901ec51e98a303a19c8152655c43bd51583f/underthesea-1.2.3-py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (4.41.1)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (7.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (2.23.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (0.8.9)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (3.13)\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[?25hCollecting scikit-learn<0.22,>=0.20\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/c5/e5267eb84994e9a92a2c6a6ee768514f255d036f3c8378acfa694e9f2c99/scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 26.3MB/s \n",
            "\u001b[?25hCollecting python-crfsuite>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea==1.2.3) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (3.0.4)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval->underthesea==1.2.3) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.22,>=0.20->underthesea==1.2.3) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=8363720c301a5bb035a66c685c330f04e268b1dde7fec2d99638183458607c7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: unidecode, scikit-learn, seqeval, python-crfsuite, underthesea\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed python-crfsuite-0.9.7 scikit-learn-0.21.3 seqeval-1.2.2 underthesea-1.2.3 unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjsyeDFRKpfF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579a7025-4186-4fd3-b656-f6044157e36a"
      },
      "source": [
        "from underthesea import sent_tokenize\n",
        "text = 'Quảng Bình : Cát tặc lộng hành, hiểm họa rình rập cầu Long Đại và dòng sông? Hiện tượng khai thác cát “lậu” trên sông cách cầu Long Đại vài trăm mét về phía hạ lưu, khiến cầu và sông Long Đại đang đứng trước hiểm họa khó lường? Vừa qua Pháp luật Plus nhận phản ánh của những người dân sống ở xã Xuân Ninh , huyện Quảng Ninh , tỉnh Quảng Bình về việc hiện nay ở xã này, cụ thể là tại thôn Xuân Dục 1 khu vực ven sông Long Đại lâu này xuất hiện những bãi tập kết cát trái phép và hiện tượng khai thác cát “lậu” trên sông cả ngày lẫn đêm gây ảnh hưởng đến đến cuộc sống thường nhật của người dân nơi đây. Từ những nguồn tin nêu trên sáng ngày 21/9, PV đã tiếp cận hiện trường đoạn sông Long Đại thuộc thôn Xuân Dụ 1 , xã Xuân Ninh , huyện Quảng Ninh , tỉnh Quảng Bình nơi người dân phản ánh để củng cố thông tin. Tại đây, PV nhận thấy nhiều bãi tập kết cát gần khu vực dân cư sinh sống, hàng ngày nhiều tàu chở cát vào đây để tập kết cát, gây ra tiếng ồn khó chịu ảnh hưởng không nhỏ đến sinh hoạt của người dân. Những bãi tập kết cát trái phép. Hơn thế nữa theo tìm hiểu của PV được biết, những vị trí có bãi tập kết cát kể trên không đủ tiêu chuẩn để tàu cập bến tập kết?. Trưa cùng ngày PV đã đi theo hướng thượng nguồn sông Long Đại mà theo phản ánh là xảy ra tình trạng khai thác cát trái phép thường xuyên diễn ra. PV nhận thấy một chiếc thuyền đang neo đậu cách bờ chừng vài chục mét và cách móng cầu Long Đại chừng vài trăm mét theo hướng hạ nguồn đang hút cát lên thuyền. Chiếc thuyền (ô đỏ) đang khai thác cát trái phép cách cầu Long Đại không xa. Tiếp tục ghi nhận và theo dõi vụ việc khoảng chừng hơn 30 phút, chiếc thuyền đã đầy cát đã được đi chuyển đi tập kết. Chiếc thuyền sau khi hút cát trái phép di chuyển về bãi tập kết. Qua tìm hiểu của PV được biết, cát ở khu vực gần cầu Long Đại là cát nhiễm mặn nếu dùng vào việc thi công công trình sẽ ảnh hưởng đến chất lượng của công trình đó… Và cát này được chủ thuyền bán lại cho người sử dụng với giá rẻ hơn so với cát được khai thác ở mỏ được cấp phép gây nên sự cạnh tranh không lành mạnh về giá cát. Tuy nhiên nhiều người dân chưa nhận thấy đến việc chất lượng của công trình sau này khi sử dụng cát nhiễm mặn này. Điều đáng nói là việc khai thác cát trái phép lại diễn ra khu vực gần móng cầu Long Đại (cả đường sắt lẫn đường bộ) nguy cơ sạc lở đất khu vực móng cầu, khiến cầu Long Đại đứng trước hiểm họa khó lường?. Liên quan đến vấn đề này, trao đổi với PV ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh cho biết về phía xã cũng đã nhiều lần xử lý nhắc nhở người dân trong vấn đề tập kết cát đúng nơi quy định. “Ngoài ra, xã đang hướng dẫn và hoàn thành các thủ tục nhằm đưa các điểm tập kết trái phép này đúng vào nơi quy định trong thời gian sớm nhất”, ông Tiến cho biết thêm. Ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh (bên phải) tại buổi làm việc với PV. Khi được PV cũng cấp bằng chứng về việc thuyền khai thác cát trái phép ngay giữa ban ngày gần khu vực móng cầu Long Đại , ông Tiến đã hết sức bất ngờ nói “Như vậy là không được rồi, không được rồi… sẽ cho xử lý ngay” Tiếp đó PV liên lạc qua điện thoại với ông Phạm Trung Đông – Chủ tịch UBND huyện Quảng Ninh để phản ánh sự việc thì ông Đông cho biết, đang bận và hướng dẫn PV liên hệ với ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh để làm việc. Tại buổi là việc với ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh PV đã cung cấp clip về việc nạn khai thác trái phép diễn ra ngay trên sông Long Đại đoạn gần móng cầu ông Giai cũng đã kiên quyết và hứa sẽ đấu tranh xử lý, đồng thời phối hợp với các cơ quan chức năng khác thường xuyên kiểm tra để chấm dứt tình trạng này. Ông Nguyễn Viết Giai cho biết sẽ đấu tranh xử lý Còn về việc các bãi tập kết trái phép, ông Giai cho biết sẽ xử lý dứt điểm trong thời gian sớm nhất để không ảnh hưởng tới cuộc sống người dân xung quanh. Khi được PV hỏi thời gian sớm nhất là bao lâu ông Giai cho biết: “ở đây đang còn vướng một khâu thủ tục. thời gian giải quyết sớm nhất cũng phải mất chừng 7 đến 10 ngày.” Việc khai thác cát trái phép gần cầu Long Đại (cả đường sắt lẫn đường bộ) nguy cơ sạt lở đất khu vực móng cầu, khiến cầu Long Đại đứng trước hiểm họa khó lường? Tuy là vậy nhưng trong sáng 22/9, PV một lần nữa đến tại hiện trường chiếc thuyền khai thác trái phép thì nhận thấy tình hình khai thác cát trái phép vẫn không hề thay đổi. Một lần nữa PV đã gọi điện thoại cho ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh và ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh để phản ánh thì lại được 2 vị hứa “sẽ xử lý”. Trong sáng 22/9 việc khai thác cát trái phép vẫn diễn ra mà không có sự can thiệp của cơ quyan chức năng? Từ những việc nêu trên, dư luận không thể không đặt ra câu hỏi liệu những việc xảy ra ở đây có phải là có sự “bảo kê” hoặc có sự tiếp tay của lực lượng chức năng có thẩm thẩm quyền hay không? Pháp luật Plus sẽ tiếp tục thông tin vụ việc đến bạn đọc.'\n",
        "sent_tokenize(text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Quảng Bình : Cát tặc lộng hành, hiểm họa rình rập cầu Long Đại và dòng sông?',\n",
              " 'Hiện tượng khai thác cát “lậu” trên sông cách cầu Long Đại vài trăm mét về phía hạ lưu, khiến cầu và sông Long Đại đang đứng trước hiểm họa khó lường?',\n",
              " 'Vừa qua Pháp luật Plus nhận phản ánh của những người dân sống ở xã Xuân Ninh , huyện Quảng Ninh , tỉnh Quảng Bình về việc hiện nay ở xã này, cụ thể là tại thôn Xuân Dục 1 khu vực ven sông Long Đại lâu này xuất hiện những bãi tập kết cát trái phép và hiện tượng khai thác cát “lậu” trên sông cả ngày lẫn đêm gây ảnh hưởng đến đến cuộc sống thường nhật của người dân nơi đây.',\n",
              " 'Từ những nguồn tin nêu trên sáng ngày 21/9, PV đã tiếp cận hiện trường đoạn sông Long Đại thuộc thôn Xuân Dụ 1 , xã Xuân Ninh , huyện Quảng Ninh , tỉnh Quảng Bình nơi người dân phản ánh để củng cố thông tin.',\n",
              " 'Tại đây, PV nhận thấy nhiều bãi tập kết cát gần khu vực dân cư sinh sống, hàng ngày nhiều tàu chở cát vào đây để tập kết cát, gây ra tiếng ồn khó chịu ảnh hưởng không nhỏ đến sinh hoạt của người dân.',\n",
              " 'Những bãi tập kết cát trái phép.',\n",
              " 'Hơn thế nữa theo tìm hiểu của PV được biết, những vị trí có bãi tập kết cát kể trên không đủ tiêu chuẩn để tàu cập bến tập kết?.',\n",
              " 'Trưa cùng ngày PV đã đi theo hướng thượng nguồn sông Long Đại mà theo phản ánh là xảy ra tình trạng khai thác cát trái phép thường xuyên diễn ra.',\n",
              " 'PV nhận thấy một chiếc thuyền đang neo đậu cách bờ chừng vài chục mét và cách móng cầu Long Đại chừng vài trăm mét theo hướng hạ nguồn đang hút cát lên thuyền.',\n",
              " 'Chiếc thuyền (ô đỏ) đang khai thác cát trái phép cách cầu Long Đại không xa.',\n",
              " 'Tiếp tục ghi nhận và theo dõi vụ việc khoảng chừng hơn 30 phút, chiếc thuyền đã đầy cát đã được đi chuyển đi tập kết.',\n",
              " 'Chiếc thuyền sau khi hút cát trái phép di chuyển về bãi tập kết.',\n",
              " 'Qua tìm hiểu của PV được biết, cát ở khu vực gần cầu Long Đại là cát nhiễm mặn nếu dùng vào việc thi công công trình sẽ ảnh hưởng đến chất lượng của công trình đó… Và cát này được chủ thuyền bán lại cho người sử dụng với giá rẻ hơn so với cát được khai thác ở mỏ được cấp phép gây nên sự cạnh tranh không lành mạnh về giá cát.',\n",
              " 'Tuy nhiên nhiều người dân chưa nhận thấy đến việc chất lượng của công trình sau này khi sử dụng cát nhiễm mặn này.',\n",
              " 'Điều đáng nói là việc khai thác cát trái phép lại diễn ra khu vực gần móng cầu Long Đại (cả đường sắt lẫn đường bộ) nguy cơ sạc lở đất khu vực móng cầu, khiến cầu Long Đại đứng trước hiểm họa khó lường?.',\n",
              " 'Liên quan đến vấn đề này, trao đổi với PV ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh cho biết về phía xã cũng đã nhiều lần xử lý nhắc nhở người dân trong vấn đề tập kết cát đúng nơi quy định.',\n",
              " '“Ngoài ra, xã đang hướng dẫn và hoàn thành các thủ tục nhằm đưa các điểm tập kết trái phép này đúng vào nơi quy định trong thời gian sớm nhất”, ông Tiến cho biết thêm.',\n",
              " 'Ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh (bên phải) tại buổi làm việc với PV.',\n",
              " 'Khi được PV cũng cấp bằng chứng về việc thuyền khai thác cát trái phép ngay giữa ban ngày gần khu vực móng cầu Long Đại , ông Tiến đã hết sức bất ngờ nói “Như vậy là không được rồi, không được rồi… sẽ cho xử lý ngay” Tiếp đó PV liên lạc qua điện thoại với ông Phạm Trung Đông – Chủ tịch UBND huyện Quảng Ninh để phản ánh sự việc thì ông Đông cho biết, đang bận và hướng dẫn PV liên hệ với ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh để làm việc.',\n",
              " 'Tại buổi là việc với ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh PV đã cung cấp clip về việc nạn khai thác trái phép diễn ra ngay trên sông Long Đại đoạn gần móng cầu ông Giai cũng đã kiên quyết và hứa sẽ đấu tranh xử lý, đồng thời phối hợp với các cơ quan chức năng khác thường xuyên kiểm tra để chấm dứt tình trạng này.',\n",
              " 'Ông Nguyễn Viết Giai cho biết sẽ đấu tranh xử lý Còn về việc các bãi tập kết trái phép, ông Giai cho biết sẽ xử lý dứt điểm trong thời gian sớm nhất để không ảnh hưởng tới cuộc sống người dân xung quanh.',\n",
              " 'Khi được PV hỏi thời gian sớm nhất là bao lâu ông Giai cho biết: “ở đây đang còn vướng một khâu thủ tục.',\n",
              " 'thời gian giải quyết sớm nhất cũng phải mất chừng 7 đến 10 ngày.” Việc khai thác cát trái phép gần cầu Long Đại (cả đường sắt lẫn đường bộ) nguy cơ sạt lở đất khu vực móng cầu, khiến cầu Long Đại đứng trước hiểm họa khó lường?',\n",
              " 'Tuy là vậy nhưng trong sáng 22/9, PV một lần nữa đến tại hiện trường chiếc thuyền khai thác trái phép thì nhận thấy tình hình khai thác cát trái phép vẫn không hề thay đổi.',\n",
              " 'Một lần nữa PV đã gọi điện thoại cho ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh và ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh để phản ánh thì lại được 2 vị hứa “sẽ xử lý”.',\n",
              " 'Trong sáng 22/9 việc khai thác cát trái phép vẫn diễn ra mà không có sự can thiệp của cơ quyan chức năng?',\n",
              " 'Từ những việc nêu trên, dư luận không thể không đặt ra câu hỏi liệu những việc xảy ra ở đây có phải là có sự “bảo kê” hoặc có sự tiếp tay của lực lượng chức năng có thẩm thẩm quyền hay không?',\n",
              " 'Pháp luật Plus sẽ tiếp tục thông tin vụ việc đến bạn đọc.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chNxSZ0mx5wS"
      },
      "source": [
        "# Extract raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phjF0edvNGpc"
      },
      "source": [
        "import os\n",
        "import re"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pTphYOiMYho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304b4eea-c141-42e9-d8be-9e0ec48e28ca"
      },
      "source": [
        "# get all subfolers and files in subfolers\n",
        "# [(\"top_subfolders\", [subfolders_in_top_subfolders], [files_in_top_subfolders])]\n",
        "sub_folders = [f for f in os.walk(\"VLSP2020_RE_dev\")][1:]\n",
        "sub_folders = sorted(sub_folders, key=lambda x: x[0])   # sort by top_subfolder name\n",
        "\n",
        "## top subfolder contain only 1 single file.\n",
        "check = False\n",
        "for i in sub_folders:\n",
        "    if i[1] or len(i[-1])!= 1:\n",
        "        print(\"ALERT!!!\")\n",
        "        check = True\n",
        "\n",
        "if not check:\n",
        "    print(\"There is \", len(sub_folders), \" subfolders. All subfolders contain only 1 file.\",\n",
        "          \" So that we have \", len(sub_folders), \" files.\")\n",
        "\n",
        "# generate data files name\n",
        "files_path = [os.path.join(i[0], i[-1][0]) for i in sub_folders]\n",
        "\n",
        "# print(*files_path, sep=\"\\n\")\n",
        "\n",
        "# print(files_path)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is  250  subfolders. All subfolders contain only 1 file.  So that we have  250  files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LJk2k0vD0dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f96a01a-4250-4fa2-9a38-179f2208b55e"
      },
      "source": [
        "# Xem trong bộ dữ liệu có những character gì\n",
        "\n",
        "character_lst = []\n",
        "for file in files_path:\n",
        "    with open(file, mode='r') as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "        # find line start with \"#Text=\"\n",
        "        textline_id = []\n",
        "        for i, text in enumerate(lines):\n",
        "            if (\"#Text=\" == text[0:6]):\n",
        "                textline_id.append(i)\n",
        "\n",
        "        # every data file has only one line that start with \"#Text=\"\"\n",
        "        assert (len(textline_id) == 1), str(\"1 is not number of line start with #Text=. \\nDoc: \" + file)\n",
        "\n",
        "        for c in lines[textline_id[0]][6:]:\n",
        "            if c not in character_lst:\n",
        "                character_lst.append(c)\n",
        "\n",
        "\n",
        "# Print all of the single characters, 30 per row.\n",
        "# For every batch of 30 tokens...\n",
        "for i in range(0, len(character_lst), 30):\n",
        "    \n",
        "    # Limit the end index so we don't go past the end of the list.\n",
        "    end = min(i + 30, len(character_lst) + 1)\n",
        "    \n",
        "    # Print out the tokens, separated by a space.\n",
        "    print(repr(' '.join(character_lst[i:end])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"U 1 6   V i ệ t N a m d ộ ' ư g ô n v à o l ớ M C ổ K h ằ ự\"\n",
            "'đ á , ã c ó ế ắ ễ r . T ỷ s ố u 9 - 0 ậ y ạ b ả I ò â Á 2 8'\n",
            "'A ể H ẽ ầ p ứ ề 4 / q ặ ù ị ỡ ấ ờ ở ú ẫ ơ x 5 B ủ ý ọ k ă ợ'\n",
            "'ê 3 ( 7 ) ừ ỗ ữ ụ é ỏ ỉ S L ũ ồ õ R í + ì e : Đ “ ” ‘ ’ ẻ z'\n",
            "'G Z ử P J D ? X ẩ Q % | O Ả f – \" ẳ Ô ỹ \\xa0 ĩ \\u200b ẵ ỳ E W F ; w'\n",
            "'Â ẹ ỵ Ý j … & Ủ Y \\ufeff Ă Ở è Ê Ấ Ầ ! Ð * Ố Í ́ ̀ ̣ Ư Ọ Ộ Ú Ỳ Ó'\n",
            "'Ồ ñ Ệ [ ] ± ~ Ì Ế À Ậ Ẩ • Ể Ợ Ờ ö ü ̉ Ạ'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugs-iTcldECs"
      },
      "source": [
        "constant = {\"entity_name\": [\"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"MISCELLANEOUS\"],\n",
        "          \"relation_name\": [\"LOCATED\", \"PART – WHOLE\", \"AFFILIATION\", \"PERSONAL - SOCIAL\"]\n",
        "           }"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rqndrEtimOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b219873-d925-488d-f285-edbabd8bc2ed"
      },
      "source": [
        "\"\"\"\n",
        "            dict = {\"doc_id\": id of folder contain doc, \n",
        "                      \"text\": doc, \n",
        "                 \"token_ids\": [                                        tokens_id, ...], \n",
        "              \"subtoken_ids\": [                                   None or sub-id, ...],\n",
        "                       \"pos\": [                             [start pos, end pos], ...],\n",
        "                    \"tokens\": [                                      tokens_text, ...],\n",
        "                    \"entity\": [                        [entity_ids, entity_name], ...],\n",
        "                  \"relation\": [    [relation, stoken_id, ssubtoken_id direction], ...]\n",
        "                   }\n",
        "\n",
        "\n",
        "                      doc_id: id of folder contain doc                                            (str)\n",
        "                         doc: doc. line start with \"#Text=\"                                       (str)\n",
        "                   token_ids: ids of tokens.                                    first column       (list int)\n",
        "                subtoken_ids: int if crr token is a subtoken, otherwise None    first column      (list int, None)\n",
        "                         pos: posittion of tokens.                              second column     (list list int) \n",
        "                              [\n",
        "                                  [start pos, end pos],\n",
        "                                  ...\n",
        "                              ]\n",
        "                       token: tokens.                                           third column      (list str)\n",
        "                      entity: entity infor if token is entity, else None.       4th, 5th column   (list list, None)\n",
        "                              [\n",
        "                                  [entity_id, entity_name],\n",
        "                                  ...\n",
        "                              ]\n",
        "                    relation: relation if token is in a relation, else None.    other column      (list list, None)\n",
        "                              [\n",
        "                                  [[relation1_name, relation1_start_tokenID, relation1_start_subtokenID, [start_entity_id, end_entity_id]], ...],  \n",
        "                                                                         --> relation1_start_subtokenID may be None if start token is not a sub token\n",
        "                                  [[relation1_name, relation1_start_tokenID, relation1_start_subtokenID,                             None], ...],  <-- dataset has mistake. Don't have direction.\n",
        "                                                                                 \n",
        "                                  \n",
        "                                  ....\n",
        "                              ]\n",
        "                    \n",
        "\"\"\"\n",
        "\n",
        "import copy\n",
        "\n",
        "raw_tdata = []\n",
        "\n",
        "for file in files_path:\n",
        "    docif = {}\n",
        "    with open(file, mode='r') as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "        # example: VLSP2020_RE_training/23351113.conll/CURATION_USER.tsv -> 23351113\n",
        "        docif[\"doc_id\"] = copy.deepcopy(file[(file.find(\"/\") + 1): file.find(\".\")])\n",
        "\n",
        "        # find line start with \"#Text=\"\n",
        "        textline_id = []\n",
        "        for i, text in enumerate(lines):\n",
        "            if (\"#Text=\" == text[0:6]):\n",
        "                textline_id.append(i)\n",
        "\n",
        "        # every data file has only one line that start with \"#Text=\"\"\n",
        "        assert (len(textline_id) == 1), str(\"1 is not number of line start with #Text=. \\nDoc: \" + file)\n",
        "\n",
        "        docif[\"text\"] = copy.deepcopy(lines[textline_id[0]][6:])\n",
        "\n",
        "        first_cline = lines[(textline_id[0] + 1)].rstrip(\"\\t\").split(\"\\t\")   # first column_line\n",
        "        assert (len(first_cline) in [3, 5, 7, 8]), str(\"Doc has problem. doc: \" + file)\n",
        "\n",
        "\n",
        "        token_ids, subtoken_ids, pos, tokens = [], [], [], []\n",
        "        entity = []\n",
        "        relation = []\n",
        "\n",
        "        pretk_id = 0\n",
        "\n",
        "        for tk_id, line in enumerate(lines[(textline_id[0] + 1):]):\n",
        "            lineif = line.rstrip(\"\\t\").split(\"\\t\")   # seperate by one \\t between columns: [abc\\txyz\\t]\n",
        "\n",
        "            # check if columns is seperated by only one single Tab character '\\t'\n",
        "            lineif1 = re.split(r'\\t+', line.rstrip('\\t'))   # seperate by all \\t between column: [abc\\t\\t\\txyz\\t]\n",
        "            assert (lineif == lineif1), str(\"Columns is not seperated by only one single TAB '\\\\t'. doc: \" + file + \" line: \" + line)\n",
        "\n",
        "            # check if inside a doc, only exist one number of (no) columns\n",
        "            # above we check if len(lineif) in [3, 5, 7, 8], too. so we can make sure that\n",
        "            # in a doc, number of columns only in [3, 5, 7, 8]\n",
        "            # and all line in a doc has same no columns\n",
        "            assert len(lineif) == len(first_cline), str(\"Number of columns in doc is not consistent. \\nDoc: \" + file + \" line: \" + line)\n",
        "\n",
        "\n",
        "            # remove all \"_\" in lineif because we don't need it\n",
        "            # [3, 5, 7, 8] -> [3, 4, 5, 7]\n",
        "            # and all data has first three column. (4th and 5th) is a pair, (6th and 7th) is a pair\n",
        "            # after removing all \"_\", if:\n",
        "            # len(lineif) = 3 -> token_ids, pos, no entity, no relation\n",
        "            # len(lineif) = 5 -> token_ids, pos, entity, no relation\n",
        "            # len(lineif) = 7 -> token_ids, pos, entity, relation\n",
        "\n",
        "            # len(lineif) = 4 --> token_ids, pos, no entity, no relation (this is a mistake in dataset, in data file has 8 columns)\n",
        "\n",
        "            lineif = [col for col in lineif if col != \"_\"]\n",
        "\n",
        "            assert (len(lineif) in [3, 4, 5, 7]), str(\"Problem with number of columns after remove \\'_\\'.\\nIn doc: \" + file + \" line \" + line)\n",
        "\n",
        "            # match first column format\n",
        "            # startwith (\"1-\") then (number) end:   1-id\n",
        "            pattern_token_ids = re.compile(\"^(1-)([\\d]+)$\")\n",
        "\n",
        "            # a token may has many subtokens\n",
        "            # startwith (\"1-\") then (number) then (. char) then (number) end:   1-id.subid \n",
        "            # pattern_subtoken_ids = re.compile(\"^(1-)([\\d]+)(\\.)([\\d]+)$\")\n",
        "\n",
        "            # Currently in train dataset, number of subtoken of a token is 0 or 1\n",
        "            # startwith (\"1-\") then (number) then (.1) end:   1-id.1 \n",
        "            pattern_subtoken_ids = re.compile(\"^(1-)([\\d]+)(\\.1)$\")\n",
        "\n",
        "            assert (pattern_token_ids.match(lineif[0]) or pattern_subtoken_ids.match(lineif[0])), \\\n",
        "            str(\"Unexpected first column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "            if pattern_token_ids.match(lineif[0]):\n",
        "                # 1-id\n",
        "                # Check if token id is increased by one in each line or not.\n",
        "                assert (int(lineif[0][2:]) == (pretk_id + 1)), str(\"First column, Token_ID is not increased by one in each line. \\nIn doc: \" + file + \" \\nline: \" + line)\n",
        "\n",
        "                token_ids.append(int(lineif[0][2:]))\n",
        "                subtoken_ids.append(None)   # Not a subtoken\n",
        "\n",
        "                pretk_id += 1\n",
        "            \n",
        "            else:\n",
        "                # 1-id.1\n",
        "                # 1-id.subid\n",
        "                tmp = lineif[0].find(\".\")\n",
        "                tokenID = int(lineif[0][2:tmp])\n",
        "                subtokenID = int(lineif[0][(tmp+1):])\n",
        "                \n",
        "                assert (tokenID == token_ids[-1]), str(\"Exist subtoken without a token before it. \\nIn doc: \" + file + \" \\nline\" + line)\n",
        "\n",
        "                token_ids.append(tokenID)\n",
        "                subtoken_ids.append(subtokenID)\n",
        "\n",
        "                print(\"\\nTHERE IS A SUBTOKEN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "\n",
        "            # match second column format\n",
        "            # startwith (number) then (\"-\" char) then (number) end\n",
        "            pattern_pos = re.compile(\"^([\\d]+)(\\-)([\\d]+)$\")\n",
        "            assert (pattern_pos.match(lineif[1])), str(\"Unexpected second column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "            pos.append([int(ele) for ele in lineif[1].split(\"-\")])    # example: \"3-6\" -> [3, 6]\n",
        "            \n",
        "            # if current token is a subtoken, check if pos subtoken is inside father token or not.\n",
        "            if pattern_subtoken_ids.match(lineif[0]):\n",
        "                father_token = token_ids.index(token_ids[-1])\n",
        "\n",
        "                assert (pos[father_token][0] <= pos[-1][0]) and (pos[-1][1] <= pos[father_token][1]), \\\n",
        "                str(\"Subtoken\\'s position is not inside father token\\'s position. \\nIndoc: \" + file + \"\\Line: \" + line)\n",
        "\n",
        "\n",
        "            # third column\n",
        "            #check if token is matched with pos (second column) or not\n",
        "            crr_token_pos = [int(ele) for ele in lineif[1].split(\"-\")]\n",
        "            if lineif[2] == lines[textline_id[0]][6:][crr_token_pos[0]:crr_token_pos[1]]:\n",
        "                tokens.append(lineif[2])\n",
        "            else:\n",
        "                assert False, str(\"Token in 3th column not match with position at 2th column. \\nIn doc: \" + file + \" \\nline: \" + line)\n",
        "            \n",
        "\n",
        "            if (len(lineif) == 3) or (len(lineif) == 4):\n",
        "                entity.append(None)\n",
        "                relation.append(None)\n",
        "\n",
        "                if len(lineif) == 4:\n",
        "                    print(\"\\n4 COLUMNS.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "\n",
        "            # because we removed all \"_\", \n",
        "            # so when len(lineif) = 5 or len(lineif) = 7, this line must has: token_ids, pos, tokens and entity.\n",
        "            # (we don't have to check if 4th, 5th column is \"_\" anymore, since we removed all \"_\")\n",
        "            if (len(lineif) == 5) or (len(lineif) == 7):\n",
        "                # 4th column now only have two posibilities: \"*\" or \"*[number]\"\n",
        "                pattern_entity_id = re.compile(\"^(\\*)(\\[)([\\d]+)(\\])$\")\n",
        "                assert ((lineif[3] == \"*\") or pattern_entity_id.match(lineif[3])), str(\"Unexpected fourth column's format. In doc: \" + file + \" line \" + line)\n",
        "\n",
        "                # in doc: 23352816\n",
        "                # line: 1-23\t126-136\t</ENAMEX>)\t*\t*\t_\t_\t_\t\n",
        "                # there is a mistake in 5th column. Unknow enity name\n",
        "                # I will let this token entity is None.\n",
        "\n",
        "                # We can just let all token entity is None\n",
        "                # if 5th column is not in constant[\"entity_name\"]\n",
        "                # but below, I just code for this specific case\n",
        "                # because I want to know more about dataset\n",
        "\n",
        "                if (lineif[3] == \"*\"):\n",
        "\n",
        "                    if (lineif[4] == \"*\"):   # specific mistake case\n",
        "                        entity.append(None)\n",
        "                        print(\"\\nENTITY NAME MISTAKE IN 5TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                    else:\n",
        "                        assert (lineif[4] in constant[\"entity_name\"]), str(\"Unknown entity name. \\nDoc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "                        entity_id = 0\n",
        "                        entity_n = lineif[4]\n",
        "\n",
        "                        entity.append([entity_id, entity_n])\n",
        "                \n",
        "                elif pattern_entity_id.match(lineif[3]):\n",
        "                    # *[number]: *[26] -> 26\n",
        "                    entity_id = int(lineif[3][2:-1])\n",
        "                    \n",
        "                    # PERSON[26]\n",
        "                    tmp = lineif[4].find(\"[\")\n",
        "                    \n",
        "                    assert (entity_id == int(lineif[4][(tmp+1):-1])), str(\"Entity ID in 4th and 5th column are not the same. In doc: \" + file + \" line \" + line)\n",
        "                    \n",
        "                    assert (lineif[4][:tmp] in constant[\"entity_name\"]), str(\"Unknown entity name in doc: \" + file + \" line \" + line)\n",
        "                    \n",
        "                    entity_n = lineif[4][:tmp]\n",
        "\n",
        "                    entity.append([entity_id, entity_n])\n",
        "\n",
        "                # may be we dont need this last else because we use regex above\n",
        "                else:\n",
        "                    assert False, str(\"4th, 5th column has UNKNOWN MISTAKE. In Doc: \" + file + \"\\nline: \" + line)\n",
        "\n",
        "\n",
        "\n",
        "            if len(lineif) == 5:\n",
        "                relation.append(None)\n",
        "            \n",
        "\n",
        "            if len(lineif) == 7:\n",
        "                # example:\n",
        "                # AFFILIATION\t1-593[13_14]\n",
        "                # PART – WHOLE\t1-42[1_2]\n",
        "                # PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-80[3_8]|1-105[7_8]\n",
        "\n",
        "                # PART – WHOLE\t1-42    (an error in dataset that need to be handled)\n",
        "\n",
        "                rel_names = lineif[5].split(\"|\")    # PERSONAL - SOCIAL|PERSONAL - SOCIAL --> [\"PERSONAL - SOCIAL\", \"PERSONAL - SOCIAL\"]\n",
        "                rel_oifs = lineif[6].split(\"|\")     # 1-80[3_8]|1-105[7_8] --> [\"1-80[3_8]\", \"1-105[7_8]\"]\n",
        "\n",
        "                # in doc: 23351515\n",
        "                # line: 1-318\n",
        "                # 6th column: PART – WHOLE|LOCATED|PART – WHOLE|*\n",
        "                # last relation name is: *  -> mistake\n",
        "                # We can read data and change it to right one \n",
        "                # but I will remove this \"*\" relation in 6th and 7th column\n",
        "\n",
        "                if '*' in rel_names:\n",
        "                    tmp = rel_names.index('*')\n",
        "\n",
        "                    del rel_names[tmp]\n",
        "                    del rel_oifs[tmp]\n",
        "\n",
        "                    print(\"\\nRELATION MISTAKE IN 6TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                # MISTAKE In doc: 23351856\n",
        "                # line: 1-185\t807-812\tTriều\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
        "                # assert ((len(rel_names) == len(rel_oifs)) and (len(rel_names) >= 1)), str(\"Number of relations in 6th and 7th columns is different to each other. In doc: \" + file + \" line \" + line )\n",
        "                # handle later\n",
        "\n",
        "                assert (len(rel_names) == len(rel_oifs)), str(\"Number of relations in 6th and 7th columns is different to each other. \\nIn doc: \" + file + \" \\nline \" + line )\n",
        "\n",
        "\n",
        "                rels = []\n",
        "                for i in range(len(rel_names)):\n",
        "                    assert (rel_names[i] in constant[\"relation_name\"]), \\\n",
        "                    str(\"Unknown relation_name in doc: \" + file + \" \\nline \" + line)\n",
        "                    \n",
        "                    relation_n = rel_names[i]\n",
        "\n",
        "                    # (startwith \"1-\") then (number) then ([ char) then (number) then (_ char) then (number) then (] char) end\n",
        "                    #             1-         26            [             3             _             0             ]   \n",
        "                    pattern_relation_oif = re.compile(\"^(1-)([\\d]+)(\\[)([\\d]+)(\\_)([\\d]+)(\\])$\")\n",
        "\n",
        "                    # (startwith \"1-\") then (number) then (.1) then ([ char) then (number) then (_ char) then (number) then (] char) end\n",
        "                    #             1-         26            .1        [             3             _             0             ]   \n",
        "                    pattern_relation_oif_1 = re.compile(\"^(1-)([\\d]+)(\\.1)(\\[)([\\d]+)(\\_)([\\d]+)(\\])$\")\n",
        "                    \n",
        "                    # below is a mistake in dataset\n",
        "                    # but currently, this type mistake has only below form (only has token id).    <--- TRAIN DOES NOT HAVE, BUT DEV HAS\n",
        "                    # (don't have subtoken id mistake type, yet)   <--- TRAIN DOES NOT HAVE, BUT DEV HAS\n",
        "                    # (startwith \"1-\") then (number)  end\n",
        "                    #             1-         26          \n",
        "                    pattern_relation_oif_mistake_1 = re.compile(\"^(1-)([\\d]+)$\")\n",
        "\n",
        "\n",
        "                    # below is a mistake in dataset <--- subtoken id only\n",
        "                    # (startwith \"1-\") then (number) then (.1) end\n",
        "                    #             1-         26            .1\n",
        "                    pattern_relation_oif_mistake_2 = re.compile(\"^(1-)([\\d]+)(\\.1)$\")\n",
        "\n",
        "\n",
        "\n",
        "                    assert (pattern_relation_oif.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_1.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_mistake_1.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_mistake_2.match(rel_oifs[i])), \\\n",
        "                            str(\"Unexpected seventh column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "                    \n",
        "\n",
        "                    # NOTICE:\n",
        "                    # IN BELOW CODE, I DONT CHECK IF ONE OF TWO ENTITIES OF A RELATION\n",
        "                    # IS \"MISCELLANEOUS\" OR NOT. \n",
        "                    # MISCELLANEOUS IS A LEGIT ENTITY NAME, BUT IT IS NOT USED IN ANY RELATION TYPE.\n",
        "                    # I WONDER IF DATASET HAS THIS MISTAKE OR NOT.\n",
        "                    # I WILL CHECK IT WHEN I CREATE SENTENCES AS INPUT OF BERT.\n",
        "\n",
        "\n",
        "                    if pattern_relation_oif.match(rel_oifs[i]):\n",
        "                        # 1-id[id_id]\n",
        "\n",
        "                        tmp_stkid = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_etkid = rel_oifs[i].find(\"[\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp_stkid:tmp_etkid]\n",
        "\n",
        "                        # start subtoken id\n",
        "                        sstoken_id = None\n",
        "\n",
        "                        direction = rel_oifs[i][(tmp_etkid+1):-1]\n",
        "\n",
        "                        direction = direction.split(\"_\")   # [sentity_id, eentity_id]\n",
        "\n",
        "                        rels.append([relation_n, int(stoken_id), sstoken_id, [int(direction[0]), int(direction[1])]])\n",
        "\n",
        "                    elif pattern_relation_oif_1.match(rel_oifs[i]):\n",
        "                        # 1-id.subid[id_id]\n",
        "\n",
        "                        tmp_sid = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_eid = rel_oifs[i].find(\".\")\n",
        "\n",
        "                        tmp_ssid = rel_oifs[i].find(\".\") + 1\n",
        "                        tmp_esid = rel_oifs[i].find(\"[\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp_sid:tmp_eid]\n",
        "\n",
        "                        sstoken_id = rel_oifs[i][tmp_ssid:tmp_esid]\n",
        "\n",
        "\n",
        "                        direction = rel_oifs[i][(tmp_esid+1):-1]\n",
        "                        direction = direction.split(\"_\")\n",
        "\n",
        "                        rels.append([relation_n, int(stoken_id), int(sstoken_id), [int(direction[0]), int(direction[1])]])\n",
        "\n",
        "                        print(\"\\nSPECIAL SUBTOKEN IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "                    \n",
        "                    elif pattern_relation_oif_mistake_1.match(rel_oifs[i]):\n",
        "                        # 1-id\n",
        "                        tmp = rel_oifs[i].find(\"-\") + 1\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp:]\n",
        "                        \n",
        "                        sstoken_id = None\n",
        "                        direction = None\n",
        "\n",
        "                        # rels.append([relation_n, stoken_id, sstoken_id, direction])\n",
        "                        rels.append([relation_n, int(stoken_id), None, None])\n",
        "\n",
        "                        print(\"\\nMISTAKE IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "                    \n",
        "                    elif pattern_relation_oif_mistake_2.match(rel_oifs[i]):\n",
        "                        # 1-id.1\n",
        "                        tmp = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_1 = rel_oifs[i].find(\".1\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp:tmp_1]\n",
        "                        \n",
        "                        sstoken_id = 1\n",
        "                        direction = None\n",
        "\n",
        "                        # rels.append([relation_n, stoken_id, sstoken_id, direction])\n",
        "                        rels.append([relation_n, int(stoken_id), sstoken_id, None])\n",
        "\n",
        "                        print(\"\\nMISTAKE IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "                        print(stoken_id, ' - ', sstoken_id)\n",
        "                    \n",
        "                    else:\n",
        "                        assert False, str(\"Unexpected seventh column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "\n",
        "                \n",
        "                if len(rels) == 0:\n",
        "                    # MISTAKE In doc: 23351856\n",
        "                    # line: 1-185\t807-812\tTriều\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
        "                    relation.append(None)\n",
        "                    print(\"\\nREALTION NAME MISTAKE IN 6TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                else:\n",
        "                    relation.append(rels)\n",
        "\n",
        "\n",
        "        docif[\"token_ids\"] = copy.deepcopy(token_ids)\n",
        "        docif[\"subtoken_ids\"] = copy.deepcopy(subtoken_ids)\n",
        "        docif[\"pos\"] = copy.deepcopy(pos)\n",
        "        docif[\"tokens\"] = copy.deepcopy(tokens)\n",
        "        docif[\"entity\"] = copy.deepcopy(entity)\n",
        "        docif[\"relation\"] = copy.deepcopy(relation)\n",
        "\n",
        "    raw_tdata.append(copy.deepcopy(docif))\n",
        "\n",
        "\n",
        "print(len(raw_tdata))           \n",
        "                \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23351997.conll/CURATION_USER.tsv \n",
            "Line:  1-488\t2269-2273\tReal\t*\tORGANIZATION\tAFFILIATION\t1-490\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352002.conll/CURATION_USER.tsv \n",
            "Line:  1-240\t1117-1123\tTrường\t*\tPERSON\tPERSONAL - SOCIAL\t1-234\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352009.conll/CURATION_USER.tsv \n",
            "Line:  1-169\t759-762\tNga\t*\tLOCATION\tLOCATED\t1-170\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-3\t9-15\tMerkel\t*\tPERSON\tPERSONAL - SOCIAL\t1-1\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-15\t64-66\tMỹ\t*\tLOCATION\tAFFILIATION\t1-43\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-17\t69-72\tĐức\t*\tLOCATION\tAFFILIATION\t1-45\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-222\t1004-1007\tĐức\t*\tLOCATION\tAFFILIATION\t1-213\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-275\t1242-1244\tMỹ\t*\tLOCATION\tAFFILIATION|AFFILIATION|AFFILIATION\t1-271|1-294[6_0]|1-298[7_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-387\t1740-1742\tMỹ\t*\tLOCATION\tAFFILIATION\t1-375\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-444\t2000-2002\tMỹ\t*\tLOCATION\tAFFILIATION\t1-441\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-563\t2524-2526\tMỹ\t*\tLOCATION\tAFFILIATION\t1-590\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-627\t2803-2805\tMỹ\t*\tLOCATION\tAFFILIATION\t1-625\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-915\t4096-4098\tMỹ\t*\tLOCATION\tAFFILIATION\t1-907\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-1012\t4546-4549\tĐức\t*\tLOCATION\tAFFILIATION\t1-1021\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352024.conll/CURATION_USER.tsv \n",
            "Line:  1-1215\t5433-5442\tOceanbank\t*\tORGANIZATION\tAFFILIATION\t1-1212\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352024.conll/CURATION_USER.tsv \n",
            "Line:  1-1248\t5579-5582\tPVN\t*\tORGANIZATION\tAFFILIATION\t1-1243\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352024.conll/CURATION_USER.tsv \n",
            "Line:  1-1325\t5930-5933\tSơn\t*\tPERSON\tPERSONAL - SOCIAL\t1-1320\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352024.conll/CURATION_USER.tsv \n",
            "Line:  1-1856\t8271-8280\tOceanbank\t*\tORGANIZATION\tAFFILIATION\t1-1849\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352024.conll/CURATION_USER.tsv \n",
            "Line:  1-2198\t9783-9792\tOceanbank\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-2201[63_0]|1-2194\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352024.conll/CURATION_USER.tsv \n",
            "Line:  1-2426\t10789-10792\tSơn\t*\tPERSON\tPERSONAL - SOCIAL\t1-2419\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352027.conll/CURATION_USER.tsv \n",
            "Line:  1-59\t293-301\tal-Qaeda\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE\t1-49[4_0]|1-52|1-54[5_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352027.conll/CURATION_USER.tsv \n",
            "Line:  1-60\t302-307\tSyria\t*\tLOCATION\tLOCATED\t1-59\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352033.conll/CURATION_USER.tsv \n",
            "Line:  1-982\t4571-4575\tPC67\t*\tORGANIZATION\tAFFILIATION\t1-965\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352065.conll/CURATION_USER.tsv \n",
            "Line:  1-824\t3986-3994\tLebannon\t*\tLOCATION\tLOCATED\t1-822\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352066.conll/CURATION_USER.tsv \n",
            "Line:  1-55\t250-254\tNhật\t*\tLOCATION\tAFFILIATION\t1-57\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352090.conll/CURATION_USER.tsv \n",
            "Line:  1-22\t95-97\tMỹ\t*\tLOCATION\tPART – WHOLE|LOCATED\t1-20|1-16[1_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352099.conll/CURATION_USER.tsv \n",
            "Line:  1-109\t459-462\tĐạt\t*\tPERSON\tPERSONAL - SOCIAL\t1-125\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352122.conll/CURATION_USER.tsv \n",
            "Line:  1-128\t575-579\tUBKT\t*[10]\tORGANIZATION[10]\t*\t1-111[8_10]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352122.conll/CURATION_USER.tsv \n",
            "Line:  1-128\t575-579\tUBKT\t*[10]\tORGANIZATION[10]\t*\t1-111[8_10]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352126.conll/CURATION_USER.tsv \n",
            "Line:  1-327\t1470-1472\tTứ\t*\tPERSON\tPERSONAL - SOCIAL\t1-324\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352126.conll/CURATION_USER.tsv \n",
            "Line:  1-425\t1905-1910\tGiang\t*\tPERSON\tPERSONAL - SOCIAL\t1-418\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352161.conll/CURATION_USER.tsv \n",
            "Line:  1-115\t512-516\tTổng\t*[14]\tORGANIZATION[14]\t*\t1-96[12_14]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352161.conll/CURATION_USER.tsv \n",
            "Line:  1-115\t512-516\tTổng\t*[14]\tORGANIZATION[14]\t*\t1-96[12_14]\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352190.conll/CURATION_USER.tsv \n",
            "Line:  1-281\t1275-1278\tSài\t*[25]\tLOCATION[25]\t*\t1-273[24_25]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352190.conll/CURATION_USER.tsv \n",
            "Line:  1-281\t1275-1278\tSài\t*[25]\tLOCATION[25]\t*\t1-273[24_25]\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352190.conll/CURATION_USER.tsv \n",
            "Line:  1-399\t1830-1835\tPhòng\t*[35]\tORGANIZATION[35]\t*\t1-388[34_35]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352190.conll/CURATION_USER.tsv \n",
            "Line:  1-399\t1830-1835\tPhòng\t*[35]\tORGANIZATION[35]\t*\t1-388[34_35]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352200.conll/CURATION_USER.tsv \n",
            "Line:  1-100\t468-470\tMU\t*\tORGANIZATION\tAFFILIATION\t1-93\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352331.conll/CURATION_USER.tsv \n",
            "Line:  1-19\t88-94\tBrazil\t*\tLOCATION\tAFFILIATION\t1-14\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352331.conll/CURATION_USER.tsv \n",
            "Line:  1-81\t393-397\tReal\t*\tORGANIZATION\tAFFILIATION\t1-79\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352337.conll/CURATION_USER.tsv \n",
            "Line:  1-152\t688-694\t/TTXVN\t*\tORGANIZATION\tPART – WHOLE\t1-151\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352337.conll/CURATION_USER.tsv \n",
            "Line:  1-224\t1012-1014\tEU\t*\tORGANIZATION\tPART – WHOLE\t1-217\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352337.conll/CURATION_USER.tsv \n",
            "Line:  1-436\t1938-1944\tBerlin\t*\tLOCATION\tLOCATED|LOCATED\t1-429|1-446[16_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352337.conll/CURATION_USER.tsv \n",
            "Line:  1-464\t2065-2069\tAlps\t*\tLOCATION\tLOCATED\t1-429\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352337.conll/CURATION_USER.tsv \n",
            "Line:  1-598\t2659-2661\tEU\t*\tORGANIZATION\tAFFILIATION\t1-592\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352337.conll/CURATION_USER.tsv \n",
            "Line:  1-935\t4186-4193\tHamburg\t*\tLOCATION\tAFFILIATION\t1-938\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352346.conll/CURATION_USER.tsv \n",
            "Line:  1-378\t1785-1794\tCampuchia\t*\tLOCATION\tLOCATED\t1-368\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352357.conll/CURATION_USER.tsv \n",
            "Line:  1-283\t1288-1293\tTuyết\t*\tPERSON\tPERSONAL - SOCIAL\t1-267\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352361.conll/CURATION_USER.tsv \n",
            "Line:  1-412\t1854-1863\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-420\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352361.conll/CURATION_USER.tsv \n",
            "Line:  1-497\t2237-2246\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-490\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352393.conll/CURATION_USER.tsv \n",
            "Line:  1-71\t354-360\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED\t1-68[2_0]|1-64\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352393.conll/CURATION_USER.tsv \n",
            "Line:  1-91\t444-453\tSingapore\t*\tLOCATION\tLOCATED\t1-64\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352393.conll/CURATION_USER.tsv \n",
            "Line:  1-139\t675-679\tQ.10\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE\t1-137|1-123[4_0]|1-131[5_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352393.conll/CURATION_USER.tsv \n",
            "Line:  1-141\t682-688\tTP.HCM\t*\tLOCATION\tLOCATED|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-123[4_0]|1-131[5_0]|1-137|1-139\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352393.conll/CURATION_USER.tsv \n",
            "Line:  1-141\t682-688\tTP.HCM\t*\tLOCATION\tLOCATED|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-123[4_0]|1-131[5_0]|1-137|1-139\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352396.conll/CURATION_USER.tsv \n",
            "Line:  1-394\t1705-1714\t/Vietnam+\t*\tORGANIZATION\tPART – WHOLE\t1-393\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352402.conll/CURATION_USER.tsv \n",
            "Line:  1-516\t2337-2339\tFA\t*\tORGANIZATION\tAFFILIATION\t1-525\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352410.conll/CURATION_USER.tsv \n",
            "Line:  1-338\t1467-1470\tPVN\t*\tORGANIZATION\tAFFILIATION\t1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352410.conll/CURATION_USER.tsv \n",
            "Line:  1-340\t1475-1484\tOceanBank\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-338|1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352410.conll/CURATION_USER.tsv \n",
            "Line:  1-340\t1475-1484\tOceanBank\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-338|1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352410.conll/CURATION_USER.tsv \n",
            "Line:  1-366\t1599-1602\tPVN\t*\tORGANIZATION\tAFFILIATION\t1-351\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352410.conll/CURATION_USER.tsv \n",
            "Line:  1-368\t1607-1616\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-351\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_dev/23352414.conll/CURATION_USER.tsv \n",
            "Line:  1-413.1\t1850-1852\tBộ\t*[19]\tORGANIZATION[19]\tAFFILIATION|AFFILIATION\t1-417.1[20_19]|1-423[21_19]\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352414.conll/CURATION_USER.tsv \n",
            "Line:  1-413.1\t1850-1852\tBộ\t*[19]\tORGANIZATION[19]\tAFFILIATION|AFFILIATION\t1-417.1[20_19]|1-423[21_19]\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_dev/23352414.conll/CURATION_USER.tsv \n",
            "Line:  1-417.1\t1872-1878\tNguyễn\t*[20]\tPERSON[20]\t_\t_\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352437.conll/CURATION_USER.tsv \n",
            "Line:  1-263\t1244-1247\tAnh\t*\tLOCATION\tLOCATED\t1-259\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352445.conll/CURATION_USER.tsv \n",
            "Line:  1-44\t201-203\tMU\t*\tORGANIZATION\tAFFILIATION\t1-47\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352445.conll/CURATION_USER.tsv \n",
            "Line:  1-134\t587-589\tMU\t*\tORGANIZATION\tAFFILIATION\t1-126\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352445.conll/CURATION_USER.tsv \n",
            "Line:  1-175\t763-765\tMU\t*\tORGANIZATION\tAFFILIATION\t1-162\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352445.conll/CURATION_USER.tsv \n",
            "Line:  1-198\t860-870\t(Indonesia\t*\tLOCATION\tPART – WHOLE\t1-197\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352457.conll/CURATION_USER.tsv \n",
            "Line:  1-19\t102-108\tParana\t*\tLOCATION\tPART – WHOLE|AFFILIATION\t1-17|1-11[1_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352457.conll/CURATION_USER.tsv \n",
            "Line:  1-21\t111-117\tBrazil\t*\tLOCATION\tAFFILIATION|PART – WHOLE|PART – WHOLE\t1-11[1_0]|1-19|1-17\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352457.conll/CURATION_USER.tsv \n",
            "Line:  1-21\t111-117\tBrazil\t*\tLOCATION\tAFFILIATION|PART – WHOLE|PART – WHOLE\t1-11[1_0]|1-19|1-17\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352461.conll/CURATION_USER.tsv \n",
            "Line:  1-229\t1081-1088\tSonbong\t*\tLOCATION\tPART – WHOLE\t1-227\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352470.conll/CURATION_USER.tsv \n",
            "Line:  1-272\t1207-1210\tHòa\t*\tPERSON\tPERSONAL - SOCIAL\t1-268\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352482.conll/CURATION_USER.tsv \n",
            "Line:  1-120\t545-548\tAnh\t*\tLOCATION\tLOCATED\t1-121\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352482.conll/CURATION_USER.tsv \n",
            "Line:  1-982\t4359-4361\tÚc\t*\tLOCATION\tLOCATED\t1-977\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352488.conll/CURATION_USER.tsv \n",
            "Line:  1-546\t2371-2376\tThắng\t*\tPERSON\tPERSONAL - SOCIAL\t1-543\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352488.conll/CURATION_USER.tsv \n",
            "Line:  1-551\t2394-2397\tNam\t*\tPERSON\tPERSONAL - SOCIAL\t1-554\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352489.conll/CURATION_USER.tsv \n",
            "Line:  1-91\t412-416\tReal\t*\tORGANIZATION\tAFFILIATION\t1-110\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352489.conll/CURATION_USER.tsv \n",
            "Line:  1-217\t976-983\tAsensio\t*\tPERSON\tPERSONAL - SOCIAL\t1-207\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-61\t282-285\tĐức\t*\tLOCATION\tAFFILIATION\t1-58\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-236\t1040-1043\tĐức\t*\tLOCATION\tAFFILIATION\t1-256\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-364\t1606-1609\tĐức\t*\tLOCATION\tAFFILIATION\t1-357\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-471\t2095-2098\tCDU\t*\tORGANIZATION\t*\t1-456\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-471\t2095-2098\tCDU\t*\tORGANIZATION\t*\t1-456\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-584\t2606-2609\tĐức\t*\tLOCATION\tPART – WHOLE\t1-582\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-711\t3187-3190\tCDU\t*\tORGANIZATION\tAFFILIATION\t1-704\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-742\t3324-3327\t/AP\t*\tORGANIZATION\tPART – WHOLE\t1-741\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-752\t3370-3373\tĐức\t*\tLOCATION\tPART – WHOLE\t1-745\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-839\t3771-3774\tĐức\t*\tLOCATION\tPART – WHOLE|LOCATED\t1-837|1-833[25_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-845\t3803-3808\tSauer\t*\tPERSON\tPERSONAL - SOCIAL\t1-843\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-881\t3972-3975\tĐức\t*\tLOCATION\tPART – WHOLE\t1-879\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-910\t4117-4119\tMỹ\t*\tLOCATION\tAFFILIATION\t1-911\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-1023\t4613-4615\tEU\t*\tORGANIZATION\tAFFILIATION\t1-1036\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-1136\t5133-5136\tĐức\t*\tLOCATION\tPART – WHOLE\t1-1134\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-1157\t5225-5228\tĐức\t*\tORGANIZATION\tAFFILIATION\t1-1144\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352495.conll/CURATION_USER.tsv \n",
            "Line:  1-16\t88-89\tÝ\t*\tLOCATION\tAFFILIATION|AFFILIATION\t1-8[1_0]|1-10\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv \n",
            "Line:  1-150\t672-674\tMỹ\t*\tLOCATION\tPART – WHOLE\t1-154\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv \n",
            "Line:  1-338\t1546-1551\tSyria\t*\tLOCATION\tLOCATED\t1-336\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv \n",
            "Line:  1-369\t1692-1695\tNga\t*\tLOCATION\tPART – WHOLE\t1-368\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv \n",
            "Line:  1-594\t2713-2716\tNga\t*\tLOCATION\tPART – WHOLE\t1-593\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv \n",
            "Line:  1-628\t2874-2876\tMỹ\t*\tLOCATION\tPART – WHOLE\t1-629\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv \n",
            "Line:  1-687\t3131-3134\tHTS\t*\tORGANIZATION\tAFFILIATION\t1-679\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv \n",
            "Line:  1-689\t3139-3144\tIdlid\t*\tLOCATION\tLOCATED\t1-687\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-177\t767-774\tChelsea\t*\tORGANIZATION\tAFFILIATION\t1-188\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-194\t855-857\tMU\t*\tORGANIZATION\tAFFILIATION|*\t1-208|1-210\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-194\t855-857\tMU\t*\tORGANIZATION\tAFFILIATION|*\t1-208|1-210\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-516\t2339-2346\tChelsea\t*\tORGANIZATION\tAFFILIATION|AFFILIATION|AFFILIATION\t1-538|1-530|1-532\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-516\t2339-2346\tChelsea\t*\tORGANIZATION\tAFFILIATION|AFFILIATION|AFFILIATION\t1-538|1-530|1-532\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-516\t2339-2346\tChelsea\t*\tORGANIZATION\tAFFILIATION|AFFILIATION|AFFILIATION\t1-538|1-530|1-532\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-538\t2446-2452\tMorata\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-532|1-530\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-538\t2446-2452\tMorata\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-532|1-530\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352543.conll/CURATION_USER.tsv \n",
            "Line:  1-494\t2295-2300\tRaisa\t*\tPERSON\tPERSONAL - SOCIAL\t1-492\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352550.conll/CURATION_USER.tsv \n",
            "Line:  1-287\t1353-1359\tZidane\t*\tPERSON\tPERSONAL - SOCIAL\t1-282\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352550.conll/CURATION_USER.tsv \n",
            "Line:  1-581\t2736-2741\tStoke\t*\tORGANIZATION\t*\t1-567[23_0]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352550.conll/CURATION_USER.tsv \n",
            "Line:  1-581\t2736-2741\tStoke\t*\tORGANIZATION\t*\t1-567[23_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352551.conll/CURATION_USER.tsv \n",
            "Line:  1-178\t812-818\tBrazil\t*\tLOCATION\tLOCATED\t1-171\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352563.conll/CURATION_USER.tsv \n",
            "Line:  1-283\t1263-1269\tMálaga\t*\tLOCATION\tPART – WHOLE\t1-280\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352585.conll/CURATION_USER.tsv \n",
            "Line:  1-207\t963-967\tPháp\t*\tPERSON\tAFFILIATION\t1-208\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352586.conll/CURATION_USER.tsv \n",
            "Line:  1-170\t809-813\t(VAS\t*\tORGANIZATION\t*\t1-156[10_0]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352586.conll/CURATION_USER.tsv \n",
            "Line:  1-170\t809-813\t(VAS\t*\tORGANIZATION\t*\t1-156[10_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352593.conll/CURATION_USER.tsv \n",
            "Line:  1-125\t582-589\tChelsea\t*\tORGANIZATION\tAFFILIATION\t1-130\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352593.conll/CURATION_USER.tsv \n",
            "Line:  1-133\t618-626\tAtletico\t*\tORGANIZATION\tAFFILIATION\t1-130\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352593.conll/CURATION_USER.tsv \n",
            "Line:  1-206\t946-954\tAtletico\t*\tORGANIZATION\tAFFILIATION\t1-198\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352593.conll/CURATION_USER.tsv \n",
            "Line:  1-210\t966-973\tChelsea\t*\tORGANIZATION\tAFFILIATION\t1-198\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_dev/23352600.conll/CURATION_USER.tsv \n",
            "Line:  1-91.1\t411-413\tVũ\t*[9]\tPERSON[9]\t_\t_\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352600.conll/CURATION_USER.tsv \n",
            "Line:  1-98\t442-446\tThái\t*[10]\tLOCATION[10]\tAFFILIATION\t1-91.1[9_10]\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352600.conll/CURATION_USER.tsv \n",
            "Line:  1-112\t494-498\tPhát\t*[11]\tLOCATION[11]\tLOCATED\t1-91.1[9_11]\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352600.conll/CURATION_USER.tsv \n",
            "Line:  1-115\t506-510\tNinh\t*[12]\tLOCATION[12]\tPART – WHOLE|LOCATED\t1-112[11_12]|1-91.1[9_12]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352601.conll/CURATION_USER.tsv \n",
            "Line:  1-344\t1631-1637\tItalia\t*\tLOCATION\tAFFILIATION\t1-333\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352602.conll/CURATION_USER.tsv \n",
            "Line:  1-297\t1361-1368\tL'Oreal\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE\t1-306|1-308|1-310[4_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352602.conll/CURATION_USER.tsv \n",
            "Line:  1-297\t1361-1368\tL'Oreal\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE\t1-306|1-308|1-310[4_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352602.conll/CURATION_USER.tsv \n",
            "Line:  1-341\t1577-1581\tPháp\t*\tLOCATION\tPART – WHOLE|LOCATED\t1-339|1-322[5_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-155\t676-679\tĐức\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-151|1-153[7_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-242\t1056-1059\tĐức\t*\tLOCATION\tAFFILIATION|PART – WHOLE\t1-257[12_0]|1-250\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-299\t1311-1314\tĐức\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-278|1-279\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-299\t1311-1314\tĐức\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-278|1-279\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-336\t1482-1484\tEU\t*\tORGANIZATION\tPART – WHOLE\t1-333\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-387\t1717-1719\tEU\t*\tORGANIZATION\tAFFILIATION\t1-370\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-421\t1863-1866\tCDU\t*\tORGANIZATION\t*\t1-427[14_0]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-421\t1863-1866\tCDU\t*\tORGANIZATION\t*\t1-427[14_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-758\t3340-3343\tSPD\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE\t1-780|1-781\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-758\t3340-3343\tSPD\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE\t1-780|1-781\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-774\t3414-3417\tĐức\t*\tLOCATION\tPART – WHOLE\t1-758\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-829\t3658-3661\tCDU\t*\tORGANIZATION\tPART – WHOLE\t1-819\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-830\t3662-3666\t/CSU\t*\tORGANIZATION\t*\t1-819\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-830\t3662-3666\t/CSU\t*\tORGANIZATION\t*\t1-819\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-957\t4236-4239\tĐức\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-960[20_0]|1-965|1-972[21_0]|1-974[22_0]|1-978[23_0]\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-1152.1\t5052-5054\tFN\t*\tORGANIZATION\t_\t_\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-1160\t5086-5090\tPháp\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-1147[29_0]|1-1152.1\t\n",
            "1152  -  1\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-1310\t5731-5734\tĐức\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-1296|1-1297\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-1310\t5731-5734\tĐức\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-1296|1-1297\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-1499\t6540-6542\tEU\t*\tORGANIZATION\tAFFILIATION\t1-1516\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352614.conll/CURATION_USER.tsv \n",
            "Line:  1-111\t487-490\tNga\t*\tLOCATION\tLOCATED\t1-109\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352615.conll/CURATION_USER.tsv \n",
            "Line:  1-334\t1530-1537\tBigbang\t*\tORGANIZATION\tAFFILIATION\t1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352617.conll/CURATION_USER.tsv \n",
            "Line:  1-338\t1597-1606\tAmsterdam\t*\tLOCATION\tLOCATED\t1-343\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352620.conll/CURATION_USER.tsv \n",
            "Line:  1-37\t167-170\tNga\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-32|1-34[1_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352629.conll/CURATION_USER.tsv \n",
            "Line:  1-46\t190-192\tL.\t*\tPERSON\tPERSONAL - SOCIAL\t1-43\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352631.conll/CURATION_USER.tsv \n",
            "Line:  1-673\t3089-3092\tDan\t*\tPERSON\tPERSONAL - SOCIAL\t1-653\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352631.conll/CURATION_USER.tsv \n",
            "Line:  1-714\t3261-3266\tBlair\t*\tPERSON\tPERSONAL - SOCIAL\t1-712\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352650.conll/CURATION_USER.tsv \n",
            "Line:  1-270\t1228-1231\tchi\t*[26]\tORGANIZATION[26]\t*\t1-265[25_26]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352650.conll/CURATION_USER.tsv \n",
            "Line:  1-270\t1228-1231\tchi\t*[26]\tORGANIZATION[26]\t*\t1-265[25_26]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-240\t1079-1082\tPax\t*\tPERSON\tPERSONAL - SOCIAL\t1-237\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-243\t1089-1095\tZahara\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-240|1-237\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-243\t1089-1095\tZahara\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-240|1-237\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-246\t1102-1108\tShiloh\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-243|1-237|1-240\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-246\t1102-1108\tShiloh\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-243|1-237|1-240\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-246\t1102-1108\tShiloh\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-243|1-237|1-240\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-249\t1115-1123\tVivienne\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-246|1-237|1-240|1-243\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-249\t1115-1123\tVivienne\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-246|1-237|1-240|1-243\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-249\t1115-1123\tVivienne\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-246|1-237|1-240|1-243\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-249\t1115-1123\tVivienne\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-246|1-237|1-240|1-243\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-251\t1128-1132\tKnox\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-249|1-237|1-240|1-243|1-246\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-251\t1128-1132\tKnox\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-249|1-237|1-240|1-243|1-246\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-251\t1128-1132\tKnox\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-249|1-237|1-240|1-243|1-246\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-251\t1128-1132\tKnox\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-249|1-237|1-240|1-243|1-246\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-251\t1128-1132\tKnox\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-249|1-237|1-240|1-243|1-246\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-519\t2333-2336\tPax\t*\tPERSON\tPERSONAL - SOCIAL\t1-517\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-564\t2535-2541\tMaddox\t*\tPERSON\tPERSONAL - SOCIAL\t1-560\t\n",
            "250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wzBGIIgKYPD"
      },
      "source": [
        "# Fix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruagt4UGbqL5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3d034cef-d1ed-41a5-de9d-a64729764c36"
      },
      "source": [
        "\"\"\"\n",
        "raw_data: - is a list (len = 506)\n",
        "          - each row is a dict {}, information from a single doc (506 doc)\n",
        "            - doc_id: number id in name of folder that contain doc\n",
        "            - text: text in line start with \"#Text=\"\n",
        "            - token_ids: list of int. (1st column)\n",
        "            - subtoken_ids: list. (1st column)\n",
        "                            an element can be None if token is not a subtoken: 1-id -> 1.26,\n",
        "                                           or int (subtoken_id) if token is a subtoken: 1-id.subid -> 1.26.1.\n",
        "                                           currently in train data, only exist subtoken id 1.\n",
        "                            (in extract raw data code, my code can get any subid, not just specify subid = 1.\n",
        "                             but i check if in data has other subid, it will return error -> to know more about data)\n",
        "            - pos: list of child list. (2st column)\n",
        "                   each child list has two int elements.\n",
        "                   [start_position, end_position]\n",
        "            - tokens: list of strings. (3th column)\n",
        "            - entity: list.\n",
        "                      an element is: None if crr token is not entity\n",
        "                                     a list with: 2 element if crr token is an entity.\n",
        "                                                  [entity_id, entity_name]\n",
        "                                                  entity_id: int, from 4th column\n",
        "                                                  entity_name: string, from 5th column\n",
        "            - relation: list\n",
        "                        an element is: None if there is no relation in 6ht, 7th column.\n",
        "                                       a list of child list. number of child list is number of relation in 6th, 7th column.\n",
        "                                                 each child list has: 4 elemnt\n",
        "                                                 [relation_name, stoken_id, sstoken_id, direction[sentity_id, eentity_id]]\n",
        "                                                 relation_name: string, from 6h column\n",
        "                                                 stoken_id: int, tokenid from 7th column\n",
        "                                                 sstoken_id: from 7th column\n",
        "                                                             None, if entity_1 is a token\n",
        "                                                             else: int, subid if entity_1 is subtoken\n",
        "                                                 direction: from 7th column\n",
        "                                                            None, if there is a mistake in dataset\n",
        "                                                            else: [entity_1_id, entity_2_id]\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nraw_data: - is a list (len = 506)\\n          - each row is a dict {}, information from a single doc (506 doc)\\n            - doc_id: number id in name of folder that contain doc\\n            - text: text in line start with \"#Text=\"\\n            - token_ids: list of int. (1st column)\\n            - subtoken_ids: list. (1st column)\\n                            an element can be None if token is not a subtoken: 1-id -> 1.26,\\n                                           or int (subtoken_id) if token is a subtoken: 1-id.subid -> 1.26.1.\\n                                           currently in train data, only exist subtoken id 1.\\n                            (in extract raw data code, my code can get any subid, not just specify subid = 1.\\n                             but i check if in data has other subid, it will return error -> to know more about data)\\n            - pos: list of child list. (2st column)\\n                   each child list has two int elements.\\n                   [start_position, end_position]\\n            - tokens: list of strings. (3th column)\\n            - entity: list.\\n                      an element is: None if crr token is not entity\\n                                     a list with: 2 element if crr token is an entity.\\n                                                  [entity_id, entity_name]\\n                                                  entity_id: int, from 4th column\\n                                                  entity_name: string, from 5th column\\n            - relation: list\\n                        an element is: None if there is no relation in 6ht, 7th column.\\n                                       a list of child list. number of child list is number of relation in 6th, 7th column.\\n                                                 each child list has: 4 elemnt\\n                                                 [relation_name, stoken_id, sstoken_id, direction[sentity_id, eentity_id]]\\n                                                 relation_name: string, from 6h column\\n                                                 stoken_id: int, tokenid from 7th column\\n                                                 sstoken_id: from 7th column\\n                                                             None, if entity_1 is a token\\n                                                             else: int, subid if entity_1 is subtoken\\n                                                 direction: from 7th column\\n                                                            None, if there is a mistake in dataset\\n                                                            else: [entity_1_id, entity_2_id]\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYMqFdP5eTPL"
      },
      "source": [
        "## Fix1: Subtoken to token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdwVXKOHs1Ql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78895ebe-4701-41d3-c4ca-018a7369e4e0"
      },
      "source": [
        "# Trong data có subtoken. Mặc dù hiện tại trong train data 1 token nếu có subtoken thì chỉ có 1 subtoken \n",
        "# và subtoken này thường là entity nên mới cần tách ra. ngoài ra subtoken thường nằm ở cuối hoặc đầu token nên ta có dễ dàng tách ra được.\n",
        "# nên để giữ ngữ nghĩa tốt nhất cho câu thì có thể chia token thành 2 token: subtoken và phần còn lại từ token gốc\n",
        "\n",
        "# Tuy nhiên, nếu trong tương lai, với bộ dev và test,\n",
        "# - nếu có nhiều hơn 1 subtoken và các subtoken k bị overlap thì cũng chia như trên\n",
        "# - còn nếu xảy ra hiện tượng overlap (subtoken bị đè lên nhau) giữa các subtoken\n",
        "# thì lúc này ta không thể tách token ra được nữa\n",
        "# lúc này ta sẽ chọn cách xử lý sẽ là không tách token gốc ra nữa. chèn thêm các subtoken vào câu và coi chúng như 1 token bình thường\n",
        "\n",
        "\n",
        "# ở đây do có 1 subtoken nên chọn cách tách token gốc ra cho đơn giản và giữ được ngữ nghĩa tốt nhất.\n",
        "# đoạn code bên dưới chỉ cho trường hợp 1 subtoken\n",
        "# trường hợp nhiều subtoken nhưng không overlap thì ý tưởng cũng tương tự, \n",
        "# nhưng cần chú ý gom các subtoken của 1 token lại và chọn thứ tự xử lý\n",
        "# và chú ý tới có dấu bằng hay không khi so sánh pos\n",
        "\n",
        "import copy\n",
        "\n",
        "raw_tdata_new = copy.deepcopy(raw_tdata)\n",
        "\n",
        "for docif in raw_tdata_new:\n",
        "    for i in range(len(docif['subtoken_ids'])):\n",
        "        \n",
        "        if docif['subtoken_ids'][i] != None:\n",
        "            \n",
        "            print('\\n\\n-----Doc: ', docif['doc_id'])\n",
        "            print('Before: ')\n",
        "            for key in docif:\n",
        "                if key not in ['doc_id', 'text']:\n",
        "                    print(docif[key][i-1], end='\\t')\n",
        "            print('', end='\\n')\n",
        "            for key in docif:\n",
        "                if key not in ['doc_id', 'text']:\n",
        "                    print(docif[key][i], end='\\t')\n",
        "\n",
        "\n",
        "\n",
        "            #print(docif['token_ids'][i-1], '\\t', docif['pos'][i-1][0], '-', docif['pos'][i-1][1], '\\t', docif['tokens'][i-1])\n",
        "            #print(docif['token_ids'][i] , '.', docif['subtoken_ids'], '\\t', docif['pos'][i][0], '-', docif['pos'][i][1], '\\t', docif['tokens'][i])\n",
        "\n",
        "            # subtoken là một đoạn đầu của token\n",
        "            # 1-200\t    902-905\tmôi\t        *[10]\tORGANIZATION[10]\t_\t_\t\n",
        "            # 1-201\t    906-913\ttrường,\t     _     _\t                _\t_\t        <--- i-1\n",
        "            # 1-201.1\t906-912\ttrường  \t*[10]\tORGANIZATION[10]\t_\t_           <--- i\n",
        "\n",
        "            if (docif['pos'][i][0] == docif['pos'][i-1][0]) and (docif['pos'][i][1] < docif['pos'][i-1][1]):\n",
        "                \n",
        "                # đổi chỗ 2 dòng, dòng subtoken lên trên, dòng token gốc xuống dưới\n",
        "                # i đang là dòng chứa subtoken sẽ thành dòng chứa token\n",
        "                # i-1 đang là dòng chứa token sẽ thành dòng chứa subtoken\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        tmp = copy.deepcopy(docif[key][i])\n",
        "                        docif[key][i] = copy.deepcopy(docif[key][i-1]) \n",
        "                        docif[key][i-1] = copy.deepcopy(tmp)\n",
        "\n",
        "                assert (docif['subtoken_ids'][i-1] != None), str(\"Swap failed.\")\n",
        "\n",
        "                ## xóa bỏ subtoken ids\n",
        "                docif['subtoken_ids'][i-1] = None\n",
        "\n",
        "                ## thay đổi tokens, bỏ phần subtoken đã tách ra\n",
        "                docif['tokens'][i] = docif['tokens'][i].replace(docif['tokens'][i-1], '')\n",
        "\n",
        "                # tuy nhiên, có trường hợp sau khi bỏ phần subtoken đi bị thừa dấu cách, nên ta cần xử lý\n",
        "                # do subtoken nằm ở đầu token, nên ta chỉ đếm dấu cách ở bên trái phần còn lại thôi\n",
        "                space_count = 0\n",
        "                for s in docif['tokens'][i]:\n",
        "                    if s in [' ', '\\xa0']:\n",
        "                        space_count += 1\n",
        "                    else:\n",
        "                        break\n",
        "                \n",
        "\n",
        "                docif['tokens'][i] = docif['tokens'][i].lstrip()\n",
        "\n",
        "\n",
        "                ## thay đổi pos\n",
        "                docif['pos'][i] = [(docif['pos'][i-1][1] + space_count), docif['pos'][i][1]]\n",
        "                \n",
        "                \n",
        "\n",
        "                assert ((docif['text'][docif['pos'][i][0]:docif['pos'][i][1]]) == docif['tokens'][i]), \\\n",
        "                str('\\nWrong postions \\npos' + str(docif['pos'][i][0]) + '-' + str(docif['pos'][i][1]) + '  token: ' + str(docif['tokens'][i]))\n",
        "\n",
        "                # thay đổi token_ids của toàn bộ phần dưới, nếu có subid trong relation ở đâu thì thay đổi, thay đổi stoken_id trong relation\n",
        "                # trường hợp này stoken_id sẽ không đổi nên không cần thay đổi\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if j >= i:\n",
        "                        docif['token_ids'][j] += 1\n",
        "\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][1] == docif['token_ids'][i-1]:   # tìm xem có relation nào trỏ tới subtoken trước kia không\n",
        "                                docif['relation'][j][k][2] = None   # nếu có thì thay bằng None\n",
        "\n",
        "                            # do ở trên, toàn bộ token_ids phía sau (>= i) sẽ bị thay đổi (cộng thêm 1)\n",
        "                            # nên những relation có stoken_id nằm ở phần phía sau này cũng sẽ cần thay đổi theo (cộng thêm 1)\n",
        "                            elif (docif['relation'][j][k][1] > docif['token_ids'][i-1]):\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1\n",
        "\n",
        "\n",
        "                '''\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][2] != None:\n",
        "                                assert False, str('Failed to replace subid in relation')\n",
        "                '''\n",
        "                            \n",
        "\n",
        "                print('\\n\\nAfter:')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i-1], end='\\t')\n",
        "                print('', end='\\n')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i], end='\\t')\n",
        "\n",
        "\n",
        "            # subtoken là một đoạn cuối của token\n",
        "            # 1-583\t    2567-2573\tmẹ-Tập\t _\t    _\t                _\t_\t<--- i-1\n",
        "            # 1-583.1\t2570-2573\tTập\t    *[19]\tORGANIZATION[19]\t_\t_\t<--- i\n",
        "\n",
        "            elif (docif['pos'][i][1] == docif['pos'][i-1][1]) and (docif['pos'][i][0] > docif['pos'][i-1][0]):\n",
        "                ## thay đổi subid\n",
        "                docif['subtoken_ids'][i] = None\n",
        "\n",
        "                ## thay đổi tokens\n",
        "                docif['tokens'][i-1] = docif['tokens'][i-1].replace(docif['tokens'][i], '')\n",
        "\n",
        "                print(repr(docif['tokens'][i-1]))\n",
        "                # tuy nhiên, có trường hợp sau khi bỏ phần subtoken đi bị thừa dấu cách, nên ta cần xử lý\n",
        "                # do subtoken nằm ở cuối token, nên ta chỉ đếm dấu cách ở bên phải phần còn lại thôi\n",
        "                space_count = 0\n",
        "                for s in docif['tokens'][i-1][::-1]:\n",
        "                    if s in [' ', '\\xa0']:\n",
        "                        space_count += 1\n",
        "                    else:\n",
        "                        break\n",
        "                \n",
        "                \n",
        "                docif['tokens'][i-1] = docif['tokens'][i-1].rstrip()\n",
        "\n",
        "\n",
        "                ## thay đổi pos\n",
        "                docif['pos'][i-1] = [docif['pos'][i-1][0], (docif['pos'][i][0] - space_count)]\n",
        "\n",
        "                \n",
        "\n",
        "                assert ((docif['text'][docif['pos'][i-1][0]:docif['pos'][i-1][1]]) == docif['tokens'][i-1]), \\\n",
        "                str('\\nWrong postions \\npos ' + str(docif['pos'][i-1][0]) + '-' + str(docif['pos'][i-1][1]) + '  token: ' + str(docif['tokens'][i-1]))\n",
        "\n",
        "                # thay đổi token_ids của toàn bộ phần dưới, nếu có subid trong relation ở đâu thì thay đổi, thay đổi stoken_id trong relation\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if j >= i:\n",
        "                        docif['token_ids'][j] += 1\n",
        "\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][1] == docif['token_ids'][i-1]:   # tìm xem có relation nào trỏ tới subtoken trước kia không\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1   # do token_id thay đổi nên relation có stoken_id này cũng phải thay đổi \n",
        "                                docif['relation'][j][k][2] = None   # nếu có thì thay bằng None\n",
        "\n",
        "                            # do ở trên, toàn bộ token_ids phía sau (>= i) sẽ bị thay đổi (cộng thêm 1)\n",
        "                            # nên những relation có stoken_id nằm ở phần phía sau này cũng sẽ cần thay đổi theo (cộng thêm 1)\n",
        "                            elif (docif['relation'][j][k][1] > docif['token_ids'][i-1]):\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1\n",
        "\n",
        "                '''\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][2] != None:\n",
        "                                assert False, str('Failed to replace subid in relation')\n",
        "                            \n",
        "                            stoken_eid = docif['token_ids'].index(docif['relation'][j][k][1])\n",
        "                            if docif['entity'][stoken_eid] == None:\n",
        "                                print(docif['doc_id'])\n",
        "                                print(docif['relation'][j][k][1])\n",
        "                                print(stoken_eid)\n",
        "                                print(docif['token_ids'][stoken_eid])\n",
        "                                assert False, str('Failed to replace stoken_id in relation')\n",
        "                        \n",
        "                '''\n",
        "                        \n",
        "\n",
        "\n",
        "\n",
        "                print('\\n\\nAfter:')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i-1], end='\\t')\n",
        "                print('', end='\\n')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i], end='\\t')\n",
        "\n",
        "            else:\n",
        "                assert False, str(\"\\nExist subtoken in middle of token.\\nDoc: \" + docif['doc_id'] + \"\\ntoken_id\" + str(docif['token_ids'][i]))\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "\n",
        "\n",
        "# kiểm tra xem code trên có lỗi gì không\n",
        "print('\\n\\nCHECKING')\n",
        "for idoc, docif in enumerate(raw_tdata_new):\n",
        "\n",
        "    relation_lst = []\n",
        "    for i in range(len(raw_tdata[idoc]['relation'])):\n",
        "        if raw_tdata[idoc]['relation'][i] != None:\n",
        "            relation_lst.append(raw_tdata[idoc]['relation'][i])\n",
        "\n",
        "    \n",
        "    relation_ith = 0\n",
        "    for i in range(len(docif['token_ids'])):\n",
        "        \n",
        "        # nếu code chạy đúng thì sẽ không còn subid\n",
        "        if (docif['subtoken_ids'][i] != None):\n",
        "            assert False, str('ERROR CODE 1')\n",
        "\n",
        "        if docif['relation'][i] != None:\n",
        "            for j in range(len(docif['relation'][i])):\n",
        "                # nếu code chạy đúng thì sẽ không còn subid\n",
        "                if docif['relation'][i][j][2] != None:\n",
        "                    assert False, str('ERROR CODE 2')\n",
        "\n",
        "                # so sánh xem thay đổi stoken_id có đúng không\n",
        "                # string của token và pos của token sẽ không đổi so với raw_tdata\n",
        "                stoken_new = docif['relation'][i][j][1]\n",
        "                stoken_new_ele_id = docif['token_ids'].index(stoken_new)\n",
        "                \n",
        "                stoken = relation_lst[relation_ith][j][1]\n",
        "                if relation_lst[relation_ith][j][2] == 1:\n",
        "                    stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken) + 1\n",
        "                else:\n",
        "                    stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken)\n",
        "\n",
        "                if docif['pos'][stoken_new_ele_id] != raw_tdata[idoc]['pos'][stoken_ele_id]:\n",
        "                    '''\n",
        "                    print('\\n-----Doc: ', docif['doc_id'])\n",
        "                    print(relation_ith)\n",
        "                    print(docif['relation'][i])\n",
        "                    print(relation_lst[relation_ith])\n",
        "                    print(docif['pos'][stoken_new_ele_id])\n",
        "                    print(raw_tdata[idoc]['pos'][stoken_ele_id])\n",
        "                    '''\n",
        "                    assert False, str('ERROR CODE 3')\n",
        "                \n",
        "            relation_ith += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('DONE. EVERYTHINGS SEEM TO BE CORRECTED :D')\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "   "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "-----Doc:  23352414\n",
            "Before: \n",
            "413\tNone\t[1843, 1852]\tquyền. Bộ\tNone\tNone\t\n",
            "413\t1\t[1850, 1852]\tBộ\t[19, 'ORGANIZATION']\t[['AFFILIATION', 417, 1, [20, 19]], ['AFFILIATION', 423, None, [21, 19]]]\t'quyền.\\xa0'\n",
            "\n",
            "\n",
            "After:\n",
            "413\tNone\t[1843, 1849]\tquyền.\tNone\tNone\t\n",
            "414\tNone\t[1850, 1852]\tBộ\t[19, 'ORGANIZATION']\t[['AFFILIATION', 418, 1, [20, 19]], ['AFFILIATION', 424, None, [21, 19]]]\t\n",
            "\n",
            "-----Doc:  23352414\n",
            "Before: \n",
            "418\tNone\t[1865, 1878]\ttrưởng Nguyễn\tNone\tNone\t\n",
            "418\t1\t[1872, 1878]\tNguyễn\t[20, 'PERSON']\tNone\t'trưởng\\xa0'\n",
            "\n",
            "\n",
            "After:\n",
            "418\tNone\t[1865, 1871]\ttrưởng\tNone\tNone\t\n",
            "419\tNone\t[1872, 1878]\tNguyễn\t[20, 'PERSON']\tNone\t\n",
            "\n",
            "-----Doc:  23352600\n",
            "Before: \n",
            "91\tNone\t[405, 413]\tvấn\". Vũ\tNone\tNone\t\n",
            "91\t1\t[411, 413]\tVũ\t[9, 'PERSON']\tNone\t'vấn\".\\xa0'\n",
            "\n",
            "\n",
            "After:\n",
            "91\tNone\t[405, 410]\tvấn\".\tNone\tNone\t\n",
            "92\tNone\t[411, 413]\tVũ\t[9, 'PERSON']\tNone\t\n",
            "\n",
            "-----Doc:  23352605\n",
            "Before: \n",
            "1152\tNone\t[5051, 5054]\t(FN\tNone\tNone\t\n",
            "1152\t1\t[5052, 5054]\tFN\t[0, 'ORGANIZATION']\tNone\t'('\n",
            "\n",
            "\n",
            "After:\n",
            "1152\tNone\t[5051, 5052]\t(\tNone\tNone\t\n",
            "1153\tNone\t[5052, 5054]\tFN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "CHECKING\n",
            "DONE. EVERYTHINGS SEEM TO BE CORRECTED :D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBr6Gx-Y2oj2"
      },
      "source": [
        "## Find all entity in a doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVl3FKeND9dk"
      },
      "source": [
        "# Tìm tất cả các entity trong một doc:\n",
        "\n",
        "def find_all_entity_in_doc(raw_data, doc_id):\n",
        "    \n",
        "    for docif in raw_data:\n",
        "\n",
        "        ######## Tìm tất cả các entity trong doc hiện tại:\n",
        "\n",
        "        #                      -                                          entity_1                                              -  ...\n",
        "        #                      | -                 token_1                  -  -                   token_2                -  ...|\n",
        "        ##### doc_entity_lst = [ [ [ele_id, token_id, entity_id, entity_name], [ele_id, token_id, entity_id, , entity_name], ...], ...]\n",
        "\n",
        "        ### Lưu ý: ele_id ở đây là element index của token đấy trong docif[\"entity\"]\n",
        "        ### chứ không phải là token_ids\n",
        "        ### lưu cái ele_id thay vì token_ids để truy cập token bằng index nhanh hơn\n",
        "        ### nếu lưu token_ids thì phải từ từ token_ids tìm xem token này nó nằm vị trí nào thì mới ra index (ele_id) để truy\n",
        "        ### thường thì, vì token_ids bắt đầu = 1, element index bắt đầu = 0 nên: token_ids tương ứng sẽ có: token_ids = ele_id + 1\n",
        "        ### nhưng nếu trong doc có subtoken thì điều trên sẽ không được đảm bảo\n",
        "\n",
        "        # token có entity_id giống nhau mà không đứng cạnh nhau (các dòng chứa token không liên tiếp nhau) thì thuộc 2 entity khác nhau. \n",
        "        # Trường hợp này là do dataset lỗi, bị lặp entity_id (bên trên có entity_id = 0 rồi xuống dưới (token không cạnh nhau) lại thấy entity_id = 0)\n",
        "        # Hiện tại mới thấy có entity_id = 0 là bị lặp lại cho nhiều entity khác nhau\n",
        "\n",
        "        \n",
        "\n",
        "        if docif['doc_id'] == doc_id:\n",
        "\n",
        "            doc_entity_lst = []\n",
        "            tmplst = []\n",
        "\n",
        "            for i in range(len(docif[\"entity\"])):\n",
        "                if docif[\"entity\"][i] != None:\n",
        "                    tmplst.append([i, docif[\"token_ids\"][i], docif[\"entity\"][i][0], docif[\"entity\"][i][1]])\n",
        "\n",
        "                    if (i < (len(docif[\"entity\"]) - 1)):\n",
        "\n",
        "                        if docif[\"entity\"][i+1] == None:\n",
        "                            doc_entity_lst.append(tmplst)\n",
        "                            tmplst = []\n",
        "\n",
        "                        elif (docif[\"entity\"][i][0] != docif[\"entity\"][i+1][0]) or (docif[\"entity\"][i][1] != docif[\"entity\"][i+1][1]):\n",
        "                            doc_entity_lst.append(tmplst)\n",
        "                            tmplst = []\n",
        "\n",
        "                    if i == (len(docif[\"entity\"]) - 1):\n",
        "                        doc_entity_lst.append(tmplst)\n",
        "\n",
        "\n",
        "            ###################### Fix lỗi hai entity khác nhau nhưng đứng cạnh nhau và bị trùng entity_id\n",
        "            # tuy nhiên, ta chỉ tìm các entity_id = 0 thôi, vì chúng dễ lỗi nhất, dễ bị trùng id nhất\n",
        "            # mọi cặp entity_id = 0 đứng cạnh nhau trong các doc trong list bên dưới đều sẽ bị tách ra thành các entity riêng\n",
        "\n",
        "            ##### comment of V1 <-- trong notebook extract_train thì code cũ hoạt động ổn do không có cụm entity lỗi nào > 2 entity\n",
        "            ##### tuy nhiên trong dev thì xuất hiện một số cụm 3 entity id 0 bị lỗi\n",
        "            # tuy nhiên ta sẽ xử lý từng cặp một trong từng lần xử lý, run_times là số cặp trùng trong doc\n",
        "            # nếu có doc nào vừa có cặp entity_0 lỗi vừa có cặp không lỗi thì phải sẽ không được thêm list dưới vào mà phải xử lý riêng\n",
        "            # trừ khi cặp bị lỗi là cặp đầu tiên thì run_times đặt là 1\n",
        "            # data cũng không có quá nhiều những cặp như này\n",
        "            #####\n",
        "            \n",
        "\n",
        "            # chạy 2 cell code bên dưới trước để tìm các cụm entity_id = 0 có từ 2 token trở lên\n",
        "            # để xem những cụm nào bị lỗi, cụm nào không bị, cụm nào lỗi mà cần sửa\n",
        "            # ta sẽ cần lấy doc_id và ith của cụm bị lỗi dể sửa\n",
        "            # sau khi sửa trong này, những cụm được sửa sẽ không còn xuất hiện khi chạy 2 cell bên dưới nữa\n",
        "            # những cụm không được sửa (không lỗi hoặc lỗi mà chọn không sửa) vẫn sẽ được in ra\n",
        "\n",
        "            # ngoài ra, do chỉ đích danh entity cần sửa nên dù doc có cả entity lỗi lẫn entity không lỗi thì vẫn sửa được\n",
        "\n",
        "            # lưu ý: mọi cặp entity lỗi có bao nhiêu token thì sẽ được tách hết ra thành từng đấy entity.\n",
        "            # tức là ví dụ entity id 0 lỗi có 3 token thì được tách ra thành 3 entity id 0 riêng biệt, mỗi entity chỉ là 1 token\n",
        "            # code bên dưới chỉ xử lý trường hợp này.\n",
        "            # không xử lý trường hợp kiểu entity id 0 lỗi có 3 token, \n",
        "            # nhưng lại cần tách ra 2 entity (thay vì 3), 1 entity gồm 2 token đầu, 1 entity là 1 token cuối\n",
        "            # lý do: lười quá, và số cụm lỗi rất ít, trường hợp như kia thì càng rất ít hơn và có khi k xảy ra\n",
        "\n",
        "\n",
        "            \n",
        "            doc_error_lst = [{'doc_error_id': '23352161', 'ith_er_lst': [68, 77]},\n",
        "                             {'doc_error_id': '23352337', 'ith_er_lst': [9]},\n",
        "                             {'doc_error_id': '23352396', 'ith_er_lst': [29]},\n",
        "                             {'doc_error_id': '23352419', 'ith_er_lst': [0]},\n",
        "                             {'doc_error_id': '23352445', 'ith_er_lst': [17]},\n",
        "                             {'doc_error_id': '23352491', 'ith_er_lst': [66]},\n",
        "                             {'doc_error_id': '23352499', 'ith_er_lst': [1, 3, 27]},\n",
        "                             {'doc_error_id': '23352585', 'ith_er_lst': [10]},\n",
        "                             {'doc_error_id': '23352601', 'ith_er_lst': [0, 4, 28]},\n",
        "                             {'doc_error_id': '23352642', 'ith_er_lst': [21]},\n",
        "                             {'doc_error_id': '23352605', 'ith_er_lst': [24, 26, 44, 48, 50, 52, 57, 61, 64, 69, 93, 96, 99, 102, 107]}      \n",
        "                            ]\n",
        "\n",
        "            doc_error_id_lst = [doc_error['doc_error_id'] for doc_error in doc_error_lst]\n",
        "\n",
        "            increase_ids = 0\n",
        "\n",
        "            if doc_id in doc_error_id_lst:\n",
        "                ith_doc_id = doc_error_id_lst.index(doc_id)\n",
        "\n",
        "                assert (doc_error_lst[ith_doc_id]['doc_error_id'] == doc_id), str('PRBOLEM')\n",
        "\n",
        "                #print('\\n\\n------', doc_id)\n",
        "\n",
        "                ith_er_list = sorted(doc_error_lst[ith_doc_id]['ith_er_lst'])\n",
        "\n",
        "                for irun, ith_er_id in enumerate(ith_er_list):\n",
        "                    #print('--', irun)\n",
        "                    doc_entity_lst_copy = None\n",
        "                    doc_entity_lst_copy = copy.deepcopy(doc_entity_lst)\n",
        "\n",
        "                    for ient, ent in enumerate(doc_entity_lst):\n",
        "                        if ient == (ith_er_id + increase_ids): # do bị dịch nên cần cộng với số id bị dịch\n",
        "\n",
        "                            assert ((ent[0][2] == 0) and (len(ent) > 1)), \\\n",
        "                            str('\\nWrong ith_er_lst. \\nDoc: ' + str(doc_id) + '\\nith_er_id: ' + str(ith_er_id))\n",
        "\n",
        "                            #print(doc_entity_lst_copy)\n",
        "\n",
        "                            # ví dụ: 3 token thì chỉ cần chạy 3 - 1 = 2 lần\n",
        "                            # lần đầu (itk = 0) thì lấy token cuối entity (token thứ 3: ent[-1]) chèn vào sau vị trí hiện tại của entity\n",
        "                            # lần hai (itk = 1) thì lấy token ngay trước token cuối (token thứ 2 từ cuối lên: ent[-2]) chèn vào sau vị trí hiện tại của entity\n",
        "                            for itk in range(len(ent) - 1):\n",
        "                                doc_entity_lst_copy.insert((ient+1), copy.deepcopy([ent[(-1 - itk)]]))\n",
        "\n",
        "                            # cuối cùng thì biến entity lỗi hiện tại thành token đầu của entity lỗi hiện tại là xong\n",
        "                            doc_entity_lst_copy[ient] = copy.deepcopy([ent[0]])\n",
        "\n",
        "                            #print(doc_entity_lst_copy)\n",
        "\n",
        "                            # do bên trên ta chèn thêm (len(ent) - 1) entity mới vào entity list\n",
        "                            # nên id của entity bị lỗi phía sau sẽ bị tăng lên lượng tương ứng\n",
        "                            # là tổng (len(ent)-1) của mọi ent bị sửa trước nó\n",
        "                            increase_ids += (len(ent) - 1)\n",
        "\n",
        "                            break\n",
        "                            \n",
        "                    doc_entity_lst = copy.deepcopy(doc_entity_lst_copy)\n",
        "                    #print(doc_entity_lst)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "            \n",
        "\n",
        "\n",
        "            '''\n",
        "            # Doc: 23351965\n",
        "            # hai token cạnh nhau (320, 321), cùng entity_id 0, cùng entity_name location nhưng không phải là 1 entity <-- lỗi data\n",
        "            # mà là hai entity, vì entity này link tới entity kia.\n",
        "\n",
        "            if docif['doc_id'] == '23351965':\n",
        "                doc_entity_lst_copy = copy.deepcopy(doc_entity_lst)\n",
        "                for ient, ent in enumerate(doc_entity_lst):\n",
        "                    if (len(ent) == 2) and (ent[0][2] == 0) and (ent[1][2] == 0):\n",
        "                        #print(doc_entity_lst_copy)\n",
        "                        doc_entity_lst_copy.insert((ient+1), [doc_entity_lst_copy[ient][1]])\n",
        "                        doc_entity_lst_copy[ient] = [doc_entity_lst_copy[ient][0]]\n",
        "                        #print(doc_entity_lst_copy)\n",
        "                        \n",
        "                doc_entity_lst = copy.deepcopy(doc_entity_lst_copy)\n",
        "                #print(doc_entity_lst)\n",
        "\n",
        "\n",
        "            # Doc: 23352753  bị giống bên trên\n",
        "            # hai token cạnh nhau (884, 885), cùng entity_id 0, cùng entity_name location nhưng không phải là 1 entity <-- lỗi data\n",
        "            # mà là hai entity, vì entity này link tới entity kia.\n",
        "\n",
        "            if docif['doc_id'] == '23352753':\n",
        "                doc_entity_lst_copy_2 = copy.deepcopy(doc_entity_lst)\n",
        "                for ient, ent in enumerate(doc_entity_lst):\n",
        "                    if (len(ent) == 2) and (ent[0][2] == 0) and (ent[1][2] == 0):\n",
        "                        #print(doc_entity_lst_copy_2)\n",
        "                        doc_entity_lst_copy_2.insert((ient+1), [doc_entity_lst_copy_2[ient][1]])\n",
        "                        doc_entity_lst_copy_2[ient] = [doc_entity_lst_copy_2[ient][0]]\n",
        "                        #print(doc_entity_lst_copy_2)\n",
        "                        \n",
        "                doc_entity_lst = copy.deepcopy(doc_entity_lst_copy_2)\n",
        "                #print(doc_entity_lst)\n",
        "            \n",
        "            '''\n",
        "\n",
        "\n",
        "            \n",
        "            return doc_entity_lst\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIA5LQry20nQ"
      },
      "source": [
        "# thường thì những entity cạnh nhau và trùng id khiến ta dễ lầm thành 1 entity\n",
        "# thì những entity này thường có entity_id = 0\n",
        "# nên ta sẽ tìm các cặp entity_id = 0 này và xem cặp nào là lỗi cặp nào không lỗi\n",
        "# sau đó đọc xem trong doc này đây là lỗi hay không phải lỗi (có cặp không phải lỗi)\n",
        "# nếu là cặp lỗi thì sẽ phải thêm trường hợp ở cell code bên trên\n",
        "# để về sau dùng hàm tìm entity bên trên sẽ không bị lỗi\n",
        "\n",
        "# MỘT LƯU Ý LÀ: \n",
        "# VÍ DỤ: \n",
        "# 1-209\t935-944\tFrankfurt\t*\tLOCATION\t\n",
        "# 1-210\t945-949\t(Đức\t*\tLOCATION\t\n",
        "\n",
        "# 1-161\t739-743\tBali\t*\tLOCATION\t_\t_\t\n",
        "# 1-162\t744-754\t(Indonesia\t*\tLOCATION\tPART – WHOLE\t1-161\t\n",
        "               \n",
        "# gán không chính xác, bên trên không có relation nhưng bên dưới lại có\n",
        "# nên nếu có relation như bên dưới thì tách ra làm 2 entity\n",
        "# còn không có relation như bên trên thì để nó là một entity\n",
        "# vì nếu không có relation mà vẫn tách ra làm 2 entity thì label giữa chúng sẽ là others, không chính xác\n",
        "# nếu để là cùng 1 entity thì sẽ hợp lý hơn. miễn là để cùng entity không ảnh hưởng gì\n",
        "# và trong data có rất nhiều label other giữa các location (như Mỹ với Anh có thể là others)\n",
        "# nên nếu ta chia ra thì sau trong test set cũng bị để làm others, tức là không hợp lý\n",
        "# thà để thành 1, thì train data cũng thế mà test data cũng thế\n",
        "# hoặc ta có thể tách nhưng phải thêm nhãn part-whole vào\n",
        "\n",
        "# cũng có thể toàn bộ các entity_id = 0 cạnh nhau đều là các entity khác nha, nhưng nếu không phải thì sẽ không hoàn hảo\n",
        "# nên có thể xét các trường hợp riêng thay vì tự động tách các entity_0 cạnh nhau thành các entity khác nhau\n",
        "\n",
        "def find_all_fault_entity_id_0(raw_data):\n",
        "\n",
        "    for docif in raw_data:\n",
        "        ent_lst = find_all_entity_in_doc(raw_data, docif['doc_id'])\n",
        "        \n",
        "        for i in range(len(ent_lst)):\n",
        "\n",
        "            if (len(ent_lst[i]) > 1) and (ent_lst[i][0][2] == 0):\n",
        "                print('\\n\\n------', docif['doc_id'], ' -ith: ', i)\n",
        "                for j in range(len(ent_lst[i])):\n",
        "                    first_tk_eleid = ent_lst[i][j][0]\n",
        "                    for key in docif:\n",
        "                        if key not in ['doc_id', 'text']:\n",
        "                            print(docif[key][first_tk_eleid], end='\\t')\n",
        "                    \n",
        "                    print('\\n')\n",
        "        \n",
        "\n",
        "\n",
        "                \n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POdyqoDS5dA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed063dd-16be-4714-b45d-9acef623d7c1"
      },
      "source": [
        "# tên hàm hơi gây nhầm\n",
        "# các entity được in ra có thể là lỗi hoặc là không lỗi, đa phần là lỗi\n",
        "# và các enity lỗi bên dưới là quyết định không sửa\n",
        "#find_all_fault_entity_id_0(raw_tdata_new_v4)\n",
        "find_all_fault_entity_id_0(raw_tdata_new)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "------ 23352316  -ith:  5\n",
            "80\tNone\t[344, 347]\tAFP\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "81\tNone\t[348, 354]\t/TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352316  -ith:  24\n",
            "341\tNone\t[1496, 1502]\t(TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "342\tNone\t[1503, 1512]\t/Vietnam+\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352347  -ith:  7\n",
            "47\tNone\t[196, 200]\tNhật\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "48\tNone\t[201, 206]\t/Nhật\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352396  -ith:  5\n",
            "67\tNone\t[281, 284]\tAFP\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "68\tNone\t[285, 291]\t/TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352442  -ith:  11\n",
            "109\tNone\t[514, 517]\tAFP\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "110\tNone\t[518, 524]\t/TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352495  -ith:  6\n",
            "92\tNone\t[413, 419]\td’Este\t[0, 'PERSON']\tNone\t\n",
            "\n",
            "93\tNone\t[420, 427]\t(Đe-xtơ\t[0, 'PERSON']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352495  -ith:  7\n",
            "105\tNone\t[475, 482]\tFerrara\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "106\tNone\t[483, 493]\t(Phe-ra-ra\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352495  -ith:  10\n",
            "137\tNone\t[636, 647]\tCharlemagne\t[0, 'PERSON']\tNone\t\n",
            "\n",
            "138\tNone\t[648, 664]\t(Sar’-lơ-ma-nhơ)\t[0, 'PERSON']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352605  -ith:  15\n",
            "159\tNone\t[696, 699]\tAFP\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "160\tNone\t[700, 706]\t/TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfXerQf-fQSv"
      },
      "source": [
        "# Create train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qce_9RiBsIZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "53c777d1-a3f3-4e42-cd80-71d12a9150ba"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "                    label: \n",
        "                    Dấu x là ký hiệu relation label (docif[\"relation\"]) xuất hiện ở chỗ của entity nào\n",
        "\n",
        "                              Entity:                 entity_1    -    entity_2\n",
        "                    Thứ tự trong câu:                 trước            sau\n",
        "                      \n",
        "                      Relation label:     LOCATED                         x     (per/org - loc)\n",
        "                                       IS_LOCATED         x                     (loc     - per/org)\n",
        "                                       PART–WHOLE\t                      x     (part    - whole)\n",
        "                                       WHOLE-PART         x                     (whole   - part)\n",
        "                                  PERSONAL–SOCIAL                               (Undirected)\n",
        "                                      AFFILIATION\t                      x     \n",
        "                                   AFFILIATION_TO         x\n",
        "                                           OTHERS                               (là nhãn giữ 2 entity cùng 1 câu mà không có relation trong data)\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n                    label: \\n                    Dấu x là ký hiệu relation label (docif[\"relation\"]) xuất hiện ở chỗ của entity nào\\n\\n                              Entity:                 entity_1    -    entity_2\\n                    Thứ tự trong câu:                 trước            sau\\n                      \\n                      Relation label:     LOCATED                         x     (per/org - loc)\\n                                       IS_LOCATED         x                     (loc     - per/org)\\n                                       PART–WHOLE\\t                      x     (part    - whole)\\n                                       WHOLE-PART         x                     (whole   - part)\\n                                  PERSONAL–SOCIAL                               (Undirected)\\n                                      AFFILIATION\\t                      x     \\n                                   AFFILIATION_TO         x\\n                                           OTHERS                               (là nhãn giữ 2 entity cùng 1 câu mà không có relation trong data)\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqEMUOpfqHZp"
      },
      "source": [
        "original_labels = ['LOCATED', 'PART – WHOLE', 'PERSONAL - SOCIAL', 'AFFILIATION']\n",
        "\n",
        "# entity chứa relation nằm ở phía sau thì là label gốc\n",
        "labels = {'LOCATED': 'LOCATED', 'IS_LOCATED': 'IS_LOCATED', \n",
        "         'PART_WHOLE': 'PART_WHOLE', 'WHOLE_PART': 'WHOLE_PART', \n",
        "         'PERSONAL_SOCIAL': 'PERSONAL_SOCIAL', \n",
        "         'AFFILIATION': 'AFFILIATION', 'AFFILIATION_TO': 'AFFILIATION_TO', \n",
        "         'OTHERS': 'OTHERS'\n",
        "         }"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8crlbk4MJTlx"
      },
      "source": [
        "### Func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBWuvOzUJUCP"
      },
      "source": [
        "def split_by_colon_punc(doc_sent_tokenize):\n",
        "\n",
        "    new_doc_sent_tokenize = []\n",
        "    for isent, sent in enumerate(doc_sent_tokenize):\n",
        "        if ':' not in sent:\n",
        "            new_doc_sent_tokenize.append(sent)\n",
        "        \n",
        "        else:\n",
        "            new_sents = sent.split(\":\")\n",
        "                \n",
        "            for inew_sent, new_sent in enumerate(new_sents):\n",
        "                if inew_sent != (len(new_sents) - 1):\n",
        "                    new_doc_sent_tokenize.append(str(new_sent.lstrip() + ':'))\n",
        "                else:\n",
        "                    new_doc_sent_tokenize.append(str(new_sent.strip()))\n",
        "\n",
        "    return new_doc_sent_tokenize\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt1PwbNGJG_W"
      },
      "source": [
        "from underthesea import sent_tokenize, word_tokenize\n",
        "\n",
        "def my_sentences_tokenize(doc_id, text):\n",
        "    ######## split sentence from docif[\"text\"] using Underthesea library\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # check if sum length of all sentence < len(text)\n",
        "    len_sentences = [len(s) for s in sentences]\n",
        "    assert (sum(len_sentences) <= len(text)), str(\"\\nSentence tokenize has problem. \\nDoc: \" + docif[\"doc_id\"])\n",
        "\n",
        "\n",
        "\n",
        "    ######\n",
        "    ### trong doc này việc chia sentence bằng Underthesea bị lỗi dẫn tới việc một entity nằm ở 2 câu.\n",
        "    new_sentences = []\n",
        "    \n",
        "    if doc_id == \"23352190\":  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (sent[-3:] != \"Ng.\") and (sent[:5] != \"Hưởng\"):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (sent[-3:] == \"Ng.\") and (sentences[isent + 1] == \"Hưởng\"):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "    elif doc_id in [\"23352299\", \"23352417\"]:  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"How Do I Look?\" not in sent) and ('Asia' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"How Do I Look?\" in sent) and ('Asia' in sentences[isent + 1]):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "        \n",
        "    ### trong các doc bên dưới việc chia sentence bằng Underthesea bị lỗi dẫn tới việc một relation link tới một entity thuộc câu khác.\n",
        "    ### thường do sau tên người viết tắt có dấu chấm\n",
        "    \n",
        "    elif doc_id == \"23352323\":  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('Lê Quý D' not in sent) and ('Hòa Thái' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif ('Lê Quý D' in sent) and ('Hòa Thái' in sentences[isent + 1]):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "\n",
        "    \n",
        "    elif doc_id == \"23352491\":  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('Reuters' not in sent) and ('Tuyết Mai' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif ('Reuters' in sent) and ('Tuyết Mai' in sentences[isent + 1]):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == \"23352572\":  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"William\" not in sent) and (\"B. Rosen\" not in sent) and ('E. Cashman' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"William\" in sent) and (\"B. Rosen\" in sentences[isent + 1]) and ('E. Cashman' in sentences[isent + 2]):\n",
        "                new_sentences.append(str('Chỉ huy lực lượng Mỹ ở đây là Thiếu tướng William. B. Rosen và Thiếu tướng Thủy quân lục chiến Robert. E. Cashman đã đề xuất kế hoạch rút quân khỏi Khe Sanh .'))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "        new_sentences_2 = []\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            assert ('…' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            if (\"...\" not in sent):\n",
        "                new_sentences_2.append(sent)\n",
        "            \n",
        "            elif (\"...\" in sent):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_2.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences_2.append(str(new_sent.lstrip()))\n",
        "        \n",
        "        #print(new_sentences_2)\n",
        "        sentences = copy.deepcopy(new_sentences_2)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    ######\n",
        "    # có một số doc có ...\n",
        "\n",
        "    elif doc_id == '23352499':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"giải phóng... Hôm nay\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"giải phóng... Hôm nay\" in sent):\n",
        "                tmppp_1 = sent.find(\". Hôm nay\")\n",
        "                tmppp_2 = sent.find(\". Một\")\n",
        "                new_sentences.append(str(sent[:(tmppp_1+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp_1+2):(tmppp_2+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp_2+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "        new_sentences_1 = []\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            \n",
        "\n",
        "            if (\"…\" not in sent) or ('UAV…nhưng' in sent):\n",
        "                new_sentences_1.append(sent)\n",
        "            \n",
        "            elif (\"…\" in sent) and ('UAV…nhưng' not in sent):\n",
        "                new_sents = sent.split(\"…\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_1.append(str(new_sent.lstrip() + '…'))\n",
        "                    else:\n",
        "                        new_sentences_1.append(str(new_sent.lstrip()))\n",
        "        \n",
        "        #print(new_sentences_1)\n",
        "        sentences = copy.deepcopy(new_sentences_1)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == '23352432':  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"tai nạn… Tuy nhiên\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"tai nạn… Tuy nhiên\" in sent):\n",
        "                tmppp = sent.find(\"… Tuy\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    ### những doc mà ta chỉ tách theo … dev\n",
        "    # các doc các câu chỉ có … hoặc một số doc có hai loại nhưng ta chỉ chia câu có …\n",
        "    elif doc_id in ['23352085', '23352087', '23352378', '23352433', '23352456', \\\n",
        "                    '23352507', '23352594', \\\n",
        "                    '23352332', '23352468']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "\n",
        "            if ((doc_id != '23352332') and (doc_id != '23352468')):\n",
        "                assert ('...' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            if (\"…\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"…\" in sent):\n",
        "                new_sents = sent.split(\"…\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '…'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.lstrip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "    ### những doc mà ta chỉ tách theo ... dev\n",
        "    # các doc các câu chỉ có ... hoặc một số doc có hai loại nhưng ta chỉ chia câu có ...\n",
        "    elif doc_id in ['23352016', '23352070', '23352073', '23352348', '23352370', \\\n",
        "                    '23352381', '23352436', '23352470', '23352648', \\\n",
        "                    '23352161', '23352314', '23352425', \\\n",
        "                    '23352122', '23352260', '23352573', '23352623']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ((doc_id != '23352122') and (doc_id != '23352260') and (doc_id != '23352573') \\\n",
        "                and (doc_id != '23352623')):   # doc nay co ca 2 nhung ta chi chia theo ...\n",
        "                assert ('…' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            if (\"...\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"...\" in sent):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.lstrip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    \n",
        "\n",
        "    ###### dev\n",
        "    # trong doc này việc chia câu bị lỗi, 2 câu bị gộp thành 1 câu\n",
        "    # những câu này thường có kí tự: .”\n",
        "    \n",
        "    elif doc_id in ['23351997', '23352066', '23352331', '23352416', '23352538']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('.”' in sent) and ('.”' != sent[-2:]):\n",
        "                new_sents = sent.split('.”')\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '.”'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ('.”' not in sent) or ('.”' == sent[-2:]):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == '23352317':  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"thuốc trừ sâu.. Thấy người quen\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"thuốc trừ sâu.. Thấy người quen\" in sent):\n",
        "                tmppp = sent.find(\". Thấy người quen\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    \n",
        "    elif doc_id == '23352605':  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"và SPD .. Nếu vậy, đây\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"và SPD .. Nếu vậy, đây\" in sent):\n",
        "                tmppp = sent.find(\". Nếu vậy\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    # tach dau .\n",
        "    \n",
        "    if doc_id in ['23352065', '23352190', '23352517', '23352603']:\n",
        "        \n",
        "        # ith_sent là thứ tự các câu cần tách dấu . trong doc (thứ tự câu bắt đầu từ 0)\n",
        "        # ith_dot là số thứ tự của các dấu . trong câu mà tại các dấu . này ta sẽ tách câu thành các phần khác nhau\n",
        "        # ith_dot cần đếm thứ tự cẩn thận bằng tay (thứ tự dấu . bắt đầu từ 1)\n",
        "        # ith_dot là một list của list. list thứ n của ith_dot là danh sách vị trí những dấu chấm mà ta sẽ dùng để tách câu thứ n tương ứn\n",
        "        # trong ith_sent   <- cần lưu ý để đúng thứ tự, và ith_sent cần xếp theo thứ tự tăng dần\n",
        "        # cần làm vậy vì có thể 1 doc có nhiều cần cần tách, rồi trong các câu này lại có câu có nhiều dấu . cần tách\n",
        "\n",
        "        # ý tưởng: ta duyệt các câu trong doc, dựa vào ith_sent_lst để biết câu nào cần tách dấu .\n",
        "        # câu nào không cần tách thì ta thêm luôn vào danh sách các câu trong doc\n",
        "        # câu nào cần tách thì: ta dựa tiếp vào ith_dot_lst để biết ta sẽ tách tại những dấu . nào trong câu\n",
        "        # ví dụ câu cần tách tại 2 dấu .: thứ 2 và thứ 3 trong câu (-> từ 1 câu tách thành 3 câu)\n",
        "        # ta sẽ tìm vị trí index của các dấu . này trong câu cần tách\n",
        "        # rồi dựa vào index đó để cắt câu thành các phần cần chia\n",
        "\n",
        "        doc_nfix_lst = [{'doc_id': '23352065', 'ith_sent_lst': [20], 'ith_dot_lst': [[1, 2]]},\n",
        "                        {'doc_id': '23352190', 'ith_sent_lst': [7], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '23352517', 'ith_sent_lst': [13], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '23352603', 'ith_sent_lst': [9, 11], 'ith_dot_lst': [[1], [1, 2]]}\n",
        "                        ]\n",
        "        \n",
        "        ##### to find pos of ith dot\n",
        "        def find_ith_dot_pos(haystack, needle, n):\n",
        "            start = haystack.find(needle)\n",
        "            while start >= 0 and n > 1:\n",
        "                start = haystack.find(needle, start+len(needle))\n",
        "                n -= 1\n",
        "            return start\n",
        "        #####\n",
        "\n",
        "\n",
        "        crr_doc_nfix = None\n",
        "        for doc_nfix in doc_nfix_lst:\n",
        "            if doc_nfix['doc_id'] == doc_id:\n",
        "                crr_doc_nfix = doc_nfix\n",
        "\n",
        "        new_sentences_6 = []\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            #if doc_id not in ['23353721', '23357394', '23352663']:\n",
        "            assert (('...' not in sent) and (\"…\" not in sent)), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            assert (len(crr_doc_nfix['ith_sent_lst']) == len(crr_doc_nfix['ith_dot_lst'])), \\\n",
        "            str('\\nLEN ith_sent_lst not equal to LEN ith_dot_lst. \\nDoc: ' + doc_id)\n",
        "\n",
        "            if isent not in crr_doc_nfix['ith_sent_lst']:\n",
        "                new_sentences_6.append(sent)\n",
        "            \n",
        "            else:\n",
        "                \n",
        "                assert (('...' not in sent) and (\"…\" not in sent)), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "                crr_ith_sent = crr_doc_nfix['ith_sent_lst'].index(isent)\n",
        "\n",
        "                ith_dot_pos_lst = [-1, (len(sent)-1)]\n",
        "\n",
        "                for ith_dot in crr_doc_nfix['ith_dot_lst'][crr_ith_sent]:\n",
        "                    dot_pos = find_ith_dot_pos(sent, '.', ith_dot)\n",
        "\n",
        "                    assert (dot_pos >= 0), str('\\nNot found ith dot. \\nDoc: ' + doc_id + '\\nSent: ' + sent)\n",
        "\n",
        "                    ith_dot_pos_lst.insert(-1, dot_pos)\n",
        "                \n",
        "                for iith in range(len(ith_dot_pos_lst) - 1):\n",
        "                    correct_sent = sent[(ith_dot_pos_lst[iith] + 1):(ith_dot_pos_lst[iith+1] + 1)]\n",
        "                    new_sentences_6.append(str(correct_sent).strip())\n",
        "\n",
        "\n",
        "        #print(new_sentences_6)\n",
        "        sentences = copy.deepcopy(new_sentences_6)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if doc_id not in ['23352024', '23352110', '23352190', '23352322', '23352428', '23352603']:\n",
        "        sentences = copy.deepcopy(split_by_colon_punc(sentences))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return sentences\n",
        "\n",
        "    ###### \n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_riTWvT_OHxy"
      },
      "source": [
        "def get_sentence_entities(docif, doc_entity_lst, sent, sspos, espos):\n",
        "    \n",
        "    sen_entity_lst = []\n",
        "\n",
        "    for i in range(len(doc_entity_lst)):   # each entity\n",
        "        first_token_eid = doc_entity_lst[i][0][0]\n",
        "        last_token_eid = doc_entity_lst[i][-1][0]\n",
        "            \n",
        "        # nếu điểm đầu của câu < điểm đầu của token đầu và điểm cuối của token cuối < điểm cuối của câu\n",
        "        # thì entity này là entity của câu này\n",
        "        if (sspos <= docif[\"pos\"][first_token_eid][0]) and (docif[\"pos\"][last_token_eid][1] <= espos):\n",
        "            sen_entity_lst.append(doc_entity_lst[i])\n",
        "\n",
        "        # điểm đầu của entity < điểm đầu của câu nhưng điểm cuối của entity lại lớn hơn điểm đầu của câu\n",
        "        # có lỗi: 1 entity nhưng thuộc 2 câu, tức là 1 phần của entity thuộc câu trước, phần còn lại lại thuộc câu đang xét.\n",
        "        # điều này có thể do chia câu bằng Underthesea có vấn đề hoặc dataset có vấn đề\n",
        "        elif (docif[\"pos\"][first_token_eid][0] < sspos) and (sspos < docif[\"pos\"][last_token_eid][1]):\n",
        "                \n",
        "        # trong dataset có lỗi này. \n",
        "        # tuy nhiên không có nhiều, nên để hiểu thêm về dataset, tôi chỉ sửa chính xác các lỗi này\n",
        "        # và đã sửa bên trên\n",
        "\n",
        "            assert False, str(\"\\n--- An entity belongs to two sentences instead of just one. (Error Code 1) \\nIn doc: \" + docif[\"doc_id\"] + \"\\nSentence: \" + repr(sent))\n",
        "                \n",
        "        # điểm đầu của entity < điểm cuối của câu nhưng điểm cuối của entity lại lớn hơn điểm cuối của câu\n",
        "        # có lỗi: 1 entity nhưng thuộc 2 câu, tức là 1 phần của entity thuộc câu đang xét, phần còn lại lại thuộc câu sau.\n",
        "        # điều này có thể do chia câu bằng Underthesea có vấn đề hoặc dataset có vấn đề\n",
        "        elif (docif[\"pos\"][first_token_eid][0] < espos) and (espos < docif[\"pos\"][last_token_eid][1]):\n",
        "                \n",
        "            assert False, str(\"\\nAn entity belongs to two sentences instead of just one. (Error Code 2) \\nIn doc: \" + docif[\"doc_id\"] + \"\\nSentence: \" + repr(sent))\n",
        "\n",
        "\n",
        "    # có thể xảy ra trường hợp chia câu bị lỗi, một câu to bị chia thành hai câu nhỏ\n",
        "    # mỗi câu nhỏ lại chứa các entity\n",
        "    # nhưng entity câu nhỏ này link tới câu nhỏ kia -> lỗi\n",
        "    # hoặc trong data có lỗi, entity câu này link tới câu khác.\n",
        "    # nên cần xem xem các relation trong câu có link tới các entity tìm thấy trong câu không\n",
        "        \n",
        "    # hay relation giữa 2 entity là đúng, nhưng stoken_id trong relation không trỏ vào token đầu tiên của entity id kia\n",
        "    # mà lại trỏ vào token giữa hoặc cuối entity kia (đã fix lỗi này bên trên)\n",
        "\n",
        "    # vì relation chỉ link tới token_ids của token đầu tiên trong entity khác\n",
        "    # nên ta sẽ thu thập danh sách token_ids của các token đầu tiên các entity trong câu\n",
        "    first_tkids_lst = []\n",
        "    for i in range(len(sen_entity_lst)):\n",
        "        first_tkids_lst.append(sen_entity_lst[i][0][1])\n",
        "\n",
        "\n",
        "    for i in range(len(sen_entity_lst)):\n",
        "        first_tkeid = sen_entity_lst[i][0][0]\n",
        "\n",
        "        if docif['relation'][first_tkeid] != None:   # từng relation trong câu\n",
        "            for j in range(len(docif['relation'][first_tkeid])):\n",
        "                if docif['relation'][first_tkeid][j][1] not in first_tkids_lst:\n",
        "                        \n",
        "                    '''\n",
        "                    print(str('\\nSentence tokenize has problem. \\nDoc: ' + docif['doc_id'] + '\\nRelation stoken ID: ' + str(docif['relation'][first_tkeid][j][1])  + ' \\nprvSent: ' + sentences[isent-1] + '\\nSent: ' + sent))\n",
        "\n",
        "                    print(docif['relation'][first_tkeid])\n",
        "                    '''\n",
        "\n",
        "                    assert False, \\\n",
        "                    str('\\nSentence tokenize has problem. \\nDoc: ' + docif['doc_id'] + '\\nRelation stoken ID: ' + str(docif['relation'][first_tkeid][j][1])  + ' \\nprvSent: ' + sentences[isent-1] + '\\nSent: ' + sent)\n",
        "\n",
        "\n",
        "\n",
        "    return sen_entity_lst\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZremDvk8tP7"
      },
      "source": [
        "def relation_name_to_sentence_label(relation_name, relation_entity):\n",
        "\n",
        "    sentence_label = None\n",
        "    # nếu entity chứa relation là entity 1 thì label ngược lại\n",
        "    if relation_entity == 1:\n",
        "        if relation_name == 'LOCATED':\n",
        "            sentence_label = 'IS_LOCATED'\n",
        "\n",
        "        elif relation_name == 'PART – WHOLE':\n",
        "            sentence_label = 'WHOLE_PART'\n",
        "\n",
        "        elif relation_name == 'PERSONAL - SOCIAL':\n",
        "            sentence_label = 'PERSONAL_SOCIAL'\n",
        "\n",
        "        elif relation_name == 'AFFILIATION':\n",
        "            sentence_label = 'AFFILIATION_TO'\n",
        "\n",
        "        else:\n",
        "            assert False, str('UNKNOW RELATION NAME: ' + relation_name)\n",
        "    \n",
        "    # nếu entity chứa relation là entity 2 thì label giữ nguyên\n",
        "    elif relation_entity == 2:\n",
        "        if relation_name == 'LOCATED':\n",
        "            sentence_label = 'LOCATED'\n",
        "\n",
        "        elif relation_name == 'PART – WHOLE':\n",
        "            sentence_label = 'PART_WHOLE'\n",
        "\n",
        "        elif relation_name == 'PERSONAL - SOCIAL':\n",
        "            sentence_label = 'PERSONAL_SOCIAL'\n",
        "\n",
        "        elif relation_name == 'AFFILIATION':\n",
        "            sentence_label = 'AFFILIATION'\n",
        "\n",
        "        else:\n",
        "            assert False, str('UNKNOW RELATION NAME: ' + relation_name)\n",
        "\n",
        "    else:\n",
        "        assert False, (\"Unexpect relation_entity. Expect 1 or 2 but got: \" + relation_entity + \".\")\n",
        "\n",
        "    \n",
        "    return sentence_label"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyIgy3tfLDTs"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKFuhu7TYFB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c8efdb9-7521-4f9f-8519-842a632029fd"
      },
      "source": [
        "# Tạo train data (câu chứa cặp entity và label)\n",
        "\n",
        "tdata = []\n",
        "\n",
        "#raw_data = raw_tdata_new_v6\n",
        "raw_data = raw_tdata_new\n",
        "\n",
        "sent_id = 0\n",
        "\n",
        "for docif in raw_data:\n",
        "\n",
        "    # find all entity in current doc    \n",
        "    doc_entity_lst = find_all_entity_in_doc(raw_data, docif['doc_id'])\n",
        "\n",
        "    # if whole doc has 0 or 1 entity -->  no relation in this doc --> skip\n",
        "    # there is many doc like this (like doc has only 3 columns,...)\n",
        "    if len(doc_entity_lst) <= 1:\n",
        "        continue\n",
        "\n",
        "\n",
        "    text = docif[\"text\"]\n",
        "    sentences = my_sentences_tokenize(docif['doc_id'], text)\n",
        "\n",
        "    \n",
        "    ######## extract training sentence\n",
        " \n",
        "    pre_espos = 0   # end of pre sentence\n",
        "\n",
        "    for isent, sent in enumerate(sentences):\n",
        "\n",
        "        sentif = {}\n",
        "        '''\n",
        "        sentif[\"doc_id\"] = docif[\"doc_id\"]\n",
        "        sentif[\"sentence\"] = sent\n",
        "        '''\n",
        "        ###### sentence position\n",
        "        # tìm vị trí của câu để dựa vào đó biết entity (các tokens) thuộc câu nào\n",
        "\n",
        "        # text may have two indentical sentences\n",
        "        # so we have to find start position of current sentence in the rest of the text that not contain previous sentences.\n",
        "\n",
        "        assert (text[pre_espos:].find(sent) >= 0), str(\"Position has problem. \\nDoc: \" + docif[\"doc_id\"] + \"\\nCurrent sentence: \" + sent)\n",
        "\n",
        "        sspos = text[pre_espos:].find(sent) + pre_espos\n",
        "        espos = sspos + len(sent)\n",
        "\n",
        "        # update pre_espos\n",
        "        pre_espos = espos\n",
        "        \n",
        "        assert (sent == text[sspos:espos]), str(\"Position founded is not matched in text. \\nDoc: \" + docif[\"doc_id\"] + \"\\nCurrent sentence: \" + sent)\n",
        "\n",
        "        '''\n",
        "        sentif[\"spos\"] = [sspos, espos]\n",
        "        '''\n",
        "\n",
        "        ###### get all entity in current sentence\n",
        "        \n",
        "        sen_entity_lst = get_sentence_entities(docif, doc_entity_lst, sent, sspos, espos)\n",
        "\n",
        "        # if current sentence has 0 or 1 entity -> no relation availabel to classify -> skip\n",
        "        if len(sen_entity_lst) <= 1: \n",
        "            continue\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "        ###### create n*(n-1)/2 sentence\n",
        "        \n",
        "        for ient, ent_1 in enumerate(sen_entity_lst):\n",
        "            for jent, ent_2 in enumerate(sen_entity_lst[(ient+1):]):\n",
        "                \n",
        "                first_tkeid_ent1 = ent_1[0][0]   # dòng chứa token đầu trong entity\n",
        "                first_tkeid_ent2 = ent_2[0][0]\n",
        "                \n",
        "                last_tkeid_ent1 = ent_1[-1][0]   # dòng chứa token cuối trong entity\n",
        "                last_tkeid_ent2 = ent_2[-1][0]\n",
        "\n",
        "                if (docif['entity'][first_tkeid_ent1][1] != \"MISCELLANEOUS\") and (docif['entity'][first_tkeid_ent2][1] != \"MISCELLANEOUS\"):\n",
        "                    \n",
        "                    # pos của entity trong doc: start của token đầu và end của token cuối trong entity\n",
        "                    ent1_pos_doc = [docif['pos'][first_tkeid_ent1][0], docif['pos'][last_tkeid_ent1][1]]\n",
        "                    ent2_pos_doc = [docif['pos'][first_tkeid_ent2][0], docif['pos'][last_tkeid_ent2][1]]\n",
        "\n",
        "                    # pos của entity trong câu chứa entity\n",
        "                    ent1_pos_sent = [(ent1_pos_doc[0] - sspos), (ent1_pos_doc[1] - sspos)]\n",
        "                    ent2_pos_sent = [(ent2_pos_doc[0] - sspos), (ent2_pos_doc[1] - sspos)]\n",
        "\n",
        "                    \n",
        "                    # kiểm tra xem pos trong doc và sent có khớp, trả về cùng entity không\n",
        "                    assert (sent[ent1_pos_sent[0]:ent1_pos_sent[1]] == text[ent1_pos_doc[0]:ent1_pos_doc[1]]), \\\n",
        "                    str('Entity 1: pos_doc and pos_sent not matched. \\nDoc: ' + docif['doc_id'] + '\\nSent: ' + sent)\n",
        "\n",
        "                    assert (sent[ent2_pos_sent[0]:ent2_pos_sent[1]] == text[ent2_pos_doc[0]:ent2_pos_doc[1]]), \\\n",
        "                    str('Entity 1: pos_doc and pos_sent not matched. \\nDoc: ' + docif['doc_id'] + '\\nSent: ' + sent)\n",
        "\n",
        "\n",
        "                    ent1_text = sent[ent1_pos_sent[0]:ent1_pos_sent[1]]\n",
        "                    ent2_text = sent[ent2_pos_sent[0]:ent2_pos_sent[1]]\n",
        "\n",
        "                    ###############\n",
        "                    # kiểm tra xem mọi token trong entity đã có mặt trong entity lấy từ pos hay chưa\n",
        "                    for itk, tk in enumerate(ent_1):\n",
        "                        eid_tk = tk[0]\n",
        "\n",
        "                        if itk < (len(ent_1) - 1):\n",
        "                            eid_n_tk = ent_1[itk+1][0]\n",
        "\n",
        "                            assert (docif['pos'][eid_tk][1] < docif['pos'][eid_n_tk][0]), \\\n",
        "                            str(\"Position not increase. Doc: \" + docif['doc_id'] + \"\\nSent: \" + sent + \"\\ncrr-pos: \" + str(docif['pos'][eid_tk][1]) + \"\\nnpos: \" + str(docif['pos'][eid_ntk][0]))\n",
        "\n",
        "\n",
        "                        assert (ent1_pos_doc[0] <= docif['pos'][eid_tk][0]) and (docif['pos'][eid_tk][1] <= ent1_pos_doc[1]), \\\n",
        "                        str('Entity\\'s token pos not inside entity pos')\n",
        "                    \n",
        "                    ###############\n",
        "\n",
        "                    ##########\n",
        "\n",
        "                    assert (ent_1[0][1] == docif['token_ids'][first_tkeid_ent1]), str('NOT MATCHED entity first token id')\n",
        "                    assert (ent_2[0][1] == docif['token_ids'][first_tkeid_ent2]), str('NOT MATCHED entity first token id')\n",
        "\n",
        "                    sentence_label = None\n",
        "\n",
        "                    # nếu cả 2 entity không có relation\n",
        "                    if (docif['relation'][first_tkeid_ent1] == None) and (docif['relation'][first_tkeid_ent2] == None):\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                    # nếu cả 2 entity có relation\n",
        "                    elif (docif['relation'][first_tkeid_ent1] != None) and (docif['relation'][first_tkeid_ent2] != None):\n",
        "                        \n",
        "                        # mặc định là OTHERS, nếu bên dưới tìm thấy relation link tới thì sẽ được thay đổi\n",
        "                        # còn nếu bên dưới tìm không thấy (tức là không có relation) thì sẽ không bị thay đổi và vẫn là OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                        for rel_1 in docif['relation'][first_tkeid_ent1]:\n",
        "                            if rel_1[1] == ent_2[0][1]:   # relation ở entity 1 link tới token đầu entity 2\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_1[0], 1)\n",
        "\n",
        "                                # kiem tra relation direction\n",
        "                                # do relation nam o ent_1 nen direction la: [ent_2, ent_1]\n",
        "                                if rel_1[3] != None:\n",
        "                                    assert (rel_1[3] == [ent_2[0][2], ent_1[0][2]]), str('CODE 1: Not match direction')\n",
        "                                \n",
        "\n",
        "                        for rel_2 in docif['relation'][first_tkeid_ent2]:\n",
        "                            if rel_2[1] == ent_1[0][1]:   # relation ở entity 2 link tới token đầu entity 1\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_2[0], 2)\n",
        "\n",
        "                                # do relation nam o ent_2 nen direction la: [ent_1, ent_2]\n",
        "                                if rel_2[3] != None:\n",
        "                                    assert (rel_2[3] == [ent_1[0][2], ent_2[0][2]]), str('CODE 2: Not match direction')\n",
        "        \n",
        "\n",
        "                    # nếu entity 1 có relation, entity 2 không có\n",
        "                    elif (docif['relation'][first_tkeid_ent1] != None) and (docif['relation'][first_tkeid_ent2] == None):\n",
        "                        \n",
        "                        # mặc định là OTHERS, nếu bên dưới tìm thấy relation link tới thì sẽ được thay đổi\n",
        "                        # còn nếu bên dưới tìm không thấy (tức là không có relation) thì sẽ không bị thay đổi và vẫn là OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                        for rel_1 in docif['relation'][first_tkeid_ent1]:\n",
        "                            if rel_1[1] == ent_2[0][1]:   # relation ở entity 1 link tới token đầu entity 2\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_1[0], 1)\n",
        "\n",
        "                                # do relation nam o ent_1 nen direction la: [ent_2, ent_1]\n",
        "                                if rel_1[3] != None:\n",
        "                                    assert (rel_1[3] == [ent_2[0][2], ent_1[0][2]]), str('CODE 3: Not match direction')\n",
        "\n",
        "                                \n",
        "\n",
        "                    # nếu entity 2 có relation, entity 1 không có\n",
        "                    elif (docif['relation'][first_tkeid_ent1] == None) and (docif['relation'][first_tkeid_ent2] != None):\n",
        "                        \n",
        "                        # mặc định là OTHERS, nếu bên dưới tìm thấy relation link tới thì sẽ được thay đổi\n",
        "                        # còn nếu bên dưới tìm không thấy (tức là không có relation) thì sẽ không bị thay đổi và vẫn là OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "                        \n",
        "                        for rel_2 in docif['relation'][first_tkeid_ent2]:\n",
        "                            if rel_2[1] == ent_1[0][1]:   # relation ở entity 2 link tới token đầu entity 1\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_2[0], 2)\n",
        "\n",
        "                                # do relation nam o ent_2 nen direction la: [ent_1, ent_2]\n",
        "                                if rel_2[3] != None:\n",
        "                                    assert (rel_2[3] == [ent_1[0][2], ent_2[0][2]]), str('CODE 4: Not match direction')\n",
        "\n",
        "                        \n",
        "                    ##########\n",
        "\n",
        "                    sentif[\"doc_id\"] = docif[\"doc_id\"]\n",
        "\n",
        "                    sent_id += 1\n",
        "                    sentif['sent_id'] = sent_id\n",
        "\n",
        "                    sentif[\"sentence\"] = sent\n",
        "                    sentif[\"spos\"] = [sspos, espos]\n",
        "\n",
        "                    entity_1 = {'text': copy.deepcopy(ent1_text), 'pos': copy.deepcopy(ent1_pos_sent)}\n",
        "                    entity_2 = {'text': copy.deepcopy(ent2_text), 'pos': copy.deepcopy(ent2_pos_sent)}\n",
        "\n",
        "                    sentif['entity_1'] = copy.deepcopy(entity_1)\n",
        "                    sentif['entity_2'] = copy.deepcopy(entity_2)\n",
        "\n",
        "                    sentif['label'] = copy.deepcopy(sentence_label)\n",
        "\n",
        "                    \n",
        "                    \n",
        "                    # may dong tren co the khong co copy.deepcopy nhung dong ben duoi khong co la bi loi\n",
        "                    tdata.append(copy.deepcopy(sentif))\n",
        "\n",
        "print(\"DONE\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu7DbslJN1i4",
        "outputId": "45c8df1a-d4c4-4d20-9115-0ead3af46415"
      },
      "source": [
        "count_label_others = 0\n",
        "\n",
        "for tdata_point in tdata:\n",
        "    if tdata_point['label'] == 'OTHERS':\n",
        "        count_label_others += 1\n",
        "\n",
        "print(count_label_others)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SfkCqCFN35t",
        "outputId": "d6aa7ecf-1708-4be6-be8c-456415577a1b"
      },
      "source": [
        "print('Count of labels that is not OTHERS: ', (len(tdata) - count_label_others))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of labels that is not OTHERS:  1470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3O1aoGj796m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b3d7721-d017-4b29-a74a-afdcb674de18"
      },
      "source": [
        "print(len(tdata))\n",
        "print(*tdata[0:15], sep='\\n')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9235\n",
            "{'doc_id': '23351996', 'sent_id': 1, 'sentence': \"U16 Việt Nam dội 'mưa gôn' vào lưới Mông Cổ Không nằm ngoài dự đoán, U16 Việt Nam đã có chiến thắng dễ dàng trước U16 Mông Cổ .\", 'spos': [0, 127], 'entity_1': {'text': 'U16 Việt Nam', 'pos': [0, 12]}, 'entity_2': {'text': 'Mông Cổ', 'pos': [36, 43]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 2, 'sentence': \"U16 Việt Nam dội 'mưa gôn' vào lưới Mông Cổ Không nằm ngoài dự đoán, U16 Việt Nam đã có chiến thắng dễ dàng trước U16 Mông Cổ .\", 'spos': [0, 127], 'entity_1': {'text': 'U16 Việt Nam', 'pos': [0, 12]}, 'entity_2': {'text': 'U16 Việt Nam', 'pos': [69, 81]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 3, 'sentence': \"U16 Việt Nam dội 'mưa gôn' vào lưới Mông Cổ Không nằm ngoài dự đoán, U16 Việt Nam đã có chiến thắng dễ dàng trước U16 Mông Cổ .\", 'spos': [0, 127], 'entity_1': {'text': 'U16 Việt Nam', 'pos': [0, 12]}, 'entity_2': {'text': 'U16 Mông Cổ', 'pos': [114, 125]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 4, 'sentence': \"U16 Việt Nam dội 'mưa gôn' vào lưới Mông Cổ Không nằm ngoài dự đoán, U16 Việt Nam đã có chiến thắng dễ dàng trước U16 Mông Cổ .\", 'spos': [0, 127], 'entity_1': {'text': 'Mông Cổ', 'pos': [36, 43]}, 'entity_2': {'text': 'U16 Việt Nam', 'pos': [69, 81]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 5, 'sentence': \"U16 Việt Nam dội 'mưa gôn' vào lưới Mông Cổ Không nằm ngoài dự đoán, U16 Việt Nam đã có chiến thắng dễ dàng trước U16 Mông Cổ .\", 'spos': [0, 127], 'entity_1': {'text': 'Mông Cổ', 'pos': [36, 43]}, 'entity_2': {'text': 'U16 Mông Cổ', 'pos': [114, 125]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 6, 'sentence': \"U16 Việt Nam dội 'mưa gôn' vào lưới Mông Cổ Không nằm ngoài dự đoán, U16 Việt Nam đã có chiến thắng dễ dàng trước U16 Mông Cổ .\", 'spos': [0, 127], 'entity_1': {'text': 'U16 Việt Nam', 'pos': [69, 81]}, 'entity_2': {'text': 'U16 Mông Cổ', 'pos': [114, 125]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 7, 'sentence': 'Như vậy tại bảng I vòng loại U16 châu Á 2018 , U16 Việt Nam và U16 Australia tạm bằng điểm nhau.', 'spos': [153, 249], 'entity_1': {'text': 'U16 Việt Nam', 'pos': [47, 59]}, 'entity_2': {'text': 'U16 Australia', 'pos': [63, 76]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 8, 'sentence': 'U16 Việt Nam thắng dễ U16 Mông Cổ .', 'spos': [316, 351], 'entity_1': {'text': 'U16 Việt Nam', 'pos': [0, 12]}, 'entity_2': {'text': 'U16 Mông Cổ', 'pos': [22, 33]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 9, 'sentence': 'Trong trận ra quân tại bảng I vòng loại U16 châu Á 2018 gặp U16 Campuchia , dù bị gỡ hòa 1-1 và bị mất người ở phút 20, nhưng U16 Việt Nam vẫn chơi xuất sắc để có chiến thắng chung cuộc 5-2.', 'spos': [352, 542], 'entity_1': {'text': 'U16 Campuchia', 'pos': [60, 73]}, 'entity_2': {'text': 'U16 Việt Nam', 'pos': [126, 138]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 10, 'sentence': 'Bước vào trận thứ 2 gặp chủ nhà U16 Mông Cổ , U16 Việt Nam tràn đầy tự tin hướng tới một chiến thắng đậm nhằm tạo đà tâm lý trước cuộc quyết đấu với U16 Australia vào chiều ngày 24/9 tới.', 'spos': [543, 730], 'entity_1': {'text': 'U16 Mông Cổ', 'pos': [32, 43]}, 'entity_2': {'text': 'U16 Việt Nam', 'pos': [46, 58]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 11, 'sentence': 'Bước vào trận thứ 2 gặp chủ nhà U16 Mông Cổ , U16 Việt Nam tràn đầy tự tin hướng tới một chiến thắng đậm nhằm tạo đà tâm lý trước cuộc quyết đấu với U16 Australia vào chiều ngày 24/9 tới.', 'spos': [543, 730], 'entity_1': {'text': 'U16 Mông Cổ', 'pos': [32, 43]}, 'entity_2': {'text': 'U16 Australia', 'pos': [149, 162]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 12, 'sentence': 'Bước vào trận thứ 2 gặp chủ nhà U16 Mông Cổ , U16 Việt Nam tràn đầy tự tin hướng tới một chiến thắng đậm nhằm tạo đà tâm lý trước cuộc quyết đấu với U16 Australia vào chiều ngày 24/9 tới.', 'spos': [543, 730], 'entity_1': {'text': 'U16 Việt Nam', 'pos': [46, 58]}, 'entity_2': {'text': 'U16 Australia', 'pos': [149, 162]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 13, 'sentence': 'Trước một đối thủ bị đánh giá thấp hơn về mọi mặt, U16 Việt Nam không gặp nhiều khó khăn để làm chủ cuộc chơi và nhanh chóng có bàn vượt lên dẫn trước do công của Nguyên Hoàng ngay phút thứ 13.', 'spos': [731, 924], 'entity_1': {'text': 'U16 Việt Nam', 'pos': [51, 63]}, 'entity_2': {'text': 'Nguyên Hoàng', 'pos': [163, 175]}, 'label': 'AFFILIATION_TO'}\n",
            "{'doc_id': '23351996', 'sent_id': 14, 'sentence': '10 phút sau, Thanh Trung (số 7) nâng tỷ số lên 2-0 từ chấm phạt đền sau khi thủ môn đối phương phạm lỗi với Nguyên Hoàng trong vòng cấm.', 'spos': [925, 1061], 'entity_1': {'text': 'Thanh Trung', 'pos': [13, 24]}, 'entity_2': {'text': 'Nguyên Hoàng', 'pos': [108, 120]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 15, 'sentence': 'Những phút còn lại của hiệp 1, U16 Việt Nam tiếp tục tạo ra một sức ép cực lớn lên phần sân của U16 Mông Cổ .', 'spos': [1062, 1171], 'entity_1': {'text': 'U16 Việt Nam', 'pos': [31, 43]}, 'entity_2': {'text': 'U16 Mông Cổ', 'pos': [96, 107]}, 'label': 'OTHERS'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0mBtqdZthaa"
      },
      "source": [
        "# Write to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTM1DITfqJtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14147b46-3706-4bb9-de6e-8618a667b88c"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xLBzf43i3NC"
      },
      "source": [
        "# write to file\n",
        "import codecs\n",
        "import json\n",
        "\n",
        "with codecs.open('dev_data.json', 'w', encoding='utf-8') as fout:\n",
        "    json.dump(tdata, fout, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72boeFzXgq-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f158ed02-02a8-450b-a4fb-1aff5516aac4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B8JEv6kDzuV"
      },
      "source": [
        "!mkdir \"/gdrive/MyDrive/VLSP2020_RE\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ng7tJuLD4_V"
      },
      "source": [
        "!mkdir \"/gdrive/MyDrive/VLSP2020_RE/json_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU0dYmmshRFZ"
      },
      "source": [
        "!cp -i dev_data.json \"/gdrive/MyDrive/VLSP2020_RE/json_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eWnRTZYSwm2",
        "outputId": "7ff49420-23a3-47a1-f08f-5f6d36effee2"
      },
      "source": [
        "import filecmp\n",
        "filecmp.cmp('dev_data.json', '/gdrive/MyDrive/VLSP2020_RE/json_data/dev_data.json')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VLSP2020_RE_extract_dev_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "H3ICier0JyzI",
        "PcS3CiYZxv8a",
        "jhOuiCNgaVqk",
        "chNxSZ0mx5wS",
        "-wzBGIIgKYPD",
        "EYMqFdP5eTPL",
        "kBr6Gx-Y2oj2",
        "dfXerQf-fQSv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQDbz_q5irs0"
      },
      "source": [
        "# Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxh7foXeJtrf"
      },
      "source": [
        "## Unrar dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etKZzzHSF6_i"
      },
      "source": [
        "Please upload VLSP2020_RE_dev.rar to Colab then */content* folder then unrar it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXjtAzVJHNI_",
        "outputId": "108b2f62-ff97-40ce-df63-264f53757142"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6x3Nl_tfBf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cecfe03-7595-4be0-c476-afce6e3fed8c"
      },
      "source": [
        "!pip install unrar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unrar\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/0b/53130ccd483e3db8c8a460cb579bdb21b458d5494d67a261e1a5b273fbb9/unrar-0.4-py3-none-any.whl\n",
            "Installing collected packages: unrar\n",
            "Successfully installed unrar-0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnlpfnYxiSiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c473b773-ec98-4102-c459-7cf1279eaae5"
      },
      "source": [
        "!unrar x VLSP2020_RE_dev.rar"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from VLSP2020_RE_dev.rar\n",
            "\n",
            "Creating    VLSP2020_RE_dev                                           OK\n",
            "Creating    VLSP2020_RE_dev/23351996.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23351996.conll/CURATION_USER.tsv             \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23351997.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23351997.conll/CURATION_USER.tsv             \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23351998.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23351998.conll/CURATION_USER.tsv             \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352000.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352000.conll/CURATION_USER.tsv             \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352001.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352001.conll/CURATION_USER.tsv             \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352002.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352002.conll/CURATION_USER.tsv             \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352003.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352003.conll/CURATION_USER.tsv             \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352006.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352006.conll/CURATION_USER.tsv             \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352009.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352009.conll/CURATION_USER.tsv             \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352013.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352013.conll/CURATION_USER.tsv             \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352014.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv             \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352016.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352016.conll/CURATION_USER.tsv             \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352018.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352018.conll/CURATION_USER.tsv             \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352019.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352019.conll/CURATION_USER.tsv             \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352020.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352020.conll/CURATION_USER.tsv             \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352021.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352021.conll/CURATION_USER.tsv             \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352024.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352024.conll/CURATION_USER.tsv             \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352026.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352026.conll/CURATION_USER.tsv             \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352027.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352027.conll/CURATION_USER.tsv             \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352028.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352028.conll/CURATION_USER.tsv             \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352030.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352030.conll/CURATION_USER.tsv             \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352033.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352033.conll/CURATION_USER.tsv             \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352065.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352065.conll/CURATION_USER.tsv             \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352066.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352066.conll/CURATION_USER.tsv             \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352070.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352070.conll/CURATION_USER.tsv             \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352071.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352071.conll/CURATION_USER.tsv             \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352073.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352073.conll/CURATION_USER.tsv             \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352075.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352075.conll/CURATION_USER.tsv             \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352078.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352078.conll/CURATION_USER.tsv             \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352079.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352079.conll/CURATION_USER.tsv             \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352081.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352081.conll/CURATION_USER.tsv             \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352084.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352084.conll/CURATION_USER.tsv             \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352085.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352085.conll/CURATION_USER.tsv             \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352087.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352087.conll/CURATION_USER.tsv             \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352089.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352089.conll/CURATION_USER.tsv             \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352090.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352090.conll/CURATION_USER.tsv             \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352099.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352099.conll/CURATION_USER.tsv             \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352107.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352107.conll/CURATION_USER.tsv             \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352110.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352110.conll/CURATION_USER.tsv             \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352117.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352117.conll/CURATION_USER.tsv             \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352122.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352122.conll/CURATION_USER.tsv             \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352125.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352125.conll/CURATION_USER.tsv             \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352126.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352126.conll/CURATION_USER.tsv             \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352143.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352143.conll/CURATION_USER.tsv             \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352161.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352161.conll/CURATION_USER.tsv             \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352163.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352163.conll/CURATION_USER.tsv             \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352173.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352173.conll/CURATION_USER.tsv             \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352190.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352190.conll/CURATION_USER.tsv             \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352200.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352200.conll/CURATION_USER.tsv             \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352232.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352232.conll/CURATION_USER.tsv             \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352236.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352236.conll/CURATION_USER.tsv             \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352239.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352239.conll/CURATION_USER.tsv             \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352240.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352240.conll/CURATION_USER.tsv             \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352260.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352260.conll/CURATION_USER.tsv             \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352265.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352265.conll/CURATION_USER.tsv             \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352283.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352283.conll/CURATION_USER.tsv             \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352292.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352292.conll/CURATION_USER.tsv             \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352296.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352296.conll/CURATION_USER.tsv             \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352297.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352297.conll/CURATION_USER.tsv             \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352298.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352298.conll/CURATION_USER.tsv             \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352299.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352299.conll/CURATION_USER.tsv             \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352300.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352300.conll/CURATION_USER.tsv             \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352301.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352301.conll/CURATION_USER.tsv             \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352303.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352303.conll/CURATION_USER.tsv             \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352305.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352305.conll/CURATION_USER.tsv             \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352309.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352309.conll/CURATION_USER.tsv             \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352314.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352314.conll/CURATION_USER.tsv             \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352316.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352316.conll/CURATION_USER.tsv             \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352317.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352317.conll/CURATION_USER.tsv             \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352319.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352319.conll/CURATION_USER.tsv             \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352320.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352320.conll/CURATION_USER.tsv             \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352321.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352321.conll/CURATION_USER.tsv             \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352322.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352322.conll/CURATION_USER.tsv             \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352323.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352323.conll/CURATION_USER.tsv             \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352326.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352326.conll/CURATION_USER.tsv             \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352327.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352327.conll/CURATION_USER.tsv             \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352328.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352328.conll/CURATION_USER.tsv             \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352329.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352329.conll/CURATION_USER.tsv             \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352331.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352331.conll/CURATION_USER.tsv             \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352332.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352332.conll/CURATION_USER.tsv             \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352334.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352334.conll/CURATION_USER.tsv             \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352335.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352335.conll/CURATION_USER.tsv             \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352337.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352337.conll/CURATION_USER.tsv             \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352343.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352343.conll/CURATION_USER.tsv             \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352345.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352345.conll/CURATION_USER.tsv             \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352346.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352346.conll/CURATION_USER.tsv             \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352347.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352347.conll/CURATION_USER.tsv             \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352348.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352348.conll/CURATION_USER.tsv             \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352352.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352352.conll/CURATION_USER.tsv             \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352357.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352357.conll/CURATION_USER.tsv             \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352359.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352359.conll/CURATION_USER.tsv             \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352361.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352361.conll/CURATION_USER.tsv             \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352363.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352363.conll/CURATION_USER.tsv             \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352366.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352366.conll/CURATION_USER.tsv             \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352370.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352370.conll/CURATION_USER.tsv             \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352372.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352372.conll/CURATION_USER.tsv             \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352373.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352373.conll/CURATION_USER.tsv             \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352376.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352376.conll/CURATION_USER.tsv             \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352378.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352378.conll/CURATION_USER.tsv             \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352381.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352381.conll/CURATION_USER.tsv             \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352382.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352382.conll/CURATION_USER.tsv             \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352385.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352385.conll/CURATION_USER.tsv             \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352390.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352390.conll/CURATION_USER.tsv             \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352393.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352393.conll/CURATION_USER.tsv             \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352396.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352396.conll/CURATION_USER.tsv             \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352397.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352397.conll/CURATION_USER.tsv             \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352401.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352401.conll/CURATION_USER.tsv             \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352402.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352402.conll/CURATION_USER.tsv             \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352405.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352405.conll/CURATION_USER.tsv             \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352408.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352408.conll/CURATION_USER.tsv             \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352409.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352409.conll/CURATION_USER.tsv             \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352410.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352410.conll/CURATION_USER.tsv             \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352411.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352411.conll/CURATION_USER.tsv             \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352412.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352412.conll/CURATION_USER.tsv             \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352413.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352413.conll/CURATION_USER.tsv             \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352414.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352414.conll/CURATION_USER.tsv             \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352415.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352415.conll/CURATION_USER.tsv             \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352416.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352416.conll/CURATION_USER.tsv             \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352417.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352417.conll/CURATION_USER.tsv             \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352419.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352419.conll/CURATION_USER.tsv             \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352421.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352421.conll/CURATION_USER.tsv             \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352425.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352425.conll/CURATION_USER.tsv             \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352427.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352427.conll/CURATION_USER.tsv             \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352428.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352428.conll/CURATION_USER.tsv             \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352429.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352429.conll/CURATION_USER.tsv             \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352432.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352432.conll/CURATION_USER.tsv             \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352433.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352433.conll/CURATION_USER.tsv             \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352434.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352434.conll/CURATION_USER.tsv             \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352436.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352436.conll/CURATION_USER.tsv             \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352437.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352437.conll/CURATION_USER.tsv             \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352438.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352438.conll/CURATION_USER.tsv             \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352440.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352440.conll/CURATION_USER.tsv             \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352442.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352442.conll/CURATION_USER.tsv             \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352443.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352443.conll/CURATION_USER.tsv             \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352444.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352444.conll/CURATION_USER.tsv             \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352445.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352445.conll/CURATION_USER.tsv             \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352448.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352448.conll/CURATION_USER.tsv             \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352449.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352449.conll/CURATION_USER.tsv             \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352450.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352450.conll/CURATION_USER.tsv             \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352451.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352451.conll/CURATION_USER.tsv             \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352453.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352453.conll/CURATION_USER.tsv             \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352456.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352456.conll/CURATION_USER.tsv             \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352457.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352457.conll/CURATION_USER.tsv             \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352460.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352460.conll/CURATION_USER.tsv             \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352461.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352461.conll/CURATION_USER.tsv             \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352462.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352462.conll/CURATION_USER.tsv             \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352467.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352467.conll/CURATION_USER.tsv             \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352468.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352468.conll/CURATION_USER.tsv             \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352470.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352470.conll/CURATION_USER.tsv             \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352471.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352471.conll/CURATION_USER.tsv             \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352473.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352473.conll/CURATION_USER.tsv             \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352476.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352476.conll/CURATION_USER.tsv             \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352477.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352477.conll/CURATION_USER.tsv             \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352480.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352480.conll/CURATION_USER.tsv             \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352482.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352482.conll/CURATION_USER.tsv             \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352486.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352486.conll/CURATION_USER.tsv             \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352487.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352487.conll/CURATION_USER.tsv             \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352488.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352488.conll/CURATION_USER.tsv             \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352489.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352489.conll/CURATION_USER.tsv             \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352491.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv             \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352492.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352492.conll/CURATION_USER.tsv             \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352494.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352494.conll/CURATION_USER.tsv             \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352495.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352495.conll/CURATION_USER.tsv             \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352497.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352497.conll/CURATION_USER.tsv             \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352498.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352498.conll/CURATION_USER.tsv             \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352499.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv             \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352507.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352507.conll/CURATION_USER.tsv             \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352508.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352508.conll/CURATION_USER.tsv             \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352517.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv             \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352518.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352518.conll/CURATION_USER.tsv             \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352522.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352522.conll/CURATION_USER.tsv             \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352523.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352523.conll/CURATION_USER.tsv             \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352524.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352524.conll/CURATION_USER.tsv             \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352526.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352526.conll/CURATION_USER.tsv             \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352530.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352530.conll/CURATION_USER.tsv             \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352532.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352532.conll/CURATION_USER.tsv             \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352535.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352535.conll/CURATION_USER.tsv             \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352536.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352536.conll/CURATION_USER.tsv             \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352538.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352538.conll/CURATION_USER.tsv             \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352539.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352539.conll/CURATION_USER.tsv             \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352540.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352540.conll/CURATION_USER.tsv             \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352543.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352543.conll/CURATION_USER.tsv             \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352545.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352545.conll/CURATION_USER.tsv             \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352546.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352546.conll/CURATION_USER.tsv             \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352548.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352548.conll/CURATION_USER.tsv             \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352550.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352550.conll/CURATION_USER.tsv             \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352551.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352551.conll/CURATION_USER.tsv             \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352552.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352552.conll/CURATION_USER.tsv             \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352560.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352560.conll/CURATION_USER.tsv             \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352561.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352561.conll/CURATION_USER.tsv             \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352562.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352562.conll/CURATION_USER.tsv             \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352563.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352563.conll/CURATION_USER.tsv             \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352568.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352568.conll/CURATION_USER.tsv             \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352571.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352571.conll/CURATION_USER.tsv             \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352572.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352572.conll/CURATION_USER.tsv             \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352573.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352573.conll/CURATION_USER.tsv             \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352575.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352575.conll/CURATION_USER.tsv             \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352577.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352577.conll/CURATION_USER.tsv             \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352578.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352578.conll/CURATION_USER.tsv             \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352580.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352580.conll/CURATION_USER.tsv             \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352581.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352581.conll/CURATION_USER.tsv             \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352583.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352583.conll/CURATION_USER.tsv             \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352585.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352585.conll/CURATION_USER.tsv             \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352586.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352586.conll/CURATION_USER.tsv             \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352588.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352588.conll/CURATION_USER.tsv             \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352589.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352589.conll/CURATION_USER.tsv             \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352590.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352590.conll/CURATION_USER.tsv             \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352591.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352591.conll/CURATION_USER.tsv             \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352592.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352592.conll/CURATION_USER.tsv             \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352593.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352593.conll/CURATION_USER.tsv             \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352594.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352594.conll/CURATION_USER.tsv             \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352596.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352596.conll/CURATION_USER.tsv             \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352597.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352597.conll/CURATION_USER.tsv             \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352600.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352600.conll/CURATION_USER.tsv             \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352601.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352601.conll/CURATION_USER.tsv             \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352602.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352602.conll/CURATION_USER.tsv             \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352603.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352603.conll/CURATION_USER.tsv             \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352605.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv             \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352609.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352609.conll/CURATION_USER.tsv             \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352610.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352610.conll/CURATION_USER.tsv             \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352612.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352612.conll/CURATION_USER.tsv             \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352614.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352614.conll/CURATION_USER.tsv             \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352615.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352615.conll/CURATION_USER.tsv             \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352617.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352617.conll/CURATION_USER.tsv             \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352618.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352618.conll/CURATION_USER.tsv             \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352619.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352619.conll/CURATION_USER.tsv             \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352620.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352620.conll/CURATION_USER.tsv             \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352623.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352623.conll/CURATION_USER.tsv             \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352624.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352624.conll/CURATION_USER.tsv             \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352625.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352625.conll/CURATION_USER.tsv             \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352626.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352626.conll/CURATION_USER.tsv             \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352628.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352628.conll/CURATION_USER.tsv             \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352629.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352629.conll/CURATION_USER.tsv             \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352631.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352631.conll/CURATION_USER.tsv             \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352634.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352634.conll/CURATION_USER.tsv             \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352635.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352635.conll/CURATION_USER.tsv             \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352637.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352637.conll/CURATION_USER.tsv             \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352640.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352640.conll/CURATION_USER.tsv             \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352641.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352641.conll/CURATION_USER.tsv             \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352642.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352642.conll/CURATION_USER.tsv             \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352644.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352644.conll/CURATION_USER.tsv             \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352648.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352648.conll/CURATION_USER.tsv             \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352649.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352649.conll/CURATION_USER.tsv             \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352650.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352650.conll/CURATION_USER.tsv             \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352651.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352651.conll/CURATION_USER.tsv             \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352653.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352653.conll/CURATION_USER.tsv             \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352654.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv             \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352733.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352733.conll/CURATION_USER.tsv             \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352734.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352734.conll/CURATION_USER.tsv             \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_dev/23352736.conll                            OK\n",
            "Extracting  VLSP2020_RE_dev/23352736.conll/CURATION_USER.tsv             \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3ICier0JyzI"
      },
      "source": [
        "## Install Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcS3CiYZxv8a"
      },
      "source": [
        "### Install VNCoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzsgsAb4uET6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ea2ac7-a9b8-4cc1-ef77-ae648aa2fcf5"
      },
      "source": [
        "# Install the vncorenlp python wrapper\n",
        "!pip install vncorenlp==1.0.3"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vncorenlp==1.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n",
            "\u001b[K     || 2.7MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp==1.0.3) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (2.10)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp37-none-any.whl size=2645936 sha256=193f48b0db8b29cfead3cd048f0529f53579bfae5360f0ac5975ecded0805f11\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxVE9cR6yZ3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe410fa3-9ab6-499d-8632-5d7e8800e4e2"
      },
      "source": [
        "# Download VnCoreNLP-1.1.1.jar & all of its  component (i.e. RDRSegmenter, pos, ner, deprel) \n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!mkdir -p vncorenlp/models/dep\n",
        "!mkdir -p vncorenlp/models/ner\n",
        "!mkdir -p vncorenlp/models/postagger\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/dep/vi-dep.xz\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger\n",
        "\n",
        "\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "\n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/\n",
        "\n",
        "!mv vi-dep.xz vncorenlp/models/dep/\n",
        "\n",
        "!mv vi-500brownclusters.xz vncorenlp/models/ner/\n",
        "!mv vi-ner.xz vncorenlp/models/ner/\n",
        "!mv vi-pretrainedembeddings.xz vncorenlp/models/ner/\n",
        "\n",
        "!mv vi-tagger vncorenlp/models/postagger/\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-09 12:15:46--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: VnCoreNLP-1.1.1.jar\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M  59.5MB/s    in 0.4s    \n",
            "\n",
            "2021-05-09 12:15:47 (59.5 MB/s) - VnCoreNLP-1.1.1.jar saved [27412575/27412575]\n",
            "\n",
            "--2021-05-09 12:15:47--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: vi-vocab\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-05-09 12:15:48 (12.2 MB/s) - vi-vocab saved [526544/526544]\n",
            "\n",
            "--2021-05-09 12:15:48--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: wordsegmenter.rdr\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-05-09 12:15:48 (5.96 MB/s) - wordsegmenter.rdr saved [128508/128508]\n",
            "\n",
            "--2021-05-09 12:15:48--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/dep/vi-dep.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16048864 (15M) [application/octet-stream]\n",
            "Saving to: vi-dep.xz\n",
            "\n",
            "vi-dep.xz           100%[===================>]  15.30M  46.6MB/s    in 0.3s    \n",
            "\n",
            "2021-05-09 12:15:49 (46.6 MB/s) - vi-dep.xz saved [16048864/16048864]\n",
            "\n",
            "--2021-05-09 12:15:49--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5599844 (5.3M) [application/octet-stream]\n",
            "Saving to: vi-500brownclusters.xz\n",
            "\n",
            "vi-500brownclusters 100%[===================>]   5.34M  28.6MB/s    in 0.2s    \n",
            "\n",
            "2021-05-09 12:15:49 (28.6 MB/s) - vi-500brownclusters.xz saved [5599844/5599844]\n",
            "\n",
            "--2021-05-09 12:15:49--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9956876 (9.5M) [application/octet-stream]\n",
            "Saving to: vi-ner.xz\n",
            "\n",
            "vi-ner.xz           100%[===================>]   9.50M  21.1MB/s    in 0.5s    \n",
            "\n",
            "2021-05-09 12:15:50 (21.1 MB/s) - vi-ner.xz saved [9956876/9956876]\n",
            "\n",
            "--2021-05-09 12:15:50--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 57313672 (55M) [application/octet-stream]\n",
            "Saving to: vi-pretrainedembeddings.xz\n",
            "\n",
            "vi-pretrainedembedd 100%[===================>]  54.66M  93.5MB/s    in 0.6s    \n",
            "\n",
            "2021-05-09 12:15:52 (93.5 MB/s) - vi-pretrainedembeddings.xz saved [57313672/57313672]\n",
            "\n",
            "--2021-05-09 12:15:52--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29709468 (28M) [application/octet-stream]\n",
            "Saving to: vi-tagger\n",
            "\n",
            "vi-tagger           100%[===================>]  28.33M  79.6MB/s    in 0.4s    \n",
            "\n",
            "2021-05-09 12:15:52 (79.6 MB/s) - vi-tagger saved [29709468/29709468]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXfVgT46BB-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7782ede-9618-4f02-c815-e11f8ecaf00f"
      },
      "source": [
        "import unicodedata\n",
        "from vncorenlp import VnCoreNLP\n",
        "\n",
        "# To perform word segmentation, POS tagging, NER and then dependency parsing\n",
        "annotator1 = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx2g') \n",
        "\n",
        "# To perform word segmentation, POS tagging and then NER\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg,pos,ner\", max_heap_size='-Xmx2g') \n",
        "# To perform word segmentation and then POS tagging\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg,pos\", max_heap_size='-Xmx2g') \n",
        "# To perform word segmentation only\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
        "# Input \n",
        "text = unicodedata.normalize(\"NFD\", \"Thanh Thy\")\n",
        "\n",
        "\n",
        "# To perform word segmentation only\n",
        "word_segmented_text = annotator1.tokenize(text) \n",
        "\n",
        "print(*word_segmented_text, sep=\"\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Thanh', 'Thuy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhOuiCNgaVqk"
      },
      "source": [
        "### Install Underthesea"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IujpzlPaKfo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f25268-5d38-470e-c8a5-676a29e93b3f"
      },
      "source": [
        "!pip install underthesea==1.2.3"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting underthesea==1.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/46/1acb7e83092bbcbc9082afe3901ec51e98a303a19c8152655c43bd51583f/underthesea-1.2.3-py3-none-any.whl (7.5MB)\n",
            "\u001b[K     || 7.5MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (4.41.1)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (7.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (2.23.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (0.8.9)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     || 245kB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (3.13)\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     || 51kB 5.5MB/s \n",
            "\u001b[?25hCollecting scikit-learn<0.22,>=0.20\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/c5/e5267eb84994e9a92a2c6a6ee768514f255d036f3c8378acfa694e9f2c99/scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     || 6.7MB 26.3MB/s \n",
            "\u001b[?25hCollecting python-crfsuite>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     || 747kB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea==1.2.3) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (3.0.4)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval->underthesea==1.2.3) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.22,>=0.20->underthesea==1.2.3) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=8363720c301a5bb035a66c685c330f04e268b1dde7fec2d99638183458607c7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: unidecode, scikit-learn, seqeval, python-crfsuite, underthesea\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed python-crfsuite-0.9.7 scikit-learn-0.21.3 seqeval-1.2.2 underthesea-1.2.3 unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjsyeDFRKpfF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579a7025-4186-4fd3-b656-f6044157e36a"
      },
      "source": [
        "from underthesea import sent_tokenize\n",
        "text = 'Qung Bnh : Ct tc lng hnh, him ha rnh rp cu Long i v dng sng? Hin tng khai thc ct lu trn sng cch cu Long i vi trm mt v pha h lu, khin cu v sng Long i ang ng trc him ha kh lng? Va qua Php lut Plus nhn phn nh ca nhng ngi dn sng  x Xun Ninh , huyn Qung Ninh , tnh Qung Bnh v vic hin nay  x ny, c th l ti thn Xun Dc 1 khu vc ven sng Long i lu ny xut hin nhng bi tp kt ct tri php v hin tng khai thc ct lu trn sng c ngy ln m gy nh hng n n cuc sng thng nht ca ngi dn ni y. T nhng ngun tin nu trn sng ngy 21/9, PV  tip cn hin trng on sng Long i thuc thn Xun D 1 , x Xun Ninh , huyn Qung Ninh , tnh Qung Bnh ni ngi dn phn nh  cng c thng tin. Ti y, PV nhn thy nhiu bi tp kt ct gn khu vc dn c sinh sng, hng ngy nhiu tu ch ct vo y  tp kt ct, gy ra ting n kh chu nh hng khng nh n sinh hot ca ngi dn. Nhng bi tp kt ct tri php. Hn th na theo tm hiu ca PV c bit, nhng v tr c bi tp kt ct k trn khng  tiu chun  tu cp bn tp kt?. Tra cng ngy PV  i theo hng thng ngun sng Long i m theo phn nh l xy ra tnh trng khai thc ct tri php thng xuyn din ra. PV nhn thy mt chic thuyn ang neo u cch b chng vi chc mt v cch mng cu Long i chng vi trm mt theo hng h ngun ang ht ct ln thuyn. Chic thuyn ( ) ang khai thc ct tri php cch cu Long i khng xa. Tip tc ghi nhn v theo di v vic khong chng hn 30 pht, chic thuyn  y ct  c i chuyn i tp kt. Chic thuyn sau khi ht ct tri php di chuyn v bi tp kt. Qua tm hiu ca PV c bit, ct  khu vc gn cu Long i l ct nhim mn nu dng vo vic thi cng cng trnh s nh hng n cht lng ca cng trnh  V ct ny c ch thuyn bn li cho ngi s dng vi gi r hn so vi ct c khai thc  m c cp php gy nn s cnh tranh khng lnh mnh v gi ct. Tuy nhin nhiu ngi dn cha nhn thy n vic cht lng ca cng trnh sau ny khi s dng ct nhim mn ny. iu ng ni l vic khai thc ct tri php li din ra khu vc gn mng cu Long i (c ng st ln ng b) nguy c sc l t khu vc mng cu, khin cu Long i ng trc him ha kh lng?. Lin quan n vn  ny, trao i vi PV ng Nguyn Trng Tin  Ch tch x Xun Ninh cho bit v pha x cng  nhiu ln x l nhc nh ngi dn trong vn  tp kt ct ng ni quy nh. Ngoi ra, x ang hng dn v hon thnh cc th tc nhm a cc im tp kt tri php ny ng vo ni quy nh trong thi gian sm nht, ng Tin cho bit thm. ng Nguyn Trng Tin  Ch tch x Xun Ninh (bn phi) ti bui lm vic vi PV. Khi c PV cng cp bng chng v vic thuyn khai thc ct tri php ngay gia ban ngy gn khu vc mng cu Long i , ng Tin  ht sc bt ng ni Nh vy l khng c ri, khng c ri s cho x l ngay Tip  PV lin lc qua in thoi vi ng Phm Trung ng  Ch tch UBND huyn Qung Ninh  phn nh s vic th ng ng cho bit, ang bn v hng dn PV lin h vi ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh  lm vic. Ti bui l vic vi ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh PV  cung cp clip v vic nn khai thc tri php din ra ngay trn sng Long i on gn mng cu ng Giai cng  kin quyt v ha s u tranh x l, ng thi phi hp vi cc c quan chc nng khc thng xuyn kim tra  chm dt tnh trng ny. ng Nguyn Vit Giai cho bit s u tranh x l Cn v vic cc bi tp kt tri php, ng Giai cho bit s x l dt im trong thi gian sm nht  khng nh hng ti cuc sng ngi dn xung quanh. Khi c PV hi thi gian sm nht l bao lu ng Giai cho bit:  y ang cn vng mt khu th tc. thi gian gii quyt sm nht cng phi mt chng 7 n 10 ngy. Vic khai thc ct tri php gn cu Long i (c ng st ln ng b) nguy c st l t khu vc mng cu, khin cu Long i ng trc him ha kh lng? Tuy l vy nhng trong sng 22/9, PV mt ln na n ti hin trng chic thuyn khai thc tri php th nhn thy tnh hnh khai thc ct tri php vn khng h thay i. Mt ln na PV  gi in thoi cho ng Nguyn Trng Tin  Ch tch x Xun Ninh v ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh  phn nh th li c 2 v ha s x l. Trong sng 22/9 vic khai thc ct tri php vn din ra m khng c s can thip ca c quyan chc nng? T nhng vic nu trn, d lun khng th khng t ra cu hi liu nhng vic xy ra  y c phi l c s bo k hoc c s tip tay ca lc lng chc nng c thm thm quyn hay khng? Php lut Plus s tip tc thng tin v vic n bn c.'\n",
        "sent_tokenize(text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Qung Bnh : Ct tc lng hnh, him ha rnh rp cu Long i v dng sng?',\n",
              " 'Hin tng khai thc ct lu trn sng cch cu Long i vi trm mt v pha h lu, khin cu v sng Long i ang ng trc him ha kh lng?',\n",
              " 'Va qua Php lut Plus nhn phn nh ca nhng ngi dn sng  x Xun Ninh , huyn Qung Ninh , tnh Qung Bnh v vic hin nay  x ny, c th l ti thn Xun Dc 1 khu vc ven sng Long i lu ny xut hin nhng bi tp kt ct tri php v hin tng khai thc ct lu trn sng c ngy ln m gy nh hng n n cuc sng thng nht ca ngi dn ni y.',\n",
              " 'T nhng ngun tin nu trn sng ngy 21/9, PV  tip cn hin trng on sng Long i thuc thn Xun D 1 , x Xun Ninh , huyn Qung Ninh , tnh Qung Bnh ni ngi dn phn nh  cng c thng tin.',\n",
              " 'Ti y, PV nhn thy nhiu bi tp kt ct gn khu vc dn c sinh sng, hng ngy nhiu tu ch ct vo y  tp kt ct, gy ra ting n kh chu nh hng khng nh n sinh hot ca ngi dn.',\n",
              " 'Nhng bi tp kt ct tri php.',\n",
              " 'Hn th na theo tm hiu ca PV c bit, nhng v tr c bi tp kt ct k trn khng  tiu chun  tu cp bn tp kt?.',\n",
              " 'Tra cng ngy PV  i theo hng thng ngun sng Long i m theo phn nh l xy ra tnh trng khai thc ct tri php thng xuyn din ra.',\n",
              " 'PV nhn thy mt chic thuyn ang neo u cch b chng vi chc mt v cch mng cu Long i chng vi trm mt theo hng h ngun ang ht ct ln thuyn.',\n",
              " 'Chic thuyn ( ) ang khai thc ct tri php cch cu Long i khng xa.',\n",
              " 'Tip tc ghi nhn v theo di v vic khong chng hn 30 pht, chic thuyn  y ct  c i chuyn i tp kt.',\n",
              " 'Chic thuyn sau khi ht ct tri php di chuyn v bi tp kt.',\n",
              " 'Qua tm hiu ca PV c bit, ct  khu vc gn cu Long i l ct nhim mn nu dng vo vic thi cng cng trnh s nh hng n cht lng ca cng trnh  V ct ny c ch thuyn bn li cho ngi s dng vi gi r hn so vi ct c khai thc  m c cp php gy nn s cnh tranh khng lnh mnh v gi ct.',\n",
              " 'Tuy nhin nhiu ngi dn cha nhn thy n vic cht lng ca cng trnh sau ny khi s dng ct nhim mn ny.',\n",
              " 'iu ng ni l vic khai thc ct tri php li din ra khu vc gn mng cu Long i (c ng st ln ng b) nguy c sc l t khu vc mng cu, khin cu Long i ng trc him ha kh lng?.',\n",
              " 'Lin quan n vn  ny, trao i vi PV ng Nguyn Trng Tin  Ch tch x Xun Ninh cho bit v pha x cng  nhiu ln x l nhc nh ngi dn trong vn  tp kt ct ng ni quy nh.',\n",
              " 'Ngoi ra, x ang hng dn v hon thnh cc th tc nhm a cc im tp kt tri php ny ng vo ni quy nh trong thi gian sm nht, ng Tin cho bit thm.',\n",
              " 'ng Nguyn Trng Tin  Ch tch x Xun Ninh (bn phi) ti bui lm vic vi PV.',\n",
              " 'Khi c PV cng cp bng chng v vic thuyn khai thc ct tri php ngay gia ban ngy gn khu vc mng cu Long i , ng Tin  ht sc bt ng ni Nh vy l khng c ri, khng c ri s cho x l ngay Tip  PV lin lc qua in thoi vi ng Phm Trung ng  Ch tch UBND huyn Qung Ninh  phn nh s vic th ng ng cho bit, ang bn v hng dn PV lin h vi ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh  lm vic.',\n",
              " 'Ti bui l vic vi ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh PV  cung cp clip v vic nn khai thc tri php din ra ngay trn sng Long i on gn mng cu ng Giai cng  kin quyt v ha s u tranh x l, ng thi phi hp vi cc c quan chc nng khc thng xuyn kim tra  chm dt tnh trng ny.',\n",
              " 'ng Nguyn Vit Giai cho bit s u tranh x l Cn v vic cc bi tp kt tri php, ng Giai cho bit s x l dt im trong thi gian sm nht  khng nh hng ti cuc sng ngi dn xung quanh.',\n",
              " 'Khi c PV hi thi gian sm nht l bao lu ng Giai cho bit:  y ang cn vng mt khu th tc.',\n",
              " 'thi gian gii quyt sm nht cng phi mt chng 7 n 10 ngy. Vic khai thc ct tri php gn cu Long i (c ng st ln ng b) nguy c st l t khu vc mng cu, khin cu Long i ng trc him ha kh lng?',\n",
              " 'Tuy l vy nhng trong sng 22/9, PV mt ln na n ti hin trng chic thuyn khai thc tri php th nhn thy tnh hnh khai thc ct tri php vn khng h thay i.',\n",
              " 'Mt ln na PV  gi in thoi cho ng Nguyn Trng Tin  Ch tch x Xun Ninh v ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh  phn nh th li c 2 v ha s x l.',\n",
              " 'Trong sng 22/9 vic khai thc ct tri php vn din ra m khng c s can thip ca c quyan chc nng?',\n",
              " 'T nhng vic nu trn, d lun khng th khng t ra cu hi liu nhng vic xy ra  y c phi l c s bo k hoc c s tip tay ca lc lng chc nng c thm thm quyn hay khng?',\n",
              " 'Php lut Plus s tip tc thng tin v vic n bn c.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chNxSZ0mx5wS"
      },
      "source": [
        "# Extract raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phjF0edvNGpc"
      },
      "source": [
        "import os\n",
        "import re"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pTphYOiMYho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304b4eea-c141-42e9-d8be-9e0ec48e28ca"
      },
      "source": [
        "# get all subfolers and files in subfolers\n",
        "# [(\"top_subfolders\", [subfolders_in_top_subfolders], [files_in_top_subfolders])]\n",
        "sub_folders = [f for f in os.walk(\"VLSP2020_RE_dev\")][1:]\n",
        "sub_folders = sorted(sub_folders, key=lambda x: x[0])   # sort by top_subfolder name\n",
        "\n",
        "## top subfolder contain only 1 single file.\n",
        "check = False\n",
        "for i in sub_folders:\n",
        "    if i[1] or len(i[-1])!= 1:\n",
        "        print(\"ALERT!!!\")\n",
        "        check = True\n",
        "\n",
        "if not check:\n",
        "    print(\"There is \", len(sub_folders), \" subfolders. All subfolders contain only 1 file.\",\n",
        "          \" So that we have \", len(sub_folders), \" files.\")\n",
        "\n",
        "# generate data files name\n",
        "files_path = [os.path.join(i[0], i[-1][0]) for i in sub_folders]\n",
        "\n",
        "# print(*files_path, sep=\"\\n\")\n",
        "\n",
        "# print(files_path)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is  250  subfolders. All subfolders contain only 1 file.  So that we have  250  files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LJk2k0vD0dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f96a01a-4250-4fa2-9a38-179f2208b55e"
      },
      "source": [
        "# Xem trong b d liu c nhng character g\n",
        "\n",
        "character_lst = []\n",
        "for file in files_path:\n",
        "    with open(file, mode='r') as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "        # find line start with \"#Text=\"\n",
        "        textline_id = []\n",
        "        for i, text in enumerate(lines):\n",
        "            if (\"#Text=\" == text[0:6]):\n",
        "                textline_id.append(i)\n",
        "\n",
        "        # every data file has only one line that start with \"#Text=\"\"\n",
        "        assert (len(textline_id) == 1), str(\"1 is not number of line start with #Text=. \\nDoc: \" + file)\n",
        "\n",
        "        for c in lines[textline_id[0]][6:]:\n",
        "            if c not in character_lst:\n",
        "                character_lst.append(c)\n",
        "\n",
        "\n",
        "# Print all of the single characters, 30 per row.\n",
        "# For every batch of 30 tokens...\n",
        "for i in range(0, len(character_lst), 30):\n",
        "    \n",
        "    # Limit the end index so we don't go past the end of the list.\n",
        "    end = min(i + 30, len(character_lst) + 1)\n",
        "    \n",
        "    # Print out the tokens, separated by a space.\n",
        "    print(repr(' '.join(character_lst[i:end])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"U 1 6   V i  t N a m d  '  g  n v  o l  M C  K h  \"\n",
            "'  ,  c     r . T  s  u 9 - 0  y  b  I    2 8'\n",
            "'A  H   p   4 / q           x 5 B    k  '\n",
            "' 3 ( 7 )        S L    R  +  e :       z'\n",
            "'G Z  P J D ? X  Q % | O  f  \"    \\xa0  \\u200b   E W F ; w'\n",
            "'    j  &  Y \\ufeff       !  *           '\n",
            "'   [ ]  ~             '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugs-iTcldECs"
      },
      "source": [
        "constant = {\"entity_name\": [\"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"MISCELLANEOUS\"],\n",
        "          \"relation_name\": [\"LOCATED\", \"PART  WHOLE\", \"AFFILIATION\", \"PERSONAL - SOCIAL\"]\n",
        "           }"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rqndrEtimOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b219873-d925-488d-f285-edbabd8bc2ed"
      },
      "source": [
        "\"\"\"\n",
        "            dict = {\"doc_id\": id of folder contain doc, \n",
        "                      \"text\": doc, \n",
        "                 \"token_ids\": [                                        tokens_id, ...], \n",
        "              \"subtoken_ids\": [                                   None or sub-id, ...],\n",
        "                       \"pos\": [                             [start pos, end pos], ...],\n",
        "                    \"tokens\": [                                      tokens_text, ...],\n",
        "                    \"entity\": [                        [entity_ids, entity_name], ...],\n",
        "                  \"relation\": [    [relation, stoken_id, ssubtoken_id direction], ...]\n",
        "                   }\n",
        "\n",
        "\n",
        "                      doc_id: id of folder contain doc                                            (str)\n",
        "                         doc: doc. line start with \"#Text=\"                                       (str)\n",
        "                   token_ids: ids of tokens.                                    first column       (list int)\n",
        "                subtoken_ids: int if crr token is a subtoken, otherwise None    first column      (list int, None)\n",
        "                         pos: posittion of tokens.                              second column     (list list int) \n",
        "                              [\n",
        "                                  [start pos, end pos],\n",
        "                                  ...\n",
        "                              ]\n",
        "                       token: tokens.                                           third column      (list str)\n",
        "                      entity: entity infor if token is entity, else None.       4th, 5th column   (list list, None)\n",
        "                              [\n",
        "                                  [entity_id, entity_name],\n",
        "                                  ...\n",
        "                              ]\n",
        "                    relation: relation if token is in a relation, else None.    other column      (list list, None)\n",
        "                              [\n",
        "                                  [[relation1_name, relation1_start_tokenID, relation1_start_subtokenID, [start_entity_id, end_entity_id]], ...],  \n",
        "                                                                         --> relation1_start_subtokenID may be None if start token is not a sub token\n",
        "                                  [[relation1_name, relation1_start_tokenID, relation1_start_subtokenID,                             None], ...],  <-- dataset has mistake. Don't have direction.\n",
        "                                                                                 \n",
        "                                  \n",
        "                                  ....\n",
        "                              ]\n",
        "                    \n",
        "\"\"\"\n",
        "\n",
        "import copy\n",
        "\n",
        "raw_tdata = []\n",
        "\n",
        "for file in files_path:\n",
        "    docif = {}\n",
        "    with open(file, mode='r') as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "        # example: VLSP2020_RE_training/23351113.conll/CURATION_USER.tsv -> 23351113\n",
        "        docif[\"doc_id\"] = copy.deepcopy(file[(file.find(\"/\") + 1): file.find(\".\")])\n",
        "\n",
        "        # find line start with \"#Text=\"\n",
        "        textline_id = []\n",
        "        for i, text in enumerate(lines):\n",
        "            if (\"#Text=\" == text[0:6]):\n",
        "                textline_id.append(i)\n",
        "\n",
        "        # every data file has only one line that start with \"#Text=\"\"\n",
        "        assert (len(textline_id) == 1), str(\"1 is not number of line start with #Text=. \\nDoc: \" + file)\n",
        "\n",
        "        docif[\"text\"] = copy.deepcopy(lines[textline_id[0]][6:])\n",
        "\n",
        "        first_cline = lines[(textline_id[0] + 1)].rstrip(\"\\t\").split(\"\\t\")   # first column_line\n",
        "        assert (len(first_cline) in [3, 5, 7, 8]), str(\"Doc has problem. doc: \" + file)\n",
        "\n",
        "\n",
        "        token_ids, subtoken_ids, pos, tokens = [], [], [], []\n",
        "        entity = []\n",
        "        relation = []\n",
        "\n",
        "        pretk_id = 0\n",
        "\n",
        "        for tk_id, line in enumerate(lines[(textline_id[0] + 1):]):\n",
        "            lineif = line.rstrip(\"\\t\").split(\"\\t\")   # seperate by one \\t between columns: [abc\\txyz\\t]\n",
        "\n",
        "            # check if columns is seperated by only one single Tab character '\\t'\n",
        "            lineif1 = re.split(r'\\t+', line.rstrip('\\t'))   # seperate by all \\t between column: [abc\\t\\t\\txyz\\t]\n",
        "            assert (lineif == lineif1), str(\"Columns is not seperated by only one single TAB '\\\\t'. doc: \" + file + \" line: \" + line)\n",
        "\n",
        "            # check if inside a doc, only exist one number of (no) columns\n",
        "            # above we check if len(lineif) in [3, 5, 7, 8], too. so we can make sure that\n",
        "            # in a doc, number of columns only in [3, 5, 7, 8]\n",
        "            # and all line in a doc has same no columns\n",
        "            assert len(lineif) == len(first_cline), str(\"Number of columns in doc is not consistent. \\nDoc: \" + file + \" line: \" + line)\n",
        "\n",
        "\n",
        "            # remove all \"_\" in lineif because we don't need it\n",
        "            # [3, 5, 7, 8] -> [3, 4, 5, 7]\n",
        "            # and all data has first three column. (4th and 5th) is a pair, (6th and 7th) is a pair\n",
        "            # after removing all \"_\", if:\n",
        "            # len(lineif) = 3 -> token_ids, pos, no entity, no relation\n",
        "            # len(lineif) = 5 -> token_ids, pos, entity, no relation\n",
        "            # len(lineif) = 7 -> token_ids, pos, entity, relation\n",
        "\n",
        "            # len(lineif) = 4 --> token_ids, pos, no entity, no relation (this is a mistake in dataset, in data file has 8 columns)\n",
        "\n",
        "            lineif = [col for col in lineif if col != \"_\"]\n",
        "\n",
        "            assert (len(lineif) in [3, 4, 5, 7]), str(\"Problem with number of columns after remove \\'_\\'.\\nIn doc: \" + file + \" line \" + line)\n",
        "\n",
        "            # match first column format\n",
        "            # startwith (\"1-\") then (number) end:   1-id\n",
        "            pattern_token_ids = re.compile(\"^(1-)([\\d]+)$\")\n",
        "\n",
        "            # a token may has many subtokens\n",
        "            # startwith (\"1-\") then (number) then (. char) then (number) end:   1-id.subid \n",
        "            # pattern_subtoken_ids = re.compile(\"^(1-)([\\d]+)(\\.)([\\d]+)$\")\n",
        "\n",
        "            # Currently in train dataset, number of subtoken of a token is 0 or 1\n",
        "            # startwith (\"1-\") then (number) then (.1) end:   1-id.1 \n",
        "            pattern_subtoken_ids = re.compile(\"^(1-)([\\d]+)(\\.1)$\")\n",
        "\n",
        "            assert (pattern_token_ids.match(lineif[0]) or pattern_subtoken_ids.match(lineif[0])), \\\n",
        "            str(\"Unexpected first column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "            if pattern_token_ids.match(lineif[0]):\n",
        "                # 1-id\n",
        "                # Check if token id is increased by one in each line or not.\n",
        "                assert (int(lineif[0][2:]) == (pretk_id + 1)), str(\"First column, Token_ID is not increased by one in each line. \\nIn doc: \" + file + \" \\nline: \" + line)\n",
        "\n",
        "                token_ids.append(int(lineif[0][2:]))\n",
        "                subtoken_ids.append(None)   # Not a subtoken\n",
        "\n",
        "                pretk_id += 1\n",
        "            \n",
        "            else:\n",
        "                # 1-id.1\n",
        "                # 1-id.subid\n",
        "                tmp = lineif[0].find(\".\")\n",
        "                tokenID = int(lineif[0][2:tmp])\n",
        "                subtokenID = int(lineif[0][(tmp+1):])\n",
        "                \n",
        "                assert (tokenID == token_ids[-1]), str(\"Exist subtoken without a token before it. \\nIn doc: \" + file + \" \\nline\" + line)\n",
        "\n",
        "                token_ids.append(tokenID)\n",
        "                subtoken_ids.append(subtokenID)\n",
        "\n",
        "                print(\"\\nTHERE IS A SUBTOKEN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "\n",
        "            # match second column format\n",
        "            # startwith (number) then (\"-\" char) then (number) end\n",
        "            pattern_pos = re.compile(\"^([\\d]+)(\\-)([\\d]+)$\")\n",
        "            assert (pattern_pos.match(lineif[1])), str(\"Unexpected second column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "            pos.append([int(ele) for ele in lineif[1].split(\"-\")])    # example: \"3-6\" -> [3, 6]\n",
        "            \n",
        "            # if current token is a subtoken, check if pos subtoken is inside father token or not.\n",
        "            if pattern_subtoken_ids.match(lineif[0]):\n",
        "                father_token = token_ids.index(token_ids[-1])\n",
        "\n",
        "                assert (pos[father_token][0] <= pos[-1][0]) and (pos[-1][1] <= pos[father_token][1]), \\\n",
        "                str(\"Subtoken\\'s position is not inside father token\\'s position. \\nIndoc: \" + file + \"\\Line: \" + line)\n",
        "\n",
        "\n",
        "            # third column\n",
        "            #check if token is matched with pos (second column) or not\n",
        "            crr_token_pos = [int(ele) for ele in lineif[1].split(\"-\")]\n",
        "            if lineif[2] == lines[textline_id[0]][6:][crr_token_pos[0]:crr_token_pos[1]]:\n",
        "                tokens.append(lineif[2])\n",
        "            else:\n",
        "                assert False, str(\"Token in 3th column not match with position at 2th column. \\nIn doc: \" + file + \" \\nline: \" + line)\n",
        "            \n",
        "\n",
        "            if (len(lineif) == 3) or (len(lineif) == 4):\n",
        "                entity.append(None)\n",
        "                relation.append(None)\n",
        "\n",
        "                if len(lineif) == 4:\n",
        "                    print(\"\\n4 COLUMNS.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "\n",
        "            # because we removed all \"_\", \n",
        "            # so when len(lineif) = 5 or len(lineif) = 7, this line must has: token_ids, pos, tokens and entity.\n",
        "            # (we don't have to check if 4th, 5th column is \"_\" anymore, since we removed all \"_\")\n",
        "            if (len(lineif) == 5) or (len(lineif) == 7):\n",
        "                # 4th column now only have two posibilities: \"*\" or \"*[number]\"\n",
        "                pattern_entity_id = re.compile(\"^(\\*)(\\[)([\\d]+)(\\])$\")\n",
        "                assert ((lineif[3] == \"*\") or pattern_entity_id.match(lineif[3])), str(\"Unexpected fourth column's format. In doc: \" + file + \" line \" + line)\n",
        "\n",
        "                # in doc: 23352816\n",
        "                # line: 1-23\t126-136\t</ENAMEX>)\t*\t*\t_\t_\t_\t\n",
        "                # there is a mistake in 5th column. Unknow enity name\n",
        "                # I will let this token entity is None.\n",
        "\n",
        "                # We can just let all token entity is None\n",
        "                # if 5th column is not in constant[\"entity_name\"]\n",
        "                # but below, I just code for this specific case\n",
        "                # because I want to know more about dataset\n",
        "\n",
        "                if (lineif[3] == \"*\"):\n",
        "\n",
        "                    if (lineif[4] == \"*\"):   # specific mistake case\n",
        "                        entity.append(None)\n",
        "                        print(\"\\nENTITY NAME MISTAKE IN 5TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                    else:\n",
        "                        assert (lineif[4] in constant[\"entity_name\"]), str(\"Unknown entity name. \\nDoc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "                        entity_id = 0\n",
        "                        entity_n = lineif[4]\n",
        "\n",
        "                        entity.append([entity_id, entity_n])\n",
        "                \n",
        "                elif pattern_entity_id.match(lineif[3]):\n",
        "                    # *[number]: *[26] -> 26\n",
        "                    entity_id = int(lineif[3][2:-1])\n",
        "                    \n",
        "                    # PERSON[26]\n",
        "                    tmp = lineif[4].find(\"[\")\n",
        "                    \n",
        "                    assert (entity_id == int(lineif[4][(tmp+1):-1])), str(\"Entity ID in 4th and 5th column are not the same. In doc: \" + file + \" line \" + line)\n",
        "                    \n",
        "                    assert (lineif[4][:tmp] in constant[\"entity_name\"]), str(\"Unknown entity name in doc: \" + file + \" line \" + line)\n",
        "                    \n",
        "                    entity_n = lineif[4][:tmp]\n",
        "\n",
        "                    entity.append([entity_id, entity_n])\n",
        "\n",
        "                # may be we dont need this last else because we use regex above\n",
        "                else:\n",
        "                    assert False, str(\"4th, 5th column has UNKNOWN MISTAKE. In Doc: \" + file + \"\\nline: \" + line)\n",
        "\n",
        "\n",
        "\n",
        "            if len(lineif) == 5:\n",
        "                relation.append(None)\n",
        "            \n",
        "\n",
        "            if len(lineif) == 7:\n",
        "                # example:\n",
        "                # AFFILIATION\t1-593[13_14]\n",
        "                # PART  WHOLE\t1-42[1_2]\n",
        "                # PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-80[3_8]|1-105[7_8]\n",
        "\n",
        "                # PART  WHOLE\t1-42    (an error in dataset that need to be handled)\n",
        "\n",
        "                rel_names = lineif[5].split(\"|\")    # PERSONAL - SOCIAL|PERSONAL - SOCIAL --> [\"PERSONAL - SOCIAL\", \"PERSONAL - SOCIAL\"]\n",
        "                rel_oifs = lineif[6].split(\"|\")     # 1-80[3_8]|1-105[7_8] --> [\"1-80[3_8]\", \"1-105[7_8]\"]\n",
        "\n",
        "                # in doc: 23351515\n",
        "                # line: 1-318\n",
        "                # 6th column: PART  WHOLE|LOCATED|PART  WHOLE|*\n",
        "                # last relation name is: *  -> mistake\n",
        "                # We can read data and change it to right one \n",
        "                # but I will remove this \"*\" relation in 6th and 7th column\n",
        "\n",
        "                if '*' in rel_names:\n",
        "                    tmp = rel_names.index('*')\n",
        "\n",
        "                    del rel_names[tmp]\n",
        "                    del rel_oifs[tmp]\n",
        "\n",
        "                    print(\"\\nRELATION MISTAKE IN 6TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                # MISTAKE In doc: 23351856\n",
        "                # line: 1-185\t807-812\tTriu\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
        "                # assert ((len(rel_names) == len(rel_oifs)) and (len(rel_names) >= 1)), str(\"Number of relations in 6th and 7th columns is different to each other. In doc: \" + file + \" line \" + line )\n",
        "                # handle later\n",
        "\n",
        "                assert (len(rel_names) == len(rel_oifs)), str(\"Number of relations in 6th and 7th columns is different to each other. \\nIn doc: \" + file + \" \\nline \" + line )\n",
        "\n",
        "\n",
        "                rels = []\n",
        "                for i in range(len(rel_names)):\n",
        "                    assert (rel_names[i] in constant[\"relation_name\"]), \\\n",
        "                    str(\"Unknown relation_name in doc: \" + file + \" \\nline \" + line)\n",
        "                    \n",
        "                    relation_n = rel_names[i]\n",
        "\n",
        "                    # (startwith \"1-\") then (number) then ([ char) then (number) then (_ char) then (number) then (] char) end\n",
        "                    #             1-         26            [             3             _             0             ]   \n",
        "                    pattern_relation_oif = re.compile(\"^(1-)([\\d]+)(\\[)([\\d]+)(\\_)([\\d]+)(\\])$\")\n",
        "\n",
        "                    # (startwith \"1-\") then (number) then (.1) then ([ char) then (number) then (_ char) then (number) then (] char) end\n",
        "                    #             1-         26            .1        [             3             _             0             ]   \n",
        "                    pattern_relation_oif_1 = re.compile(\"^(1-)([\\d]+)(\\.1)(\\[)([\\d]+)(\\_)([\\d]+)(\\])$\")\n",
        "                    \n",
        "                    # below is a mistake in dataset\n",
        "                    # but currently, this type mistake has only below form (only has token id).    <--- TRAIN DOES NOT HAVE, BUT DEV HAS\n",
        "                    # (don't have subtoken id mistake type, yet)   <--- TRAIN DOES NOT HAVE, BUT DEV HAS\n",
        "                    # (startwith \"1-\") then (number)  end\n",
        "                    #             1-         26          \n",
        "                    pattern_relation_oif_mistake_1 = re.compile(\"^(1-)([\\d]+)$\")\n",
        "\n",
        "\n",
        "                    # below is a mistake in dataset <--- subtoken id only\n",
        "                    # (startwith \"1-\") then (number) then (.1) end\n",
        "                    #             1-         26            .1\n",
        "                    pattern_relation_oif_mistake_2 = re.compile(\"^(1-)([\\d]+)(\\.1)$\")\n",
        "\n",
        "\n",
        "\n",
        "                    assert (pattern_relation_oif.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_1.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_mistake_1.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_mistake_2.match(rel_oifs[i])), \\\n",
        "                            str(\"Unexpected seventh column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "                    \n",
        "\n",
        "                    # NOTICE:\n",
        "                    # IN BELOW CODE, I DONT CHECK IF ONE OF TWO ENTITIES OF A RELATION\n",
        "                    # IS \"MISCELLANEOUS\" OR NOT. \n",
        "                    # MISCELLANEOUS IS A LEGIT ENTITY NAME, BUT IT IS NOT USED IN ANY RELATION TYPE.\n",
        "                    # I WONDER IF DATASET HAS THIS MISTAKE OR NOT.\n",
        "                    # I WILL CHECK IT WHEN I CREATE SENTENCES AS INPUT OF BERT.\n",
        "\n",
        "\n",
        "                    if pattern_relation_oif.match(rel_oifs[i]):\n",
        "                        # 1-id[id_id]\n",
        "\n",
        "                        tmp_stkid = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_etkid = rel_oifs[i].find(\"[\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp_stkid:tmp_etkid]\n",
        "\n",
        "                        # start subtoken id\n",
        "                        sstoken_id = None\n",
        "\n",
        "                        direction = rel_oifs[i][(tmp_etkid+1):-1]\n",
        "\n",
        "                        direction = direction.split(\"_\")   # [sentity_id, eentity_id]\n",
        "\n",
        "                        rels.append([relation_n, int(stoken_id), sstoken_id, [int(direction[0]), int(direction[1])]])\n",
        "\n",
        "                    elif pattern_relation_oif_1.match(rel_oifs[i]):\n",
        "                        # 1-id.subid[id_id]\n",
        "\n",
        "                        tmp_sid = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_eid = rel_oifs[i].find(\".\")\n",
        "\n",
        "                        tmp_ssid = rel_oifs[i].find(\".\") + 1\n",
        "                        tmp_esid = rel_oifs[i].find(\"[\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp_sid:tmp_eid]\n",
        "\n",
        "                        sstoken_id = rel_oifs[i][tmp_ssid:tmp_esid]\n",
        "\n",
        "\n",
        "                        direction = rel_oifs[i][(tmp_esid+1):-1]\n",
        "                        direction = direction.split(\"_\")\n",
        "\n",
        "                        rels.append([relation_n, int(stoken_id), int(sstoken_id), [int(direction[0]), int(direction[1])]])\n",
        "\n",
        "                        print(\"\\nSPECIAL SUBTOKEN IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "                    \n",
        "                    elif pattern_relation_oif_mistake_1.match(rel_oifs[i]):\n",
        "                        # 1-id\n",
        "                        tmp = rel_oifs[i].find(\"-\") + 1\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp:]\n",
        "                        \n",
        "                        sstoken_id = None\n",
        "                        direction = None\n",
        "\n",
        "                        # rels.append([relation_n, stoken_id, sstoken_id, direction])\n",
        "                        rels.append([relation_n, int(stoken_id), None, None])\n",
        "\n",
        "                        print(\"\\nMISTAKE IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "                    \n",
        "                    elif pattern_relation_oif_mistake_2.match(rel_oifs[i]):\n",
        "                        # 1-id.1\n",
        "                        tmp = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_1 = rel_oifs[i].find(\".1\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp:tmp_1]\n",
        "                        \n",
        "                        sstoken_id = 1\n",
        "                        direction = None\n",
        "\n",
        "                        # rels.append([relation_n, stoken_id, sstoken_id, direction])\n",
        "                        rels.append([relation_n, int(stoken_id), sstoken_id, None])\n",
        "\n",
        "                        print(\"\\nMISTAKE IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "                        print(stoken_id, ' - ', sstoken_id)\n",
        "                    \n",
        "                    else:\n",
        "                        assert False, str(\"Unexpected seventh column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "\n",
        "                \n",
        "                if len(rels) == 0:\n",
        "                    # MISTAKE In doc: 23351856\n",
        "                    # line: 1-185\t807-812\tTriu\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
        "                    relation.append(None)\n",
        "                    print(\"\\nREALTION NAME MISTAKE IN 6TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                else:\n",
        "                    relation.append(rels)\n",
        "\n",
        "\n",
        "        docif[\"token_ids\"] = copy.deepcopy(token_ids)\n",
        "        docif[\"subtoken_ids\"] = copy.deepcopy(subtoken_ids)\n",
        "        docif[\"pos\"] = copy.deepcopy(pos)\n",
        "        docif[\"tokens\"] = copy.deepcopy(tokens)\n",
        "        docif[\"entity\"] = copy.deepcopy(entity)\n",
        "        docif[\"relation\"] = copy.deepcopy(relation)\n",
        "\n",
        "    raw_tdata.append(copy.deepcopy(docif))\n",
        "\n",
        "\n",
        "print(len(raw_tdata))           \n",
        "                \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23351997.conll/CURATION_USER.tsv \n",
            "Line:  1-488\t2269-2273\tReal\t*\tORGANIZATION\tAFFILIATION\t1-490\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352002.conll/CURATION_USER.tsv \n",
            "Line:  1-240\t1117-1123\tTrng\t*\tPERSON\tPERSONAL - SOCIAL\t1-234\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352009.conll/CURATION_USER.tsv \n",
            "Line:  1-169\t759-762\tNga\t*\tLOCATION\tLOCATED\t1-170\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-3\t9-15\tMerkel\t*\tPERSON\tPERSONAL - SOCIAL\t1-1\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-15\t64-66\tM\t*\tLOCATION\tAFFILIATION\t1-43\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-17\t69-72\tc\t*\tLOCATION\tAFFILIATION\t1-45\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-222\t1004-1007\tc\t*\tLOCATION\tAFFILIATION\t1-213\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-275\t1242-1244\tM\t*\tLOCATION\tAFFILIATION|AFFILIATION|AFFILIATION\t1-271|1-294[6_0]|1-298[7_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-387\t1740-1742\tM\t*\tLOCATION\tAFFILIATION\t1-375\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-444\t2000-2002\tM\t*\tLOCATION\tAFFILIATION\t1-441\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-563\t2524-2526\tM\t*\tLOCATION\tAFFILIATION\t1-590\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-627\t2803-2805\tM\t*\tLOCATION\tAFFILIATION\t1-625\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-915\t4096-4098\tM\t*\tLOCATION\tAFFILIATION\t1-907\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352014.conll/CURATION_USER.tsv \n",
            "Line:  1-1012\t4546-4549\tc\t*\tLOCATION\tAFFILIATION\t1-1021\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352024.conll/CURATION_USER.tsv \n",
            "Line:  1-1215\t5433-5442\tOceanbank\t*\tORGANIZATION\tAFFILIATION\t1-1212\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352024.conll/CURATION_USER.tsv \n",
            "Line:  1-1248\t5579-5582\tPVN\t*\tORGANIZATION\tAFFILIATION\t1-1243\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352024.conll/CURATION_USER.tsv \n",
            "Line:  1-1325\t5930-5933\tSn\t*\tPERSON\tPERSONAL - SOCIAL\t1-1320\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352024.conll/CURATION_USER.tsv \n",
            "Line:  1-1856\t8271-8280\tOceanbank\t*\tORGANIZATION\tAFFILIATION\t1-1849\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352024.conll/CURATION_USER.tsv \n",
            "Line:  1-2198\t9783-9792\tOceanbank\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-2201[63_0]|1-2194\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352024.conll/CURATION_USER.tsv \n",
            "Line:  1-2426\t10789-10792\tSn\t*\tPERSON\tPERSONAL - SOCIAL\t1-2419\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352027.conll/CURATION_USER.tsv \n",
            "Line:  1-59\t293-301\tal-Qaeda\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE\t1-49[4_0]|1-52|1-54[5_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352027.conll/CURATION_USER.tsv \n",
            "Line:  1-60\t302-307\tSyria\t*\tLOCATION\tLOCATED\t1-59\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352033.conll/CURATION_USER.tsv \n",
            "Line:  1-982\t4571-4575\tPC67\t*\tORGANIZATION\tAFFILIATION\t1-965\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352065.conll/CURATION_USER.tsv \n",
            "Line:  1-824\t3986-3994\tLebannon\t*\tLOCATION\tLOCATED\t1-822\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352066.conll/CURATION_USER.tsv \n",
            "Line:  1-55\t250-254\tNht\t*\tLOCATION\tAFFILIATION\t1-57\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352090.conll/CURATION_USER.tsv \n",
            "Line:  1-22\t95-97\tM\t*\tLOCATION\tPART  WHOLE|LOCATED\t1-20|1-16[1_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352099.conll/CURATION_USER.tsv \n",
            "Line:  1-109\t459-462\tt\t*\tPERSON\tPERSONAL - SOCIAL\t1-125\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352122.conll/CURATION_USER.tsv \n",
            "Line:  1-128\t575-579\tUBKT\t*[10]\tORGANIZATION[10]\t*\t1-111[8_10]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352122.conll/CURATION_USER.tsv \n",
            "Line:  1-128\t575-579\tUBKT\t*[10]\tORGANIZATION[10]\t*\t1-111[8_10]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352126.conll/CURATION_USER.tsv \n",
            "Line:  1-327\t1470-1472\tT\t*\tPERSON\tPERSONAL - SOCIAL\t1-324\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352126.conll/CURATION_USER.tsv \n",
            "Line:  1-425\t1905-1910\tGiang\t*\tPERSON\tPERSONAL - SOCIAL\t1-418\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352161.conll/CURATION_USER.tsv \n",
            "Line:  1-115\t512-516\tTng\t*[14]\tORGANIZATION[14]\t*\t1-96[12_14]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352161.conll/CURATION_USER.tsv \n",
            "Line:  1-115\t512-516\tTng\t*[14]\tORGANIZATION[14]\t*\t1-96[12_14]\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352190.conll/CURATION_USER.tsv \n",
            "Line:  1-281\t1275-1278\tSi\t*[25]\tLOCATION[25]\t*\t1-273[24_25]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352190.conll/CURATION_USER.tsv \n",
            "Line:  1-281\t1275-1278\tSi\t*[25]\tLOCATION[25]\t*\t1-273[24_25]\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352190.conll/CURATION_USER.tsv \n",
            "Line:  1-399\t1830-1835\tPhng\t*[35]\tORGANIZATION[35]\t*\t1-388[34_35]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352190.conll/CURATION_USER.tsv \n",
            "Line:  1-399\t1830-1835\tPhng\t*[35]\tORGANIZATION[35]\t*\t1-388[34_35]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352200.conll/CURATION_USER.tsv \n",
            "Line:  1-100\t468-470\tMU\t*\tORGANIZATION\tAFFILIATION\t1-93\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352331.conll/CURATION_USER.tsv \n",
            "Line:  1-19\t88-94\tBrazil\t*\tLOCATION\tAFFILIATION\t1-14\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352331.conll/CURATION_USER.tsv \n",
            "Line:  1-81\t393-397\tReal\t*\tORGANIZATION\tAFFILIATION\t1-79\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352337.conll/CURATION_USER.tsv \n",
            "Line:  1-152\t688-694\t/TTXVN\t*\tORGANIZATION\tPART  WHOLE\t1-151\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352337.conll/CURATION_USER.tsv \n",
            "Line:  1-224\t1012-1014\tEU\t*\tORGANIZATION\tPART  WHOLE\t1-217\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352337.conll/CURATION_USER.tsv \n",
            "Line:  1-436\t1938-1944\tBerlin\t*\tLOCATION\tLOCATED|LOCATED\t1-429|1-446[16_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352337.conll/CURATION_USER.tsv \n",
            "Line:  1-464\t2065-2069\tAlps\t*\tLOCATION\tLOCATED\t1-429\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352337.conll/CURATION_USER.tsv \n",
            "Line:  1-598\t2659-2661\tEU\t*\tORGANIZATION\tAFFILIATION\t1-592\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352337.conll/CURATION_USER.tsv \n",
            "Line:  1-935\t4186-4193\tHamburg\t*\tLOCATION\tAFFILIATION\t1-938\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352346.conll/CURATION_USER.tsv \n",
            "Line:  1-378\t1785-1794\tCampuchia\t*\tLOCATION\tLOCATED\t1-368\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352357.conll/CURATION_USER.tsv \n",
            "Line:  1-283\t1288-1293\tTuyt\t*\tPERSON\tPERSONAL - SOCIAL\t1-267\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352361.conll/CURATION_USER.tsv \n",
            "Line:  1-412\t1854-1863\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-420\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352361.conll/CURATION_USER.tsv \n",
            "Line:  1-497\t2237-2246\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-490\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352393.conll/CURATION_USER.tsv \n",
            "Line:  1-71\t354-360\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED\t1-68[2_0]|1-64\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352393.conll/CURATION_USER.tsv \n",
            "Line:  1-91\t444-453\tSingapore\t*\tLOCATION\tLOCATED\t1-64\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352393.conll/CURATION_USER.tsv \n",
            "Line:  1-139\t675-679\tQ.10\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE\t1-137|1-123[4_0]|1-131[5_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352393.conll/CURATION_USER.tsv \n",
            "Line:  1-141\t682-688\tTP.HCM\t*\tLOCATION\tLOCATED|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-123[4_0]|1-131[5_0]|1-137|1-139\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352393.conll/CURATION_USER.tsv \n",
            "Line:  1-141\t682-688\tTP.HCM\t*\tLOCATION\tLOCATED|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-123[4_0]|1-131[5_0]|1-137|1-139\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352396.conll/CURATION_USER.tsv \n",
            "Line:  1-394\t1705-1714\t/Vietnam+\t*\tORGANIZATION\tPART  WHOLE\t1-393\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352402.conll/CURATION_USER.tsv \n",
            "Line:  1-516\t2337-2339\tFA\t*\tORGANIZATION\tAFFILIATION\t1-525\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352410.conll/CURATION_USER.tsv \n",
            "Line:  1-338\t1467-1470\tPVN\t*\tORGANIZATION\tAFFILIATION\t1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352410.conll/CURATION_USER.tsv \n",
            "Line:  1-340\t1475-1484\tOceanBank\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-338|1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352410.conll/CURATION_USER.tsv \n",
            "Line:  1-340\t1475-1484\tOceanBank\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-338|1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352410.conll/CURATION_USER.tsv \n",
            "Line:  1-366\t1599-1602\tPVN\t*\tORGANIZATION\tAFFILIATION\t1-351\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352410.conll/CURATION_USER.tsv \n",
            "Line:  1-368\t1607-1616\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-351\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_dev/23352414.conll/CURATION_USER.tsv \n",
            "Line:  1-413.1\t1850-1852\tB\t*[19]\tORGANIZATION[19]\tAFFILIATION|AFFILIATION\t1-417.1[20_19]|1-423[21_19]\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352414.conll/CURATION_USER.tsv \n",
            "Line:  1-413.1\t1850-1852\tB\t*[19]\tORGANIZATION[19]\tAFFILIATION|AFFILIATION\t1-417.1[20_19]|1-423[21_19]\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_dev/23352414.conll/CURATION_USER.tsv \n",
            "Line:  1-417.1\t1872-1878\tNguyn\t*[20]\tPERSON[20]\t_\t_\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352437.conll/CURATION_USER.tsv \n",
            "Line:  1-263\t1244-1247\tAnh\t*\tLOCATION\tLOCATED\t1-259\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352445.conll/CURATION_USER.tsv \n",
            "Line:  1-44\t201-203\tMU\t*\tORGANIZATION\tAFFILIATION\t1-47\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352445.conll/CURATION_USER.tsv \n",
            "Line:  1-134\t587-589\tMU\t*\tORGANIZATION\tAFFILIATION\t1-126\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352445.conll/CURATION_USER.tsv \n",
            "Line:  1-175\t763-765\tMU\t*\tORGANIZATION\tAFFILIATION\t1-162\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352445.conll/CURATION_USER.tsv \n",
            "Line:  1-198\t860-870\t(Indonesia\t*\tLOCATION\tPART  WHOLE\t1-197\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352457.conll/CURATION_USER.tsv \n",
            "Line:  1-19\t102-108\tParana\t*\tLOCATION\tPART  WHOLE|AFFILIATION\t1-17|1-11[1_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352457.conll/CURATION_USER.tsv \n",
            "Line:  1-21\t111-117\tBrazil\t*\tLOCATION\tAFFILIATION|PART  WHOLE|PART  WHOLE\t1-11[1_0]|1-19|1-17\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352457.conll/CURATION_USER.tsv \n",
            "Line:  1-21\t111-117\tBrazil\t*\tLOCATION\tAFFILIATION|PART  WHOLE|PART  WHOLE\t1-11[1_0]|1-19|1-17\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352461.conll/CURATION_USER.tsv \n",
            "Line:  1-229\t1081-1088\tSonbong\t*\tLOCATION\tPART  WHOLE\t1-227\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352470.conll/CURATION_USER.tsv \n",
            "Line:  1-272\t1207-1210\tHa\t*\tPERSON\tPERSONAL - SOCIAL\t1-268\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352482.conll/CURATION_USER.tsv \n",
            "Line:  1-120\t545-548\tAnh\t*\tLOCATION\tLOCATED\t1-121\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352482.conll/CURATION_USER.tsv \n",
            "Line:  1-982\t4359-4361\tc\t*\tLOCATION\tLOCATED\t1-977\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352488.conll/CURATION_USER.tsv \n",
            "Line:  1-546\t2371-2376\tThng\t*\tPERSON\tPERSONAL - SOCIAL\t1-543\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352488.conll/CURATION_USER.tsv \n",
            "Line:  1-551\t2394-2397\tNam\t*\tPERSON\tPERSONAL - SOCIAL\t1-554\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352489.conll/CURATION_USER.tsv \n",
            "Line:  1-91\t412-416\tReal\t*\tORGANIZATION\tAFFILIATION\t1-110\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352489.conll/CURATION_USER.tsv \n",
            "Line:  1-217\t976-983\tAsensio\t*\tPERSON\tPERSONAL - SOCIAL\t1-207\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-61\t282-285\tc\t*\tLOCATION\tAFFILIATION\t1-58\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-236\t1040-1043\tc\t*\tLOCATION\tAFFILIATION\t1-256\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-364\t1606-1609\tc\t*\tLOCATION\tAFFILIATION\t1-357\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-471\t2095-2098\tCDU\t*\tORGANIZATION\t*\t1-456\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-471\t2095-2098\tCDU\t*\tORGANIZATION\t*\t1-456\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-584\t2606-2609\tc\t*\tLOCATION\tPART  WHOLE\t1-582\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-711\t3187-3190\tCDU\t*\tORGANIZATION\tAFFILIATION\t1-704\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-742\t3324-3327\t/AP\t*\tORGANIZATION\tPART  WHOLE\t1-741\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-752\t3370-3373\tc\t*\tLOCATION\tPART  WHOLE\t1-745\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-839\t3771-3774\tc\t*\tLOCATION\tPART  WHOLE|LOCATED\t1-837|1-833[25_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-845\t3803-3808\tSauer\t*\tPERSON\tPERSONAL - SOCIAL\t1-843\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-881\t3972-3975\tc\t*\tLOCATION\tPART  WHOLE\t1-879\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-910\t4117-4119\tM\t*\tLOCATION\tAFFILIATION\t1-911\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-1023\t4613-4615\tEU\t*\tORGANIZATION\tAFFILIATION\t1-1036\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-1136\t5133-5136\tc\t*\tLOCATION\tPART  WHOLE\t1-1134\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352491.conll/CURATION_USER.tsv \n",
            "Line:  1-1157\t5225-5228\tc\t*\tORGANIZATION\tAFFILIATION\t1-1144\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352495.conll/CURATION_USER.tsv \n",
            "Line:  1-16\t88-89\t\t*\tLOCATION\tAFFILIATION|AFFILIATION\t1-8[1_0]|1-10\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv \n",
            "Line:  1-150\t672-674\tM\t*\tLOCATION\tPART  WHOLE\t1-154\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv \n",
            "Line:  1-338\t1546-1551\tSyria\t*\tLOCATION\tLOCATED\t1-336\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv \n",
            "Line:  1-369\t1692-1695\tNga\t*\tLOCATION\tPART  WHOLE\t1-368\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv \n",
            "Line:  1-594\t2713-2716\tNga\t*\tLOCATION\tPART  WHOLE\t1-593\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv \n",
            "Line:  1-628\t2874-2876\tM\t*\tLOCATION\tPART  WHOLE\t1-629\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv \n",
            "Line:  1-687\t3131-3134\tHTS\t*\tORGANIZATION\tAFFILIATION\t1-679\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352499.conll/CURATION_USER.tsv \n",
            "Line:  1-689\t3139-3144\tIdlid\t*\tLOCATION\tLOCATED\t1-687\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-177\t767-774\tChelsea\t*\tORGANIZATION\tAFFILIATION\t1-188\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-194\t855-857\tMU\t*\tORGANIZATION\tAFFILIATION|*\t1-208|1-210\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-194\t855-857\tMU\t*\tORGANIZATION\tAFFILIATION|*\t1-208|1-210\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-516\t2339-2346\tChelsea\t*\tORGANIZATION\tAFFILIATION|AFFILIATION|AFFILIATION\t1-538|1-530|1-532\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-516\t2339-2346\tChelsea\t*\tORGANIZATION\tAFFILIATION|AFFILIATION|AFFILIATION\t1-538|1-530|1-532\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-516\t2339-2346\tChelsea\t*\tORGANIZATION\tAFFILIATION|AFFILIATION|AFFILIATION\t1-538|1-530|1-532\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-538\t2446-2452\tMorata\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-532|1-530\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352517.conll/CURATION_USER.tsv \n",
            "Line:  1-538\t2446-2452\tMorata\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-532|1-530\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352543.conll/CURATION_USER.tsv \n",
            "Line:  1-494\t2295-2300\tRaisa\t*\tPERSON\tPERSONAL - SOCIAL\t1-492\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352550.conll/CURATION_USER.tsv \n",
            "Line:  1-287\t1353-1359\tZidane\t*\tPERSON\tPERSONAL - SOCIAL\t1-282\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352550.conll/CURATION_USER.tsv \n",
            "Line:  1-581\t2736-2741\tStoke\t*\tORGANIZATION\t*\t1-567[23_0]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352550.conll/CURATION_USER.tsv \n",
            "Line:  1-581\t2736-2741\tStoke\t*\tORGANIZATION\t*\t1-567[23_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352551.conll/CURATION_USER.tsv \n",
            "Line:  1-178\t812-818\tBrazil\t*\tLOCATION\tLOCATED\t1-171\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352563.conll/CURATION_USER.tsv \n",
            "Line:  1-283\t1263-1269\tMlaga\t*\tLOCATION\tPART  WHOLE\t1-280\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352585.conll/CURATION_USER.tsv \n",
            "Line:  1-207\t963-967\tPhp\t*\tPERSON\tAFFILIATION\t1-208\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352586.conll/CURATION_USER.tsv \n",
            "Line:  1-170\t809-813\t(VAS\t*\tORGANIZATION\t*\t1-156[10_0]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352586.conll/CURATION_USER.tsv \n",
            "Line:  1-170\t809-813\t(VAS\t*\tORGANIZATION\t*\t1-156[10_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352593.conll/CURATION_USER.tsv \n",
            "Line:  1-125\t582-589\tChelsea\t*\tORGANIZATION\tAFFILIATION\t1-130\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352593.conll/CURATION_USER.tsv \n",
            "Line:  1-133\t618-626\tAtletico\t*\tORGANIZATION\tAFFILIATION\t1-130\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352593.conll/CURATION_USER.tsv \n",
            "Line:  1-206\t946-954\tAtletico\t*\tORGANIZATION\tAFFILIATION\t1-198\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352593.conll/CURATION_USER.tsv \n",
            "Line:  1-210\t966-973\tChelsea\t*\tORGANIZATION\tAFFILIATION\t1-198\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_dev/23352600.conll/CURATION_USER.tsv \n",
            "Line:  1-91.1\t411-413\tV\t*[9]\tPERSON[9]\t_\t_\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352600.conll/CURATION_USER.tsv \n",
            "Line:  1-98\t442-446\tThi\t*[10]\tLOCATION[10]\tAFFILIATION\t1-91.1[9_10]\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352600.conll/CURATION_USER.tsv \n",
            "Line:  1-112\t494-498\tPht\t*[11]\tLOCATION[11]\tLOCATED\t1-91.1[9_11]\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352600.conll/CURATION_USER.tsv \n",
            "Line:  1-115\t506-510\tNinh\t*[12]\tLOCATION[12]\tPART  WHOLE|LOCATED\t1-112[11_12]|1-91.1[9_12]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352601.conll/CURATION_USER.tsv \n",
            "Line:  1-344\t1631-1637\tItalia\t*\tLOCATION\tAFFILIATION\t1-333\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352602.conll/CURATION_USER.tsv \n",
            "Line:  1-297\t1361-1368\tL'Oreal\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE\t1-306|1-308|1-310[4_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352602.conll/CURATION_USER.tsv \n",
            "Line:  1-297\t1361-1368\tL'Oreal\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE\t1-306|1-308|1-310[4_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352602.conll/CURATION_USER.tsv \n",
            "Line:  1-341\t1577-1581\tPhp\t*\tLOCATION\tPART  WHOLE|LOCATED\t1-339|1-322[5_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-155\t676-679\tc\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-151|1-153[7_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-242\t1056-1059\tc\t*\tLOCATION\tAFFILIATION|PART  WHOLE\t1-257[12_0]|1-250\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-299\t1311-1314\tc\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-278|1-279\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-299\t1311-1314\tc\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-278|1-279\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-336\t1482-1484\tEU\t*\tORGANIZATION\tPART  WHOLE\t1-333\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-387\t1717-1719\tEU\t*\tORGANIZATION\tAFFILIATION\t1-370\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-421\t1863-1866\tCDU\t*\tORGANIZATION\t*\t1-427[14_0]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-421\t1863-1866\tCDU\t*\tORGANIZATION\t*\t1-427[14_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-758\t3340-3343\tSPD\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE\t1-780|1-781\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-758\t3340-3343\tSPD\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE\t1-780|1-781\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-774\t3414-3417\tc\t*\tLOCATION\tPART  WHOLE\t1-758\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-829\t3658-3661\tCDU\t*\tORGANIZATION\tPART  WHOLE\t1-819\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-830\t3662-3666\t/CSU\t*\tORGANIZATION\t*\t1-819\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-830\t3662-3666\t/CSU\t*\tORGANIZATION\t*\t1-819\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-957\t4236-4239\tc\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-960[20_0]|1-965|1-972[21_0]|1-974[22_0]|1-978[23_0]\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-1152.1\t5052-5054\tFN\t*\tORGANIZATION\t_\t_\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-1160\t5086-5090\tPhp\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-1147[29_0]|1-1152.1\t\n",
            "1152  -  1\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-1310\t5731-5734\tc\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-1296|1-1297\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-1310\t5731-5734\tc\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-1296|1-1297\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352605.conll/CURATION_USER.tsv \n",
            "Line:  1-1499\t6540-6542\tEU\t*\tORGANIZATION\tAFFILIATION\t1-1516\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352614.conll/CURATION_USER.tsv \n",
            "Line:  1-111\t487-490\tNga\t*\tLOCATION\tLOCATED\t1-109\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352615.conll/CURATION_USER.tsv \n",
            "Line:  1-334\t1530-1537\tBigbang\t*\tORGANIZATION\tAFFILIATION\t1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352617.conll/CURATION_USER.tsv \n",
            "Line:  1-338\t1597-1606\tAmsterdam\t*\tLOCATION\tLOCATED\t1-343\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352620.conll/CURATION_USER.tsv \n",
            "Line:  1-37\t167-170\tNga\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-32|1-34[1_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352629.conll/CURATION_USER.tsv \n",
            "Line:  1-46\t190-192\tL.\t*\tPERSON\tPERSONAL - SOCIAL\t1-43\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352631.conll/CURATION_USER.tsv \n",
            "Line:  1-673\t3089-3092\tDan\t*\tPERSON\tPERSONAL - SOCIAL\t1-653\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352631.conll/CURATION_USER.tsv \n",
            "Line:  1-714\t3261-3266\tBlair\t*\tPERSON\tPERSONAL - SOCIAL\t1-712\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352650.conll/CURATION_USER.tsv \n",
            "Line:  1-270\t1228-1231\tchi\t*[26]\tORGANIZATION[26]\t*\t1-265[25_26]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352650.conll/CURATION_USER.tsv \n",
            "Line:  1-270\t1228-1231\tchi\t*[26]\tORGANIZATION[26]\t*\t1-265[25_26]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-240\t1079-1082\tPax\t*\tPERSON\tPERSONAL - SOCIAL\t1-237\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-243\t1089-1095\tZahara\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-240|1-237\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-243\t1089-1095\tZahara\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-240|1-237\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-246\t1102-1108\tShiloh\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-243|1-237|1-240\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-246\t1102-1108\tShiloh\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-243|1-237|1-240\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-246\t1102-1108\tShiloh\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-243|1-237|1-240\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-249\t1115-1123\tVivienne\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-246|1-237|1-240|1-243\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-249\t1115-1123\tVivienne\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-246|1-237|1-240|1-243\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-249\t1115-1123\tVivienne\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-246|1-237|1-240|1-243\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-249\t1115-1123\tVivienne\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-246|1-237|1-240|1-243\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-251\t1128-1132\tKnox\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-249|1-237|1-240|1-243|1-246\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-251\t1128-1132\tKnox\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-249|1-237|1-240|1-243|1-246\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-251\t1128-1132\tKnox\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-249|1-237|1-240|1-243|1-246\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-251\t1128-1132\tKnox\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-249|1-237|1-240|1-243|1-246\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-251\t1128-1132\tKnox\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-249|1-237|1-240|1-243|1-246\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-519\t2333-2336\tPax\t*\tPERSON\tPERSONAL - SOCIAL\t1-517\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_dev/23352654.conll/CURATION_USER.tsv \n",
            "Line:  1-564\t2535-2541\tMaddox\t*\tPERSON\tPERSONAL - SOCIAL\t1-560\t\n",
            "250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wzBGIIgKYPD"
      },
      "source": [
        "# Fix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruagt4UGbqL5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3d034cef-d1ed-41a5-de9d-a64729764c36"
      },
      "source": [
        "\"\"\"\n",
        "raw_data: - is a list (len = 506)\n",
        "          - each row is a dict {}, information from a single doc (506 doc)\n",
        "            - doc_id: number id in name of folder that contain doc\n",
        "            - text: text in line start with \"#Text=\"\n",
        "            - token_ids: list of int. (1st column)\n",
        "            - subtoken_ids: list. (1st column)\n",
        "                            an element can be None if token is not a subtoken: 1-id -> 1.26,\n",
        "                                           or int (subtoken_id) if token is a subtoken: 1-id.subid -> 1.26.1.\n",
        "                                           currently in train data, only exist subtoken id 1.\n",
        "                            (in extract raw data code, my code can get any subid, not just specify subid = 1.\n",
        "                             but i check if in data has other subid, it will return error -> to know more about data)\n",
        "            - pos: list of child list. (2st column)\n",
        "                   each child list has two int elements.\n",
        "                   [start_position, end_position]\n",
        "            - tokens: list of strings. (3th column)\n",
        "            - entity: list.\n",
        "                      an element is: None if crr token is not entity\n",
        "                                     a list with: 2 element if crr token is an entity.\n",
        "                                                  [entity_id, entity_name]\n",
        "                                                  entity_id: int, from 4th column\n",
        "                                                  entity_name: string, from 5th column\n",
        "            - relation: list\n",
        "                        an element is: None if there is no relation in 6ht, 7th column.\n",
        "                                       a list of child list. number of child list is number of relation in 6th, 7th column.\n",
        "                                                 each child list has: 4 elemnt\n",
        "                                                 [relation_name, stoken_id, sstoken_id, direction[sentity_id, eentity_id]]\n",
        "                                                 relation_name: string, from 6h column\n",
        "                                                 stoken_id: int, tokenid from 7th column\n",
        "                                                 sstoken_id: from 7th column\n",
        "                                                             None, if entity_1 is a token\n",
        "                                                             else: int, subid if entity_1 is subtoken\n",
        "                                                 direction: from 7th column\n",
        "                                                            None, if there is a mistake in dataset\n",
        "                                                            else: [entity_1_id, entity_2_id]\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nraw_data: - is a list (len = 506)\\n          - each row is a dict {}, information from a single doc (506 doc)\\n            - doc_id: number id in name of folder that contain doc\\n            - text: text in line start with \"#Text=\"\\n            - token_ids: list of int. (1st column)\\n            - subtoken_ids: list. (1st column)\\n                            an element can be None if token is not a subtoken: 1-id -> 1.26,\\n                                           or int (subtoken_id) if token is a subtoken: 1-id.subid -> 1.26.1.\\n                                           currently in train data, only exist subtoken id 1.\\n                            (in extract raw data code, my code can get any subid, not just specify subid = 1.\\n                             but i check if in data has other subid, it will return error -> to know more about data)\\n            - pos: list of child list. (2st column)\\n                   each child list has two int elements.\\n                   [start_position, end_position]\\n            - tokens: list of strings. (3th column)\\n            - entity: list.\\n                      an element is: None if crr token is not entity\\n                                     a list with: 2 element if crr token is an entity.\\n                                                  [entity_id, entity_name]\\n                                                  entity_id: int, from 4th column\\n                                                  entity_name: string, from 5th column\\n            - relation: list\\n                        an element is: None if there is no relation in 6ht, 7th column.\\n                                       a list of child list. number of child list is number of relation in 6th, 7th column.\\n                                                 each child list has: 4 elemnt\\n                                                 [relation_name, stoken_id, sstoken_id, direction[sentity_id, eentity_id]]\\n                                                 relation_name: string, from 6h column\\n                                                 stoken_id: int, tokenid from 7th column\\n                                                 sstoken_id: from 7th column\\n                                                             None, if entity_1 is a token\\n                                                             else: int, subid if entity_1 is subtoken\\n                                                 direction: from 7th column\\n                                                            None, if there is a mistake in dataset\\n                                                            else: [entity_1_id, entity_2_id]\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYMqFdP5eTPL"
      },
      "source": [
        "## Fix1: Subtoken to token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdwVXKOHs1Ql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78895ebe-4701-41d3-c4ca-018a7369e4e0"
      },
      "source": [
        "# Trong data c subtoken. Mc d hin ti trong train data 1 token nu c subtoken th ch c 1 subtoken \n",
        "# v subtoken ny thng l entity nn mi cn tch ra. ngoi ra subtoken thng nm  cui hoc u token nn ta c d dng tch ra c.\n",
        "# nn  gi ng ngha tt nht cho cu th c th chia token thnh 2 token: subtoken v phn cn li t token gc\n",
        "\n",
        "# Tuy nhin, nu trong tng lai, vi b dev v test,\n",
        "# - nu c nhiu hn 1 subtoken v cc subtoken k b overlap th cng chia nh trn\n",
        "# - cn nu xy ra hin tng overlap (subtoken b  ln nhau) gia cc subtoken\n",
        "# th lc ny ta khng th tch token ra c na\n",
        "# lc ny ta s chn cch x l s l khng tch token gc ra na. chn thm cc subtoken vo cu v coi chng nh 1 token bnh thng\n",
        "\n",
        "\n",
        "#  y do c 1 subtoken nn chn cch tch token gc ra cho n gin v gi c ng ngha tt nht.\n",
        "# on code bn di ch cho trng hp 1 subtoken\n",
        "# trng hp nhiu subtoken nhng khng overlap th  tng cng tng t, \n",
        "# nhng cn ch  gom cc subtoken ca 1 token li v chn th t x l\n",
        "# v ch  ti c du bng hay khng khi so snh pos\n",
        "\n",
        "import copy\n",
        "\n",
        "raw_tdata_new = copy.deepcopy(raw_tdata)\n",
        "\n",
        "for docif in raw_tdata_new:\n",
        "    for i in range(len(docif['subtoken_ids'])):\n",
        "        \n",
        "        if docif['subtoken_ids'][i] != None:\n",
        "            \n",
        "            print('\\n\\n-----Doc: ', docif['doc_id'])\n",
        "            print('Before: ')\n",
        "            for key in docif:\n",
        "                if key not in ['doc_id', 'text']:\n",
        "                    print(docif[key][i-1], end='\\t')\n",
        "            print('', end='\\n')\n",
        "            for key in docif:\n",
        "                if key not in ['doc_id', 'text']:\n",
        "                    print(docif[key][i], end='\\t')\n",
        "\n",
        "\n",
        "\n",
        "            #print(docif['token_ids'][i-1], '\\t', docif['pos'][i-1][0], '-', docif['pos'][i-1][1], '\\t', docif['tokens'][i-1])\n",
        "            #print(docif['token_ids'][i] , '.', docif['subtoken_ids'], '\\t', docif['pos'][i][0], '-', docif['pos'][i][1], '\\t', docif['tokens'][i])\n",
        "\n",
        "            # subtoken l mt on u ca token\n",
        "            # 1-200\t    902-905\tmi\t        *[10]\tORGANIZATION[10]\t_\t_\t\n",
        "            # 1-201\t    906-913\ttrng,\t     _     _\t                _\t_\t        <--- i-1\n",
        "            # 1-201.1\t906-912\ttrng  \t*[10]\tORGANIZATION[10]\t_\t_           <--- i\n",
        "\n",
        "            if (docif['pos'][i][0] == docif['pos'][i-1][0]) and (docif['pos'][i][1] < docif['pos'][i-1][1]):\n",
        "                \n",
        "                # i ch 2 dng, dng subtoken ln trn, dng token gc xung di\n",
        "                # i ang l dng cha subtoken s thnh dng cha token\n",
        "                # i-1 ang l dng cha token s thnh dng cha subtoken\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        tmp = copy.deepcopy(docif[key][i])\n",
        "                        docif[key][i] = copy.deepcopy(docif[key][i-1]) \n",
        "                        docif[key][i-1] = copy.deepcopy(tmp)\n",
        "\n",
        "                assert (docif['subtoken_ids'][i-1] != None), str(\"Swap failed.\")\n",
        "\n",
        "                ## xa b subtoken ids\n",
        "                docif['subtoken_ids'][i-1] = None\n",
        "\n",
        "                ## thay i tokens, b phn subtoken  tch ra\n",
        "                docif['tokens'][i] = docif['tokens'][i].replace(docif['tokens'][i-1], '')\n",
        "\n",
        "                # tuy nhin, c trng hp sau khi b phn subtoken i b tha du cch, nn ta cn x l\n",
        "                # do subtoken nm  u token, nn ta ch m du cch  bn tri phn cn li thi\n",
        "                space_count = 0\n",
        "                for s in docif['tokens'][i]:\n",
        "                    if s in [' ', '\\xa0']:\n",
        "                        space_count += 1\n",
        "                    else:\n",
        "                        break\n",
        "                \n",
        "\n",
        "                docif['tokens'][i] = docif['tokens'][i].lstrip()\n",
        "\n",
        "\n",
        "                ## thay i pos\n",
        "                docif['pos'][i] = [(docif['pos'][i-1][1] + space_count), docif['pos'][i][1]]\n",
        "                \n",
        "                \n",
        "\n",
        "                assert ((docif['text'][docif['pos'][i][0]:docif['pos'][i][1]]) == docif['tokens'][i]), \\\n",
        "                str('\\nWrong postions \\npos' + str(docif['pos'][i][0]) + '-' + str(docif['pos'][i][1]) + '  token: ' + str(docif['tokens'][i]))\n",
        "\n",
        "                # thay i token_ids ca ton b phn di, nu c subid trong relation  u th thay i, thay i stoken_id trong relation\n",
        "                # trng hp ny stoken_id s khng i nn khng cn thay i\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if j >= i:\n",
        "                        docif['token_ids'][j] += 1\n",
        "\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][1] == docif['token_ids'][i-1]:   # tm xem c relation no tr ti subtoken trc kia khng\n",
        "                                docif['relation'][j][k][2] = None   # nu c th thay bng None\n",
        "\n",
        "                            # do  trn, ton b token_ids pha sau (>= i) s b thay i (cng thm 1)\n",
        "                            # nn nhng relation c stoken_id nm  phn pha sau ny cng s cn thay i theo (cng thm 1)\n",
        "                            elif (docif['relation'][j][k][1] > docif['token_ids'][i-1]):\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1\n",
        "\n",
        "\n",
        "                '''\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][2] != None:\n",
        "                                assert False, str('Failed to replace subid in relation')\n",
        "                '''\n",
        "                            \n",
        "\n",
        "                print('\\n\\nAfter:')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i-1], end='\\t')\n",
        "                print('', end='\\n')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i], end='\\t')\n",
        "\n",
        "\n",
        "            # subtoken l mt on cui ca token\n",
        "            # 1-583\t    2567-2573\tm-Tp\t _\t    _\t                _\t_\t<--- i-1\n",
        "            # 1-583.1\t2570-2573\tTp\t    *[19]\tORGANIZATION[19]\t_\t_\t<--- i\n",
        "\n",
        "            elif (docif['pos'][i][1] == docif['pos'][i-1][1]) and (docif['pos'][i][0] > docif['pos'][i-1][0]):\n",
        "                ## thay i subid\n",
        "                docif['subtoken_ids'][i] = None\n",
        "\n",
        "                ## thay i tokens\n",
        "                docif['tokens'][i-1] = docif['tokens'][i-1].replace(docif['tokens'][i], '')\n",
        "\n",
        "                print(repr(docif['tokens'][i-1]))\n",
        "                # tuy nhin, c trng hp sau khi b phn subtoken i b tha du cch, nn ta cn x l\n",
        "                # do subtoken nm  cui token, nn ta ch m du cch  bn phi phn cn li thi\n",
        "                space_count = 0\n",
        "                for s in docif['tokens'][i-1][::-1]:\n",
        "                    if s in [' ', '\\xa0']:\n",
        "                        space_count += 1\n",
        "                    else:\n",
        "                        break\n",
        "                \n",
        "                \n",
        "                docif['tokens'][i-1] = docif['tokens'][i-1].rstrip()\n",
        "\n",
        "\n",
        "                ## thay i pos\n",
        "                docif['pos'][i-1] = [docif['pos'][i-1][0], (docif['pos'][i][0] - space_count)]\n",
        "\n",
        "                \n",
        "\n",
        "                assert ((docif['text'][docif['pos'][i-1][0]:docif['pos'][i-1][1]]) == docif['tokens'][i-1]), \\\n",
        "                str('\\nWrong postions \\npos ' + str(docif['pos'][i-1][0]) + '-' + str(docif['pos'][i-1][1]) + '  token: ' + str(docif['tokens'][i-1]))\n",
        "\n",
        "                # thay i token_ids ca ton b phn di, nu c subid trong relation  u th thay i, thay i stoken_id trong relation\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if j >= i:\n",
        "                        docif['token_ids'][j] += 1\n",
        "\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][1] == docif['token_ids'][i-1]:   # tm xem c relation no tr ti subtoken trc kia khng\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1   # do token_id thay i nn relation c stoken_id ny cng phi thay i \n",
        "                                docif['relation'][j][k][2] = None   # nu c th thay bng None\n",
        "\n",
        "                            # do  trn, ton b token_ids pha sau (>= i) s b thay i (cng thm 1)\n",
        "                            # nn nhng relation c stoken_id nm  phn pha sau ny cng s cn thay i theo (cng thm 1)\n",
        "                            elif (docif['relation'][j][k][1] > docif['token_ids'][i-1]):\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1\n",
        "\n",
        "                '''\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][2] != None:\n",
        "                                assert False, str('Failed to replace subid in relation')\n",
        "                            \n",
        "                            stoken_eid = docif['token_ids'].index(docif['relation'][j][k][1])\n",
        "                            if docif['entity'][stoken_eid] == None:\n",
        "                                print(docif['doc_id'])\n",
        "                                print(docif['relation'][j][k][1])\n",
        "                                print(stoken_eid)\n",
        "                                print(docif['token_ids'][stoken_eid])\n",
        "                                assert False, str('Failed to replace stoken_id in relation')\n",
        "                        \n",
        "                '''\n",
        "                        \n",
        "\n",
        "\n",
        "\n",
        "                print('\\n\\nAfter:')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i-1], end='\\t')\n",
        "                print('', end='\\n')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i], end='\\t')\n",
        "\n",
        "            else:\n",
        "                assert False, str(\"\\nExist subtoken in middle of token.\\nDoc: \" + docif['doc_id'] + \"\\ntoken_id\" + str(docif['token_ids'][i]))\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "\n",
        "\n",
        "# kim tra xem code trn c li g khng\n",
        "print('\\n\\nCHECKING')\n",
        "for idoc, docif in enumerate(raw_tdata_new):\n",
        "\n",
        "    relation_lst = []\n",
        "    for i in range(len(raw_tdata[idoc]['relation'])):\n",
        "        if raw_tdata[idoc]['relation'][i] != None:\n",
        "            relation_lst.append(raw_tdata[idoc]['relation'][i])\n",
        "\n",
        "    \n",
        "    relation_ith = 0\n",
        "    for i in range(len(docif['token_ids'])):\n",
        "        \n",
        "        # nu code chy ng th s khng cn subid\n",
        "        if (docif['subtoken_ids'][i] != None):\n",
        "            assert False, str('ERROR CODE 1')\n",
        "\n",
        "        if docif['relation'][i] != None:\n",
        "            for j in range(len(docif['relation'][i])):\n",
        "                # nu code chy ng th s khng cn subid\n",
        "                if docif['relation'][i][j][2] != None:\n",
        "                    assert False, str('ERROR CODE 2')\n",
        "\n",
        "                # so snh xem thay i stoken_id c ng khng\n",
        "                # string ca token v pos ca token s khng i so vi raw_tdata\n",
        "                stoken_new = docif['relation'][i][j][1]\n",
        "                stoken_new_ele_id = docif['token_ids'].index(stoken_new)\n",
        "                \n",
        "                stoken = relation_lst[relation_ith][j][1]\n",
        "                if relation_lst[relation_ith][j][2] == 1:\n",
        "                    stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken) + 1\n",
        "                else:\n",
        "                    stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken)\n",
        "\n",
        "                if docif['pos'][stoken_new_ele_id] != raw_tdata[idoc]['pos'][stoken_ele_id]:\n",
        "                    '''\n",
        "                    print('\\n-----Doc: ', docif['doc_id'])\n",
        "                    print(relation_ith)\n",
        "                    print(docif['relation'][i])\n",
        "                    print(relation_lst[relation_ith])\n",
        "                    print(docif['pos'][stoken_new_ele_id])\n",
        "                    print(raw_tdata[idoc]['pos'][stoken_ele_id])\n",
        "                    '''\n",
        "                    assert False, str('ERROR CODE 3')\n",
        "                \n",
        "            relation_ith += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('DONE. EVERYTHINGS SEEM TO BE CORRECTED :D')\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "   "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "-----Doc:  23352414\n",
            "Before: \n",
            "413\tNone\t[1843, 1852]\tquyn.B\tNone\tNone\t\n",
            "413\t1\t[1850, 1852]\tB\t[19, 'ORGANIZATION']\t[['AFFILIATION', 417, 1, [20, 19]], ['AFFILIATION', 423, None, [21, 19]]]\t'quyn.\\xa0'\n",
            "\n",
            "\n",
            "After:\n",
            "413\tNone\t[1843, 1849]\tquyn.\tNone\tNone\t\n",
            "414\tNone\t[1850, 1852]\tB\t[19, 'ORGANIZATION']\t[['AFFILIATION', 418, 1, [20, 19]], ['AFFILIATION', 424, None, [21, 19]]]\t\n",
            "\n",
            "-----Doc:  23352414\n",
            "Before: \n",
            "418\tNone\t[1865, 1878]\ttrngNguyn\tNone\tNone\t\n",
            "418\t1\t[1872, 1878]\tNguyn\t[20, 'PERSON']\tNone\t'trng\\xa0'\n",
            "\n",
            "\n",
            "After:\n",
            "418\tNone\t[1865, 1871]\ttrng\tNone\tNone\t\n",
            "419\tNone\t[1872, 1878]\tNguyn\t[20, 'PERSON']\tNone\t\n",
            "\n",
            "-----Doc:  23352600\n",
            "Before: \n",
            "91\tNone\t[405, 413]\tvn\".V\tNone\tNone\t\n",
            "91\t1\t[411, 413]\tV\t[9, 'PERSON']\tNone\t'vn\".\\xa0'\n",
            "\n",
            "\n",
            "After:\n",
            "91\tNone\t[405, 410]\tvn\".\tNone\tNone\t\n",
            "92\tNone\t[411, 413]\tV\t[9, 'PERSON']\tNone\t\n",
            "\n",
            "-----Doc:  23352605\n",
            "Before: \n",
            "1152\tNone\t[5051, 5054]\t(FN\tNone\tNone\t\n",
            "1152\t1\t[5052, 5054]\tFN\t[0, 'ORGANIZATION']\tNone\t'('\n",
            "\n",
            "\n",
            "After:\n",
            "1152\tNone\t[5051, 5052]\t(\tNone\tNone\t\n",
            "1153\tNone\t[5052, 5054]\tFN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "CHECKING\n",
            "DONE. EVERYTHINGS SEEM TO BE CORRECTED :D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBr6Gx-Y2oj2"
      },
      "source": [
        "## Find all entity in a doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVl3FKeND9dk"
      },
      "source": [
        "# Tm tt c cc entity trong mt doc:\n",
        "\n",
        "def find_all_entity_in_doc(raw_data, doc_id):\n",
        "    \n",
        "    for docif in raw_data:\n",
        "\n",
        "        ######## Tm tt c cc entity trong doc hin ti:\n",
        "\n",
        "        #                      -                                          entity_1                                              -  ...\n",
        "        #                      | -                 token_1                  -  -                   token_2                -  ...|\n",
        "        ##### doc_entity_lst = [ [ [ele_id, token_id, entity_id, entity_name], [ele_id, token_id, entity_id, , entity_name], ...], ...]\n",
        "\n",
        "        ### Lu : ele_id  y l element index ca token y trong docif[\"entity\"]\n",
        "        ### ch khng phi l token_ids\n",
        "        ### lu ci ele_id thay v token_ids  truy cp token bng index nhanh hn\n",
        "        ### nu lu token_ids th phi t t token_ids tm xem token ny n nm v tr no th mi ra index (ele_id)  truy\n",
        "        ### thng th, v token_ids bt u = 1, element index bt u = 0 nn: token_ids tng ng s c: token_ids = ele_id + 1\n",
        "        ### nhng nu trong doc c subtoken th iu trn s khng c m bo\n",
        "\n",
        "        # token c entity_id ging nhau m khng ng cnh nhau (cc dng cha token khng lin tip nhau) th thuc 2 entity khc nhau. \n",
        "        # Trng hp ny l do dataset li, b lp entity_id (bn trn c entity_id = 0 ri xung di (token khng cnh nhau) li thy entity_id = 0)\n",
        "        # Hin ti mi thy c entity_id = 0 l b lp li cho nhiu entity khc nhau\n",
        "\n",
        "        \n",
        "\n",
        "        if docif['doc_id'] == doc_id:\n",
        "\n",
        "            doc_entity_lst = []\n",
        "            tmplst = []\n",
        "\n",
        "            for i in range(len(docif[\"entity\"])):\n",
        "                if docif[\"entity\"][i] != None:\n",
        "                    tmplst.append([i, docif[\"token_ids\"][i], docif[\"entity\"][i][0], docif[\"entity\"][i][1]])\n",
        "\n",
        "                    if (i < (len(docif[\"entity\"]) - 1)):\n",
        "\n",
        "                        if docif[\"entity\"][i+1] == None:\n",
        "                            doc_entity_lst.append(tmplst)\n",
        "                            tmplst = []\n",
        "\n",
        "                        elif (docif[\"entity\"][i][0] != docif[\"entity\"][i+1][0]) or (docif[\"entity\"][i][1] != docif[\"entity\"][i+1][1]):\n",
        "                            doc_entity_lst.append(tmplst)\n",
        "                            tmplst = []\n",
        "\n",
        "                    if i == (len(docif[\"entity\"]) - 1):\n",
        "                        doc_entity_lst.append(tmplst)\n",
        "\n",
        "\n",
        "            ###################### Fix li hai entity khc nhau nhng ng cnh nhau v b trng entity_id\n",
        "            # tuy nhin, ta ch tm cc entity_id = 0 thi, v chng d li nht, d b trng id nht\n",
        "            # mi cp entity_id = 0 ng cnh nhau trong cc doc trong list bn di u s b tch ra thnh cc entity ring\n",
        "\n",
        "            ##### comment of V1 <-- trong notebook extract_train th code c hot ng n do khng c cm entity li no > 2 entity\n",
        "            ##### tuy nhin trong dev th xut hin mt s cm 3 entity id 0 b li\n",
        "            # tuy nhin ta s x l tng cp mt trong tng ln x l, run_times l s cp trng trong doc\n",
        "            # nu c doc no va c cp entity_0 li va c cp khng li th phi s khng c thm list di vo m phi x l ring\n",
        "            # tr khi cp b li l cp u tin th run_times t l 1\n",
        "            # data cng khng c qu nhiu nhng cp nh ny\n",
        "            #####\n",
        "            \n",
        "\n",
        "            # chy 2 cell code bn di trc  tm cc cm entity_id = 0 c t 2 token tr ln\n",
        "            #  xem nhng cm no b li, cm no khng b, cm no li m cn sa\n",
        "            # ta s cn ly doc_id v ith ca cm b li d sa\n",
        "            # sau khi sa trong ny, nhng cm c sa s khng cn xut hin khi chy 2 cell bn di na\n",
        "            # nhng cm khng c sa (khng li hoc li m chn khng sa) vn s c in ra\n",
        "\n",
        "            # ngoi ra, do ch ch danh entity cn sa nn d doc c c entity li ln entity khng li th vn sa c\n",
        "\n",
        "            # lu : mi cp entity li c bao nhiu token th s c tch ht ra thnh tng y entity.\n",
        "            # tc l v d entity id 0 li c 3 token th c tch ra thnh 3 entity id 0 ring bit, mi entity ch l 1 token\n",
        "            # code bn di ch x l trng hp ny.\n",
        "            # khng x l trng hp kiu entity id 0 li c 3 token, \n",
        "            # nhng li cn tch ra 2 entity (thay v 3), 1 entity gm 2 token u, 1 entity l 1 token cui\n",
        "            # l do: li qu, v s cm li rt t, trng hp nh kia th cng rt t hn v c khi k xy ra\n",
        "\n",
        "\n",
        "            \n",
        "            doc_error_lst = [{'doc_error_id': '23352161', 'ith_er_lst': [68, 77]},\n",
        "                             {'doc_error_id': '23352337', 'ith_er_lst': [9]},\n",
        "                             {'doc_error_id': '23352396', 'ith_er_lst': [29]},\n",
        "                             {'doc_error_id': '23352419', 'ith_er_lst': [0]},\n",
        "                             {'doc_error_id': '23352445', 'ith_er_lst': [17]},\n",
        "                             {'doc_error_id': '23352491', 'ith_er_lst': [66]},\n",
        "                             {'doc_error_id': '23352499', 'ith_er_lst': [1, 3, 27]},\n",
        "                             {'doc_error_id': '23352585', 'ith_er_lst': [10]},\n",
        "                             {'doc_error_id': '23352601', 'ith_er_lst': [0, 4, 28]},\n",
        "                             {'doc_error_id': '23352642', 'ith_er_lst': [21]},\n",
        "                             {'doc_error_id': '23352605', 'ith_er_lst': [24, 26, 44, 48, 50, 52, 57, 61, 64, 69, 93, 96, 99, 102, 107]}      \n",
        "                            ]\n",
        "\n",
        "            doc_error_id_lst = [doc_error['doc_error_id'] for doc_error in doc_error_lst]\n",
        "\n",
        "            increase_ids = 0\n",
        "\n",
        "            if doc_id in doc_error_id_lst:\n",
        "                ith_doc_id = doc_error_id_lst.index(doc_id)\n",
        "\n",
        "                assert (doc_error_lst[ith_doc_id]['doc_error_id'] == doc_id), str('PRBOLEM')\n",
        "\n",
        "                #print('\\n\\n------', doc_id)\n",
        "\n",
        "                ith_er_list = sorted(doc_error_lst[ith_doc_id]['ith_er_lst'])\n",
        "\n",
        "                for irun, ith_er_id in enumerate(ith_er_list):\n",
        "                    #print('--', irun)\n",
        "                    doc_entity_lst_copy = None\n",
        "                    doc_entity_lst_copy = copy.deepcopy(doc_entity_lst)\n",
        "\n",
        "                    for ient, ent in enumerate(doc_entity_lst):\n",
        "                        if ient == (ith_er_id + increase_ids): # do b dch nn cn cng vi s id b dch\n",
        "\n",
        "                            assert ((ent[0][2] == 0) and (len(ent) > 1)), \\\n",
        "                            str('\\nWrong ith_er_lst. \\nDoc: ' + str(doc_id) + '\\nith_er_id: ' + str(ith_er_id))\n",
        "\n",
        "                            #print(doc_entity_lst_copy)\n",
        "\n",
        "                            # v d: 3 token th ch cn chy 3 - 1 = 2 ln\n",
        "                            # ln u (itk = 0) th ly token cui entity (token th 3: ent[-1]) chn vo sau v tr hin ti ca entity\n",
        "                            # ln hai (itk = 1) th ly token ngay trc token cui (token th 2 t cui ln: ent[-2]) chn vo sau v tr hin ti ca entity\n",
        "                            for itk in range(len(ent) - 1):\n",
        "                                doc_entity_lst_copy.insert((ient+1), copy.deepcopy([ent[(-1 - itk)]]))\n",
        "\n",
        "                            # cui cng th bin entity li hin ti thnh token u ca entity li hin ti l xong\n",
        "                            doc_entity_lst_copy[ient] = copy.deepcopy([ent[0]])\n",
        "\n",
        "                            #print(doc_entity_lst_copy)\n",
        "\n",
        "                            # do bn trn ta chn thm (len(ent) - 1) entity mi vo entity list\n",
        "                            # nn id ca entity b li pha sau s b tng ln lng tng ng\n",
        "                            # l tng (len(ent)-1) ca mi ent b sa trc n\n",
        "                            increase_ids += (len(ent) - 1)\n",
        "\n",
        "                            break\n",
        "                            \n",
        "                    doc_entity_lst = copy.deepcopy(doc_entity_lst_copy)\n",
        "                    #print(doc_entity_lst)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "            \n",
        "\n",
        "\n",
        "            '''\n",
        "            # Doc: 23351965\n",
        "            # hai token cnh nhau (320, 321), cng entity_id 0, cng entity_name location nhng khng phi l 1 entity <-- li data\n",
        "            # m l hai entity, v entity ny link ti entity kia.\n",
        "\n",
        "            if docif['doc_id'] == '23351965':\n",
        "                doc_entity_lst_copy = copy.deepcopy(doc_entity_lst)\n",
        "                for ient, ent in enumerate(doc_entity_lst):\n",
        "                    if (len(ent) == 2) and (ent[0][2] == 0) and (ent[1][2] == 0):\n",
        "                        #print(doc_entity_lst_copy)\n",
        "                        doc_entity_lst_copy.insert((ient+1), [doc_entity_lst_copy[ient][1]])\n",
        "                        doc_entity_lst_copy[ient] = [doc_entity_lst_copy[ient][0]]\n",
        "                        #print(doc_entity_lst_copy)\n",
        "                        \n",
        "                doc_entity_lst = copy.deepcopy(doc_entity_lst_copy)\n",
        "                #print(doc_entity_lst)\n",
        "\n",
        "\n",
        "            # Doc: 23352753  b ging bn trn\n",
        "            # hai token cnh nhau (884, 885), cng entity_id 0, cng entity_name location nhng khng phi l 1 entity <-- li data\n",
        "            # m l hai entity, v entity ny link ti entity kia.\n",
        "\n",
        "            if docif['doc_id'] == '23352753':\n",
        "                doc_entity_lst_copy_2 = copy.deepcopy(doc_entity_lst)\n",
        "                for ient, ent in enumerate(doc_entity_lst):\n",
        "                    if (len(ent) == 2) and (ent[0][2] == 0) and (ent[1][2] == 0):\n",
        "                        #print(doc_entity_lst_copy_2)\n",
        "                        doc_entity_lst_copy_2.insert((ient+1), [doc_entity_lst_copy_2[ient][1]])\n",
        "                        doc_entity_lst_copy_2[ient] = [doc_entity_lst_copy_2[ient][0]]\n",
        "                        #print(doc_entity_lst_copy_2)\n",
        "                        \n",
        "                doc_entity_lst = copy.deepcopy(doc_entity_lst_copy_2)\n",
        "                #print(doc_entity_lst)\n",
        "            \n",
        "            '''\n",
        "\n",
        "\n",
        "            \n",
        "            return doc_entity_lst\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIA5LQry20nQ"
      },
      "source": [
        "# thng th nhng entity cnh nhau v trng id khin ta d lm thnh 1 entity\n",
        "# th nhng entity ny thng c entity_id = 0\n",
        "# nn ta s tm cc cp entity_id = 0 ny v xem cp no l li cp no khng li\n",
        "# sau  c xem trong doc ny y l li hay khng phi li (c cp khng phi li)\n",
        "# nu l cp li th s phi thm trng hp  cell code bn trn\n",
        "#  v sau dng hm tm entity bn trn s khng b li\n",
        "\n",
        "# MT LU  L: \n",
        "# V D: \n",
        "# 1-209\t935-944\tFrankfurt\t*\tLOCATION\t\n",
        "# 1-210\t945-949\t(c\t*\tLOCATION\t\n",
        "\n",
        "# 1-161\t739-743\tBali\t*\tLOCATION\t_\t_\t\n",
        "# 1-162\t744-754\t(Indonesia\t*\tLOCATION\tPART  WHOLE\t1-161\t\n",
        "               \n",
        "# gn khng chnh xc, bn trn khng c relation nhng bn di li c\n",
        "# nn nu c relation nh bn di th tch ra lm 2 entity\n",
        "# cn khng c relation nh bn trn th  n l mt entity\n",
        "# v nu khng c relation m vn tch ra lm 2 entity th label gia chng s l others, khng chnh xc\n",
        "# nu  l cng 1 entity th s hp l hn. min l  cng entity khng nh hng g\n",
        "# v trong data c rt nhiu label other gia cc location (nh M vi Anh c th l others)\n",
        "# nn nu ta chia ra th sau trong test set cng b  lm others, tc l khng hp l\n",
        "# th  thnh 1, th train data cng th m test data cng th\n",
        "# hoc ta c th tch nhng phi thm nhn part-whole vo\n",
        "\n",
        "# cng c th ton b cc entity_id = 0 cnh nhau u l cc entity khc nha, nhng nu khng phi th s khng hon ho\n",
        "# nn c th xt cc trng hp ring thay v t ng tch cc entity_0 cnh nhau thnh cc entity khc nhau\n",
        "\n",
        "def find_all_fault_entity_id_0(raw_data):\n",
        "\n",
        "    for docif in raw_data:\n",
        "        ent_lst = find_all_entity_in_doc(raw_data, docif['doc_id'])\n",
        "        \n",
        "        for i in range(len(ent_lst)):\n",
        "\n",
        "            if (len(ent_lst[i]) > 1) and (ent_lst[i][0][2] == 0):\n",
        "                print('\\n\\n------', docif['doc_id'], ' -ith: ', i)\n",
        "                for j in range(len(ent_lst[i])):\n",
        "                    first_tk_eleid = ent_lst[i][j][0]\n",
        "                    for key in docif:\n",
        "                        if key not in ['doc_id', 'text']:\n",
        "                            print(docif[key][first_tk_eleid], end='\\t')\n",
        "                    \n",
        "                    print('\\n')\n",
        "        \n",
        "\n",
        "\n",
        "                \n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POdyqoDS5dA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed063dd-16be-4714-b45d-9acef623d7c1"
      },
      "source": [
        "# tn hm hi gy nhm\n",
        "# cc entity c in ra c th l li hoc l khng li, a phn l li\n",
        "# v cc enity li bn di l quyt nh khng sa\n",
        "#find_all_fault_entity_id_0(raw_tdata_new_v4)\n",
        "find_all_fault_entity_id_0(raw_tdata_new)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "------ 23352316  -ith:  5\n",
            "80\tNone\t[344, 347]\tAFP\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "81\tNone\t[348, 354]\t/TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352316  -ith:  24\n",
            "341\tNone\t[1496, 1502]\t(TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "342\tNone\t[1503, 1512]\t/Vietnam+\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352347  -ith:  7\n",
            "47\tNone\t[196, 200]\tNht\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "48\tNone\t[201, 206]\t/Nht\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352396  -ith:  5\n",
            "67\tNone\t[281, 284]\tAFP\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "68\tNone\t[285, 291]\t/TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352442  -ith:  11\n",
            "109\tNone\t[514, 517]\tAFP\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "110\tNone\t[518, 524]\t/TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352495  -ith:  6\n",
            "92\tNone\t[413, 419]\tdEste\t[0, 'PERSON']\tNone\t\n",
            "\n",
            "93\tNone\t[420, 427]\t(e-xt\t[0, 'PERSON']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352495  -ith:  7\n",
            "105\tNone\t[475, 482]\tFerrara\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "106\tNone\t[483, 493]\t(Phe-ra-ra\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352495  -ith:  10\n",
            "137\tNone\t[636, 647]\tCharlemagne\t[0, 'PERSON']\tNone\t\n",
            "\n",
            "138\tNone\t[648, 664]\t(Sar-l-ma-nh)\t[0, 'PERSON']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352605  -ith:  15\n",
            "159\tNone\t[696, 699]\tAFP\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "160\tNone\t[700, 706]\t/TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfXerQf-fQSv"
      },
      "source": [
        "# Create train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qce_9RiBsIZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "53c777d1-a3f3-4e42-cd80-71d12a9150ba"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "                    label: \n",
        "                    Du x l k hiu relation label (docif[\"relation\"]) xut hin  ch ca entity no\n",
        "\n",
        "                              Entity:                 entity_1    -    entity_2\n",
        "                    Th t trong cu:                 trc            sau\n",
        "                      \n",
        "                      Relation label:     LOCATED                         x     (per/org - loc)\n",
        "                                       IS_LOCATED         x                     (loc     - per/org)\n",
        "                                       PARTWHOLE\t                      x     (part    - whole)\n",
        "                                       WHOLE-PART         x                     (whole   - part)\n",
        "                                  PERSONALSOCIAL                               (Undirected)\n",
        "                                      AFFILIATION\t                      x     \n",
        "                                   AFFILIATION_TO         x\n",
        "                                           OTHERS                               (l nhn gi 2 entity cng 1 cu m khng c relation trong data)\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n                    label: \\n                    Du x l k hiu relation label (docif[\"relation\"]) xut hin  ch ca entity no\\n\\n                              Entity:                 entity_1    -    entity_2\\n                    Th t trong cu:                 trc            sau\\n                      \\n                      Relation label:     LOCATED                         x     (per/org - loc)\\n                                       IS_LOCATED         x                     (loc     - per/org)\\n                                       PARTWHOLE\\t                      x     (part    - whole)\\n                                       WHOLE-PART         x                     (whole   - part)\\n                                  PERSONALSOCIAL                               (Undirected)\\n                                      AFFILIATION\\t                      x     \\n                                   AFFILIATION_TO         x\\n                                           OTHERS                               (l nhn gi 2 entity cng 1 cu m khng c relation trong data)\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqEMUOpfqHZp"
      },
      "source": [
        "original_labels = ['LOCATED', 'PART  WHOLE', 'PERSONAL - SOCIAL', 'AFFILIATION']\n",
        "\n",
        "# entity cha relation nm  pha sau th l label gc\n",
        "labels = {'LOCATED': 'LOCATED', 'IS_LOCATED': 'IS_LOCATED', \n",
        "         'PART_WHOLE': 'PART_WHOLE', 'WHOLE_PART': 'WHOLE_PART', \n",
        "         'PERSONAL_SOCIAL': 'PERSONAL_SOCIAL', \n",
        "         'AFFILIATION': 'AFFILIATION', 'AFFILIATION_TO': 'AFFILIATION_TO', \n",
        "         'OTHERS': 'OTHERS'\n",
        "         }"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8crlbk4MJTlx"
      },
      "source": [
        "### Func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBWuvOzUJUCP"
      },
      "source": [
        "def split_by_colon_punc(doc_sent_tokenize):\n",
        "\n",
        "    new_doc_sent_tokenize = []\n",
        "    for isent, sent in enumerate(doc_sent_tokenize):\n",
        "        if ':' not in sent:\n",
        "            new_doc_sent_tokenize.append(sent)\n",
        "        \n",
        "        else:\n",
        "            new_sents = sent.split(\":\")\n",
        "                \n",
        "            for inew_sent, new_sent in enumerate(new_sents):\n",
        "                if inew_sent != (len(new_sents) - 1):\n",
        "                    new_doc_sent_tokenize.append(str(new_sent.lstrip() + ':'))\n",
        "                else:\n",
        "                    new_doc_sent_tokenize.append(str(new_sent.strip()))\n",
        "\n",
        "    return new_doc_sent_tokenize\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt1PwbNGJG_W"
      },
      "source": [
        "from underthesea import sent_tokenize, word_tokenize\n",
        "\n",
        "def my_sentences_tokenize(doc_id, text):\n",
        "    ######## split sentence from docif[\"text\"] using Underthesea library\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # check if sum length of all sentence < len(text)\n",
        "    len_sentences = [len(s) for s in sentences]\n",
        "    assert (sum(len_sentences) <= len(text)), str(\"\\nSentence tokenize has problem. \\nDoc: \" + docif[\"doc_id\"])\n",
        "\n",
        "\n",
        "\n",
        "    ######\n",
        "    ### trong doc ny vic chia sentence bng Underthesea b li dn ti vic mt entity nm  2 cu.\n",
        "    new_sentences = []\n",
        "    \n",
        "    if doc_id == \"23352190\":  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (sent[-3:] != \"Ng.\") and (sent[:5] != \"Hng\"):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (sent[-3:] == \"Ng.\") and (sentences[isent + 1] == \"Hng\"):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "    elif doc_id in [\"23352299\", \"23352417\"]:  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"How Do I Look?\" not in sent) and ('Asia' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"How Do I Look?\" in sent) and ('Asia' in sentences[isent + 1]):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "        \n",
        "    ### trong cc doc bn di vic chia sentence bng Underthesea b li dn ti vic mt relation link ti mt entity thuc cu khc.\n",
        "    ### thng do sau tn ngi vit tt c du chm\n",
        "    \n",
        "    elif doc_id == \"23352323\":  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('L Qu D' not in sent) and ('Ha Thi' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif ('L Qu D' in sent) and ('Ha Thi' in sentences[isent + 1]):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "\n",
        "    \n",
        "    elif doc_id == \"23352491\":  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('Reuters' not in sent) and ('Tuyt Mai' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif ('Reuters' in sent) and ('Tuyt Mai' in sentences[isent + 1]):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == \"23352572\":  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"William\" not in sent) and (\"B. Rosen\" not in sent) and ('E. Cashman' not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"William\" in sent) and (\"B. Rosen\" in sentences[isent + 1]) and ('E. Cashman' in sentences[isent + 2]):\n",
        "                new_sentences.append(str('Ch huy lc lng M  y l Thiu tng William. B. Rosen v Thiu tng Thy qun lc chin Robert. E. Cashman   xut k hoch rt qun khi Khe Sanh .'))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "        new_sentences_2 = []\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            assert ('' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            if (\"...\" not in sent):\n",
        "                new_sentences_2.append(sent)\n",
        "            \n",
        "            elif (\"...\" in sent):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_2.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences_2.append(str(new_sent.lstrip()))\n",
        "        \n",
        "        #print(new_sentences_2)\n",
        "        sentences = copy.deepcopy(new_sentences_2)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    ######\n",
        "    # c mt s doc c ...\n",
        "\n",
        "    elif doc_id == '23352499':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"gii phng... Hm nay\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"gii phng... Hm nay\" in sent):\n",
        "                tmppp_1 = sent.find(\". Hm nay\")\n",
        "                tmppp_2 = sent.find(\". Mt\")\n",
        "                new_sentences.append(str(sent[:(tmppp_1+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp_1+2):(tmppp_2+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp_2+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "        new_sentences_1 = []\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            \n",
        "\n",
        "            if (\"\" not in sent) or ('UAVnhng' in sent):\n",
        "                new_sentences_1.append(sent)\n",
        "            \n",
        "            elif (\"\" in sent) and ('UAVnhng' not in sent):\n",
        "                new_sents = sent.split(\"\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_1.append(str(new_sent.lstrip() + ''))\n",
        "                    else:\n",
        "                        new_sentences_1.append(str(new_sent.lstrip()))\n",
        "        \n",
        "        #print(new_sentences_1)\n",
        "        sentences = copy.deepcopy(new_sentences_1)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == '23352432':  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"tai nn Tuy nhin\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"tai nn Tuy nhin\" in sent):\n",
        "                tmppp = sent.find(\" Tuy\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    ### nhng doc m ta ch tch theo  dev\n",
        "    # cc doc cc cu ch c  hoc mt s doc c hai loi nhng ta ch chia cu c \n",
        "    elif doc_id in ['23352085', '23352087', '23352378', '23352433', '23352456', \\\n",
        "                    '23352507', '23352594', \\\n",
        "                    '23352332', '23352468']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "\n",
        "            if ((doc_id != '23352332') and (doc_id != '23352468')):\n",
        "                assert ('...' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            if (\"\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"\" in sent):\n",
        "                new_sents = sent.split(\"\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + ''))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.lstrip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "    ### nhng doc m ta ch tch theo ... dev\n",
        "    # cc doc cc cu ch c ... hoc mt s doc c hai loi nhng ta ch chia cu c ...\n",
        "    elif doc_id in ['23352016', '23352070', '23352073', '23352348', '23352370', \\\n",
        "                    '23352381', '23352436', '23352470', '23352648', \\\n",
        "                    '23352161', '23352314', '23352425', \\\n",
        "                    '23352122', '23352260', '23352573', '23352623']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ((doc_id != '23352122') and (doc_id != '23352260') and (doc_id != '23352573') \\\n",
        "                and (doc_id != '23352623')):   # doc nay co ca 2 nhung ta chi chia theo ...\n",
        "                assert ('' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            if (\"...\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"...\" in sent):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.lstrip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    \n",
        "\n",
        "    ###### dev\n",
        "    # trong doc ny vic chia cu b li, 2 cu b gp thnh 1 cu\n",
        "    # nhng cu ny thng c k t: .\n",
        "    \n",
        "    elif doc_id in ['23351997', '23352066', '23352331', '23352416', '23352538']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('.' in sent) and ('.' != sent[-2:]):\n",
        "                new_sents = sent.split('.')\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '.'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ('.' not in sent) or ('.' == sent[-2:]):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == '23352317':  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"thuc tr su.. Thy ngi quen\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"thuc tr su.. Thy ngi quen\" in sent):\n",
        "                tmppp = sent.find(\". Thy ngi quen\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    \n",
        "    elif doc_id == '23352605':  # dev\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"v SPD .. Nu vy, y\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"v SPD .. Nu vy, y\" in sent):\n",
        "                tmppp = sent.find(\". Nu vy\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    # tach dau .\n",
        "    \n",
        "    if doc_id in ['23352065', '23352190', '23352517', '23352603']:\n",
        "        \n",
        "        # ith_sent l th t cc cu cn tch du . trong doc (th t cu bt u t 0)\n",
        "        # ith_dot l s th t ca cc du . trong cu m ti cc du . ny ta s tch cu thnh cc phn khc nhau\n",
        "        # ith_dot cn m th t cn thn bng tay (th t du . bt u t 1)\n",
        "        # ith_dot l mt list ca list. list th n ca ith_dot l danh sch v tr nhng du chm m ta s dng  tch cu th n tng n\n",
        "        # trong ith_sent   <- cn lu   ng th t, v ith_sent cn xp theo th t tng dn\n",
        "        # cn lm vy v c th 1 doc c nhiu cn cn tch, ri trong cc cu ny li c cu c nhiu du . cn tch\n",
        "\n",
        "        #  tng: ta duyt cc cu trong doc, da vo ith_sent_lst  bit cu no cn tch du .\n",
        "        # cu no khng cn tch th ta thm lun vo danh sch cc cu trong doc\n",
        "        # cu no cn tch th: ta da tip vo ith_dot_lst  bit ta s tch ti nhng du . no trong cu\n",
        "        # v d cu cn tch ti 2 du .: th 2 v th 3 trong cu (-> t 1 cu tch thnh 3 cu)\n",
        "        # ta s tm v tr index ca cc du . ny trong cu cn tch\n",
        "        # ri da vo index   ct cu thnh cc phn cn chia\n",
        "\n",
        "        doc_nfix_lst = [{'doc_id': '23352065', 'ith_sent_lst': [20], 'ith_dot_lst': [[1, 2]]},\n",
        "                        {'doc_id': '23352190', 'ith_sent_lst': [7], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '23352517', 'ith_sent_lst': [13], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '23352603', 'ith_sent_lst': [9, 11], 'ith_dot_lst': [[1], [1, 2]]}\n",
        "                        ]\n",
        "        \n",
        "        ##### to find pos of ith dot\n",
        "        def find_ith_dot_pos(haystack, needle, n):\n",
        "            start = haystack.find(needle)\n",
        "            while start >= 0 and n > 1:\n",
        "                start = haystack.find(needle, start+len(needle))\n",
        "                n -= 1\n",
        "            return start\n",
        "        #####\n",
        "\n",
        "\n",
        "        crr_doc_nfix = None\n",
        "        for doc_nfix in doc_nfix_lst:\n",
        "            if doc_nfix['doc_id'] == doc_id:\n",
        "                crr_doc_nfix = doc_nfix\n",
        "\n",
        "        new_sentences_6 = []\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            #if doc_id not in ['23353721', '23357394', '23352663']:\n",
        "            assert (('...' not in sent) and (\"\" not in sent)), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            assert (len(crr_doc_nfix['ith_sent_lst']) == len(crr_doc_nfix['ith_dot_lst'])), \\\n",
        "            str('\\nLEN ith_sent_lst not equal to LEN ith_dot_lst. \\nDoc: ' + doc_id)\n",
        "\n",
        "            if isent not in crr_doc_nfix['ith_sent_lst']:\n",
        "                new_sentences_6.append(sent)\n",
        "            \n",
        "            else:\n",
        "                \n",
        "                assert (('...' not in sent) and (\"\" not in sent)), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "                crr_ith_sent = crr_doc_nfix['ith_sent_lst'].index(isent)\n",
        "\n",
        "                ith_dot_pos_lst = [-1, (len(sent)-1)]\n",
        "\n",
        "                for ith_dot in crr_doc_nfix['ith_dot_lst'][crr_ith_sent]:\n",
        "                    dot_pos = find_ith_dot_pos(sent, '.', ith_dot)\n",
        "\n",
        "                    assert (dot_pos >= 0), str('\\nNot found ith dot. \\nDoc: ' + doc_id + '\\nSent: ' + sent)\n",
        "\n",
        "                    ith_dot_pos_lst.insert(-1, dot_pos)\n",
        "                \n",
        "                for iith in range(len(ith_dot_pos_lst) - 1):\n",
        "                    correct_sent = sent[(ith_dot_pos_lst[iith] + 1):(ith_dot_pos_lst[iith+1] + 1)]\n",
        "                    new_sentences_6.append(str(correct_sent).strip())\n",
        "\n",
        "\n",
        "        #print(new_sentences_6)\n",
        "        sentences = copy.deepcopy(new_sentences_6)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if doc_id not in ['23352024', '23352110', '23352190', '23352322', '23352428', '23352603']:\n",
        "        sentences = copy.deepcopy(split_by_colon_punc(sentences))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return sentences\n",
        "\n",
        "    ###### \n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_riTWvT_OHxy"
      },
      "source": [
        "def get_sentence_entities(docif, doc_entity_lst, sent, sspos, espos):\n",
        "    \n",
        "    sen_entity_lst = []\n",
        "\n",
        "    for i in range(len(doc_entity_lst)):   # each entity\n",
        "        first_token_eid = doc_entity_lst[i][0][0]\n",
        "        last_token_eid = doc_entity_lst[i][-1][0]\n",
        "            \n",
        "        # nu im u ca cu < im u ca token u v im cui ca token cui < im cui ca cu\n",
        "        # th entity ny l entity ca cu ny\n",
        "        if (sspos <= docif[\"pos\"][first_token_eid][0]) and (docif[\"pos\"][last_token_eid][1] <= espos):\n",
        "            sen_entity_lst.append(doc_entity_lst[i])\n",
        "\n",
        "        # im u ca entity < im u ca cu nhng im cui ca entity li ln hn im u ca cu\n",
        "        # c li: 1 entity nhng thuc 2 cu, tc l 1 phn ca entity thuc cu trc, phn cn li li thuc cu ang xt.\n",
        "        # iu ny c th do chia cu bng Underthesea c vn  hoc dataset c vn \n",
        "        elif (docif[\"pos\"][first_token_eid][0] < sspos) and (sspos < docif[\"pos\"][last_token_eid][1]):\n",
        "                \n",
        "        # trong dataset c li ny. \n",
        "        # tuy nhin khng c nhiu, nn  hiu thm v dataset, ti ch sa chnh xc cc li ny\n",
        "        # v  sa bn trn\n",
        "\n",
        "            assert False, str(\"\\n--- An entity belongs to two sentences instead of just one. (Error Code 1) \\nIn doc: \" + docif[\"doc_id\"] + \"\\nSentence: \" + repr(sent))\n",
        "                \n",
        "        # im u ca entity < im cui ca cu nhng im cui ca entity li ln hn im cui ca cu\n",
        "        # c li: 1 entity nhng thuc 2 cu, tc l 1 phn ca entity thuc cu ang xt, phn cn li li thuc cu sau.\n",
        "        # iu ny c th do chia cu bng Underthesea c vn  hoc dataset c vn \n",
        "        elif (docif[\"pos\"][first_token_eid][0] < espos) and (espos < docif[\"pos\"][last_token_eid][1]):\n",
        "                \n",
        "            assert False, str(\"\\nAn entity belongs to two sentences instead of just one. (Error Code 2) \\nIn doc: \" + docif[\"doc_id\"] + \"\\nSentence: \" + repr(sent))\n",
        "\n",
        "\n",
        "    # c th xy ra trng hp chia cu b li, mt cu to b chia thnh hai cu nh\n",
        "    # mi cu nh li cha cc entity\n",
        "    # nhng entity cu nh ny link ti cu nh kia -> li\n",
        "    # hoc trong data c li, entity cu ny link ti cu khc.\n",
        "    # nn cn xem xem cc relation trong cu c link ti cc entity tm thy trong cu khng\n",
        "        \n",
        "    # hay relation gia 2 entity l ng, nhng stoken_id trong relation khng tr vo token u tin ca entity id kia\n",
        "    # m li tr vo token gia hoc cui entity kia ( fix li ny bn trn)\n",
        "\n",
        "    # v relation ch link ti token_ids ca token u tin trong entity khc\n",
        "    # nn ta s thu thp danh sch token_ids ca cc token u tin cc entity trong cu\n",
        "    first_tkids_lst = []\n",
        "    for i in range(len(sen_entity_lst)):\n",
        "        first_tkids_lst.append(sen_entity_lst[i][0][1])\n",
        "\n",
        "\n",
        "    for i in range(len(sen_entity_lst)):\n",
        "        first_tkeid = sen_entity_lst[i][0][0]\n",
        "\n",
        "        if docif['relation'][first_tkeid] != None:   # tng relation trong cu\n",
        "            for j in range(len(docif['relation'][first_tkeid])):\n",
        "                if docif['relation'][first_tkeid][j][1] not in first_tkids_lst:\n",
        "                        \n",
        "                    '''\n",
        "                    print(str('\\nSentence tokenize has problem. \\nDoc: ' + docif['doc_id'] + '\\nRelation stoken ID: ' + str(docif['relation'][first_tkeid][j][1])  + ' \\nprvSent: ' + sentences[isent-1] + '\\nSent: ' + sent))\n",
        "\n",
        "                    print(docif['relation'][first_tkeid])\n",
        "                    '''\n",
        "\n",
        "                    assert False, \\\n",
        "                    str('\\nSentence tokenize has problem. \\nDoc: ' + docif['doc_id'] + '\\nRelation stoken ID: ' + str(docif['relation'][first_tkeid][j][1])  + ' \\nprvSent: ' + sentences[isent-1] + '\\nSent: ' + sent)\n",
        "\n",
        "\n",
        "\n",
        "    return sen_entity_lst\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZremDvk8tP7"
      },
      "source": [
        "def relation_name_to_sentence_label(relation_name, relation_entity):\n",
        "\n",
        "    sentence_label = None\n",
        "    # nu entity cha relation l entity 1 th label ngc li\n",
        "    if relation_entity == 1:\n",
        "        if relation_name == 'LOCATED':\n",
        "            sentence_label = 'IS_LOCATED'\n",
        "\n",
        "        elif relation_name == 'PART  WHOLE':\n",
        "            sentence_label = 'WHOLE_PART'\n",
        "\n",
        "        elif relation_name == 'PERSONAL - SOCIAL':\n",
        "            sentence_label = 'PERSONAL_SOCIAL'\n",
        "\n",
        "        elif relation_name == 'AFFILIATION':\n",
        "            sentence_label = 'AFFILIATION_TO'\n",
        "\n",
        "        else:\n",
        "            assert False, str('UNKNOW RELATION NAME: ' + relation_name)\n",
        "    \n",
        "    # nu entity cha relation l entity 2 th label gi nguyn\n",
        "    elif relation_entity == 2:\n",
        "        if relation_name == 'LOCATED':\n",
        "            sentence_label = 'LOCATED'\n",
        "\n",
        "        elif relation_name == 'PART  WHOLE':\n",
        "            sentence_label = 'PART_WHOLE'\n",
        "\n",
        "        elif relation_name == 'PERSONAL - SOCIAL':\n",
        "            sentence_label = 'PERSONAL_SOCIAL'\n",
        "\n",
        "        elif relation_name == 'AFFILIATION':\n",
        "            sentence_label = 'AFFILIATION'\n",
        "\n",
        "        else:\n",
        "            assert False, str('UNKNOW RELATION NAME: ' + relation_name)\n",
        "\n",
        "    else:\n",
        "        assert False, (\"Unexpect relation_entity. Expect 1 or 2 but got: \" + relation_entity + \".\")\n",
        "\n",
        "    \n",
        "    return sentence_label"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyIgy3tfLDTs"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKFuhu7TYFB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c8efdb9-7521-4f9f-8519-842a632029fd"
      },
      "source": [
        "# To train data (cu cha cp entity v label)\n",
        "\n",
        "tdata = []\n",
        "\n",
        "#raw_data = raw_tdata_new_v6\n",
        "raw_data = raw_tdata_new\n",
        "\n",
        "sent_id = 0\n",
        "\n",
        "for docif in raw_data:\n",
        "\n",
        "    # find all entity in current doc    \n",
        "    doc_entity_lst = find_all_entity_in_doc(raw_data, docif['doc_id'])\n",
        "\n",
        "    # if whole doc has 0 or 1 entity -->  no relation in this doc --> skip\n",
        "    # there is many doc like this (like doc has only 3 columns,...)\n",
        "    if len(doc_entity_lst) <= 1:\n",
        "        continue\n",
        "\n",
        "\n",
        "    text = docif[\"text\"]\n",
        "    sentences = my_sentences_tokenize(docif['doc_id'], text)\n",
        "\n",
        "    \n",
        "    ######## extract training sentence\n",
        " \n",
        "    pre_espos = 0   # end of pre sentence\n",
        "\n",
        "    for isent, sent in enumerate(sentences):\n",
        "\n",
        "        sentif = {}\n",
        "        '''\n",
        "        sentif[\"doc_id\"] = docif[\"doc_id\"]\n",
        "        sentif[\"sentence\"] = sent\n",
        "        '''\n",
        "        ###### sentence position\n",
        "        # tm v tr ca cu  da vo  bit entity (cc tokens) thuc cu no\n",
        "\n",
        "        # text may have two indentical sentences\n",
        "        # so we have to find start position of current sentence in the rest of the text that not contain previous sentences.\n",
        "\n",
        "        assert (text[pre_espos:].find(sent) >= 0), str(\"Position has problem. \\nDoc: \" + docif[\"doc_id\"] + \"\\nCurrent sentence: \" + sent)\n",
        "\n",
        "        sspos = text[pre_espos:].find(sent) + pre_espos\n",
        "        espos = sspos + len(sent)\n",
        "\n",
        "        # update pre_espos\n",
        "        pre_espos = espos\n",
        "        \n",
        "        assert (sent == text[sspos:espos]), str(\"Position founded is not matched in text. \\nDoc: \" + docif[\"doc_id\"] + \"\\nCurrent sentence: \" + sent)\n",
        "\n",
        "        '''\n",
        "        sentif[\"spos\"] = [sspos, espos]\n",
        "        '''\n",
        "\n",
        "        ###### get all entity in current sentence\n",
        "        \n",
        "        sen_entity_lst = get_sentence_entities(docif, doc_entity_lst, sent, sspos, espos)\n",
        "\n",
        "        # if current sentence has 0 or 1 entity -> no relation availabel to classify -> skip\n",
        "        if len(sen_entity_lst) <= 1: \n",
        "            continue\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "        ###### create n*(n-1)/2 sentence\n",
        "        \n",
        "        for ient, ent_1 in enumerate(sen_entity_lst):\n",
        "            for jent, ent_2 in enumerate(sen_entity_lst[(ient+1):]):\n",
        "                \n",
        "                first_tkeid_ent1 = ent_1[0][0]   # dng cha token u trong entity\n",
        "                first_tkeid_ent2 = ent_2[0][0]\n",
        "                \n",
        "                last_tkeid_ent1 = ent_1[-1][0]   # dng cha token cui trong entity\n",
        "                last_tkeid_ent2 = ent_2[-1][0]\n",
        "\n",
        "                if (docif['entity'][first_tkeid_ent1][1] != \"MISCELLANEOUS\") and (docif['entity'][first_tkeid_ent2][1] != \"MISCELLANEOUS\"):\n",
        "                    \n",
        "                    # pos ca entity trong doc: start ca token u v end ca token cui trong entity\n",
        "                    ent1_pos_doc = [docif['pos'][first_tkeid_ent1][0], docif['pos'][last_tkeid_ent1][1]]\n",
        "                    ent2_pos_doc = [docif['pos'][first_tkeid_ent2][0], docif['pos'][last_tkeid_ent2][1]]\n",
        "\n",
        "                    # pos ca entity trong cu cha entity\n",
        "                    ent1_pos_sent = [(ent1_pos_doc[0] - sspos), (ent1_pos_doc[1] - sspos)]\n",
        "                    ent2_pos_sent = [(ent2_pos_doc[0] - sspos), (ent2_pos_doc[1] - sspos)]\n",
        "\n",
        "                    \n",
        "                    # kim tra xem pos trong doc v sent c khp, tr v cng entity khng\n",
        "                    assert (sent[ent1_pos_sent[0]:ent1_pos_sent[1]] == text[ent1_pos_doc[0]:ent1_pos_doc[1]]), \\\n",
        "                    str('Entity 1: pos_doc and pos_sent not matched. \\nDoc: ' + docif['doc_id'] + '\\nSent: ' + sent)\n",
        "\n",
        "                    assert (sent[ent2_pos_sent[0]:ent2_pos_sent[1]] == text[ent2_pos_doc[0]:ent2_pos_doc[1]]), \\\n",
        "                    str('Entity 1: pos_doc and pos_sent not matched. \\nDoc: ' + docif['doc_id'] + '\\nSent: ' + sent)\n",
        "\n",
        "\n",
        "                    ent1_text = sent[ent1_pos_sent[0]:ent1_pos_sent[1]]\n",
        "                    ent2_text = sent[ent2_pos_sent[0]:ent2_pos_sent[1]]\n",
        "\n",
        "                    ###############\n",
        "                    # kim tra xem mi token trong entity  c mt trong entity ly t pos hay cha\n",
        "                    for itk, tk in enumerate(ent_1):\n",
        "                        eid_tk = tk[0]\n",
        "\n",
        "                        if itk < (len(ent_1) - 1):\n",
        "                            eid_n_tk = ent_1[itk+1][0]\n",
        "\n",
        "                            assert (docif['pos'][eid_tk][1] < docif['pos'][eid_n_tk][0]), \\\n",
        "                            str(\"Position not increase. Doc: \" + docif['doc_id'] + \"\\nSent: \" + sent + \"\\ncrr-pos: \" + str(docif['pos'][eid_tk][1]) + \"\\nnpos: \" + str(docif['pos'][eid_ntk][0]))\n",
        "\n",
        "\n",
        "                        assert (ent1_pos_doc[0] <= docif['pos'][eid_tk][0]) and (docif['pos'][eid_tk][1] <= ent1_pos_doc[1]), \\\n",
        "                        str('Entity\\'s token pos not inside entity pos')\n",
        "                    \n",
        "                    ###############\n",
        "\n",
        "                    ##########\n",
        "\n",
        "                    assert (ent_1[0][1] == docif['token_ids'][first_tkeid_ent1]), str('NOT MATCHED entity first token id')\n",
        "                    assert (ent_2[0][1] == docif['token_ids'][first_tkeid_ent2]), str('NOT MATCHED entity first token id')\n",
        "\n",
        "                    sentence_label = None\n",
        "\n",
        "                    # nu c 2 entity khng c relation\n",
        "                    if (docif['relation'][first_tkeid_ent1] == None) and (docif['relation'][first_tkeid_ent2] == None):\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                    # nu c 2 entity c relation\n",
        "                    elif (docif['relation'][first_tkeid_ent1] != None) and (docif['relation'][first_tkeid_ent2] != None):\n",
        "                        \n",
        "                        # mc nh l OTHERS, nu bn di tm thy relation link ti th s c thay i\n",
        "                        # cn nu bn di tm khng thy (tc l khng c relation) th s khng b thay i v vn l OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                        for rel_1 in docif['relation'][first_tkeid_ent1]:\n",
        "                            if rel_1[1] == ent_2[0][1]:   # relation  entity 1 link ti token u entity 2\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_1[0], 1)\n",
        "\n",
        "                                # kiem tra relation direction\n",
        "                                # do relation nam o ent_1 nen direction la: [ent_2, ent_1]\n",
        "                                if rel_1[3] != None:\n",
        "                                    assert (rel_1[3] == [ent_2[0][2], ent_1[0][2]]), str('CODE 1: Not match direction')\n",
        "                                \n",
        "\n",
        "                        for rel_2 in docif['relation'][first_tkeid_ent2]:\n",
        "                            if rel_2[1] == ent_1[0][1]:   # relation  entity 2 link ti token u entity 1\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_2[0], 2)\n",
        "\n",
        "                                # do relation nam o ent_2 nen direction la: [ent_1, ent_2]\n",
        "                                if rel_2[3] != None:\n",
        "                                    assert (rel_2[3] == [ent_1[0][2], ent_2[0][2]]), str('CODE 2: Not match direction')\n",
        "        \n",
        "\n",
        "                    # nu entity 1 c relation, entity 2 khng c\n",
        "                    elif (docif['relation'][first_tkeid_ent1] != None) and (docif['relation'][first_tkeid_ent2] == None):\n",
        "                        \n",
        "                        # mc nh l OTHERS, nu bn di tm thy relation link ti th s c thay i\n",
        "                        # cn nu bn di tm khng thy (tc l khng c relation) th s khng b thay i v vn l OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                        for rel_1 in docif['relation'][first_tkeid_ent1]:\n",
        "                            if rel_1[1] == ent_2[0][1]:   # relation  entity 1 link ti token u entity 2\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_1[0], 1)\n",
        "\n",
        "                                # do relation nam o ent_1 nen direction la: [ent_2, ent_1]\n",
        "                                if rel_1[3] != None:\n",
        "                                    assert (rel_1[3] == [ent_2[0][2], ent_1[0][2]]), str('CODE 3: Not match direction')\n",
        "\n",
        "                                \n",
        "\n",
        "                    # nu entity 2 c relation, entity 1 khng c\n",
        "                    elif (docif['relation'][first_tkeid_ent1] == None) and (docif['relation'][first_tkeid_ent2] != None):\n",
        "                        \n",
        "                        # mc nh l OTHERS, nu bn di tm thy relation link ti th s c thay i\n",
        "                        # cn nu bn di tm khng thy (tc l khng c relation) th s khng b thay i v vn l OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "                        \n",
        "                        for rel_2 in docif['relation'][first_tkeid_ent2]:\n",
        "                            if rel_2[1] == ent_1[0][1]:   # relation  entity 2 link ti token u entity 1\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_2[0], 2)\n",
        "\n",
        "                                # do relation nam o ent_2 nen direction la: [ent_1, ent_2]\n",
        "                                if rel_2[3] != None:\n",
        "                                    assert (rel_2[3] == [ent_1[0][2], ent_2[0][2]]), str('CODE 4: Not match direction')\n",
        "\n",
        "                        \n",
        "                    ##########\n",
        "\n",
        "                    sentif[\"doc_id\"] = docif[\"doc_id\"]\n",
        "\n",
        "                    sent_id += 1\n",
        "                    sentif['sent_id'] = sent_id\n",
        "\n",
        "                    sentif[\"sentence\"] = sent\n",
        "                    sentif[\"spos\"] = [sspos, espos]\n",
        "\n",
        "                    entity_1 = {'text': copy.deepcopy(ent1_text), 'pos': copy.deepcopy(ent1_pos_sent)}\n",
        "                    entity_2 = {'text': copy.deepcopy(ent2_text), 'pos': copy.deepcopy(ent2_pos_sent)}\n",
        "\n",
        "                    sentif['entity_1'] = copy.deepcopy(entity_1)\n",
        "                    sentif['entity_2'] = copy.deepcopy(entity_2)\n",
        "\n",
        "                    sentif['label'] = copy.deepcopy(sentence_label)\n",
        "\n",
        "                    \n",
        "                    \n",
        "                    # may dong tren co the khong co copy.deepcopy nhung dong ben duoi khong co la bi loi\n",
        "                    tdata.append(copy.deepcopy(sentif))\n",
        "\n",
        "print(\"DONE\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu7DbslJN1i4",
        "outputId": "45c8df1a-d4c4-4d20-9115-0ead3af46415"
      },
      "source": [
        "count_label_others = 0\n",
        "\n",
        "for tdata_point in tdata:\n",
        "    if tdata_point['label'] == 'OTHERS':\n",
        "        count_label_others += 1\n",
        "\n",
        "print(count_label_others)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SfkCqCFN35t",
        "outputId": "d6aa7ecf-1708-4be6-be8c-456415577a1b"
      },
      "source": [
        "print('Count of labels that is not OTHERS: ', (len(tdata) - count_label_others))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of labels that is not OTHERS:  1470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3O1aoGj796m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b3d7721-d017-4b29-a74a-afdcb674de18"
      },
      "source": [
        "print(len(tdata))\n",
        "print(*tdata[0:15], sep='\\n')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9235\n",
            "{'doc_id': '23351996', 'sent_id': 1, 'sentence': \"U16 Vit Nam di 'ma gn' vo li Mng C Khng nm ngoi d on, U16 Vit Nam  c chin thng d dng trc U16 Mng C .\", 'spos': [0, 127], 'entity_1': {'text': 'U16 Vit Nam', 'pos': [0, 12]}, 'entity_2': {'text': 'Mng C', 'pos': [36, 43]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 2, 'sentence': \"U16 Vit Nam di 'ma gn' vo li Mng C Khng nm ngoi d on, U16 Vit Nam  c chin thng d dng trc U16 Mng C .\", 'spos': [0, 127], 'entity_1': {'text': 'U16 Vit Nam', 'pos': [0, 12]}, 'entity_2': {'text': 'U16 Vit Nam', 'pos': [69, 81]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 3, 'sentence': \"U16 Vit Nam di 'ma gn' vo li Mng C Khng nm ngoi d on, U16 Vit Nam  c chin thng d dng trc U16 Mng C .\", 'spos': [0, 127], 'entity_1': {'text': 'U16 Vit Nam', 'pos': [0, 12]}, 'entity_2': {'text': 'U16 Mng C', 'pos': [114, 125]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 4, 'sentence': \"U16 Vit Nam di 'ma gn' vo li Mng C Khng nm ngoi d on, U16 Vit Nam  c chin thng d dng trc U16 Mng C .\", 'spos': [0, 127], 'entity_1': {'text': 'Mng C', 'pos': [36, 43]}, 'entity_2': {'text': 'U16 Vit Nam', 'pos': [69, 81]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 5, 'sentence': \"U16 Vit Nam di 'ma gn' vo li Mng C Khng nm ngoi d on, U16 Vit Nam  c chin thng d dng trc U16 Mng C .\", 'spos': [0, 127], 'entity_1': {'text': 'Mng C', 'pos': [36, 43]}, 'entity_2': {'text': 'U16 Mng C', 'pos': [114, 125]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 6, 'sentence': \"U16 Vit Nam di 'ma gn' vo li Mng C Khng nm ngoi d on, U16 Vit Nam  c chin thng d dng trc U16 Mng C .\", 'spos': [0, 127], 'entity_1': {'text': 'U16 Vit Nam', 'pos': [69, 81]}, 'entity_2': {'text': 'U16 Mng C', 'pos': [114, 125]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 7, 'sentence': 'Nh vy ti bng I vng loi U16 chu  2018 , U16 Vit Nam v U16 Australia tm bng im nhau.', 'spos': [153, 249], 'entity_1': {'text': 'U16 Vit Nam', 'pos': [47, 59]}, 'entity_2': {'text': 'U16 Australia', 'pos': [63, 76]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 8, 'sentence': 'U16 Vit Nam thng d U16 Mng C .', 'spos': [316, 351], 'entity_1': {'text': 'U16 Vit Nam', 'pos': [0, 12]}, 'entity_2': {'text': 'U16 Mng C', 'pos': [22, 33]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 9, 'sentence': 'Trong trn ra qun ti bng I vng loi U16 chu  2018 gp U16 Campuchia , d b g ha 1-1 v b mt ngi  pht 20, nhng U16 Vit Nam vn chi xut sc  c chin thng chung cuc 5-2.', 'spos': [352, 542], 'entity_1': {'text': 'U16 Campuchia', 'pos': [60, 73]}, 'entity_2': {'text': 'U16 Vit Nam', 'pos': [126, 138]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 10, 'sentence': 'Bc vo trn th 2 gp ch nh U16 Mng C , U16 Vit Nam trn y t tin hng ti mt chin thng m nhm to  tm l trc cuc quyt u vi U16 Australia vo chiu ngy 24/9 ti.', 'spos': [543, 730], 'entity_1': {'text': 'U16 Mng C', 'pos': [32, 43]}, 'entity_2': {'text': 'U16 Vit Nam', 'pos': [46, 58]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 11, 'sentence': 'Bc vo trn th 2 gp ch nh U16 Mng C , U16 Vit Nam trn y t tin hng ti mt chin thng m nhm to  tm l trc cuc quyt u vi U16 Australia vo chiu ngy 24/9 ti.', 'spos': [543, 730], 'entity_1': {'text': 'U16 Mng C', 'pos': [32, 43]}, 'entity_2': {'text': 'U16 Australia', 'pos': [149, 162]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 12, 'sentence': 'Bc vo trn th 2 gp ch nh U16 Mng C , U16 Vit Nam trn y t tin hng ti mt chin thng m nhm to  tm l trc cuc quyt u vi U16 Australia vo chiu ngy 24/9 ti.', 'spos': [543, 730], 'entity_1': {'text': 'U16 Vit Nam', 'pos': [46, 58]}, 'entity_2': {'text': 'U16 Australia', 'pos': [149, 162]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 13, 'sentence': 'Trc mt i th b nh gi thp hn v mi mt, U16 Vit Nam khng gp nhiu kh khn  lm ch cuc chi v nhanh chng c bn vt ln dn trc do cng ca Nguyn Hong ngay pht th 13.', 'spos': [731, 924], 'entity_1': {'text': 'U16 Vit Nam', 'pos': [51, 63]}, 'entity_2': {'text': 'Nguyn Hong', 'pos': [163, 175]}, 'label': 'AFFILIATION_TO'}\n",
            "{'doc_id': '23351996', 'sent_id': 14, 'sentence': '10 pht sau, Thanh Trung (s 7) nng t s ln 2-0 t chm pht n sau khi th mn i phng phm li vi Nguyn Hong trong vng cm.', 'spos': [925, 1061], 'entity_1': {'text': 'Thanh Trung', 'pos': [13, 24]}, 'entity_2': {'text': 'Nguyn Hong', 'pos': [108, 120]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351996', 'sent_id': 15, 'sentence': 'Nhng pht cn li ca hip 1, U16 Vit Nam tip tc to ra mt sc p cc ln ln phn sn ca U16 Mng C .', 'spos': [1062, 1171], 'entity_1': {'text': 'U16 Vit Nam', 'pos': [31, 43]}, 'entity_2': {'text': 'U16 Mng C', 'pos': [96, 107]}, 'label': 'OTHERS'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0mBtqdZthaa"
      },
      "source": [
        "# Write to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTM1DITfqJtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14147b46-3706-4bb9-de6e-8618a667b88c"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xLBzf43i3NC"
      },
      "source": [
        "# write to file\n",
        "import codecs\n",
        "import json\n",
        "\n",
        "with codecs.open('dev_data.json', 'w', encoding='utf-8') as fout:\n",
        "    json.dump(tdata, fout, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72boeFzXgq-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f158ed02-02a8-450b-a4fb-1aff5516aac4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B8JEv6kDzuV"
      },
      "source": [
        "!mkdir \"/gdrive/MyDrive/VLSP2020_RE\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ng7tJuLD4_V"
      },
      "source": [
        "!mkdir \"/gdrive/MyDrive/VLSP2020_RE/json_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU0dYmmshRFZ"
      },
      "source": [
        "!cp -i dev_data.json \"/gdrive/MyDrive/VLSP2020_RE/json_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eWnRTZYSwm2",
        "outputId": "7ff49420-23a3-47a1-f08f-5f6d36effee2"
      },
      "source": [
        "import filecmp\n",
        "filecmp.cmp('dev_data.json', '/gdrive/MyDrive/VLSP2020_RE/json_data/dev_data.json')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}
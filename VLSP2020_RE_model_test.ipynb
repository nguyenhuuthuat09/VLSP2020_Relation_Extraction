{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VLSP_RE_model_GPU_test_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XFbOntuVkYZ7",
        "HsVFPOEqKivl",
        "YmLDKiKvqMvj",
        "rbbwIE69LJGt",
        "3HoxrF8WKupX",
        "fD22vnF7K4Ta",
        "InJpM7aDEtHB",
        "eCOWz9EfFPzn",
        "0jyBD2u1NP3r",
        "5F2EbtGIhZY_",
        "aF7krGOHhZY_",
        "eUyGGJYyhZY_",
        "cJJS0PLthZZE",
        "JcwMQApng86X",
        "7COIdSRHZww2",
        "tb5kMJ-VC2Iz",
        "1OWIkCz9aYyV",
        "IADdZhElb4D4",
        "OWb98FjG46cV",
        "hKLlYYYY4-r4",
        "2CuCMaFHwNuV",
        "9pQSd9DT2nAR",
        "EAeHqITIgWdI",
        "5xfu-7oPFyIA",
        "-2w44XoqKy_P",
        "k7SbTS2ol2bC"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3b4914b965714f99af3c84f41c557e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1a2dbf0a41084d14b21517d7f7306b73",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1389102cb5f14ae9b9af203ec5039e9e",
              "IPY_MODEL_1b1da9e4470c44c0a21ab79ed89556ed"
            ]
          }
        },
        "1a2dbf0a41084d14b21517d7f7306b73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1389102cb5f14ae9b9af203ec5039e9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c3e8916e15084fe0bd0dd96504ec6a3b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 557,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 557,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2109fc3dac3646b28bb9020f18e81964"
          }
        },
        "1b1da9e4470c44c0a21ab79ed89556ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0ad3661d994a4f99bd823bca42b71c1b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 557/557 [00:01&lt;00:00, 315B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c07e7b5d6d9140c98bbf0efd578d6783"
          }
        },
        "c3e8916e15084fe0bd0dd96504ec6a3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2109fc3dac3646b28bb9020f18e81964": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ad3661d994a4f99bd823bca42b71c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c07e7b5d6d9140c98bbf0efd578d6783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f6f1a73374b4ec689b24732c38d5a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_692521ef3c59480da0a1fc822e37aecf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b2c1f316dead466bb0b2a355c171a095",
              "IPY_MODEL_a81aa834510f47bda6cf9eb148352227"
            ]
          }
        },
        "692521ef3c59480da0a1fc822e37aecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2c1f316dead466bb0b2a355c171a095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2c06913a935c42528c255546a26c2f05",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 895321,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 895321,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6dbc2545ee13463095f9ab150b46ab29"
          }
        },
        "a81aa834510f47bda6cf9eb148352227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4f7c1a5ce5bb44f2989918102982a5cd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 895k/895k [00:01&lt;00:00, 689kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9713508a9e204469929c3e139dd413c5"
          }
        },
        "2c06913a935c42528c255546a26c2f05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6dbc2545ee13463095f9ab150b46ab29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4f7c1a5ce5bb44f2989918102982a5cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9713508a9e204469929c3e139dd413c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66522d14d090439bbe2feb6a1292a554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d91789c4a44341509837c3b85084b75a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_09af6a98c4b547d8af96651865c54036",
              "IPY_MODEL_01773a58409e487e94a8133e1b3a8078"
            ]
          }
        },
        "d91789c4a44341509837c3b85084b75a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "09af6a98c4b547d8af96651865c54036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bb75464a31434356ba8e753e568455f8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1135173,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1135173,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b7d9f7eba2b8449caff7635cdbefc9df"
          }
        },
        "01773a58409e487e94a8133e1b3a8078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_325916b51bb1482d9b0d57b471e80731",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.14M/1.14M [00:00&lt;00:00, 2.40MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c6fe7852aa334d2586e93d3b523a9bfa"
          }
        },
        "bb75464a31434356ba8e753e568455f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b7d9f7eba2b8449caff7635cdbefc9df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "325916b51bb1482d9b0d57b471e80731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c6fe7852aa334d2586e93d3b523a9bfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c786160f313442fba41d1a1e14161294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_37c74f19e47840ae914fdcb88b52b500",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_634d88480adc4ae48146d0e4f4b2b716",
              "IPY_MODEL_958076a125154442ab3ac83281bad961"
            ]
          }
        },
        "37c74f19e47840ae914fdcb88b52b500": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "634d88480adc4ae48146d0e4f4b2b716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_efb71f459f614454951fe40689ab69b6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 558,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 558,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77b601f2cb4b441d80925a50f9f7eb72"
          }
        },
        "958076a125154442ab3ac83281bad961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_499c8787b8be463d9e0ec3587e99a3fa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 558/558 [00:01&lt;00:00, 424B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0fa2070b54a142fd9b7f53044e360f57"
          }
        },
        "efb71f459f614454951fe40689ab69b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77b601f2cb4b441d80925a50f9f7eb72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "499c8787b8be463d9e0ec3587e99a3fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0fa2070b54a142fd9b7f53044e360f57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e80b886cea5e4c9db3379578295e0cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_84b5ddeeb4ef458c811dbdb9ef883e50",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9a7527ed660f4825a124ff3fdceb2efc",
              "IPY_MODEL_ccf88d2944a94cae9d6c458a6f698cf3"
            ]
          }
        },
        "84b5ddeeb4ef458c811dbdb9ef883e50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a7527ed660f4825a124ff3fdceb2efc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_235ff5f78c4f40a1bde12cc78fa1ae1b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 895321,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 895321,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a908a9cc874b429eb062f05a716102e1"
          }
        },
        "ccf88d2944a94cae9d6c458a6f698cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5215bca1d1614145aee3902a15a03b83",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 895k/895k [00:00&lt;00:00, 1.06MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba0bdf6881a64ea4adee7d98006719bd"
          }
        },
        "235ff5f78c4f40a1bde12cc78fa1ae1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a908a9cc874b429eb062f05a716102e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5215bca1d1614145aee3902a15a03b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba0bdf6881a64ea4adee7d98006719bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2ef2129e42764d3e8428aa6809b62cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_65a27361ab054ed9a77327bf9c386500",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_68e1612d2fa244dd9be0335c51eb345b",
              "IPY_MODEL_5e7ef50b4b294fba9c647f84532d9e3a"
            ]
          }
        },
        "65a27361ab054ed9a77327bf9c386500": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68e1612d2fa244dd9be0335c51eb345b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b97cd71315c24580997aabcae6ee8e92",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1135173,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1135173,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_07c685d035824b42b947b3612c359ab3"
          }
        },
        "5e7ef50b4b294fba9c647f84532d9e3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_00d577cb2e584112beb93888e60d02da",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.14M/1.14M [00:13&lt;00:00, 82.9kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_878a6a271e9b42f59377487a8a9224ba"
          }
        },
        "b97cd71315c24580997aabcae6ee8e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "07c685d035824b42b947b3612c359ab3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00d577cb2e584112beb93888e60d02da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "878a6a271e9b42f59377487a8a9224ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd60906f1baf464c8274b68700d21c1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_795ac19284be47fbb287aa9f41d38cc8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ce82f626aaab41ce9f2ea3b4fdd4569d",
              "IPY_MODEL_4c7500d70a564b7b916accf239b5dd0a"
            ]
          }
        },
        "795ac19284be47fbb287aa9f41d38cc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce82f626aaab41ce9f2ea3b4fdd4569d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a1ebb8452e0c4786940aafcb890603b6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5069051,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5069051,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1eb2b370959f4f3eaf9aecf5b123dfa6"
          }
        },
        "4c7500d70a564b7b916accf239b5dd0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_16ea42b255434efda55d312216fbcf0b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5.07M/5.07M [00:01&lt;00:00, 3.35MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_37f057d05e1a4c8895ada124e454bd5f"
          }
        },
        "a1ebb8452e0c4786940aafcb890603b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1eb2b370959f4f3eaf9aecf5b123dfa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16ea42b255434efda55d312216fbcf0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "37f057d05e1a4c8895ada124e454bd5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be06c71b047946f0badc4bfb917c0fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6299a9ef76244965af5d0a6322514ede",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9e46b6e3b9a74d6c883d10bd06028503",
              "IPY_MODEL_961fecb7da8c4b79a656a518be131d8b"
            ]
          }
        },
        "6299a9ef76244965af5d0a6322514ede": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e46b6e3b9a74d6c883d10bd06028503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b80f9cdbdee44c66804e8d35f6da4084",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5069051,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5069051,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7475d11c72f943e6b42bf2ed3c7762b2"
          }
        },
        "961fecb7da8c4b79a656a518be131d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eac68e038ea64c26b4ed2b8a3dce6277",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5.07M/5.07M [00:01&lt;00:00, 3.22MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c7e860bce3d48d5a02a8e37f71fef31"
          }
        },
        "b80f9cdbdee44c66804e8d35f6da4084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7475d11c72f943e6b42bf2ed3c7762b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eac68e038ea64c26b4ed2b8a3dce6277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c7e860bce3d48d5a02a8e37f71fef31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFbOntuVkYZ7"
      },
      "source": [
        "# Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBXHW6YDkJGo",
        "outputId": "d8520b0d-cb35-47ae-d725-0852fa7b7acc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i008g3mMQjV",
        "outputId": "6b1555ae-939b-4cd0-d25a-f0403fefdd5a"
      },
      "source": [
        "!ls \"/gdrive/MyDrive/VLSP2020_RE/json_data\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev_data_extend.json  test_data_v2.json       train_data.json\n",
            "dev_data.json\t      train_data_extend.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-e5X6WSNPUk"
      },
      "source": [
        "!cp -i \"/gdrive/MyDrive/VLSP2020_RE/json_data/test_data_v2.json\" test_data.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsVFPOEqKivl"
      },
      "source": [
        "# Install Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmLDKiKvqMvj"
      },
      "source": [
        "## Intsall Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpLCC4CRqP2A",
        "outputId": "26b486fa-9409-4aeb-9011-58761a912353"
      },
      "source": [
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (735.4MB)\n",
            "\u001b[K     |████████████████████████████████| 735.4MB 24kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 238kB/s \n",
            "\u001b[?25hCollecting torchaudio==0.7.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/16/ecdb9eb09ec6b8133d6c9536ea9e49cd13c9b5873c8488b8b765a39028da/torchaudio-0.7.2-cp37-cp37m-manylinux1_x86_64.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu101) (7.1.2)\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1+cu101 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "Successfully installed torch-1.7.1+cu101 torchaudio-0.7.2 torchvision-0.8.2+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbbwIE69LJGt"
      },
      "source": [
        "## Install Huggingface Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0wOcO4VKUgl",
        "outputId": "09090349-5c62-4b35-dcb8-11a04280c2b2"
      },
      "source": [
        "# new command in huggingface v4.x has some anoy change. so better use old version for now\n",
        "# https://github.com/huggingface/transformers/releases\n",
        "\n",
        "!pip install transformers==3.5.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (3.0.12)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 26.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (3.12.4)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/ac/f5ba028f0f097d855e1541301e946d4672eb0f30b6e25cb2369075f916d2/tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 37.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 49.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.5.1) (56.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.5.1) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.5.1) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.1) (1.0.1)\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HoxrF8WKupX"
      },
      "source": [
        "## Install VNCoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYXTRsYRKoJn",
        "outputId": "ab1eae28-b2bd-47fd-be1f-c176a0c994e2"
      },
      "source": [
        "# Install the vncorenlp python wrapper\n",
        "!pip install vncorenlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vncorenlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp37-none-any.whl size=2645936 sha256=36c7bdde73875bc0e92f9ef6fa66a698f30908578b5d081e001eaacbddbbea54\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G95nbje5KxnD",
        "outputId": "903cb81a-73ba-4062-b841-3196128d9546"
      },
      "source": [
        "# Download VnCoreNLP-1.1.1.jar & all of its  component (i.e. RDRSegmenter, pos, ner, deprel) \n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!mkdir -p vncorenlp/models/dep\n",
        "!mkdir -p vncorenlp/models/ner\n",
        "!mkdir -p vncorenlp/models/postagger\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/dep/vi-dep.xz\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger\n",
        "\n",
        "\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "\n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/\n",
        "\n",
        "!mv vi-dep.xz vncorenlp/models/dep/\n",
        "\n",
        "!mv vi-500brownclusters.xz vncorenlp/models/ner/\n",
        "!mv vi-ner.xz vncorenlp/models/ner/\n",
        "!mv vi-pretrainedembeddings.xz vncorenlp/models/ner/\n",
        "\n",
        "!mv vi-tagger vncorenlp/models/postagger/\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-07 11:45:29--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M  83.5MB/s    in 0.3s    \n",
            "\n",
            "2021-05-07 11:45:30 (83.5 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2021-05-07 11:45:30--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-05-07 11:45:31 (17.7 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2021-05-07 11:45:31--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-05-07 11:45:31 (8.17 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n",
            "--2021-05-07 11:45:31--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/dep/vi-dep.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16048864 (15M) [application/octet-stream]\n",
            "Saving to: ‘vi-dep.xz’\n",
            "\n",
            "vi-dep.xz           100%[===================>]  15.30M  38.8MB/s    in 0.4s    \n",
            "\n",
            "2021-05-07 11:45:32 (38.8 MB/s) - ‘vi-dep.xz’ saved [16048864/16048864]\n",
            "\n",
            "--2021-05-07 11:45:32--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5599844 (5.3M) [application/octet-stream]\n",
            "Saving to: ‘vi-500brownclusters.xz’\n",
            "\n",
            "vi-500brownclusters 100%[===================>]   5.34M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-07 11:45:33 (40.1 MB/s) - ‘vi-500brownclusters.xz’ saved [5599844/5599844]\n",
            "\n",
            "--2021-05-07 11:45:33--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9956876 (9.5M) [application/octet-stream]\n",
            "Saving to: ‘vi-ner.xz’\n",
            "\n",
            "vi-ner.xz           100%[===================>]   9.50M  59.9MB/s    in 0.2s    \n",
            "\n",
            "2021-05-07 11:45:34 (59.9 MB/s) - ‘vi-ner.xz’ saved [9956876/9956876]\n",
            "\n",
            "--2021-05-07 11:45:34--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 57313672 (55M) [application/octet-stream]\n",
            "Saving to: ‘vi-pretrainedembeddings.xz’\n",
            "\n",
            "vi-pretrainedembedd 100%[===================>]  54.66M   142MB/s    in 0.4s    \n",
            "\n",
            "2021-05-07 11:45:36 (142 MB/s) - ‘vi-pretrainedembeddings.xz’ saved [57313672/57313672]\n",
            "\n",
            "--2021-05-07 11:45:36--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29709468 (28M) [application/octet-stream]\n",
            "Saving to: ‘vi-tagger’\n",
            "\n",
            "vi-tagger           100%[===================>]  28.33M   101MB/s    in 0.3s    \n",
            "\n",
            "2021-05-07 11:45:38 (101 MB/s) - ‘vi-tagger’ saved [29709468/29709468]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD22vnF7K4Ta"
      },
      "source": [
        "## Install UndertheSea"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZnrrGI2LB_D",
        "outputId": "817a48c3-b4ce-40ee-9f1c-1d7ab57b09f1"
      },
      "source": [
        "!pip install underthesea==1.2.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting underthesea==1.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/46/1acb7e83092bbcbc9082afe3901ec51e98a303a19c8152655c43bd51583f/underthesea-1.2.3-py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (0.8.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (2.23.0)\n",
            "Collecting scikit-learn<0.22,>=0.20\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/c5/e5267eb84994e9a92a2c6a6ee768514f255d036f3c8378acfa694e9f2c99/scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 33.9MB/s \n",
            "\u001b[?25hCollecting python-crfsuite>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 52.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (3.2.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (3.13)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (1.0.1)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (4.41.1)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 53.6MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (1.24.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.22,>=0.20->underthesea==1.2.3) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.22,>=0.20->underthesea==1.2.3) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea==1.2.3) (1.15.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=2dbcabe986e4b0dedc34b322c83f7d8ce372d61a999b56a46217a2dca7b51cb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: scikit-learn, python-crfsuite, unidecode, seqeval, underthesea\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed python-crfsuite-0.9.7 scikit-learn-0.21.3 seqeval-1.2.2 underthesea-1.2.3 unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InJpM7aDEtHB"
      },
      "source": [
        "# Read and Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcW3ncfLE_sB"
      },
      "source": [
        "import unicodedata\n",
        "import os, time\n",
        "import copy, json\n",
        "\n",
        "from underthesea import sent_tokenize, word_tokenize\n",
        "from vncorenlp import VnCoreNLP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCOWz9EfFPzn"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OukJU79aE1pl",
        "outputId": "bde3d211-0516-41e4-e63e-52ae4fff0745"
      },
      "source": [
        "with open('test_data.json') as test_data_json:\n",
        "  jtest_data = json.load(test_data_json)\n",
        "\n",
        "print(type(jtest_data))\n",
        "print(*jtest_data[0:6], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "{'doc_id': '23352901', 'sent_id': 1, 'sentence': \"Đây là lý do khiến Yoon Ah quyết định cắt mái tóc dài 'nữ thần' Mái tóc cũ của thành viên SNSD bị hư hỏng nặng nề và Yoon Ah thậm chí không muốn nuôi tóc lại.\", 'spos': [0, 158], 'entity_1': {'text': 'Yoon Ah', 'pos': [19, 26]}, 'entity_2': {'text': 'SNSD', 'pos': [90, 94]}, 'entity_1_info': {'first_token': [5, 6, 1, 'PERSON'], 'last_token': [6, 7, 1, 'PERSON']}, 'entity_2_info': {'first_token': [21, 22, 0, 'ORGANIZATION'], 'last_token': [21, 22, 0, 'ORGANIZATION']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 2, 'sentence': \"Đây là lý do khiến Yoon Ah quyết định cắt mái tóc dài 'nữ thần' Mái tóc cũ của thành viên SNSD bị hư hỏng nặng nề và Yoon Ah thậm chí không muốn nuôi tóc lại.\", 'spos': [0, 158], 'entity_1': {'text': 'Yoon Ah', 'pos': [19, 26]}, 'entity_2': {'text': 'Yoon Ah', 'pos': [117, 124]}, 'entity_1_info': {'first_token': [5, 6, 1, 'PERSON'], 'last_token': [6, 7, 1, 'PERSON']}, 'entity_2_info': {'first_token': [28, 29, 2, 'PERSON'], 'last_token': [29, 30, 2, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 3, 'sentence': \"Đây là lý do khiến Yoon Ah quyết định cắt mái tóc dài 'nữ thần' Mái tóc cũ của thành viên SNSD bị hư hỏng nặng nề và Yoon Ah thậm chí không muốn nuôi tóc lại.\", 'spos': [0, 158], 'entity_1': {'text': 'SNSD', 'pos': [90, 94]}, 'entity_2': {'text': 'Yoon Ah', 'pos': [117, 124]}, 'entity_1_info': {'first_token': [21, 22, 0, 'ORGANIZATION'], 'last_token': [21, 22, 0, 'ORGANIZATION']}, 'entity_2_info': {'first_token': [28, 29, 2, 'PERSON'], 'last_token': [29, 30, 2, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 4, 'sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Soo Young', 'pos': [0, 9]}, 'entity_2': {'text': 'Seo Hyun', 'pos': [13, 21]}, 'entity_1_info': {'first_token': [258, 259, 8, 'PERSON'], 'last_token': [259, 260, 8, 'PERSON']}, 'entity_2_info': {'first_token': [261, 262, 9, 'PERSON'], 'last_token': [262, 263, 9, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 5, 'sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Soo Young', 'pos': [0, 9]}, 'entity_2': {'text': 'Yoon Ah', 'pos': [40, 47]}, 'entity_1_info': {'first_token': [258, 259, 8, 'PERSON'], 'last_token': [259, 260, 8, 'PERSON']}, 'entity_2_info': {'first_token': [267, 268, 10, 'PERSON'], 'last_token': [268, 269, 10, 'PERSON']}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23352901', 'sent_id': 6, 'sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Soo Young', 'pos': [0, 9]}, 'entity_2': {'text': 'Sunny', 'pos': [51, 56]}, 'entity_1_info': {'first_token': [258, 259, 8, 'PERSON'], 'last_token': [259, 260, 8, 'PERSON']}, 'entity_2_info': {'first_token': [270, 271, 0, 'PERSON'], 'last_token': [270, 271, 0, 'PERSON']}, 'label': 'OTHERS'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jyBD2u1NP3r"
      },
      "source": [
        "## Clean func"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgaMrjV05d1k"
      },
      "source": [
        "### Normalize text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy2V2T-h5orS"
      },
      "source": [
        "Bên dưới, ta sẽ normalize lại các câu và normalize cả entity text. Tuy nhiên, ta sẽ tạo bản copy, vẫn giữ lại câu gốc và entity gốc để tiện sau này match cho tập test. Normalize sẽ giúp các mô hình BERT nhận diện, word peice câu tốt hơn.\n",
        "\n",
        "Và do nhiều câu bị lỗi unicode, chưa được normalize nên sau khi normalize thì pos entity bị thay đổi nên ta phải tìm pos mới của entity trong câu mới.\n",
        "\n",
        "\n",
        "Vấn đề: \n",
        "- do sau khi được normalize thì pos của entity sẽ thay đổi nên ta cần tìm lại pos của entity trong câu mới\n",
        "- trong câu có thể có nhiều cụm từ giống entity đang xét trong câu (nhiều entity giống nhau trong câu) nên cũng phải cẩn thận khi tìm pos của entity trong câu mới.\n",
        "\n",
        "Ý tưởng:\n",
        "- giả sử trong câu có 5 entity giống hệt nhau. và entity ta đang xét là entity thứ 3 trong số 5 entity kia.\n",
        "  + ... entity_1 ... entity_2 ... **entity_3** ... entity_4 ... entity_5 ...  (dù tên khác nhưng text sau khi được normlize sẽ là một.)\n",
        "- Từ **câu gốc** ta tạo thành 3 câu: \n",
        "  + ... entity_1 ... entity_2 ... **entity_3** ... entity_4 ... entity_5 ... (vẫn là câu gốc)\n",
        "  + ... entity_1 ... entity_2 ... (phần trước entity trong câu gốc - dùng **pos cũ** của entity trong câu cũ)\n",
        "  + ... entity_4 ... entity_5 ... (phần sau entity trong câu gốc - dùng **pos cũ** của entity trong câu cũ)\n",
        "- đầu tiên ta normalize cả 3 câu trên và normalize entity_text đang xét\n",
        "- sau đó tìm pos indice của 'normalized entity_text' trong cả 3 câu đã được nomalize trên, sẽ thu được kết quả như dưới:\n",
        "  + ``` [[ent_1_pos], [ent_2_pos], [ent_3_pos], [ent_4_pos], [ent_5_pos]] ```\n",
        "  + ``` [[ent_1_pos], [ent_2_pos]] ```\n",
        "  + ``` [[ent_4_pos], [ent_5_pos]] ```\n",
        "- cuối cùng chỉ cần lấy list đầu trừ đi tổng 2 list dưới là sẽ thu được: ```[ent_3_pos]```\n",
        "\n",
        "- Hiểu một cách đơn giản là tìm pos pos trong cả câu, rồi tìm trong phần trước xem có những cái nào, tìm trong phần sau xem có những cái nào. rồi cái còn lại chưa xuất hiện thì chính là pos entity của cái cần tìm\n",
        "\n",
        "Sau khi so một vài kết quả của cách này với cách cũ (có bug nên chỉ in ra được vài kết quả) thì thấy đều ổn, giống nhau.\n",
        "\n",
        "**Lưu ý:** cần xem entity có bị overlap không, không thì mới dùng được. do code bên dưới là dùng cho tìm pos non overlap. (ví dụ overlap: tìm ACA trong ACACA thì code bên dưới chỉ tìm được 1 pos đầu dù có 2 cái). Dùng code không overlap vì muốn xem data kĩ hơn. xem có bị xấu như kia không.\n",
        "https://stackoverflow.com/questions/4664850/how-to-find-all-occurrences-of-a-substring\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLR5EJB1uq9l",
        "outputId": "56a9ce64-e951-49e2-bec2-519a7de061f0"
      },
      "source": [
        "import re\n",
        "\n",
        "txt = \"Một số hình ảnh tư liệu về quá trình thực thi và bảo vệ chủ quyền của Việt Nam đối với hai quần đảo Hoàng Sa và Trường Sa từ những năm 1930 đến khi Trung Quốc xâm chiếm toàn bộ quần đảo Hoàng Sa bằng trận “Hải chiến Hoàng Sa ” ngày 19-1-1974.\"\n",
        "\n",
        "print([i for i in range(len(txt)) if txt.startswith('Hoàng Sa', i)])\n",
        "\n",
        "print([[a.start(), a.end()] for a in list(re.finditer('Hoàng Sa', txt))])\n",
        "\n",
        "print([[m.start(), m.end()] for m in re.finditer('Hoàng Sa', txt)])  # <--- dùng code này\n",
        "                                                                     # không dùng được cho overlap (find 'ACA' in 'ACACA' sẽ chỉ cho 1 kết quả)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[109, 200, 232]\n",
            "[[109, 117], [200, 208], [232, 240]]\n",
            "[[109, 117], [200, 208], [232, 240]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1B6xMCyZ62y",
        "outputId": "1eff19a2-c4d7-4a99-cb16-dd1e860150b4"
      },
      "source": [
        "a = [[1,2], [6,8]]\n",
        "b = [[1,2], [3,5], [6,8], [11,22]]\n",
        "print(all(i in b for i in a))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyhi7xJi5hpl"
      },
      "source": [
        "def normalize_sentif(jdata):\n",
        "\n",
        "    new_jdata = []\n",
        "\n",
        "    count_changed_sent = 0\n",
        "\n",
        "    for sentif in jdata:\n",
        "\n",
        "        new_sentence = copy.deepcopy(sentif['sentence'])\n",
        "        new_entity_1_text, new_entity_1_pos = copy.deepcopy(sentif['entity_1']['text']), copy.deepcopy(sentif['entity_1']['pos'])\n",
        "        new_entity_2_text, new_entity_2_pos = copy.deepcopy(sentif['entity_2']['text']), copy.deepcopy(sentif['entity_2']['pos'])\n",
        "\n",
        "        if sentif['sentence'] != unicodedata.normalize(\"NFC\", sentif['sentence']):\n",
        "\n",
        "            new_sentence = copy.deepcopy(unicodedata.normalize(\"NFC\", sentif['sentence']))\n",
        "            new_entity_1_text = copy.deepcopy(unicodedata.normalize(\"NFC\", sentif['entity_1']['text']))\n",
        "            new_entity_2_text = copy.deepcopy(unicodedata.normalize(\"NFC\", sentif['entity_2']['text']))\n",
        "\n",
        "            # tìm trong cả câu đã được normalized:  A B C entity D E F\n",
        "            entity_1_all_pos = [[m.start(), m.end()] for m in re.finditer(re.escape(new_entity_1_text), new_sentence)]\n",
        "            entity_2_all_pos = [[m.start(), m.end()] for m in re.finditer(re.escape(new_entity_2_text), new_sentence)]\n",
        "\n",
        "            # tìm trong phần đằng trước entity: A B C\n",
        "            sent_before_entity_1 = sentif['sentence'][:sentif['entity_1']['pos'][0]]\n",
        "            sent_before_entity_1 = unicodedata.normalize(\"NFC\", sent_before_entity_1)\n",
        "\n",
        "            sent_before_entity_2 = sentif['sentence'][:sentif['entity_2']['pos'][0]]\n",
        "            sent_before_entity_2 = unicodedata.normalize(\"NFC\", sent_before_entity_2)\n",
        "\n",
        "            entity_1_before_pos = [[m.start(), m.end()] for m in re.finditer(re.escape(new_entity_1_text), sent_before_entity_1)]\n",
        "            entity_2_before_pos = [[m.start(), m.end()] for m in re.finditer(re.escape(new_entity_2_text), sent_before_entity_2)]\n",
        "\n",
        "            assert (all(i in entity_1_all_pos for i in entity_1_before_pos)), str('\\nPROBLEM WITH POS BEFORE ENTITY 1')\n",
        "            assert (all(i in entity_2_all_pos for i in entity_2_before_pos)), str('\\nPROBLEM WITH POS BEFORE ENTITY 2')\n",
        "\n",
        "            # tìm trong phần đằng sau entity: D E F\n",
        "            sent_after_entity_1 = sentif['sentence'][sentif['entity_1']['pos'][1]:]\n",
        "            sent_after_entity_1 = unicodedata.normalize(\"NFC\", sent_after_entity_1)\n",
        "\n",
        "            sent_after_entity_2 = sentif['sentence'][sentif['entity_2']['pos'][1]:]\n",
        "            sent_after_entity_2 = unicodedata.normalize(\"NFC\", sent_after_entity_2)\n",
        "\n",
        "            entity_1_after_pos = [[m.start(), m.end()] for m in re.finditer(re.escape(new_entity_1_text), sent_after_entity_1)]\n",
        "            entity_2_after_pos = [[m.start(), m.end()] for m in re.finditer(re.escape(new_entity_2_text), sent_after_entity_2)]\n",
        "\n",
        "            change_index_after_1 = len(unicodedata.normalize(\"NFC\", sentif['sentence'][:sentif['entity_1']['pos'][1]]))\n",
        "            change_index_after_2 = len(unicodedata.normalize(\"NFC\", sentif['sentence'][:sentif['entity_2']['pos'][1]]))\n",
        "\n",
        "            entity_1_after_pos = [[(tmp[0]+change_index_after_1), (tmp[1]+change_index_after_1)] for tmp in entity_1_after_pos]\n",
        "            entity_2_after_pos = [[(tmp[0]+change_index_after_2), (tmp[1]+change_index_after_2)] for tmp in entity_2_after_pos]\n",
        "\n",
        "            assert (all(i in entity_1_all_pos for i in entity_1_after_pos)), str('\\nPROBLEM WITH POS AFTER ENTITY 1')\n",
        "            assert (all(i in entity_2_all_pos for i in entity_2_after_pos)), str('\\nPROBLEM WITH POS AFTER ENTITY 2')\n",
        "\n",
        "\n",
        "            new_entity_1_pos = [itm for itm in entity_1_all_pos if itm not in (entity_1_before_pos + entity_1_after_pos)]\n",
        "            new_entity_2_pos = [itm for itm in entity_2_all_pos if itm not in (entity_2_before_pos + entity_2_after_pos)]\n",
        "            \n",
        "            \n",
        "            assert (len(new_entity_1_pos) == 1), str('BELLELEELL 1')\n",
        "            assert (len(new_entity_2_pos) == 1), str('BELLELEELL 2')\n",
        "            \n",
        "            '''\n",
        "            if (len(new_entity_1_pos) != 1):\n",
        "                print(sentif)\n",
        "                print(entity_1_all_pos)\n",
        "                print(entity_1_before_pos)\n",
        "                print(entity_1_after_pos)\n",
        "\n",
        "            if (len(new_entity_2_pos) != 1):\n",
        "                print(sentif)\n",
        "                print(entity_2_all_pos)\n",
        "                print(entity_2_before_pos)\n",
        "                print(entity_2_after_pos)\n",
        "            '''\n",
        "\n",
        "            new_entity_1_pos = copy.deepcopy(new_entity_1_pos[0])\n",
        "            new_entity_2_pos = copy.deepcopy(new_entity_2_pos[0])\n",
        "            \n",
        "\n",
        "            assert (new_sentence[new_entity_1_pos[0]:new_entity_1_pos[1]] == new_entity_1_text), \\\n",
        "            str('\\nAFTER NORMALIZE, ENTITY 1 TEXT NOT MATCH ENTITY 1 POS. sent_id: ' + str(sent_id))\n",
        "\n",
        "            assert (new_sentence[new_entity_2_pos[0]:new_entity_2_pos[1]] == new_entity_2_text), \\\n",
        "            str('\\nAFTER NORMALIZE, ENTITY 2 TEXT NOT MATCH ENTITY 2 POS. sent_id: ' + str(sent_id))\n",
        "\n",
        "\n",
        "        # thêm vào new_jdata\n",
        "        new_sentif = copy.deepcopy(sentif)\n",
        "\n",
        "        new_sentif['new_sentence'] = copy.deepcopy(new_sentence)\n",
        "\n",
        "        new_entity_1 = copy.deepcopy({'text': copy.deepcopy(new_entity_1_text), 'pos': copy.deepcopy(new_entity_1_pos)})\n",
        "        new_entity_2 = copy.deepcopy({'text': copy.deepcopy(new_entity_2_text), 'pos': copy.deepcopy(new_entity_2_pos)})\n",
        "\n",
        "        new_sentif['new_entity_1'] = copy.deepcopy(new_entity_1)\n",
        "        new_sentif['new_entity_2'] = copy.deepcopy(new_entity_2)\n",
        "\n",
        "        new_jdata.append(copy.deepcopy(new_sentif))\n",
        "\n",
        "\n",
        "        \n",
        "        # in để check xem code chạy đúng không\n",
        "        if new_jdata[-1]['new_sentence'] != sentif['sentence']:\n",
        "            count_changed_sent += 1\n",
        "\n",
        "            print('\\n\\n----- ', count_changed_sent,  ' - sent_id: ', sentif['sent_id'])\n",
        "            print('Original sent:   ', sentif['sentence'])\n",
        "            print('Normalized sent: ', new_jdata[-1]['new_sentence'])\n",
        "\n",
        "            print('Previous entity 1:   ', sentif['entity_1'])\n",
        "            print('Normalized entity 1: ', new_jdata[-1]['new_entity_1'])\n",
        "\n",
        "            print('Previous entity 2:   ', sentif['entity_2'])\n",
        "            print('Normalized entity 2: ', new_jdata[-1]['new_entity_2'])\n",
        "        \n",
        "        # phần cũ vẫn phải y nguyên. bên trên chỉ thêm 'new_sentence', 'new_entity_1' và 'new_entity_2'\n",
        "        assert (new_jdata[-1]['sent_id'] == sentif['sent_id']), str('FAILED TO COPY sent_id. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['doc_id'] == sentif['doc_id']), str('FAILED TO COPY doc_id. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['sentence'] == sentif['sentence']), str('FAILED TO COPY sentence. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['entity_1'] == sentif['entity_1']), str('FAILED TO COPY entity_1. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['entity_2'] == sentif['entity_2']), str('FAILED TO COPY SENT_ID. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['label'] == sentif['label']), str('FAILED TO COPY SENT_ID. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['spos'] == sentif['spos']), str('FAILED TO COPY spos. sent_id: ' + sentif['sent_id'])\n",
        "    \n",
        "    \n",
        "    return copy.deepcopy(new_jdata)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNfcso5IDPPK"
      },
      "source": [
        "### Remove non alnum character at start and end of entity_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjsrq58pDZVj"
      },
      "source": [
        "Trong data nhiều entity ở đầu hoặc cuối (kí tự đầu hoặc cuối) bị lẫn dấu câu như (, ',... hay khoảng trắng. Điều này làm ảnh hướng tới mô hình nếu sau này ta pooling vectors của các word piece trong entity để được vector đại diện cho entity thì sẽ bị lẫn các kí tự không cần kia.\n",
        "\n",
        "Ngoài ra, cách tìm index các wordpiece của entity (để sau pooling được) bên dưới ta dùng không cho phép có khoảng trắng lẫn vào ở đầu hoặc cuối entity nên cần loại bỏ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-ly6uxrDOmI"
      },
      "source": [
        "def remove_entity_non_alnum_start_end(sent_id, sentence, entity_if):\n",
        "    '''\n",
        "    Bên trên ở phần \"check\", thấy rằng nhiều entity bị lẫn những kí tự \"thừa\" ở đầu hoặc ở cuối entity\n",
        "    ví dụ: (Thái Nguyên    <----- bị lẫn kí tự (\n",
        "\n",
        "    Sau khi xem dữ liệu, ta thấy những kí tự \"thừa\" là những kí tự không phải là chữ cái hay chữ số. (not .isalnum())\n",
        "    (trừ dấu \".\" và trường hợp entity là \"6+\")\n",
        "\n",
        "    Ngoài ra, bên trên trong phần check, thấy rằng nếu trong (giữa) entity có kí tự không phải chữ số hay chữ cái thì cũng đều hợp lý \n",
        "    chứ không phải lỗi.\n",
        "\n",
        "    Trong hàm này, ta sẽ tạo ra một sửa, loại bỏ các kí tự kia khỏi entity lỗi.\n",
        "\n",
        "    Tuy nhiên, để tránh việc sau này trên tập test khó match được entity thì ta sẽ chỉ tạo bản copy và sửa trên bản copy thôi.\n",
        "    '''\n",
        "\n",
        "    # just double check\n",
        "    assert ((not entity_if['text'][0].isalnum()) or (not entity_if['text'][-1].isalnum())), \\\n",
        "    str('Both First and last character of entity are  alnum.')\n",
        "\n",
        "    # count non alnum character at start\n",
        "    count_at_start = 0\n",
        "    for c in entity_if['text']:\n",
        "        if (not c.isalnum()):\n",
        "            count_at_start += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # count non alnum character at end\n",
        "    count_at_end = 0\n",
        "    for c in entity_if['text'][::-1]:\n",
        "        if (not c.isalnum()) and (c != '+') and (c != '.'):   # trong tập train, dev, thì + và . chỉ xuất hiện ở cuối entity nên phần count start k có\n",
        "            count_at_end += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    entity_text = entity_if['text']\n",
        "    entity_pos = entity_if['pos']   # pos trong câu\n",
        "\n",
        "    new_entity_text = entity_text[(count_at_start):(len(entity_text) - count_at_end)]   # slice trong entity không phải slice trong sentence\n",
        "    new_entity_pos = [(entity_pos[0] + count_at_start), (entity_pos[1] - count_at_end)]   # pos trong câu\n",
        "\n",
        "    assert (sentence[new_entity_pos[0]:new_entity_pos[1]] == new_entity_text), \\\n",
        "    str('\\nNew entity text not match new entity pos.')\n",
        "\n",
        "\n",
        "    return new_entity_text, new_entity_pos\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAH5RjECE32S"
      },
      "source": [
        "def fix_start_end_of_entity(jdata):\n",
        "\n",
        "    new_jdata = copy.deepcopy([])\n",
        "\n",
        "    tmpppent = []\n",
        "\n",
        "    for sentif in jdata:\n",
        "\n",
        "        # nếu bên dưới không bị thay đổi tức là không chứa \"kí tự thừa\" thì vẫn giống entity cũ\n",
        "        # ngoài ra ta sẽ truyền vào hàm này j(train/dev)_data_v1 và thay đổi trên new_entity_1\n",
        "        new_entity_1_text = copy.deepcopy(sentif['new_entity_1']['text'])\n",
        "        new_entity_1_pos = copy.deepcopy(sentif['new_entity_1']['pos'])\n",
        "\n",
        "        if (not sentif['new_entity_1']['text'][0].isalnum()) or (not sentif['new_entity_1']['text'][-1].isalnum()):\n",
        "            new_entity_1_text, new_entity_1_pos = remove_entity_non_alnum_start_end(sentif['sent_id'], sentif['new_sentence'], sentif['new_entity_1'])\n",
        "\n",
        "        # nếu bên dưới không bị thay đổi tức là không chứa \"kí tự thừa\" thì vẫn giống entity cũ\n",
        "        # ngoài ra ta sẽ truyền vào hàm này j(train/dev)_data_v1 và thay đổi trên new_entity_2\n",
        "        new_entity_2_text = copy.deepcopy(sentif['new_entity_2']['text'])\n",
        "        new_entity_2_pos = copy.deepcopy(sentif['new_entity_2']['pos'])\n",
        "\n",
        "        if (not sentif['new_entity_2']['text'][0].isalnum()) or (not sentif['new_entity_2']['text'][-1].isalnum()):\n",
        "            new_entity_2_text, new_entity_2_pos = remove_entity_non_alnum_start_end(sentif['sent_id'], sentif['new_sentence'], sentif['new_entity_2'])\n",
        "\n",
        "        \n",
        "\n",
        "        new_sentif = copy.deepcopy(sentif)\n",
        "\n",
        "        new_entity_1 = {'text': copy.deepcopy(new_entity_1_text), 'pos': copy.deepcopy(new_entity_1_pos)}\n",
        "        new_entity_2 = {'text': copy.deepcopy(new_entity_2_text), 'pos': copy.deepcopy(new_entity_2_pos)}\n",
        "\n",
        "        '''\n",
        "        del new_sentif['new_entity_1']\n",
        "        del new_sentif['new_entity_2']\n",
        "        '''\n",
        "\n",
        "        new_sentif['new_entity_1'] = copy.deepcopy(new_entity_1)\n",
        "        new_sentif['new_entity_2'] = copy.deepcopy(new_entity_2)\n",
        "\n",
        "\n",
        "        new_jdata.append(copy.deepcopy(new_sentif))\n",
        "\n",
        "\n",
        "\n",
        "        # in kết quả\n",
        "        if (new_jdata[-1]['new_entity_1'] != sentif['new_entity_1']) and (sentif['new_entity_1']['text'] not in tmpppent):\n",
        "\n",
        "            print('\\noriginal:      ', sentif['new_entity_1'])\n",
        "            print('new_entity:    ', new_jdata[-1]['new_entity_1'])\n",
        "\n",
        "            tmpppent.append(sentif['new_entity_1']['text'])\n",
        "\n",
        "        if (new_jdata[-1]['new_entity_2'] != sentif['new_entity_2']) and (sentif['new_entity_2']['text'] not in tmpppent):\n",
        "\n",
        "            print('\\noriginal:      ', sentif['new_entity_2'])\n",
        "            print('new_entity:    ', new_jdata[-1]['new_entity_2'])\n",
        "\n",
        "            tmpppent.append(sentif['new_entity_2']['text'])\n",
        "\n",
        "    return new_jdata\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F2EbtGIhZY_"
      },
      "source": [
        "## Clean test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF7krGOHhZY_"
      },
      "source": [
        "### Normalize test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUyGGJYyhZY_"
      },
      "source": [
        "#### check overlap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO9uqIffhZY_"
      },
      "source": [
        "CHÚ Ý: Cần review data để xem có xảy ra overlap entity không vì code find_nth bên dưới dùng là cho không overlap.\n",
        "\n",
        "Ví dụ: overlap: ACACA\n",
        "\n",
        "https://stackoverflow.com/questions/1883980/find-the-nth-occurrence-of-substring-in-a-string\n",
        "\n",
        "https://stackoverflow.com/questions/4664850/how-to-find-all-occurrences-of-a-substring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKRfSSVehZZA",
        "outputId": "2b890dfb-56e7-468c-8564-36ce8b3cc5ef"
      },
      "source": [
        "tmpppsent = []\n",
        "counttt = 0\n",
        "\n",
        "for sentif in jtest_data:\n",
        "\n",
        "    if (sentif['sentence'] != unicodedata.normalize(\"NFC\", sentif['sentence'])):\n",
        "        counttt += 1\n",
        "\n",
        "        if (sentif['sentence'] not in tmpppsent):\n",
        "            print(sentif['sent_id'])\n",
        "            print(sentif['sentence'])\n",
        "            tmpppsent.append(sentif['sentence'])\n",
        "\n",
        "print(counttt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYqWH67fhZZD"
      },
      "source": [
        "#### Normalize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQvV6M2rhZZD"
      },
      "source": [
        "jtest_data_v1 = copy.deepcopy(normalize_sentif(jtest_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibDYFDo5hZZE"
      },
      "source": [
        "### fix start end of entity in dev data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVKO5QHZhZZE"
      },
      "source": [
        "jtest_data_use = copy.deepcopy(jtest_data_v1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJJS0PLthZZE"
      },
      "source": [
        "#### Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du2XIqDChZZE",
        "outputId": "9faa2015-5278-448d-a3fb-7ec23f0c160b"
      },
      "source": [
        "tmpppsent = []\n",
        "allchar_lst = []\n",
        "alnum_char_lst = []\n",
        "not_alnum_char_lst = []\n",
        "\n",
        "for sentif in jtest_data_use:\n",
        "    if sentif['sentence'] not in tmpppsent:\n",
        "        for ch in sentif['sentence']:\n",
        "            if ch not in allchar_lst:\n",
        "                allchar_lst.append(ch)\n",
        "\n",
        "            if ch.isalnum() and (ch not in alnum_char_lst):\n",
        "                alnum_char_lst.append(ch)\n",
        "\n",
        "            if (not ch.isalnum()) and (ch not in not_alnum_char_lst):\n",
        "                not_alnum_char_lst.append(ch)\n",
        "              \n",
        "        tmpppsent.append(sentif['sentence'])\n",
        "\n",
        "\n",
        "print(len(allchar_lst))\n",
        "for i in range(0, len(allchar_lst), 30):\n",
        "    # Limit the end index so we don't go past the end of the list.\n",
        "    end = min(i + 30, len(allchar_lst) + 1)\n",
        "\n",
        "    # Print out the tokens, separated by a space.\n",
        "    print(repr(' '.join(allchar_lst[i:end])))\n",
        "\n",
        "\n",
        "print('\\n', len(alnum_char_lst))\n",
        "for i in range(0, len(alnum_char_lst), 20):\n",
        "    # Limit the end index so we don't go past the end of the list.\n",
        "    end = min(i + 20, len(alnum_char_lst) + 1)\n",
        "\n",
        "    # Print out the tokens, separated by a space.\n",
        "    print(repr(' '.join(alnum_char_lst[i:end])))\n",
        "\n",
        "\n",
        "print('\\n', len(not_alnum_char_lst))\n",
        "for i in range(0, len(not_alnum_char_lst), 20):\n",
        "    # Limit the end index so we don't go past the end of the list.\n",
        "    end = min(i + 20, len(not_alnum_char_lst) + 1)\n",
        "    \n",
        "    # Print out the tokens, separated by a space.\n",
        "    print(repr(' '.join(not_alnum_char_lst[i:end])))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "195\n",
            "\"Đ â y   l à ý d o k h i ế n Y A q u t đ ị c ắ m á ó ' ữ ầ M\"\n",
            "'ũ ủ a v ê S N D b ư ỏ g ặ ề ậ í ô ố ạ . e H ể p , ổ ả r ớ ỉ'\n",
            "'ẻ B ự ứ T P ờ : \" ì s ò ọ ệ ă ừ ” C ĩ ơ w ỷ 2 0 ẽ “ ợ ễ 1 4'\n",
            "'5 / ộ ở V ( ẹ ồ ) ù ấ … ú Q è ỹ - ẩ L ã R G x K ụ 9 J õ ằ ỡ'\n",
            "'é ử \\xa0 ẳ ỳ 7 I % E 8 Ả X 6 3 Ấ Z f ẫ F Á ; – ỗ + Ô Â W z j &'\n",
            "'O | ? ẵ ç U Ủ \\ufeff Ý ‘ ’ Ư Ú ! ō ä Ố ° ë Ở # [ ] ỵ ž č ě Ế Ộ À'\n",
            "'ü ′ Ì * Ứ Ă Ẩ Ầ @ Ự 徐 晃 公 明 Í'\n",
            "\n",
            " 163\n",
            "'Đ â y l à ý d o k h i ế n Y A q u t đ ị'\n",
            "'c ắ m á ó ữ ầ M ũ ủ a v ê S N D b ư ỏ g'\n",
            "'ặ ề ậ í ô ố ạ e H ể p ổ ả r ớ ỉ ẻ B ự ứ'\n",
            "'T P ờ ì s ò ọ ệ ă ừ C ĩ ơ w ỷ 2 0 ẽ ợ ễ'\n",
            "'1 4 5 ộ ở V ẹ ồ ù ấ ú Q è ỹ ẩ L ã R G x'\n",
            "'K ụ 9 J õ ằ ỡ é ử ẳ ỳ 7 I E 8 Ả X 6 3 Ấ'\n",
            "'Z f ẫ F Á ỗ Ô Â W z j O ẵ ç U Ủ Ý Ư Ú ō'\n",
            "'ä Ố ë Ở ỵ ž č ě Ế Ộ À ü Ì Ứ Ă Ẩ Ầ Ự 徐 晃'\n",
            "'公 明 Í'\n",
            "\n",
            " 32\n",
            "'  \\' . , : \" ” “ / ( ) … - \\xa0 % ; – + & |'\n",
            "'? \\ufeff ‘ ’ ! ° # [ ] ′ * @'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jMWWYbkhZZE",
        "outputId": "f71cb78e-c8ae-4bb5-f611-faaf43738589"
      },
      "source": [
        "# review data\n",
        "# có vẻ là nếu trong entity có kí tự bắt đầu hoặc kết thúc là số thì k phải lỗi.\n",
        "\n",
        "for sentif in jtest_data_use:\n",
        "    if (sentif['entity_1']['text'][0].isnumeric()) or (sentif['entity_1']['text'][-1].isnumeric()):\n",
        "        print(sentif['entity_1'])\n",
        "\n",
        "    if (sentif['entity_2']['text'][0].isnumeric()) or (sentif['entity_2']['text'][-1].isnumeric()):\n",
        "        print(sentif['entity_2'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': 'Bộ Tư lệnh tác chiến 2', 'pos': [44, 66]}\n",
            "{'text': 'Bộ Tư lệnh tác chiến 2', 'pos': [44, 66]}\n",
            "{'text': 'quận 9', 'pos': [28, 34]}\n",
            "{'text': 'quận 9', 'pos': [28, 34]}\n",
            "{'text': '(quận 9', 'pos': [108, 115]}\n",
            "{'text': '(quận 9', 'pos': [108, 115]}\n",
            "{'text': '(quận 9', 'pos': [108, 115]}\n",
            "{'text': '(quận 9', 'pos': [108, 115]}\n",
            "{'text': '(quận 9', 'pos': [108, 115]}\n",
            "{'text': '2015-2020,\\xa0ông\\xa0Nguyễn Xuân Anh', 'pos': [195, 225]}\n",
            "{'text': '2015-2020,\\xa0ông\\xa0Nguyễn Xuân Anh', 'pos': [195, 225]}\n",
            "{'text': '2015-2020,\\xa0ông\\xa0Nguyễn Xuân Anh', 'pos': [195, 225]}\n",
            "{'text': 'Tiểu đoàn 10', 'pos': [112, 124]}\n",
            "{'text': 'Trung đoàn 22', 'pos': [127, 140]}\n",
            "{'text': 'Quân khu 4', 'pos': [143, 153]}\n",
            "{'text': 'Tiểu đoàn 10', 'pos': [112, 124]}\n",
            "{'text': 'Trung đoàn 22', 'pos': [127, 140]}\n",
            "{'text': 'Quân khu 4', 'pos': [143, 153]}\n",
            "{'text': 'Tiểu đoàn 10', 'pos': [112, 124]}\n",
            "{'text': 'Trung đoàn 22', 'pos': [127, 140]}\n",
            "{'text': 'Quân khu 4', 'pos': [143, 153]}\n",
            "{'text': 'Tiểu đoàn 10', 'pos': [112, 124]}\n",
            "{'text': 'Trung đoàn 22', 'pos': [127, 140]}\n",
            "{'text': 'Tiểu đoàn 10', 'pos': [112, 124]}\n",
            "{'text': 'Quân khu 4', 'pos': [143, 153]}\n",
            "{'text': 'Tiểu đoàn 10', 'pos': [112, 124]}\n",
            "{'text': 'Trung đoàn 22', 'pos': [127, 140]}\n",
            "{'text': 'Quân khu 4', 'pos': [143, 153]}\n",
            "{'text': 'Trung đoàn 22', 'pos': [127, 140]}\n",
            "{'text': 'Quân khu 4', 'pos': [143, 153]}\n",
            "{'text': 'tiểu đoàn 10', 'pos': [18, 30]}\n",
            "{'text': 'tiểu đoàn 10', 'pos': [18, 30]}\n",
            "{'text': 'tiểu đoàn 10', 'pos': [18, 30]}\n",
            "{'text': 'sư đoàn 5', 'pos': [132, 141]}\n",
            "{'text': 'sư đoàn 5', 'pos': [132, 141]}\n",
            "{'text': 'sư đoàn 5', 'pos': [132, 141]}\n",
            "{'text': 'sư đoàn 5', 'pos': [150, 159]}\n",
            "{'text': '01/2017/TT-TANDTC', 'pos': [10, 27]}\n",
            "{'text': '218 Lý Tự Trọng', 'pos': [147, 162]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [20, 32]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [20, 32]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [20, 32]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [20, 32]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [20, 32]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [20, 32]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [20, 32]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [20, 32]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [20, 32]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [20, 32]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [20, 32]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [20, 32]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [30, 42]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [30, 42]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [30, 42]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [59, 71]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [59, 71]}\n",
            "{'text': 'tổ dân phố 7', 'pos': [59, 71]}\n",
            "{'text': 'tổ 59', 'pos': [211, 216]}\n",
            "{'text': 'tổ 26', 'pos': [288, 293]}\n",
            "{'text': 'tổ 59', 'pos': [211, 216]}\n",
            "{'text': 'tổ 26', 'pos': [288, 293]}\n",
            "{'text': 'tổ 59', 'pos': [211, 216]}\n",
            "{'text': 'tổ 26', 'pos': [288, 293]}\n",
            "{'text': 'tổ 59', 'pos': [211, 216]}\n",
            "{'text': 'tổ 26', 'pos': [288, 293]}\n",
            "{'text': 'tổ 59', 'pos': [211, 216]}\n",
            "{'text': 'tổ 26', 'pos': [288, 293]}\n",
            "{'text': 'tổ 59', 'pos': [211, 216]}\n",
            "{'text': 'tổ 59', 'pos': [211, 216]}\n",
            "{'text': 'tổ 59', 'pos': [211, 216]}\n",
            "{'text': 'tổ 59', 'pos': [211, 216]}\n",
            "{'text': 'tổ 26', 'pos': [288, 293]}\n",
            "{'text': 'tổ 59', 'pos': [211, 216]}\n",
            "{'text': 'tổ 59', 'pos': [211, 216]}\n",
            "{'text': 'tổ 26', 'pos': [288, 293]}\n",
            "{'text': 'tổ 26', 'pos': [288, 293]}\n",
            "{'text': 'tổ 26', 'pos': [288, 293]}\n",
            "{'text': 'tổ 26', 'pos': [288, 293]}\n",
            "{'text': 'tổ 26', 'pos': [288, 293]}\n",
            "{'text': 'KBS2', 'pos': [105, 109]}\n",
            "{'text': 'KBS2', 'pos': [105, 109]}\n",
            "{'text': 'Nhà máy Nhiệt điện Thái Bình 2', 'pos': [283, 313]}\n",
            "{'text': 'Nhà máy Nhiệt điện Thái Bình 2', 'pos': [283, 313]}\n",
            "{'text': 'Đại đội 4', 'pos': [56, 65]}\n",
            "{'text': 'Đại đội 4', 'pos': [56, 65]}\n",
            "{'text': 'Đại đội 4', 'pos': [56, 65]}\n",
            "{'text': 'Đại đội 4', 'pos': [56, 65]}\n",
            "{'text': 'Đại đội 4', 'pos': [70, 79]}\n",
            "{'text': 'Đại đội 4', 'pos': [70, 79]}\n",
            "{'text': 'Đại đội 4', 'pos': [102, 111]}\n",
            "{'text': 'Đại đội 4', 'pos': [102, 111]}\n",
            "{'text': 'Đại đội 4', 'pos': [102, 111]}\n",
            "{'text': 'Đại đội 4', 'pos': [19, 28]}\n",
            "{'text': 'Đại đội 4', 'pos': [19, 28]}\n",
            "{'text': 'Đại đội 4', 'pos': [42, 51]}\n",
            "{'text': 'VKS quân sự quân khu 4', 'pos': [136, 158]}\n",
            "{'text': 'VKS quân sự quân khu 4', 'pos': [136, 158]}\n",
            "{'text': 'quận 4', 'pos': [39, 45]}\n",
            "{'text': 'quận 4', 'pos': [39, 45]}\n",
            "{'text': '04/2017/TT-BKHĐT', 'pos': [81, 97]}\n",
            "{'text': '04/2017/TT-BKHĐT', 'pos': [81, 97]}\n",
            "{'text': '04/2017/TT-BKHĐT', 'pos': [81, 97]}\n",
            "{'text': '375 Greenwich Street', 'pos': [103, 123]}\n",
            "{'text': '375 Greenwich Street', 'pos': [103, 123]}\n",
            "{'text': '375 Greenwich Street', 'pos': [103, 123]}\n",
            "{'text': 'Sư đoàn 330', 'pos': [38, 49]}\n",
            "{'text': 'Quân khu 9', 'pos': [52, 62]}\n",
            "{'text': 'Quốc lộ 91', 'pos': [92, 102]}\n",
            "{'text': 'Quốc lộ 91', 'pos': [92, 102]}\n",
            "{'text': 'Quốc lộ 91', 'pos': [92, 102]}\n",
            "{'text': '\\ufeffBộ tư lệnh Quân khu 3', 'pos': [0, 22]}\n",
            "{'text': 'Bộ tư lệnh Quân khu 3', 'pos': [62, 83]}\n",
            "{'text': 'Quân khu 3', 'pos': [58, 68]}\n",
            "{'text': 'Quân khu 3', 'pos': [67, 77]}\n",
            "{'text': 'Quân khu 3', 'pos': [101, 111]}\n",
            "{'text': '(C45', 'pos': [46, 50]}\n",
            "{'text': 'quận 1', 'pos': [136, 142]}\n",
            "{'text': '(C45', 'pos': [46, 50]}\n",
            "{'text': '(C45', 'pos': [46, 50]}\n",
            "{'text': '(C45', 'pos': [46, 50]}\n",
            "{'text': 'quận 1', 'pos': [136, 142]}\n",
            "{'text': '(C45', 'pos': [46, 50]}\n",
            "{'text': '(C45', 'pos': [46, 50]}\n",
            "{'text': '(C45', 'pos': [46, 50]}\n",
            "{'text': 'quận 1', 'pos': [136, 142]}\n",
            "{'text': 'quận 1', 'pos': [136, 142]}\n",
            "{'text': 'quận 1', 'pos': [136, 142]}\n",
            "{'text': 'quận 1', 'pos': [136, 142]}\n",
            "{'text': 'quận 1', 'pos': [136, 142]}\n",
            "{'text': 'quận 4', 'pos': [55, 61]}\n",
            "{'text': 'quận 4', 'pos': [55, 61]}\n",
            "{'text': 'quận 4', 'pos': [55, 61]}\n",
            "{'text': 'quận 12', 'pos': [111, 118]}\n",
            "{'text': 'quận 12', 'pos': [111, 118]}\n",
            "{'text': 'quận 12', 'pos': [111, 118]}\n",
            "{'text': 'quận 12', 'pos': [111, 118]}\n",
            "{'text': 'quận 12', 'pos': [111, 118]}\n",
            "{'text': 'C45', 'pos': [8, 11]}\n",
            "{'text': 'C45', 'pos': [8, 11]}\n",
            "{'text': 'C45', 'pos': [8, 11]}\n",
            "{'text': 'C45', 'pos': [8, 11]}\n",
            "{'text': 'C45', 'pos': [15, 18]}\n",
            "{'text': 'C45', 'pos': [16, 19]}\n",
            "{'text': 'C45', 'pos': [29, 32]}\n",
            "{'text': 'C45', 'pos': [29, 32]}\n",
            "{'text': 'C45', 'pos': [29, 32]}\n",
            "{'text': 'C45', 'pos': [29, 32]}\n",
            "{'text': 'C45', 'pos': [17, 20]}\n",
            "{'text': 'C45', 'pos': [17, 20]}\n",
            "{'text': 'C45', 'pos': [55, 58]}\n",
            "{'text': 'C45', 'pos': [55, 58]}\n",
            "{'text': 'C45', 'pos': [55, 58]}\n",
            "{'text': 'C45', 'pos': [55, 58]}\n",
            "{'text': '144 Đội Cấn', 'pos': [163, 174]}\n",
            "{'text': '144 Đội Cấn', 'pos': [163, 174]}\n",
            "{'text': '144 Đội Cấn', 'pos': [163, 174]}\n",
            "{'text': '144 Đội Cấn', 'pos': [163, 174]}\n",
            "{'text': '144 Đội Cấn', 'pos': [163, 174]}\n",
            "{'text': 'quận 12', 'pos': [185, 192]}\n",
            "{'text': 'quận 12', 'pos': [185, 192]}\n",
            "{'text': 'quận 12', 'pos': [185, 192]}\n",
            "{'text': 'Q.5', 'pos': [51, 54]}\n",
            "{'text': 'Q.5', 'pos': [51, 54]}\n",
            "{'text': 'Q.10', 'pos': [93, 97]}\n",
            "{'text': 'Q.10', 'pos': [93, 97]}\n",
            "{'text': '163/2013/TTLT-BTC', 'pos': [93, 110]}\n",
            "{'text': '163/2013/TTLT-BTC', 'pos': [93, 110]}\n",
            "{'text': '163/2013/TTLT-BTC', 'pos': [93, 110]}\n",
            "{'text': '163/2013/TTLT-BTC', 'pos': [47, 64]}\n",
            "{'text': '163/2013/TTLT-BTC', 'pos': [85, 102]}\n",
            "{'text': 'U2', 'pos': [73, 75]}\n",
            "{'text': 'U2', 'pos': [73, 75]}\n",
            "{'text': 'U2', 'pos': [73, 75]}\n",
            "{'text': 'U2', 'pos': [73, 75]}\n",
            "{'text': '/VOV1', 'pos': [12, 17]}\n",
            "{'text': 'Xa lộ Liên tiểu bang 43', 'pos': [68, 91]}\n",
            "{'text': 'Xa lộ Liên tiểu bang 43', 'pos': [68, 91]}\n",
            "{'text': 'Xa lộ Liên tiểu bang 43', 'pos': [68, 91]}\n",
            "{'text': 'Xa lộ Liên tiểu bang 43', 'pos': [68, 91]}\n",
            "{'text': 'Xa lộ Liên tiểu bang 43', 'pos': [68, 91]}\n",
            "{'text': 'U23', 'pos': [79, 82]}\n",
            "{'text': 'U23', 'pos': [79, 82]}\n",
            "{'text': '04/2017/TT-BKHĐT', 'pos': [240, 256]}\n",
            "{'text': '04/2017/TT-BKHĐT', 'pos': [240, 256]}\n",
            "{'text': '04/2017/TT-BKHĐT', 'pos': [240, 256]}\n",
            "{'text': '04/2017/TT-BKHĐT', 'pos': [240, 256]}\n",
            "{'text': '04/2017/TT-BKHĐT', 'pos': [240, 256]}\n",
            "{'text': '07/2015/TT-BKHĐT', 'pos': [265, 281]}\n",
            "{'text': '07/2015/TT-BKHĐT', 'pos': [265, 281]}\n",
            "{'text': '07/2015/TT-BKHĐT', 'pos': [265, 281]}\n",
            "{'text': '07/2015/TT-BKHĐT', 'pos': [265, 281]}\n",
            "{'text': '04/2017/TT-BKHĐT', 'pos': [112, 128]}\n",
            "{'text': '04/2017/TT-BKHĐT', 'pos': [112, 128]}\n",
            "{'text': '04/2017/TT-BKHĐT', 'pos': [12, 28]}\n",
            "{'text': '(GOT7', 'pos': [149, 154]}\n",
            "{'text': '(GOT7', 'pos': [149, 154]}\n",
            "{'text': '(Chi hội Nhà báo VTV9', 'pos': [79, 100]}\n",
            "{'text': '(Chi hội Nhà báo VTV9', 'pos': [79, 100]}\n",
            "{'text': '(Chi hội Nhà báo VTV9', 'pos': [79, 100]}\n",
            "{'text': '(Chi hội Nhà báo VTV9', 'pos': [79, 100]}\n",
            "{'text': '(Chi hội Nhà báo VTV9', 'pos': [79, 100]}\n",
            "{'text': '50/2016/TT-BYT', 'pos': [36, 50]}\n",
            "{'text': '68/2010/TT-BNNPTNT', 'pos': [91, 109]}\n",
            "{'text': '50/2016/TT-BYT', 'pos': [36, 50]}\n",
            "{'text': '68/2010/TT-BNNPTNT', 'pos': [91, 109]}\n",
            "{'text': '50/2016/TT-BYT', 'pos': [36, 50]}\n",
            "{'text': '50/2016/TT-BYT', 'pos': [36, 50]}\n",
            "{'text': '68/2010/TT-BNNPTNT', 'pos': [91, 109]}\n",
            "{'text': '68/2010/TT-BNNPTNT', 'pos': [91, 109]}\n",
            "{'text': 'phường 1', 'pos': [23, 31]}\n",
            "{'text': 'phường 1', 'pos': [23, 31]}\n",
            "{'text': 'phường 1', 'pos': [23, 31]}\n",
            "{'text': 'U23', 'pos': [9, 12]}\n",
            "{'text': 'U23', 'pos': [9, 12]}\n",
            "{'text': 'U23', 'pos': [76, 79]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIEWzlcthZZF"
      },
      "source": [
        "for sentif in jtest_data_use:\n",
        "    if ('½' in sentif['entity_1']['text']) or ('²' in sentif['entity_1']['text']) or ('ï' in sentif['entity_1']['text']):\n",
        "        print(sentif['entity_1'])\n",
        "\n",
        "    if ('½' in sentif['entity_2']['text']) or ('²' in sentif['entity_2']['text']) or ('ï' in sentif['entity_2']['text']):\n",
        "        print(sentif['entity_2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQw7TkpyhZZF",
        "outputId": "dab120d0-79b5-487b-e643-408bde5a1166"
      },
      "source": [
        "# những entity bắt đầu hoặc kết thúc bằng kí tự không phải chữ cái hay chữ số\n",
        "tmpppent = []\n",
        "for sentif in jtest_data_use:\n",
        "    if (not sentif['entity_1']['text'][0].isalnum()) or (not sentif['entity_1']['text'][-1].isalnum()):\n",
        "        if sentif['entity_1']['text'] not in tmpppent:\n",
        "            print(sentif['entity_1'])\n",
        "            tmpppent.append(sentif['entity_1']['text'])\n",
        "\n",
        "            '''\n",
        "            if '.' == sentif['entity_1']['text'][0]:\n",
        "                print(sentif)\n",
        "            '''\n",
        "\n",
        "    if (not sentif['entity_2']['text'][0].isalnum()) or (not sentif['entity_2']['text'][-1].isalnum()):\n",
        "        if sentif['entity_2']['text'] not in tmpppent:\n",
        "            print(sentif['entity_2'])\n",
        "            tmpppent.append(sentif['entity_2']['text'])\n",
        "\n",
        "            '''\n",
        "            if '.' == sentif['entity_2']['text'][0]:\n",
        "                print(sentif)\n",
        "            '''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': '/TTXVN', 'pos': [9, 15]}\n",
            "{'text': '\"Nhà nước Hồi giáo', 'pos': [8, 26]}\n",
            "{'text': '(IS', 'pos': [29, 32]}\n",
            "{'text': '(Nga', 'pos': [49, 53]}\n",
            "{'text': '(Thổ Nhĩ Kỳ', 'pos': [47, 58]}\n",
            "{'text': '(CH Tatarstan', 'pos': [81, 94]}\n",
            "{'text': '(Mỹ', 'pos': [173, 176]}\n",
            "{'text': '(AFF', 'pos': [67, 71]}\n",
            "{'text': '(VFF', 'pos': [234, 238]}\n",
            "{'text': '/Vietnam+', 'pos': [17, 26]}\n",
            "{'text': '“Sắc”', 'pos': [25, 30]}\n",
            "{'text': '(Vietnam+', 'pos': [10, 19]}\n",
            "{'text': '(Vietlott', 'pos': [144, 153]}\n",
            "{'text': '(TP. HCM)', 'pos': [80, 89]}\n",
            "{'text': '(Vinataba', 'pos': [118, 127]}\n",
            "{'text': '“Nga', 'pos': [27, 31]}\n",
            "{'text': '\"Toni', 'pos': [59, 64]}\n",
            "{'text': '(Ajax', 'pos': [112, 117]}\n",
            "{'text': '(phường Tăng Nhơn Phú A', 'pos': [2, 25]}\n",
            "{'text': '(quận 9', 'pos': [108, 115]}\n",
            "{'text': '(CASC', 'pos': [91, 96]}\n",
            "{'text': '(CETC', 'pos': [38, 43]}\n",
            "{'text': '-Lào', 'pos': [52, 56]}\n",
            "{'text': '-Việt Nam', 'pos': [293, 302]}\n",
            "{'text': '–Lào', 'pos': [73, 77]}\n",
            "{'text': '-Trường Trung học cơ sở Saiphon', 'pos': [28, 59]}\n",
            "{'text': '/VOV', 'pos': [8, 12]}\n",
            "{'text': '-Vientiane', 'pos': [13, 23]}\n",
            "{'text': '-Miền Trung', 'pos': [7, 18]}\n",
            "{'text': '(quận Ngũ Hành Sơn', 'pos': [82, 100]}\n",
            "{'text': '(BV Đa khoa Hà Đông', 'pos': [51, 70]}\n",
            "{'text': '(xã Mường Lạn', 'pos': [54, 67]}\n",
            "{'text': '\\ufeffYanbi', 'pos': [0, 6]}\n",
            "{'text': '(NFSC', 'pos': [188, 193]}\n",
            "{'text': '(Interpol', 'pos': [105, 114]}\n",
            "{'text': '\\ufeffBaker Hughes', 'pos': [0, 13]}\n",
            "{'text': '(Tổ chức hoa hậu Mỹ', 'pos': [126, 145]}\n",
            "{'text': '/Time', 'pos': [14, 19]}\n",
            "{'text': '(số 58 Quán Sứ', 'pos': [113, 127]}\n",
            "{'text': '(Công ty CP xe khách số I Sơn La', 'pos': [169, 201]}\n",
            "{'text': '(Công ty TNHH Kim Cương', 'pos': [286, 309]}\n",
            "{'text': '(Công ty Vinasun Ánh Dương Việt Nam', 'pos': [70, 105]}\n",
            "{'text': '(Công ty CP Vận tải và Dịch vụ Liên Ninh', 'pos': [18, 58]}\n",
            "{'text': '(Xí nghiệp xe khách Nam Hà Nội', 'pos': [12, 42]}\n",
            "{'text': '(Công ty CP Vận tải Newway', 'pos': [16, 42]}\n",
            "{'text': '(Trung tâm Tân Đạt', 'pos': [16, 34]}\n",
            "{'text': '(Công ty CP Xe điện Hà Nội', 'pos': [15, 41]}\n",
            "{'text': '(Công ty TNHH Thương mại Thiên Phong', 'pos': [18, 54]}\n",
            "{'text': '(Công ty CP Tập đoàn Mai Linh', 'pos': [13, 42]}\n",
            "{'text': '(Công ty CP Thương mại du lịch và Vận chuyển khách Tình Nghĩa', 'pos': [16, 77]}\n",
            "{'text': '(Công ty CP Vận tải bộ Tân Cảng', 'pos': [14, 45]}\n",
            "{'text': '(Công ty TNHH Văn Minh', 'pos': [13, 35]}\n",
            "{'text': '(Công ty TNHH Phúc Xuyên', 'pos': [14, 38]}\n",
            "{'text': '(Công ty CP Thương mại và Du lịch Hạ Vinh', 'pos': [17, 58]}\n",
            "{'text': '(Công ty CP vận tải bộ Tân Cảng', 'pos': [15, 46]}\n",
            "{'text': '(Công ty CP Logistics Portserco', 'pos': [11, 42]}\n",
            "{'text': '(Công ty CP Cơ khí Sơn La', 'pos': [17, 42]}\n",
            "{'text': '(Công ty CP xe khách Sài Gòn', 'pos': [32, 60]}\n",
            "{'text': '(Công ty CP Giao nhận và Thương mại Quang Châu', 'pos': [17, 63]}\n",
            "{'text': '(HTX Vận tải Thống Nhất Quảng Ngãi', 'pos': [9, 43]}\n",
            "{'text': '(HTX Cơ khí vận tải và Dịch vụ Diên Hồng', 'pos': [18, 58]}\n",
            "{'text': '(Công ty CP Vận tải An Giang', 'pos': [13, 41]}\n",
            "{'text': '(Công ty TNHH Thương mại và Dịch vụ Hải Phượng', 'pos': [20, 66]}\n",
            "{'text': '(Công ty TNHH MTV Mai Linh Kon Tum', 'pos': [15, 49]}\n",
            "{'text': '(Công ty CP Du lịch Xuân Long', 'pos': [13, 42]}\n",
            "{'text': '(Công ty TNHH Minh Quốc', 'pos': [16, 39]}\n",
            "{'text': '(HTX Vận tải ô tô Thành Tuyên', 'pos': [13, 42]}\n",
            "{'text': '(Công ty CP vận tải ô tô Thanh Hóa', 'pos': [14, 48]}\n",
            "{'text': '(Công ty TNHH Mai Linh Thanh Hóa', 'pos': [13, 45]}\n",
            "{'text': '(Công ty TNHH Dịch vụ Taxi miền Bắc', 'pos': [12, 47]}\n",
            "{'text': '(Công ty CP xe khách Quảng Ninh', 'pos': [14, 45]}\n",
            "{'text': '(Công ty CP Vận tải ô tô Điện Biên', 'pos': [14, 48]}\n",
            "{'text': '(Xí nghiệp Xe buýt Cầu Bươu', 'pos': [13, 40]}\n",
            "{'text': '(Xí nghiệp Xe buýt Hà Nội', 'pos': [17, 42]}\n",
            "{'text': '(Xí nghiệp Xe buýt 10/10 Hà Nội', 'pos': [16, 47]}\n",
            "{'text': '(Công ty TNHH MTV Mai Linh Vĩnh Phúc', 'pos': [14, 50]}\n",
            "{'text': '(Công ty TNHH MTV Mai Linh Gia Lai', 'pos': [14, 48]}\n",
            "{'text': '(Công ty CP Thương mại du lịch Hà Lan', 'pos': [14, 51]}\n",
            "{'text': '(Công ty CP Giao nhận kho vận Hải Dương', 'pos': [18, 57]}\n",
            "{'text': '(Công ty CP Vận tải ô tô Quảng Ninh', 'pos': [19, 54]}\n",
            "{'text': '(Công ty TNHH Vận tải Thịnh Hưng', 'pos': [16, 48]}\n",
            "{'text': '(Xí nghiệp Xe buýt Yên Viên', 'pos': [13, 40]}\n",
            "{'text': '(HTX Vận tải hàng hóa hành khách và Du lịch Tân Việt', 'pos': [13, 65]}\n",
            "{'text': '(Hàn Quốc', 'pos': [61, 70]}\n",
            "{'text': '“Liên đoàn Taekwondo Thế giới', 'pos': [164, 193]}\n",
            "{'text': '“World Taekwondo', 'pos': [225, 241]}\n",
            "{'text': '(Taekwondo Thế giới', 'pos': [244, 263]}\n",
            "{'text': '(World Taekwondo Federation', 'pos': [139, 166]}\n",
            "{'text': '(huyện Tu Mơ Rông', 'pos': [57, 74]}\n",
            "{'text': '(huyện Đắk Glei', 'pos': [89, 104]}\n",
            "{'text': '\\ufeffHà Tĩnh', 'pos': [0, 8]}\n",
            "{'text': '(Đức Thọ', 'pos': [109, 117]}\n",
            "{'text': '(huyện Đức Thọ', 'pos': [115, 129]}\n",
            "{'text': '(Bộ Kế hoạch và Đầu tư', 'pos': [95, 117]}\n",
            "{'text': '(Bộ KH&ĐT', 'pos': [74, 83]}\n",
            "{'text': \"\\ufeff'Út Trọc\", 'pos': [0, 9]}\n",
            "{'text': \"'Vũ Nhôm\", 'pos': [13, 21]}\n",
            "{'text': \"'Út Trọc\", 'pos': [94, 102]}\n",
            "{'text': '“Khánh Trắng', 'pos': [136, 148]}\n",
            "{'text': '“Hiệp', 'pos': [152, 157]}\n",
            "{'text': '“Thuyết', 'pos': [167, 174]}\n",
            "{'text': '“Vũ nhôm', 'pos': [26, 34]}\n",
            "{'text': '“Út trọc', 'pos': [38, 46]}\n",
            "{'text': '\\ufeffTP.HCM', 'pos': [0, 7]}\n",
            "{'text': '(Nhóm Kẹo Dẻo', 'pos': [103, 116]}\n",
            "{'text': '\\ufeffKaty Perry', 'pos': [0, 11]}\n",
            "{'text': \"'Lê Doãn Hợp\", 'pos': [145, 157]}\n",
            "{'text': '(Đồng Nai', 'pos': [54, 63]}\n",
            "{'text': '“Lê Doãn Hợp', 'pos': [139, 151]}\n",
            "{'text': '\\ufeffKendall', 'pos': [0, 8]}\n",
            "{'text': '\"Kendall', 'pos': [0, 8]}\n",
            "{'text': '/Quang Hiếu', 'pos': [9, 20]}\n",
            "{'text': '(Bộ Kế hoạch - Đầu tư', 'pos': [63, 84]}\n",
            "{'text': '\\ufeffNga', 'pos': [0, 4]}\n",
            "{'text': '(Nghi Xuân', 'pos': [123, 133]}\n",
            "{'text': '(huyện Nghi Xuân', 'pos': [94, 110]}\n",
            "{'text': '\\ufeffBaek Hyun', 'pos': [0, 10]}\n",
            "{'text': '(EXO', 'pos': [11, 15]}\n",
            "{'text': '-Mỹ', 'pos': [90, 93]}\n",
            "{'text': '(NSA', 'pos': [146, 150]}\n",
            "{'text': 'Analog Devices Inc.', 'pos': [11, 30]}\n",
            "{'text': '(ADI', 'pos': [31, 35]}\n",
            "{'text': '(Girl’s Day', 'pos': [159, 170]}\n",
            "{'text': '\"Kiha & The Faces', 'pos': [130, 147]}\n",
            "{'text': '(Vitas', 'pos': [58, 64]}\n",
            "{'text': '\\ufeffNBA', 'pos': [0, 4]}\n",
            "{'text': '\\ufeffLana Del Rey', 'pos': [0, 13]}\n",
            "{'text': '‘Barcelona', 'pos': [25, 35]}\n",
            "{'text': '“Thánh Johan Cruyff', 'pos': [152, 171]}\n",
            "{'text': 'Lille O.S.C.', 'pos': [130, 142]}\n",
            "{'text': '(BTS', 'pos': [126, 130]}\n",
            "{'text': '(2PM', 'pos': [112, 116]}\n",
            "{'text': '(Hà Nội', 'pos': [37, 44]}\n",
            "{'text': 'Lê Quang D.', 'pos': [55, 66]}\n",
            "{'text': '(Hà Tĩnh', 'pos': [141, 149]}\n",
            "{'text': '(596-Tôn Đức Thắng', 'pos': [48, 66]}\n",
            "{'text': '\\ufeffMỹ', 'pos': [0, 3]}\n",
            "{'text': '(BSR', 'pos': [91, 95]}\n",
            "{'text': '\\ufeffBritney Spears', 'pos': [0, 15]}\n",
            "{'text': '\"Britney', 'pos': [103, 111]}\n",
            "{'text': '(Ryu Jun Yeol', 'pos': [136, 149]}\n",
            "{'text': '(Hyeri', 'pos': [193, 199]}\n",
            "{'text': '(Park Bo Gum', 'pos': [47, 59]}\n",
            "{'text': '(CECODES', 'pos': [82, 90]}\n",
            "{'text': '\\ufeffLeBron James', 'pos': [0, 13]}\n",
            "{'text': '(Bành Sơn', 'pos': [44, 53]}\n",
            "{'text': '\"Hoàng phần sơn', 'pos': [142, 157]}\n",
            "{'text': '(PVC', 'pos': [186, 190]}\n",
            "{'text': '(FED', 'pos': [209, 213]}\n",
            "{'text': '(huyện Tiền Hải', 'pos': [88, 103]}\n",
            "{'text': '(Quỳnh Lưu', 'pos': [19, 29]}\n",
            "{'text': 'N. \"đầu gấu”', 'pos': [106, 118]}\n",
            "{'text': 'Th.', 'pos': [76, 79]}\n",
            "{'text': '(Nghệ An', 'pos': [158, 166]}\n",
            "{'text': '(Anh', 'pos': [66, 70]}\n",
            "{'text': '(BOJ', 'pos': [46, 50]}\n",
            "{'text': '(ECB', 'pos': [101, 105]}\n",
            "{'text': '“công ty vận tải hàng không dân sự Viktor Bout', 'pos': [58, 104]}\n",
            "{'text': '(DEA', 'pos': [56, 60]}\n",
            "{'text': '(FARC', 'pos': [144, 149]}\n",
            "{'text': 'P.O.', 'pos': [35, 39]}\n",
            "{'text': '\"Zico', 'pos': [109, 114]}\n",
            "{'text': '(NYDFS', 'pos': [37, 43]}\n",
            "{'text': '\\ufeffBộ Xây dựng', 'pos': [0, 12]}\n",
            "{'text': '(Tôn Thất Trung', 'pos': [149, 164]}\n",
            "{'text': '(Bộ NN&PTNT', 'pos': [85, 96]}\n",
            "{'text': '(Viện Nước, Tưới tiêu và Môi trường', 'pos': [26, 61]}\n",
            "{'text': '(Bộ Tài nguyên và Môi trường', 'pos': [82, 110]}\n",
            "{'text': '(Bộ Y tế', 'pos': [63, 71]}\n",
            "{'text': '(UN Women', 'pos': [152, 161]}\n",
            "{'text': '(TTXVN', 'pos': [11, 17]}\n",
            "{'text': \"'Viện KSND tỉnh Nghệ An\", 'pos': [88, 111]}\n",
            "{'text': '“Viện KSND tỉnh Nghệ An', 'pos': [94, 117]}\n",
            "{'text': \"'Đường Tăng\", 'pos': [108, 119]}\n",
            "{'text': '\"Đường Tăng', 'pos': [19, 30]}\n",
            "{'text': '(Hà Tuệ', 'pos': [45, 52]}\n",
            "{'text': '(thị xã La Gi', 'pos': [30, 43]}\n",
            "{'text': '(Ifeng', 'pos': [46, 52]}\n",
            "{'text': '(Thiểm Tây', 'pos': [142, 152]}\n",
            "{'text': '\"Khương Bá Ước', 'pos': [43, 57]}\n",
            "{'text': '(Trung Quốc', 'pos': [51, 62]}\n",
            "{'text': '(Gia Cát Lượng', 'pos': [106, 120]}\n",
            "{'text': '“Chí Phèo', 'pos': [182, 191]}\n",
            "{'text': '(Hoa Kỳ', 'pos': [160, 167]}\n",
            "{'text': '(IRCJ', 'pos': [89, 94]}\n",
            "{'text': '/WB', 'pos': [69, 72]}\n",
            "{'text': '(EU', 'pos': [116, 119]}\n",
            "{'text': '(Liên đoàn Thể dục dụng cụ Mỹ', 'pos': [56, 85]}\n",
            "{'text': '(IEEE', 'pos': [82, 87]}\n",
            "{'text': 'Y Combinator Inc.', 'pos': [155, 172]}\n",
            "{'text': '(TWC', 'pos': [84, 88]}\n",
            "{'text': '(Tư Nghĩa', 'pos': [52, 61]}\n",
            "{'text': '(Bảy Đào', 'pos': [85, 93]}\n",
            "{'text': '\\ufeffThạch Bích', 'pos': [0, 11]}\n",
            "{'text': '(VAFIE', 'pos': [61, 67]}\n",
            "{'text': '\\ufeffViện Kinh tế Xây dựng', 'pos': [0, 22]}\n",
            "{'text': '/Một Thế Giới', 'pos': [15, 28]}\n",
            "{'text': '(Tiền Giang', 'pos': [68, 79]}\n",
            "{'text': '(Phú Tân', 'pos': [18, 26]}\n",
            "{'text': '(Châu Thành', 'pos': [80, 91]}\n",
            "{'text': '(Campuchia', 'pos': [147, 157]}\n",
            "{'text': '(cồn Bình Thủy', 'pos': [16, 30]}\n",
            "{'text': '\\ufeffBộ tư lệnh Quân khu 3', 'pos': [0, 22]}\n",
            "{'text': '(C45', 'pos': [46, 50]}\n",
            "{'text': '(tỉnh Đồng Nai', 'pos': [171, 185]}\n",
            "{'text': '(VnCert', 'pos': [86, 93]}\n",
            "{'text': '(KH&ĐT', 'pos': [216, 222]}\n",
            "{'text': '(ADB', 'pos': [103, 107]}\n",
            "{'text': '(EVN', 'pos': [115, 119]}\n",
            "{'text': '(Digiworld Corporation', 'pos': [95, 117]}\n",
            "{'text': '\\ufeffDana White', 'pos': [0, 11]}\n",
            "{'text': '“Conor', 'pos': [65, 71]}\n",
            "{'text': '(Leng Rjn', 'pos': [51, 60]}\n",
            "{'text': '(huyện Lắk', 'pos': [93, 103]}\n",
            "{'text': '(huyện Vĩnh Cữu', 'pos': [53, 68]}\n",
            "{'text': '(làng Kon K’tu', 'pos': [62, 76]}\n",
            "{'text': '(Lưu Diệc Phi', 'pos': [19, 32]}\n",
            "{'text': '(Song Seung Hun', 'pos': [115, 130]}\n",
            "{'text': '(Mạnh Giai', 'pos': [49, 59]}\n",
            "{'text': '(Giang Ngữ Thần', 'pos': [102, 117]}\n",
            "{'text': '(TP.HCM', 'pos': [103, 110]}\n",
            "{'text': '(ONEI', 'pos': [92, 97]}\n",
            "{'text': '(Cepal', 'pos': [192, 198]}\n",
            "{'text': \"Viện nghiên cứu và phát triển 'Burevestnik'\", 'pos': [136, 179]}\n",
            "{'text': 'Viện nghiên cứu và phát triển “Burevestnik”', 'pos': [88, 131]}\n",
            "{'text': '(NYT', 'pos': [141, 145]}\n",
            "{'text': '(Quy Nhơn', 'pos': [24, 33]}\n",
            "{'text': '-BNV', 'pos': [111, 115]}\n",
            "{'text': '(Eric Clapton', 'pos': [201, 214]}\n",
            "{'text': '(Yên Dũng', 'pos': [31, 40]}\n",
            "{'text': '(thôn Bắc Thành', 'pos': [36, 51]}\n",
            "{'text': '(huyện Ứng Hòa', 'pos': [293, 307]}\n",
            "{'text': '(xã Hợp Đồng', 'pos': [111, 123]}\n",
            "{'text': '(Công an tỉnh Nam Định', 'pos': [96, 118]}\n",
            "{'text': '-TPHCM', 'pos': [13, 19]}\n",
            "{'text': '\\ufeffAT&T', 'pos': [0, 5]}\n",
            "{'text': '“Ma Bell', 'pos': [92, 100]}\n",
            "{'text': '\\ufeffVũ Dino', 'pos': [0, 8]}\n",
            "{'text': '/VOV1', 'pos': [12, 17]}\n",
            "{'text': '\\ufeffHoa Sen Group', 'pos': [0, 14]}\n",
            "{'text': '(Hoa Sen Group', 'pos': [60, 74]}\n",
            "{'text': '\\ufeffToyota', 'pos': [0, 7]}\n",
            "{'text': '(Các Tiểu vương quốc Ả Rập Thống nhất', 'pos': [76, 113]}\n",
            "{'text': '(Bách Long', 'pos': [68, 78]}\n",
            "{'text': '(Ấn Độ', 'pos': [114, 120]}\n",
            "{'text': '(Thái Lan', 'pos': [99, 108]}\n",
            "{'text': '(VCCI', 'pos': [208, 213]}\n",
            "{'text': '(TAND tỉnh Bình Phước', 'pos': [26, 47]}\n",
            "{'text': \"\\ufeff'Federer\", 'pos': [0, 9]}\n",
            "{'text': '(huyện Cần Giờ', 'pos': [139, 153]}\n",
            "{'text': '(Sóc Trăng', 'pos': [59, 69]}\n",
            "{'text': '(xã An Thạnh Đông', 'pos': [40, 57]}\n",
            "{'text': '(Võ Thanh Quang', 'pos': [24, 39]}\n",
            "{'text': '(Trà Vinh', 'pos': [71, 80]}\n",
            "{'text': '(Lê Minh Đương', 'pos': [14, 28]}\n",
            "{'text': '\\ufeffEVNNPT', 'pos': [0, 7]}\n",
            "{'text': '(EVNNPT', 'pos': [38, 45]}\n",
            "{'text': '-BTC', 'pos': [282, 286]}\n",
            "{'text': '(Trần Tế Xương', 'pos': [17, 31]}\n",
            "{'text': '(Nam Định', 'pos': [53, 62]}\n",
            "{'text': '(Lữ', 'pos': [34, 37]}\n",
            "{'text': '(Đào Uyên Minh', 'pos': [24, 38]}\n",
            "{'text': '(Bắc Ninh', 'pos': [48, 57]}\n",
            "{'text': '\\ufeffVũ Duy Khánh', 'pos': [0, 13]}\n",
            "{'text': '(Đức', 'pos': [50, 54]}\n",
            "{'text': '(Đan Mạch', 'pos': [46, 55]}\n",
            "{'text': '(Hà Lan', 'pos': [81, 88]}\n",
            "{'text': '(GOT7', 'pos': [149, 154]}\n",
            "{'text': '(Báo Nhân Dân', 'pos': [184, 197]}\n",
            "{'text': '-Quốc Khánh', 'pos': [94, 105]}\n",
            "{'text': '(Báo Sài Gòn Giải Phóng', 'pos': [106, 129]}\n",
            "{'text': '-Nhật Nam', 'pos': [113, 122]}\n",
            "{'text': '(Báo điện tử Chính phủ', 'pos': [123, 145]}\n",
            "{'text': '(Đài Tiếng nói Việt Nam', 'pos': [87, 110]}\n",
            "{'text': '-Mỹ Hạnh', 'pos': [80, 88]}\n",
            "{'text': '-Hồng Quân', 'pos': [89, 99]}\n",
            "{'text': '-Tuấn Tú', 'pos': [100, 108]}\n",
            "{'text': '(Trung tâm Truyền hình Nhân Dân', 'pos': [109, 140]}\n",
            "{'text': '(Thông tấn xã Việt Nam', 'pos': [111, 133]}\n",
            "{'text': '(Tạp chí Nội chính', 'pos': [111, 129]}\n",
            "{'text': '-Nguyễn Minh Phong', 'pos': [139, 157]}\n",
            "{'text': '-Hoàng Gia Minh', 'pos': [158, 173]}\n",
            "{'text': '-Hồ Quang Phương', 'pos': [174, 190]}\n",
            "{'text': '(Báo Quân đội nhân dân', 'pos': [191, 213]}\n",
            "{'text': '(Báo Cần Thơ', 'pos': [103, 115]}\n",
            "{'text': '(Báo Bạc Liêu', 'pos': [66, 79]}\n",
            "{'text': '\"Xã Cổ Đô', 'pos': [12, 21]}\n",
            "{'text': '-Thanh Nhất', 'pos': [25, 36]}\n",
            "{'text': '-Đức Lựu', 'pos': [37, 45]}\n",
            "{'text': '-Sỹ Toàn', 'pos': [46, 54]}\n",
            "{'text': '-Thùy Dương', 'pos': [55, 66]}\n",
            "{'text': '-Phúc Lâm', 'pos': [67, 76]}\n",
            "{'text': '(Đài Phát thanh-Truyền hình Quảng Nam', 'pos': [77, 114]}\n",
            "{'text': '-Anh Tuấn', 'pos': [116, 125]}\n",
            "{'text': '-Đức Hiệp', 'pos': [126, 135]}\n",
            "{'text': '-Đức Phong', 'pos': [136, 146]}\n",
            "{'text': '-Xuân Hoàng', 'pos': [147, 158]}\n",
            "{'text': '(Đài Phát thanh-Truyền hình Quảng Ninh', 'pos': [159, 197]}\n",
            "{'text': '(Tạp chí Cộng sản', 'pos': [150, 167]}\n",
            "{'text': '(Tạp chí Kiểm tra', 'pos': [98, 115]}\n",
            "{'text': '-Thái Sơn', 'pos': [79, 88]}\n",
            "{'text': '-Ái Châu', 'pos': [89, 97]}\n",
            "{'text': '-Ngọc Minh', 'pos': [98, 108]}\n",
            "{'text': '(Báo Thanh Niên', 'pos': [109, 124]}\n",
            "{'text': '-Vũ Ngọc Hà', 'pos': [100, 111]}\n",
            "{'text': '-Nguyễn Thành Tâm', 'pos': [112, 129]}\n",
            "{'text': '(Báo Hà Nội Mới', 'pos': [130, 145]}\n",
            "{'text': '-Nguyễn Thị Lý', 'pos': [90, 104]}\n",
            "{'text': '-Nguyễn Phương Triều', 'pos': [105, 125]}\n",
            "{'text': '(Báo Quảng Ngãi', 'pos': [126, 141]}\n",
            "{'text': '-Minh Châm', 'pos': [173, 183]}\n",
            "{'text': '(Báo Hải Phòng', 'pos': [184, 198]}\n",
            "{'text': '(Báo Công an nhân dân', 'pos': [94, 115]}\n",
            "{'text': '(Báo điện tử VietnamPlus', 'pos': [102, 126]}\n",
            "{'text': '-Thu Hà', 'pos': [85, 92]}\n",
            "{'text': '(Báo điện tử Đảng Cộng sản Việt Nam', 'pos': [93, 128]}\n",
            "{'text': '(Báo An Giang', 'pos': [75, 88]}\n",
            "{'text': '-Đình Dương', 'pos': [46, 57]}\n",
            "{'text': '-Hữu Thành', 'pos': [58, 68]}\n",
            "{'text': '-Đình Lộc', 'pos': [69, 78]}\n",
            "{'text': '(Chi hội Nhà báo VTV9', 'pos': [79, 100]}\n",
            "{'text': '-Song Lâm', 'pos': [95, 104]}\n",
            "{'text': '-Hoàng Bạc', 'pos': [105, 115]}\n",
            "{'text': '-Thanh Miền', 'pos': [85, 96]}\n",
            "{'text': '(Báo Yên Bái', 'pos': [97, 109]}\n",
            "{'text': '(Đoàn luật sư TP Hà Nội', 'pos': [56, 79]}\n",
            "{'text': '(huyện Hóc Môn', 'pos': [40, 54]}\n",
            "{'text': '(TP Hà Nội', 'pos': [94, 104]}\n",
            "{'text': '(Phường Dịch Vọng Hậu', 'pos': [66, 87]}\n",
            "{'text': '(Nam', 'pos': [24, 28]}\n",
            "{'text': '(Kitô giáo', 'pos': [106, 116]}\n",
            "{'text': '“John Fitzgerald Kennedy', 'pos': [133, 157]}\n",
            "{'text': '(AFP', 'pos': [31, 35]}\n",
            "{'text': '\\ufeffTừ Hoảng', 'pos': [0, 9]}\n",
            "{'text': '(Vũ', 'pos': [19, 22]}\n",
            "{'text': '\"Từ Hoảng', 'pos': [56, 65]}\n",
            "{'text': '(Tào Tháo', 'pos': [112, 121]}\n",
            "{'text': '\"Từ', 'pos': [40, 43]}\n",
            "{'text': '(Từ Hoảng', 'pos': [123, 132]}\n",
            "{'text': '\\ufeffLâm Vinh Hải', 'pos': [0, 13]}\n",
            "{'text': '“Hải', 'pos': [0, 4]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZpC3s97hZZF",
        "outputId": "5a99aca3-95fd-481f-83f2-e4acc87aeae2"
      },
      "source": [
        "# những entity bắt đầu hoặc kết thúc bằng kí tự không phải chữ cái hay chữ số\n",
        "# tuy nhiên những entity bắt đầu hoặc kết thúc bằng kí tự '.' hoặc '+' thì không phải là lỗi\n",
        "tmpppent = []\n",
        "for sentif in jtest_data_use:\n",
        "    if (sentif['entity_1']['text'][0] == '+') or (sentif['entity_1']['text'][-1] == '+') \\\n",
        "    or (sentif['entity_1']['text'][0] == '.') or (sentif['entity_1']['text'][-1] == '.'):\n",
        "        if sentif['entity_1']['text'] not in tmpppent:\n",
        "            print(sentif['entity_1'])\n",
        "            tmpppent.append(sentif['entity_1']['text'])\n",
        "\n",
        "            '''\n",
        "            if '.' == sentif['entity_1']['text'][0]:\n",
        "                print(sentif)\n",
        "            '''\n",
        "\n",
        "    if (sentif['entity_2']['text'][0] == '+') or (sentif['entity_2']['text'][-1] == '.') \\\n",
        "    or (sentif['entity_2']['text'][0] == '.') or (sentif['entity_2']['text'][-1] == '+'):\n",
        "        if sentif['entity_2']['text'] not in tmpppent:\n",
        "            print(sentif['entity_2'])\n",
        "            tmpppent.append(sentif['entity_2']['text'])\n",
        "\n",
        "            '''\n",
        "            if '.' == sentif['entity_2']['text'][0]:\n",
        "                print(sentif)\n",
        "            '''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': '/Vietnam+', 'pos': [17, 26]}\n",
            "{'text': '(Vietnam+', 'pos': [10, 19]}\n",
            "{'text': 'Analog Devices Inc.', 'pos': [11, 30]}\n",
            "{'text': 'Lille O.S.C.', 'pos': [130, 142]}\n",
            "{'text': 'Lê Quang D.', 'pos': [55, 66]}\n",
            "{'text': 'Th.', 'pos': [76, 79]}\n",
            "{'text': 'P.O.', 'pos': [35, 39]}\n",
            "{'text': 'Y Combinator Inc.', 'pos': [155, 172]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzMKF60qhZZF"
      },
      "source": [
        "# lỗi\n",
        "tmpppent = []\n",
        "for sentif in jtest_data_use:\n",
        "    if (sentif['entity_1']['text'][0] == '\\xa0') or (sentif['entity_1']['text'][-1] == '\\xa0'):\n",
        "        if sentif['entity_1']['text'] not in tmpppent:\n",
        "            print(sentif['entity_1'])\n",
        "            tmpppent.append(sentif['entity_1']['text'])\n",
        "\n",
        "            '''\n",
        "            if '.' == sentif['entity_1']['text'][0]:\n",
        "                print(sentif)\n",
        "            '''\n",
        "\n",
        "    if (sentif['entity_2']['text'][0] == '\\xa0') or (sentif['entity_2']['text'][-1] == '\\xa0'):\n",
        "        if sentif['entity_2']['text'] not in tmpppent:\n",
        "            print(sentif['entity_2'])\n",
        "            tmpppent.append(sentif['entity_2']['text'])\n",
        "\n",
        "            '''\n",
        "            if '.' == sentif['entity_2']['text'][0]:\n",
        "                print(sentif)\n",
        "            '''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcokotVKhZZF",
        "outputId": "70be45dc-9776-47e1-9d3c-07954bd6e89b"
      },
      "source": [
        "# những entity bắt đầu hoặc kết thúc bằng kí tự không phải chữ cái hay chữ số\n",
        "tmpppent = []\n",
        "for sentif in jtest_data_use:\n",
        "\n",
        "    if sentif['entity_1']['text'] not in tmpppent:\n",
        "        for c in sentif['entity_1']['text'][1:-1]:\n",
        "            if (not c.isalnum()) and (c != ' ') and (c != '.'):\n",
        "                print(sentif['entity_1'])\n",
        "        \n",
        "        tmpppent.append(sentif['entity_1']['text'])\n",
        "\n",
        "        '''\n",
        "        if '- Huế' in sentif['entity_1']['text']:\n",
        "            print(sentif)\n",
        "        '''\n",
        "    \n",
        "    \n",
        "    if sentif['entity_2']['text'] not in tmpppent:\n",
        "        for c in sentif['entity_2']['text'][1:-1]:\n",
        "            if (not c.isalnum()) and (c != ' ') and (c != '.'):\n",
        "                print(sentif['entity_2'])\n",
        "        \n",
        "        tmpppent.append(sentif['entity_2']['text'])\n",
        "\n",
        "        '''\n",
        "        if '- Huế' in sentif['entity_2']['text']:\n",
        "            print(sentif)\n",
        "        '''\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': 'Jordan Vogt-Roberts', 'pos': [52, 71]}\n",
            "{'text': 'Manuela Martins-Green', 'pos': [115, 136]}\n",
            "{'text': 'Martins-Green', 'pos': [31, 44]}\n",
            "{'text': 'Park Chan-ju', 'pos': [65, 77]}\n",
            "{'text': 'Hà Nội T&T', 'pos': [140, 150]}\n",
            "{'text': 'Tập đoàn chế tạo máy bay không người lái (UAV) Tengoen', 'pos': [0, 54]}\n",
            "{'text': 'Tập đoàn chế tạo máy bay không người lái (UAV) Tengoen', 'pos': [0, 54]}\n",
            "{'text': 'Mặt trận Al-Nusra', 'pos': [91, 108]}\n",
            "{'text': 'sĩ\\xa0René Gruau', 'pos': [117, 130]}\n",
            "{'text': 'Tổng Liên đoàn Lao động (LĐLĐ) Việt Nam', 'pos': [71, 110]}\n",
            "{'text': 'Tổng Liên đoàn Lao động (LĐLĐ) Việt Nam', 'pos': [71, 110]}\n",
            "{'text': 'của\\xa0Ban Thường vụ Thành uỷ Đà Nẵng', 'pos': [151, 185]}\n",
            "{'text': '2015-2020,\\xa0ông\\xa0Nguyễn Xuân Anh', 'pos': [195, 225]}\n",
            "{'text': '2015-2020,\\xa0ông\\xa0Nguyễn Xuân Anh', 'pos': [195, 225]}\n",
            "{'text': '2015-2020,\\xa0ông\\xa0Nguyễn Xuân Anh', 'pos': [195, 225]}\n",
            "{'text': '2015-2020,\\xa0ông\\xa0Nguyễn Xuân Anh', 'pos': [195, 225]}\n",
            "{'text': 'và\\xa0ông\\xa0Huỳnh Đức Thơ', 'pos': [226, 246]}\n",
            "{'text': 'và\\xa0ông\\xa0Huỳnh Đức Thơ', 'pos': [226, 246]}\n",
            "{'text': 'PV/VOV', 'pos': [0, 6]}\n",
            "{'text': 'As-Suhnah', 'pos': [225, 234]}\n",
            "{'text': 'tại\\xa0Trung tâm Hội nghị và Triển lãm Ariyana', 'pos': [38, 81]}\n",
            "{'text': 'gái\\xa0Châu Tuyết Vân', 'pos': [39, 57]}\n",
            "{'text': 'al-Nusra Front', 'pos': [178, 192]}\n",
            "{'text': 'Deir ez-Zor', 'pos': [143, 154]}\n",
            "{'text': 'Cowen & Co', 'pos': [77, 87]}\n",
            "{'text': 'Simmons & Co', 'pos': [19, 31]}\n",
            "{'text': '(Xí nghiệp Xe buýt 10/10 Hà Nội', 'pos': [16, 47]}\n",
            "{'text': 'Hanoi Kids’ Art Center', 'pos': [428, 450]}\n",
            "{'text': 'Sở GT-VT tỉnh Kon Tum', 'pos': [111, 132]}\n",
            "{'text': 'Sở GT-VT Kon Tum', 'pos': [34, 50]}\n",
            "{'text': '(Bộ KH&ĐT', 'pos': [74, 83]}\n",
            "{'text': \"\\ufeff'Út Trọc\", 'pos': [0, 9]}\n",
            "{'text': 'Công đoàn Tổng Cty LICOGI - CTCP', 'pos': [9, 41]}\n",
            "{'text': 'Công đoàn Tổng Cty LICOGI -CTCP', 'pos': [76, 107]}\n",
            "{'text': '(Bộ Kế hoạch - Đầu tư', 'pos': [63, 84]}\n",
            "{'text': 'Angel & Royal', 'pos': [0, 13]}\n",
            "{'text': '01/2017/TT-TANDTC', 'pos': [10, 27]}\n",
            "{'text': '01/2017/TT-TANDTC', 'pos': [10, 27]}\n",
            "{'text': '01/2017/TT-TANDTC', 'pos': [10, 27]}\n",
            "{'text': 'Trường THCS-THPT Đức Trí', 'pos': [43, 67]}\n",
            "{'text': 'khoa Toán thống kê, Đại học Kinh tế TP.HCM', 'pos': [43, 85]}\n",
            "{'text': 'THCS - THPT Đức Trí', 'pos': [166, 185]}\n",
            "{'text': 'Phòng TN&MT huyện Nghi Xuân', 'pos': [120, 147]}\n",
            "{'text': 'Petropavlovsk-Kamchatsky', 'pos': [47, 71]}\n",
            "{'text': '(Girl’s Day', 'pos': [159, 170]}\n",
            "{'text': '\"Kiha & The Faces', 'pos': [130, 147]}\n",
            "{'text': 'T-ara', 'pos': [57, 62]}\n",
            "{'text': 'Karl-Anthony Towns', 'pos': [67, 85]}\n",
            "{'text': 'AT&T Center', 'pos': [10, 21]}\n",
            "{'text': 'AT&T', 'pos': [65, 69]}\n",
            "{'text': 'Sơn Tùng M-TP', 'pos': [27, 40]}\n",
            "{'text': 'Bà Rịa - Vũng Tàu', 'pos': [54, 71]}\n",
            "{'text': '(596-Tôn Đức Thắng', 'pos': [48, 66]}\n",
            "{'text': 'Las Vegas Review-Journal', 'pos': [67, 91]}\n",
            "{'text': \"del'Abbaye\", 'pos': [42, 52]}\n",
            "{'text': 'Ả-rập Xê-út', 'pos': [101, 112]}\n",
            "{'text': 'Ả-rập Xê-út', 'pos': [101, 112]}\n",
            "{'text': 'Jean-Claude Duvalier', 'pos': [29, 49]}\n",
            "{'text': 'Port-au-Prince', 'pos': [64, 78]}\n",
            "{'text': 'Port-au-Prince', 'pos': [64, 78]}\n",
            "{'text': 'Lin ‘Martin’ Jarvis', 'pos': [20, 39]}\n",
            "{'text': 'Lin ‘Martin’ Jarvis', 'pos': [20, 39]}\n",
            "{'text': 'Elizabeth Hartley-Brewer', 'pos': [3, 27]}\n",
            "{'text': 'N. \"đầu gấu”', 'pos': [106, 118]}\n",
            "{'text': 'Khoa Luật, Trường Đại học Vinh', 'pos': [127, 157]}\n",
            "{'text': 'B-Bomb', 'pos': [87, 93]}\n",
            "{'text': 'U-Kwon', 'pos': [96, 102]}\n",
            "{'text': 'Vụ Theo dõi các vụ án - Ban Nội chính Trung ương', 'pos': [63, 111]}\n",
            "{'text': 'Vụ Pháp luật - Ban Nội chính Trung ương', 'pos': [146, 185]}\n",
            "{'text': 'Vụ theo dõi và xử lý các vụ án, Ban Nội chính Trung ương', 'pos': [50, 106]}\n",
            "{'text': 'Vụ Pháp luật, Ban Nội chính Trung ương', 'pos': [211, 249]}\n",
            "{'text': '(Bộ NN&PTNT', 'pos': [85, 96]}\n",
            "{'text': '(Viện Nước, Tưới tiêu và Môi trường', 'pos': [26, 61]}\n",
            "{'text': 'Bộ NN&PTNT', 'pos': [64, 74]}\n",
            "{'text': 'Cục Phòng, chống HIV/AIDS', 'pos': [37, 62]}\n",
            "{'text': 'Cục Phòng, chống HIV/AIDS', 'pos': [37, 62]}\n",
            "{'text': 'Jan-Lennard Struff', 'pos': [29, 47]}\n",
            "{'text': 'Trường THCS&THPT Võ Văn Kiệt', 'pos': [56, 84]}\n",
            "{'text': 'Sở GD&ĐT Phú Yên', 'pos': [133, 149]}\n",
            "{'text': 'báo Giáo dục & Thời đại', 'pos': [171, 194]}\n",
            "{'text': 'Bộ Giáo dục & Đào tạo', 'pos': [301, 322]}\n",
            "{'text': '04/2017/TT-BKHĐT', 'pos': [81, 97]}\n",
            "{'text': '04/2017/TT-BKHĐT', 'pos': [81, 97]}\n",
            "{'text': '04/2017/TT-BKHĐT', 'pos': [81, 97]}\n",
            "{'text': 'Jermaine O’Neal', 'pos': [129, 144]}\n",
            "{'text': 'O’Neal', 'pos': [68, 74]}\n",
            "{'text': 'Sở KH-ĐT Quảng Nam', 'pos': [295, 313]}\n",
            "{'text': 'Viện sức khỏe tâm thần, BV Bạch Mai', 'pos': [56, 91]}\n",
            "{'text': 'đặc khu kinh tế Savan-Seno', 'pos': [8, 34]}\n",
            "{'text': \"Hãng xếp hạng tín nhiệm Standard & Poor's\", 'pos': [6, 47]}\n",
            "{'text': \"Hãng xếp hạng tín nhiệm Standard & Poor's\", 'pos': [6, 47]}\n",
            "{'text': 'S&P', 'pos': [50, 53]}\n",
            "{'text': 'Jay-Z', 'pos': [69, 74]}\n",
            "{'text': '(KH&ĐT', 'pos': [216, 222]}\n",
            "{'text': 'Cục Kế hoạch và Đầu tư, Bộ Quốc phòng', 'pos': [26, 63]}\n",
            "{'text': 'Bộ KH&ĐT', 'pos': [73, 81]}\n",
            "{'text': 'Công ty CP XNK & TM Lychee', 'pos': [122, 148]}\n",
            "{'text': 'xã Đắk-rơ-wa', 'pos': [30, 42]}\n",
            "{'text': 'xã Đắk-rơ-wa', 'pos': [30, 42]}\n",
            "{'text': 'Kon K’tu', 'pos': [98, 106]}\n",
            "{'text': 'H’rung', 'pos': [70, 76]}\n",
            "{'text': 'K’Yếu', 'pos': [156, 161]}\n",
            "{'text': '(làng Kon K’tu', 'pos': [62, 76]}\n",
            "{'text': 'Moody’s', 'pos': [34, 41]}\n",
            "{'text': 'Carmelo Mesa-Lago', 'pos': [16, 33]}\n",
            "{'text': \"Viện nghiên cứu và phát triển 'Burevestnik'\", 'pos': [136, 179]}\n",
            "{'text': 'Viện nghiên cứu và phát triển “Burevestnik”', 'pos': [88, 131]}\n",
            "{'text': '163/2013/TTLT-BTC', 'pos': [93, 110]}\n",
            "{'text': '163/2013/TTLT-BTC', 'pos': [93, 110]}\n",
            "{'text': '163/2013/TTLT-BTC', 'pos': [93, 110]}\n",
            "{'text': '\\ufeffAT&T', 'pos': [0, 5]}\n",
            "{'text': 'AT&T Crop', 'pos': [0, 9]}\n",
            "{'text': 'T – Mobile', 'pos': [159, 169]}\n",
            "{'text': 'Ủy ban Tài chính - Ngân sách', 'pos': [98, 126]}\n",
            "{'text': \"\\ufeff'Federer\", 'pos': [0, 9]}\n",
            "{'text': '07/2015/TT-BKHĐT', 'pos': [265, 281]}\n",
            "{'text': '07/2015/TT-BKHĐT', 'pos': [265, 281]}\n",
            "{'text': '07/2015/TT-BKHĐT', 'pos': [265, 281]}\n",
            "{'text': 'E’Twaun Moore', 'pos': [116, 129]}\n",
            "{'text': \"Aubrey O'Day\", 'pos': [0, 12]}\n",
            "{'text': 'King’s Garden', 'pos': [86, 99]}\n",
            "{'text': 'M&H Entertainment', 'pos': [37, 54]}\n",
            "{'text': '(Đài Phát thanh-Truyền hình Quảng Nam', 'pos': [77, 114]}\n",
            "{'text': '(Đài Phát thanh-Truyền hình Quảng Ninh', 'pos': [159, 197]}\n",
            "{'text': 'đường cao tốc Hà Nội - Hải Phòng', 'pos': [670, 702]}\n",
            "{'text': '50/2016/TT-BYT', 'pos': [36, 50]}\n",
            "{'text': '50/2016/TT-BYT', 'pos': [36, 50]}\n",
            "{'text': '50/2016/TT-BYT', 'pos': [36, 50]}\n",
            "{'text': '68/2010/TT-BNNPTNT', 'pos': [91, 109]}\n",
            "{'text': '68/2010/TT-BNNPTNT', 'pos': [91, 109]}\n",
            "{'text': '68/2010/TT-BNNPTNT', 'pos': [91, 109]}\n",
            "{'text': 'Bộ NN-PTNT', 'pos': [153, 163]}\n",
            "{'text': 'Bộ Môi trường, Thực Phẩm và Nông Nghiệp', 'pos': [49, 88]}\n",
            "{'text': 'D. Finney-Smith', 'pos': [32, 47]}\n",
            "{'text': \"Kyle O'Quinn\", 'pos': [97, 109]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpCdj0TVhZZF"
      },
      "source": [
        "#### fix start end of entity in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3v80f4whZZF",
        "outputId": "2730e296-8e12-4c5a-9c4e-202ba08e3e1e"
      },
      "source": [
        "jtest_data_v2 = copy.deepcopy(fix_start_end_of_entity(jtest_data_use))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "original:       {'text': '/TTXVN', 'pos': [9, 15]}\n",
            "new_entity:     {'text': 'TTXVN', 'pos': [10, 15]}\n",
            "\n",
            "original:       {'text': '\"Nhà nước Hồi giáo', 'pos': [8, 26]}\n",
            "new_entity:     {'text': 'Nhà nước Hồi giáo', 'pos': [9, 26]}\n",
            "\n",
            "original:       {'text': '(IS', 'pos': [29, 32]}\n",
            "new_entity:     {'text': 'IS', 'pos': [30, 32]}\n",
            "\n",
            "original:       {'text': '(Nga', 'pos': [49, 53]}\n",
            "new_entity:     {'text': 'Nga', 'pos': [50, 53]}\n",
            "\n",
            "original:       {'text': '(Thổ Nhĩ Kỳ', 'pos': [47, 58]}\n",
            "new_entity:     {'text': 'Thổ Nhĩ Kỳ', 'pos': [48, 58]}\n",
            "\n",
            "original:       {'text': '(CH Tatarstan', 'pos': [81, 94]}\n",
            "new_entity:     {'text': 'CH Tatarstan', 'pos': [82, 94]}\n",
            "\n",
            "original:       {'text': '(Mỹ', 'pos': [173, 176]}\n",
            "new_entity:     {'text': 'Mỹ', 'pos': [174, 176]}\n",
            "\n",
            "original:       {'text': '(AFF', 'pos': [67, 71]}\n",
            "new_entity:     {'text': 'AFF', 'pos': [68, 71]}\n",
            "\n",
            "original:       {'text': '(VFF', 'pos': [234, 238]}\n",
            "new_entity:     {'text': 'VFF', 'pos': [235, 238]}\n",
            "\n",
            "original:       {'text': '/Vietnam+', 'pos': [17, 26]}\n",
            "new_entity:     {'text': 'Vietnam+', 'pos': [18, 26]}\n",
            "\n",
            "original:       {'text': '“Sắc”', 'pos': [25, 30]}\n",
            "new_entity:     {'text': 'Sắc', 'pos': [26, 29]}\n",
            "\n",
            "original:       {'text': '(Vietnam+', 'pos': [10, 19]}\n",
            "new_entity:     {'text': 'Vietnam+', 'pos': [11, 19]}\n",
            "\n",
            "original:       {'text': '(Vietlott', 'pos': [144, 153]}\n",
            "new_entity:     {'text': 'Vietlott', 'pos': [145, 153]}\n",
            "\n",
            "original:       {'text': '(TP. HCM)', 'pos': [80, 89]}\n",
            "new_entity:     {'text': 'TP. HCM', 'pos': [81, 88]}\n",
            "\n",
            "original:       {'text': '(Vinataba', 'pos': [118, 127]}\n",
            "new_entity:     {'text': 'Vinataba', 'pos': [119, 127]}\n",
            "\n",
            "original:       {'text': '“Nga', 'pos': [27, 31]}\n",
            "new_entity:     {'text': 'Nga', 'pos': [28, 31]}\n",
            "\n",
            "original:       {'text': '\"Toni', 'pos': [59, 64]}\n",
            "new_entity:     {'text': 'Toni', 'pos': [60, 64]}\n",
            "\n",
            "original:       {'text': '(Ajax', 'pos': [112, 117]}\n",
            "new_entity:     {'text': 'Ajax', 'pos': [113, 117]}\n",
            "\n",
            "original:       {'text': '(phường Tăng Nhơn Phú A', 'pos': [2, 25]}\n",
            "new_entity:     {'text': 'phường Tăng Nhơn Phú A', 'pos': [3, 25]}\n",
            "\n",
            "original:       {'text': '(quận 9', 'pos': [108, 115]}\n",
            "new_entity:     {'text': 'quận 9', 'pos': [109, 115]}\n",
            "\n",
            "original:       {'text': '(CASC', 'pos': [91, 96]}\n",
            "new_entity:     {'text': 'CASC', 'pos': [92, 96]}\n",
            "\n",
            "original:       {'text': '(CETC', 'pos': [38, 43]}\n",
            "new_entity:     {'text': 'CETC', 'pos': [39, 43]}\n",
            "\n",
            "original:       {'text': '-Lào', 'pos': [52, 56]}\n",
            "new_entity:     {'text': 'Lào', 'pos': [53, 56]}\n",
            "\n",
            "original:       {'text': '-Việt Nam', 'pos': [293, 302]}\n",
            "new_entity:     {'text': 'Việt Nam', 'pos': [294, 302]}\n",
            "\n",
            "original:       {'text': '–Lào', 'pos': [73, 77]}\n",
            "new_entity:     {'text': 'Lào', 'pos': [74, 77]}\n",
            "\n",
            "original:       {'text': '-Trường Trung học cơ sở Saiphon', 'pos': [28, 59]}\n",
            "new_entity:     {'text': 'Trường Trung học cơ sở Saiphon', 'pos': [29, 59]}\n",
            "\n",
            "original:       {'text': '/VOV', 'pos': [8, 12]}\n",
            "new_entity:     {'text': 'VOV', 'pos': [9, 12]}\n",
            "\n",
            "original:       {'text': '-Vientiane', 'pos': [13, 23]}\n",
            "new_entity:     {'text': 'Vientiane', 'pos': [14, 23]}\n",
            "\n",
            "original:       {'text': '-Miền Trung', 'pos': [7, 18]}\n",
            "new_entity:     {'text': 'Miền Trung', 'pos': [8, 18]}\n",
            "\n",
            "original:       {'text': '(quận Ngũ Hành Sơn', 'pos': [82, 100]}\n",
            "new_entity:     {'text': 'quận Ngũ Hành Sơn', 'pos': [83, 100]}\n",
            "\n",
            "original:       {'text': '(BV Đa khoa Hà Đông', 'pos': [51, 70]}\n",
            "new_entity:     {'text': 'BV Đa khoa Hà Đông', 'pos': [52, 70]}\n",
            "\n",
            "original:       {'text': '(xã Mường Lạn', 'pos': [54, 67]}\n",
            "new_entity:     {'text': 'xã Mường Lạn', 'pos': [55, 67]}\n",
            "\n",
            "original:       {'text': '\\ufeffYanbi', 'pos': [0, 6]}\n",
            "new_entity:     {'text': 'Yanbi', 'pos': [1, 6]}\n",
            "\n",
            "original:       {'text': '(NFSC', 'pos': [188, 193]}\n",
            "new_entity:     {'text': 'NFSC', 'pos': [189, 193]}\n",
            "\n",
            "original:       {'text': '(Interpol', 'pos': [105, 114]}\n",
            "new_entity:     {'text': 'Interpol', 'pos': [106, 114]}\n",
            "\n",
            "original:       {'text': '\\ufeffBaker Hughes', 'pos': [0, 13]}\n",
            "new_entity:     {'text': 'Baker Hughes', 'pos': [1, 13]}\n",
            "\n",
            "original:       {'text': '(Tổ chức hoa hậu Mỹ', 'pos': [126, 145]}\n",
            "new_entity:     {'text': 'Tổ chức hoa hậu Mỹ', 'pos': [127, 145]}\n",
            "\n",
            "original:       {'text': '/Time', 'pos': [14, 19]}\n",
            "new_entity:     {'text': 'Time', 'pos': [15, 19]}\n",
            "\n",
            "original:       {'text': '(số 58 Quán Sứ', 'pos': [113, 127]}\n",
            "new_entity:     {'text': 'số 58 Quán Sứ', 'pos': [114, 127]}\n",
            "\n",
            "original:       {'text': '(Công ty CP xe khách số I Sơn La', 'pos': [169, 201]}\n",
            "new_entity:     {'text': 'Công ty CP xe khách số I Sơn La', 'pos': [170, 201]}\n",
            "\n",
            "original:       {'text': '(Công ty TNHH Kim Cương', 'pos': [286, 309]}\n",
            "new_entity:     {'text': 'Công ty TNHH Kim Cương', 'pos': [287, 309]}\n",
            "\n",
            "original:       {'text': '(Công ty Vinasun Ánh Dương Việt Nam', 'pos': [70, 105]}\n",
            "new_entity:     {'text': 'Công ty Vinasun Ánh Dương Việt Nam', 'pos': [71, 105]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Vận tải và Dịch vụ Liên Ninh', 'pos': [18, 58]}\n",
            "new_entity:     {'text': 'Công ty CP Vận tải và Dịch vụ Liên Ninh', 'pos': [19, 58]}\n",
            "\n",
            "original:       {'text': '(Xí nghiệp xe khách Nam Hà Nội', 'pos': [12, 42]}\n",
            "new_entity:     {'text': 'Xí nghiệp xe khách Nam Hà Nội', 'pos': [13, 42]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Vận tải Newway', 'pos': [16, 42]}\n",
            "new_entity:     {'text': 'Công ty CP Vận tải Newway', 'pos': [17, 42]}\n",
            "\n",
            "original:       {'text': '(Trung tâm Tân Đạt', 'pos': [16, 34]}\n",
            "new_entity:     {'text': 'Trung tâm Tân Đạt', 'pos': [17, 34]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Xe điện Hà Nội', 'pos': [15, 41]}\n",
            "new_entity:     {'text': 'Công ty CP Xe điện Hà Nội', 'pos': [16, 41]}\n",
            "\n",
            "original:       {'text': '(Công ty TNHH Thương mại Thiên Phong', 'pos': [18, 54]}\n",
            "new_entity:     {'text': 'Công ty TNHH Thương mại Thiên Phong', 'pos': [19, 54]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Tập đoàn Mai Linh', 'pos': [13, 42]}\n",
            "new_entity:     {'text': 'Công ty CP Tập đoàn Mai Linh', 'pos': [14, 42]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Thương mại du lịch và Vận chuyển khách Tình Nghĩa', 'pos': [16, 77]}\n",
            "new_entity:     {'text': 'Công ty CP Thương mại du lịch và Vận chuyển khách Tình Nghĩa', 'pos': [17, 77]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Vận tải bộ Tân Cảng', 'pos': [14, 45]}\n",
            "new_entity:     {'text': 'Công ty CP Vận tải bộ Tân Cảng', 'pos': [15, 45]}\n",
            "\n",
            "original:       {'text': '(Công ty TNHH Văn Minh', 'pos': [13, 35]}\n",
            "new_entity:     {'text': 'Công ty TNHH Văn Minh', 'pos': [14, 35]}\n",
            "\n",
            "original:       {'text': '(Công ty TNHH Phúc Xuyên', 'pos': [14, 38]}\n",
            "new_entity:     {'text': 'Công ty TNHH Phúc Xuyên', 'pos': [15, 38]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Thương mại và Du lịch Hạ Vinh', 'pos': [17, 58]}\n",
            "new_entity:     {'text': 'Công ty CP Thương mại và Du lịch Hạ Vinh', 'pos': [18, 58]}\n",
            "\n",
            "original:       {'text': '(Công ty CP vận tải bộ Tân Cảng', 'pos': [15, 46]}\n",
            "new_entity:     {'text': 'Công ty CP vận tải bộ Tân Cảng', 'pos': [16, 46]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Logistics Portserco', 'pos': [11, 42]}\n",
            "new_entity:     {'text': 'Công ty CP Logistics Portserco', 'pos': [12, 42]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Cơ khí Sơn La', 'pos': [17, 42]}\n",
            "new_entity:     {'text': 'Công ty CP Cơ khí Sơn La', 'pos': [18, 42]}\n",
            "\n",
            "original:       {'text': '(Công ty CP xe khách Sài Gòn', 'pos': [32, 60]}\n",
            "new_entity:     {'text': 'Công ty CP xe khách Sài Gòn', 'pos': [33, 60]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Giao nhận và Thương mại Quang Châu', 'pos': [17, 63]}\n",
            "new_entity:     {'text': 'Công ty CP Giao nhận và Thương mại Quang Châu', 'pos': [18, 63]}\n",
            "\n",
            "original:       {'text': '(HTX Vận tải Thống Nhất Quảng Ngãi', 'pos': [9, 43]}\n",
            "new_entity:     {'text': 'HTX Vận tải Thống Nhất Quảng Ngãi', 'pos': [10, 43]}\n",
            "\n",
            "original:       {'text': '(HTX Cơ khí vận tải và Dịch vụ Diên Hồng', 'pos': [18, 58]}\n",
            "new_entity:     {'text': 'HTX Cơ khí vận tải và Dịch vụ Diên Hồng', 'pos': [19, 58]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Vận tải An Giang', 'pos': [13, 41]}\n",
            "new_entity:     {'text': 'Công ty CP Vận tải An Giang', 'pos': [14, 41]}\n",
            "\n",
            "original:       {'text': '(Công ty TNHH Thương mại và Dịch vụ Hải Phượng', 'pos': [20, 66]}\n",
            "new_entity:     {'text': 'Công ty TNHH Thương mại và Dịch vụ Hải Phượng', 'pos': [21, 66]}\n",
            "\n",
            "original:       {'text': '(Công ty TNHH MTV Mai Linh Kon Tum', 'pos': [15, 49]}\n",
            "new_entity:     {'text': 'Công ty TNHH MTV Mai Linh Kon Tum', 'pos': [16, 49]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Du lịch Xuân Long', 'pos': [13, 42]}\n",
            "new_entity:     {'text': 'Công ty CP Du lịch Xuân Long', 'pos': [14, 42]}\n",
            "\n",
            "original:       {'text': '(Công ty TNHH Minh Quốc', 'pos': [16, 39]}\n",
            "new_entity:     {'text': 'Công ty TNHH Minh Quốc', 'pos': [17, 39]}\n",
            "\n",
            "original:       {'text': '(HTX Vận tải ô tô Thành Tuyên', 'pos': [13, 42]}\n",
            "new_entity:     {'text': 'HTX Vận tải ô tô Thành Tuyên', 'pos': [14, 42]}\n",
            "\n",
            "original:       {'text': '(Công ty CP vận tải ô tô Thanh Hóa', 'pos': [14, 48]}\n",
            "new_entity:     {'text': 'Công ty CP vận tải ô tô Thanh Hóa', 'pos': [15, 48]}\n",
            "\n",
            "original:       {'text': '(Công ty TNHH Mai Linh Thanh Hóa', 'pos': [13, 45]}\n",
            "new_entity:     {'text': 'Công ty TNHH Mai Linh Thanh Hóa', 'pos': [14, 45]}\n",
            "\n",
            "original:       {'text': '(Công ty TNHH Dịch vụ Taxi miền Bắc', 'pos': [12, 47]}\n",
            "new_entity:     {'text': 'Công ty TNHH Dịch vụ Taxi miền Bắc', 'pos': [13, 47]}\n",
            "\n",
            "original:       {'text': '(Công ty CP xe khách Quảng Ninh', 'pos': [14, 45]}\n",
            "new_entity:     {'text': 'Công ty CP xe khách Quảng Ninh', 'pos': [15, 45]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Vận tải ô tô Điện Biên', 'pos': [14, 48]}\n",
            "new_entity:     {'text': 'Công ty CP Vận tải ô tô Điện Biên', 'pos': [15, 48]}\n",
            "\n",
            "original:       {'text': '(Xí nghiệp Xe buýt Cầu Bươu', 'pos': [13, 40]}\n",
            "new_entity:     {'text': 'Xí nghiệp Xe buýt Cầu Bươu', 'pos': [14, 40]}\n",
            "\n",
            "original:       {'text': '(Xí nghiệp Xe buýt Hà Nội', 'pos': [17, 42]}\n",
            "new_entity:     {'text': 'Xí nghiệp Xe buýt Hà Nội', 'pos': [18, 42]}\n",
            "\n",
            "original:       {'text': '(Xí nghiệp Xe buýt 10/10 Hà Nội', 'pos': [16, 47]}\n",
            "new_entity:     {'text': 'Xí nghiệp Xe buýt 10/10 Hà Nội', 'pos': [17, 47]}\n",
            "\n",
            "original:       {'text': '(Công ty TNHH MTV Mai Linh Vĩnh Phúc', 'pos': [14, 50]}\n",
            "new_entity:     {'text': 'Công ty TNHH MTV Mai Linh Vĩnh Phúc', 'pos': [15, 50]}\n",
            "\n",
            "original:       {'text': '(Công ty TNHH MTV Mai Linh Gia Lai', 'pos': [14, 48]}\n",
            "new_entity:     {'text': 'Công ty TNHH MTV Mai Linh Gia Lai', 'pos': [15, 48]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Thương mại du lịch Hà Lan', 'pos': [14, 51]}\n",
            "new_entity:     {'text': 'Công ty CP Thương mại du lịch Hà Lan', 'pos': [15, 51]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Giao nhận kho vận Hải Dương', 'pos': [18, 57]}\n",
            "new_entity:     {'text': 'Công ty CP Giao nhận kho vận Hải Dương', 'pos': [19, 57]}\n",
            "\n",
            "original:       {'text': '(Công ty CP Vận tải ô tô Quảng Ninh', 'pos': [19, 54]}\n",
            "new_entity:     {'text': 'Công ty CP Vận tải ô tô Quảng Ninh', 'pos': [20, 54]}\n",
            "\n",
            "original:       {'text': '(Công ty TNHH Vận tải Thịnh Hưng', 'pos': [16, 48]}\n",
            "new_entity:     {'text': 'Công ty TNHH Vận tải Thịnh Hưng', 'pos': [17, 48]}\n",
            "\n",
            "original:       {'text': '(Xí nghiệp Xe buýt Yên Viên', 'pos': [13, 40]}\n",
            "new_entity:     {'text': 'Xí nghiệp Xe buýt Yên Viên', 'pos': [14, 40]}\n",
            "\n",
            "original:       {'text': '(HTX Vận tải hàng hóa hành khách và Du lịch Tân Việt', 'pos': [13, 65]}\n",
            "new_entity:     {'text': 'HTX Vận tải hàng hóa hành khách và Du lịch Tân Việt', 'pos': [14, 65]}\n",
            "\n",
            "original:       {'text': '(Hàn Quốc', 'pos': [61, 70]}\n",
            "new_entity:     {'text': 'Hàn Quốc', 'pos': [62, 70]}\n",
            "\n",
            "original:       {'text': '“Liên đoàn Taekwondo Thế giới', 'pos': [164, 193]}\n",
            "new_entity:     {'text': 'Liên đoàn Taekwondo Thế giới', 'pos': [165, 193]}\n",
            "\n",
            "original:       {'text': '“World Taekwondo', 'pos': [225, 241]}\n",
            "new_entity:     {'text': 'World Taekwondo', 'pos': [226, 241]}\n",
            "\n",
            "original:       {'text': '(Taekwondo Thế giới', 'pos': [244, 263]}\n",
            "new_entity:     {'text': 'Taekwondo Thế giới', 'pos': [245, 263]}\n",
            "\n",
            "original:       {'text': '(World Taekwondo Federation', 'pos': [139, 166]}\n",
            "new_entity:     {'text': 'World Taekwondo Federation', 'pos': [140, 166]}\n",
            "\n",
            "original:       {'text': '(huyện Tu Mơ Rông', 'pos': [57, 74]}\n",
            "new_entity:     {'text': 'huyện Tu Mơ Rông', 'pos': [58, 74]}\n",
            "\n",
            "original:       {'text': '(huyện Đắk Glei', 'pos': [89, 104]}\n",
            "new_entity:     {'text': 'huyện Đắk Glei', 'pos': [90, 104]}\n",
            "\n",
            "original:       {'text': '\\ufeffHà Tĩnh', 'pos': [0, 8]}\n",
            "new_entity:     {'text': 'Hà Tĩnh', 'pos': [1, 8]}\n",
            "\n",
            "original:       {'text': '(Đức Thọ', 'pos': [109, 117]}\n",
            "new_entity:     {'text': 'Đức Thọ', 'pos': [110, 117]}\n",
            "\n",
            "original:       {'text': '(huyện Đức Thọ', 'pos': [115, 129]}\n",
            "new_entity:     {'text': 'huyện Đức Thọ', 'pos': [116, 129]}\n",
            "\n",
            "original:       {'text': '(Bộ Kế hoạch và Đầu tư', 'pos': [95, 117]}\n",
            "new_entity:     {'text': 'Bộ Kế hoạch và Đầu tư', 'pos': [96, 117]}\n",
            "\n",
            "original:       {'text': '(Bộ KH&ĐT', 'pos': [74, 83]}\n",
            "new_entity:     {'text': 'Bộ KH&ĐT', 'pos': [75, 83]}\n",
            "\n",
            "original:       {'text': \"\\ufeff'Út Trọc\", 'pos': [0, 9]}\n",
            "new_entity:     {'text': 'Út Trọc', 'pos': [2, 9]}\n",
            "\n",
            "original:       {'text': \"'Vũ Nhôm\", 'pos': [13, 21]}\n",
            "new_entity:     {'text': 'Vũ Nhôm', 'pos': [14, 21]}\n",
            "\n",
            "original:       {'text': \"'Út Trọc\", 'pos': [94, 102]}\n",
            "new_entity:     {'text': 'Út Trọc', 'pos': [95, 102]}\n",
            "\n",
            "original:       {'text': '“Khánh Trắng', 'pos': [136, 148]}\n",
            "new_entity:     {'text': 'Khánh Trắng', 'pos': [137, 148]}\n",
            "\n",
            "original:       {'text': '“Hiệp', 'pos': [152, 157]}\n",
            "new_entity:     {'text': 'Hiệp', 'pos': [153, 157]}\n",
            "\n",
            "original:       {'text': '“Thuyết', 'pos': [167, 174]}\n",
            "new_entity:     {'text': 'Thuyết', 'pos': [168, 174]}\n",
            "\n",
            "original:       {'text': '“Vũ nhôm', 'pos': [26, 34]}\n",
            "new_entity:     {'text': 'Vũ nhôm', 'pos': [27, 34]}\n",
            "\n",
            "original:       {'text': '“Út trọc', 'pos': [38, 46]}\n",
            "new_entity:     {'text': 'Út trọc', 'pos': [39, 46]}\n",
            "\n",
            "original:       {'text': '\\ufeffTP.HCM', 'pos': [0, 7]}\n",
            "new_entity:     {'text': 'TP.HCM', 'pos': [1, 7]}\n",
            "\n",
            "original:       {'text': '(Nhóm Kẹo Dẻo', 'pos': [103, 116]}\n",
            "new_entity:     {'text': 'Nhóm Kẹo Dẻo', 'pos': [104, 116]}\n",
            "\n",
            "original:       {'text': '\\ufeffKaty Perry', 'pos': [0, 11]}\n",
            "new_entity:     {'text': 'Katy Perry', 'pos': [1, 11]}\n",
            "\n",
            "original:       {'text': \"'Lê Doãn Hợp\", 'pos': [145, 157]}\n",
            "new_entity:     {'text': 'Lê Doãn Hợp', 'pos': [146, 157]}\n",
            "\n",
            "original:       {'text': '(Đồng Nai', 'pos': [54, 63]}\n",
            "new_entity:     {'text': 'Đồng Nai', 'pos': [55, 63]}\n",
            "\n",
            "original:       {'text': '“Lê Doãn Hợp', 'pos': [139, 151]}\n",
            "new_entity:     {'text': 'Lê Doãn Hợp', 'pos': [140, 151]}\n",
            "\n",
            "original:       {'text': '\\ufeffKendall', 'pos': [0, 8]}\n",
            "new_entity:     {'text': 'Kendall', 'pos': [1, 8]}\n",
            "\n",
            "original:       {'text': '\"Kendall', 'pos': [0, 8]}\n",
            "new_entity:     {'text': 'Kendall', 'pos': [1, 8]}\n",
            "\n",
            "original:       {'text': '/Quang Hiếu', 'pos': [9, 20]}\n",
            "new_entity:     {'text': 'Quang Hiếu', 'pos': [10, 20]}\n",
            "\n",
            "original:       {'text': '(Bộ Kế hoạch - Đầu tư', 'pos': [63, 84]}\n",
            "new_entity:     {'text': 'Bộ Kế hoạch - Đầu tư', 'pos': [64, 84]}\n",
            "\n",
            "original:       {'text': '\\ufeffNga', 'pos': [0, 4]}\n",
            "new_entity:     {'text': 'Nga', 'pos': [1, 4]}\n",
            "\n",
            "original:       {'text': '(Nghi Xuân', 'pos': [123, 133]}\n",
            "new_entity:     {'text': 'Nghi Xuân', 'pos': [124, 133]}\n",
            "\n",
            "original:       {'text': '(huyện Nghi Xuân', 'pos': [94, 110]}\n",
            "new_entity:     {'text': 'huyện Nghi Xuân', 'pos': [95, 110]}\n",
            "\n",
            "original:       {'text': '\\ufeffBaek Hyun', 'pos': [0, 10]}\n",
            "new_entity:     {'text': 'Baek Hyun', 'pos': [1, 10]}\n",
            "\n",
            "original:       {'text': '(EXO', 'pos': [11, 15]}\n",
            "new_entity:     {'text': 'EXO', 'pos': [12, 15]}\n",
            "\n",
            "original:       {'text': '-Mỹ', 'pos': [90, 93]}\n",
            "new_entity:     {'text': 'Mỹ', 'pos': [91, 93]}\n",
            "\n",
            "original:       {'text': '(NSA', 'pos': [146, 150]}\n",
            "new_entity:     {'text': 'NSA', 'pos': [147, 150]}\n",
            "\n",
            "original:       {'text': '(ADI', 'pos': [31, 35]}\n",
            "new_entity:     {'text': 'ADI', 'pos': [32, 35]}\n",
            "\n",
            "original:       {'text': '(Girl’s Day', 'pos': [159, 170]}\n",
            "new_entity:     {'text': 'Girl’s Day', 'pos': [160, 170]}\n",
            "\n",
            "original:       {'text': '\"Kiha & The Faces', 'pos': [130, 147]}\n",
            "new_entity:     {'text': 'Kiha & The Faces', 'pos': [131, 147]}\n",
            "\n",
            "original:       {'text': '(Vitas', 'pos': [58, 64]}\n",
            "new_entity:     {'text': 'Vitas', 'pos': [59, 64]}\n",
            "\n",
            "original:       {'text': '\\ufeffNBA', 'pos': [0, 4]}\n",
            "new_entity:     {'text': 'NBA', 'pos': [1, 4]}\n",
            "\n",
            "original:       {'text': '\\ufeffLana Del Rey', 'pos': [0, 13]}\n",
            "new_entity:     {'text': 'Lana Del Rey', 'pos': [1, 13]}\n",
            "\n",
            "original:       {'text': '‘Barcelona', 'pos': [25, 35]}\n",
            "new_entity:     {'text': 'Barcelona', 'pos': [26, 35]}\n",
            "\n",
            "original:       {'text': '“Thánh Johan Cruyff', 'pos': [152, 171]}\n",
            "new_entity:     {'text': 'Thánh Johan Cruyff', 'pos': [153, 171]}\n",
            "\n",
            "original:       {'text': '(BTS', 'pos': [126, 130]}\n",
            "new_entity:     {'text': 'BTS', 'pos': [127, 130]}\n",
            "\n",
            "original:       {'text': '(2PM', 'pos': [112, 116]}\n",
            "new_entity:     {'text': '2PM', 'pos': [113, 116]}\n",
            "\n",
            "original:       {'text': '(Hà Nội', 'pos': [37, 44]}\n",
            "new_entity:     {'text': 'Hà Nội', 'pos': [38, 44]}\n",
            "\n",
            "original:       {'text': '(Hà Tĩnh', 'pos': [141, 149]}\n",
            "new_entity:     {'text': 'Hà Tĩnh', 'pos': [142, 149]}\n",
            "\n",
            "original:       {'text': '(596-Tôn Đức Thắng', 'pos': [48, 66]}\n",
            "new_entity:     {'text': '596-Tôn Đức Thắng', 'pos': [49, 66]}\n",
            "\n",
            "original:       {'text': '\\ufeffMỹ', 'pos': [0, 3]}\n",
            "new_entity:     {'text': 'Mỹ', 'pos': [1, 3]}\n",
            "\n",
            "original:       {'text': '(BSR', 'pos': [91, 95]}\n",
            "new_entity:     {'text': 'BSR', 'pos': [92, 95]}\n",
            "\n",
            "original:       {'text': '\\ufeffBritney Spears', 'pos': [0, 15]}\n",
            "new_entity:     {'text': 'Britney Spears', 'pos': [1, 15]}\n",
            "\n",
            "original:       {'text': '\"Britney', 'pos': [103, 111]}\n",
            "new_entity:     {'text': 'Britney', 'pos': [104, 111]}\n",
            "\n",
            "original:       {'text': '(Ryu Jun Yeol', 'pos': [136, 149]}\n",
            "new_entity:     {'text': 'Ryu Jun Yeol', 'pos': [137, 149]}\n",
            "\n",
            "original:       {'text': '(Hyeri', 'pos': [193, 199]}\n",
            "new_entity:     {'text': 'Hyeri', 'pos': [194, 199]}\n",
            "\n",
            "original:       {'text': '(Park Bo Gum', 'pos': [47, 59]}\n",
            "new_entity:     {'text': 'Park Bo Gum', 'pos': [48, 59]}\n",
            "\n",
            "original:       {'text': '(CECODES', 'pos': [82, 90]}\n",
            "new_entity:     {'text': 'CECODES', 'pos': [83, 90]}\n",
            "\n",
            "original:       {'text': '\\ufeffLeBron James', 'pos': [0, 13]}\n",
            "new_entity:     {'text': 'LeBron James', 'pos': [1, 13]}\n",
            "\n",
            "original:       {'text': '(Bành Sơn', 'pos': [44, 53]}\n",
            "new_entity:     {'text': 'Bành Sơn', 'pos': [45, 53]}\n",
            "\n",
            "original:       {'text': '\"Hoàng phần sơn', 'pos': [142, 157]}\n",
            "new_entity:     {'text': 'Hoàng phần sơn', 'pos': [143, 157]}\n",
            "\n",
            "original:       {'text': '(PVC', 'pos': [186, 190]}\n",
            "new_entity:     {'text': 'PVC', 'pos': [187, 190]}\n",
            "\n",
            "original:       {'text': '(FED', 'pos': [209, 213]}\n",
            "new_entity:     {'text': 'FED', 'pos': [210, 213]}\n",
            "\n",
            "original:       {'text': '(huyện Tiền Hải', 'pos': [88, 103]}\n",
            "new_entity:     {'text': 'huyện Tiền Hải', 'pos': [89, 103]}\n",
            "\n",
            "original:       {'text': '(Quỳnh Lưu', 'pos': [19, 29]}\n",
            "new_entity:     {'text': 'Quỳnh Lưu', 'pos': [20, 29]}\n",
            "\n",
            "original:       {'text': 'N. \"đầu gấu”', 'pos': [106, 118]}\n",
            "new_entity:     {'text': 'N. \"đầu gấu', 'pos': [106, 117]}\n",
            "\n",
            "original:       {'text': '(Nghệ An', 'pos': [158, 166]}\n",
            "new_entity:     {'text': 'Nghệ An', 'pos': [159, 166]}\n",
            "\n",
            "original:       {'text': '(Anh', 'pos': [66, 70]}\n",
            "new_entity:     {'text': 'Anh', 'pos': [67, 70]}\n",
            "\n",
            "original:       {'text': '(BOJ', 'pos': [46, 50]}\n",
            "new_entity:     {'text': 'BOJ', 'pos': [47, 50]}\n",
            "\n",
            "original:       {'text': '(ECB', 'pos': [101, 105]}\n",
            "new_entity:     {'text': 'ECB', 'pos': [102, 105]}\n",
            "\n",
            "original:       {'text': '“công ty vận tải hàng không dân sự Viktor Bout', 'pos': [58, 104]}\n",
            "new_entity:     {'text': 'công ty vận tải hàng không dân sự Viktor Bout', 'pos': [59, 104]}\n",
            "\n",
            "original:       {'text': '(DEA', 'pos': [56, 60]}\n",
            "new_entity:     {'text': 'DEA', 'pos': [57, 60]}\n",
            "\n",
            "original:       {'text': '(FARC', 'pos': [144, 149]}\n",
            "new_entity:     {'text': 'FARC', 'pos': [145, 149]}\n",
            "\n",
            "original:       {'text': '\"Zico', 'pos': [109, 114]}\n",
            "new_entity:     {'text': 'Zico', 'pos': [110, 114]}\n",
            "\n",
            "original:       {'text': '(NYDFS', 'pos': [37, 43]}\n",
            "new_entity:     {'text': 'NYDFS', 'pos': [38, 43]}\n",
            "\n",
            "original:       {'text': '\\ufeffBộ Xây dựng', 'pos': [0, 12]}\n",
            "new_entity:     {'text': 'Bộ Xây dựng', 'pos': [1, 12]}\n",
            "\n",
            "original:       {'text': '(Tôn Thất Trung', 'pos': [149, 164]}\n",
            "new_entity:     {'text': 'Tôn Thất Trung', 'pos': [150, 164]}\n",
            "\n",
            "original:       {'text': '(Bộ NN&PTNT', 'pos': [85, 96]}\n",
            "new_entity:     {'text': 'Bộ NN&PTNT', 'pos': [86, 96]}\n",
            "\n",
            "original:       {'text': '(Viện Nước, Tưới tiêu và Môi trường', 'pos': [26, 61]}\n",
            "new_entity:     {'text': 'Viện Nước, Tưới tiêu và Môi trường', 'pos': [27, 61]}\n",
            "\n",
            "original:       {'text': '(Bộ Tài nguyên và Môi trường', 'pos': [82, 110]}\n",
            "new_entity:     {'text': 'Bộ Tài nguyên và Môi trường', 'pos': [83, 110]}\n",
            "\n",
            "original:       {'text': '(Bộ Y tế', 'pos': [63, 71]}\n",
            "new_entity:     {'text': 'Bộ Y tế', 'pos': [64, 71]}\n",
            "\n",
            "original:       {'text': '(UN Women', 'pos': [152, 161]}\n",
            "new_entity:     {'text': 'UN Women', 'pos': [153, 161]}\n",
            "\n",
            "original:       {'text': '(TTXVN', 'pos': [11, 17]}\n",
            "new_entity:     {'text': 'TTXVN', 'pos': [12, 17]}\n",
            "\n",
            "original:       {'text': \"'Viện KSND tỉnh Nghệ An\", 'pos': [88, 111]}\n",
            "new_entity:     {'text': 'Viện KSND tỉnh Nghệ An', 'pos': [89, 111]}\n",
            "\n",
            "original:       {'text': '“Viện KSND tỉnh Nghệ An', 'pos': [94, 117]}\n",
            "new_entity:     {'text': 'Viện KSND tỉnh Nghệ An', 'pos': [95, 117]}\n",
            "\n",
            "original:       {'text': \"'Đường Tăng\", 'pos': [108, 119]}\n",
            "new_entity:     {'text': 'Đường Tăng', 'pos': [109, 119]}\n",
            "\n",
            "original:       {'text': '\"Đường Tăng', 'pos': [19, 30]}\n",
            "new_entity:     {'text': 'Đường Tăng', 'pos': [20, 30]}\n",
            "\n",
            "original:       {'text': '(Hà Tuệ', 'pos': [45, 52]}\n",
            "new_entity:     {'text': 'Hà Tuệ', 'pos': [46, 52]}\n",
            "\n",
            "original:       {'text': '(thị xã La Gi', 'pos': [30, 43]}\n",
            "new_entity:     {'text': 'thị xã La Gi', 'pos': [31, 43]}\n",
            "\n",
            "original:       {'text': '(Ifeng', 'pos': [46, 52]}\n",
            "new_entity:     {'text': 'Ifeng', 'pos': [47, 52]}\n",
            "\n",
            "original:       {'text': '(Thiểm Tây', 'pos': [142, 152]}\n",
            "new_entity:     {'text': 'Thiểm Tây', 'pos': [143, 152]}\n",
            "\n",
            "original:       {'text': '\"Khương Bá Ước', 'pos': [43, 57]}\n",
            "new_entity:     {'text': 'Khương Bá Ước', 'pos': [44, 57]}\n",
            "\n",
            "original:       {'text': '(Trung Quốc', 'pos': [51, 62]}\n",
            "new_entity:     {'text': 'Trung Quốc', 'pos': [52, 62]}\n",
            "\n",
            "original:       {'text': '(Gia Cát Lượng', 'pos': [106, 120]}\n",
            "new_entity:     {'text': 'Gia Cát Lượng', 'pos': [107, 120]}\n",
            "\n",
            "original:       {'text': '“Chí Phèo', 'pos': [182, 191]}\n",
            "new_entity:     {'text': 'Chí Phèo', 'pos': [183, 191]}\n",
            "\n",
            "original:       {'text': '(Hoa Kỳ', 'pos': [160, 167]}\n",
            "new_entity:     {'text': 'Hoa Kỳ', 'pos': [161, 167]}\n",
            "\n",
            "original:       {'text': '(IRCJ', 'pos': [89, 94]}\n",
            "new_entity:     {'text': 'IRCJ', 'pos': [90, 94]}\n",
            "\n",
            "original:       {'text': '/WB', 'pos': [69, 72]}\n",
            "new_entity:     {'text': 'WB', 'pos': [70, 72]}\n",
            "\n",
            "original:       {'text': '(EU', 'pos': [116, 119]}\n",
            "new_entity:     {'text': 'EU', 'pos': [117, 119]}\n",
            "\n",
            "original:       {'text': '(Liên đoàn Thể dục dụng cụ Mỹ', 'pos': [56, 85]}\n",
            "new_entity:     {'text': 'Liên đoàn Thể dục dụng cụ Mỹ', 'pos': [57, 85]}\n",
            "\n",
            "original:       {'text': '(IEEE', 'pos': [82, 87]}\n",
            "new_entity:     {'text': 'IEEE', 'pos': [83, 87]}\n",
            "\n",
            "original:       {'text': '(TWC', 'pos': [84, 88]}\n",
            "new_entity:     {'text': 'TWC', 'pos': [85, 88]}\n",
            "\n",
            "original:       {'text': '(Tư Nghĩa', 'pos': [52, 61]}\n",
            "new_entity:     {'text': 'Tư Nghĩa', 'pos': [53, 61]}\n",
            "\n",
            "original:       {'text': '(Bảy Đào', 'pos': [85, 93]}\n",
            "new_entity:     {'text': 'Bảy Đào', 'pos': [86, 93]}\n",
            "\n",
            "original:       {'text': '\\ufeffThạch Bích', 'pos': [0, 11]}\n",
            "new_entity:     {'text': 'Thạch Bích', 'pos': [1, 11]}\n",
            "\n",
            "original:       {'text': '(VAFIE', 'pos': [61, 67]}\n",
            "new_entity:     {'text': 'VAFIE', 'pos': [62, 67]}\n",
            "\n",
            "original:       {'text': '\\ufeffViện Kinh tế Xây dựng', 'pos': [0, 22]}\n",
            "new_entity:     {'text': 'Viện Kinh tế Xây dựng', 'pos': [1, 22]}\n",
            "\n",
            "original:       {'text': '/Một Thế Giới', 'pos': [15, 28]}\n",
            "new_entity:     {'text': 'Một Thế Giới', 'pos': [16, 28]}\n",
            "\n",
            "original:       {'text': '(Tiền Giang', 'pos': [68, 79]}\n",
            "new_entity:     {'text': 'Tiền Giang', 'pos': [69, 79]}\n",
            "\n",
            "original:       {'text': '(Phú Tân', 'pos': [18, 26]}\n",
            "new_entity:     {'text': 'Phú Tân', 'pos': [19, 26]}\n",
            "\n",
            "original:       {'text': '(Châu Thành', 'pos': [80, 91]}\n",
            "new_entity:     {'text': 'Châu Thành', 'pos': [81, 91]}\n",
            "\n",
            "original:       {'text': '(Campuchia', 'pos': [147, 157]}\n",
            "new_entity:     {'text': 'Campuchia', 'pos': [148, 157]}\n",
            "\n",
            "original:       {'text': '(cồn Bình Thủy', 'pos': [16, 30]}\n",
            "new_entity:     {'text': 'cồn Bình Thủy', 'pos': [17, 30]}\n",
            "\n",
            "original:       {'text': '\\ufeffBộ tư lệnh Quân khu 3', 'pos': [0, 22]}\n",
            "new_entity:     {'text': 'Bộ tư lệnh Quân khu 3', 'pos': [1, 22]}\n",
            "\n",
            "original:       {'text': '(C45', 'pos': [46, 50]}\n",
            "new_entity:     {'text': 'C45', 'pos': [47, 50]}\n",
            "\n",
            "original:       {'text': '(tỉnh Đồng Nai', 'pos': [171, 185]}\n",
            "new_entity:     {'text': 'tỉnh Đồng Nai', 'pos': [172, 185]}\n",
            "\n",
            "original:       {'text': '(VnCert', 'pos': [86, 93]}\n",
            "new_entity:     {'text': 'VnCert', 'pos': [87, 93]}\n",
            "\n",
            "original:       {'text': '(KH&ĐT', 'pos': [216, 222]}\n",
            "new_entity:     {'text': 'KH&ĐT', 'pos': [217, 222]}\n",
            "\n",
            "original:       {'text': '(ADB', 'pos': [103, 107]}\n",
            "new_entity:     {'text': 'ADB', 'pos': [104, 107]}\n",
            "\n",
            "original:       {'text': '(EVN', 'pos': [115, 119]}\n",
            "new_entity:     {'text': 'EVN', 'pos': [116, 119]}\n",
            "\n",
            "original:       {'text': '(Digiworld Corporation', 'pos': [95, 117]}\n",
            "new_entity:     {'text': 'Digiworld Corporation', 'pos': [96, 117]}\n",
            "\n",
            "original:       {'text': '\\ufeffDana White', 'pos': [0, 11]}\n",
            "new_entity:     {'text': 'Dana White', 'pos': [1, 11]}\n",
            "\n",
            "original:       {'text': '“Conor', 'pos': [65, 71]}\n",
            "new_entity:     {'text': 'Conor', 'pos': [66, 71]}\n",
            "\n",
            "original:       {'text': '(Leng Rjn', 'pos': [51, 60]}\n",
            "new_entity:     {'text': 'Leng Rjn', 'pos': [52, 60]}\n",
            "\n",
            "original:       {'text': '(huyện Lắk', 'pos': [93, 103]}\n",
            "new_entity:     {'text': 'huyện Lắk', 'pos': [94, 103]}\n",
            "\n",
            "original:       {'text': '(huyện Vĩnh Cữu', 'pos': [53, 68]}\n",
            "new_entity:     {'text': 'huyện Vĩnh Cữu', 'pos': [54, 68]}\n",
            "\n",
            "original:       {'text': '(làng Kon K’tu', 'pos': [62, 76]}\n",
            "new_entity:     {'text': 'làng Kon K’tu', 'pos': [63, 76]}\n",
            "\n",
            "original:       {'text': '(Lưu Diệc Phi', 'pos': [19, 32]}\n",
            "new_entity:     {'text': 'Lưu Diệc Phi', 'pos': [20, 32]}\n",
            "\n",
            "original:       {'text': '(Song Seung Hun', 'pos': [115, 130]}\n",
            "new_entity:     {'text': 'Song Seung Hun', 'pos': [116, 130]}\n",
            "\n",
            "original:       {'text': '(Mạnh Giai', 'pos': [49, 59]}\n",
            "new_entity:     {'text': 'Mạnh Giai', 'pos': [50, 59]}\n",
            "\n",
            "original:       {'text': '(Giang Ngữ Thần', 'pos': [102, 117]}\n",
            "new_entity:     {'text': 'Giang Ngữ Thần', 'pos': [103, 117]}\n",
            "\n",
            "original:       {'text': '(TP.HCM', 'pos': [103, 110]}\n",
            "new_entity:     {'text': 'TP.HCM', 'pos': [104, 110]}\n",
            "\n",
            "original:       {'text': '(ONEI', 'pos': [92, 97]}\n",
            "new_entity:     {'text': 'ONEI', 'pos': [93, 97]}\n",
            "\n",
            "original:       {'text': '(Cepal', 'pos': [192, 198]}\n",
            "new_entity:     {'text': 'Cepal', 'pos': [193, 198]}\n",
            "\n",
            "original:       {'text': \"Viện nghiên cứu và phát triển 'Burevestnik'\", 'pos': [136, 179]}\n",
            "new_entity:     {'text': \"Viện nghiên cứu và phát triển 'Burevestnik\", 'pos': [136, 178]}\n",
            "\n",
            "original:       {'text': 'Viện nghiên cứu và phát triển “Burevestnik”', 'pos': [88, 131]}\n",
            "new_entity:     {'text': 'Viện nghiên cứu và phát triển “Burevestnik', 'pos': [88, 130]}\n",
            "\n",
            "original:       {'text': '(NYT', 'pos': [141, 145]}\n",
            "new_entity:     {'text': 'NYT', 'pos': [142, 145]}\n",
            "\n",
            "original:       {'text': '(Quy Nhơn', 'pos': [24, 33]}\n",
            "new_entity:     {'text': 'Quy Nhơn', 'pos': [25, 33]}\n",
            "\n",
            "original:       {'text': '-BNV', 'pos': [111, 115]}\n",
            "new_entity:     {'text': 'BNV', 'pos': [112, 115]}\n",
            "\n",
            "original:       {'text': '(Eric Clapton', 'pos': [201, 214]}\n",
            "new_entity:     {'text': 'Eric Clapton', 'pos': [202, 214]}\n",
            "\n",
            "original:       {'text': '(Yên Dũng', 'pos': [31, 40]}\n",
            "new_entity:     {'text': 'Yên Dũng', 'pos': [32, 40]}\n",
            "\n",
            "original:       {'text': '(thôn Bắc Thành', 'pos': [36, 51]}\n",
            "new_entity:     {'text': 'thôn Bắc Thành', 'pos': [37, 51]}\n",
            "\n",
            "original:       {'text': '(huyện Ứng Hòa', 'pos': [293, 307]}\n",
            "new_entity:     {'text': 'huyện Ứng Hòa', 'pos': [294, 307]}\n",
            "\n",
            "original:       {'text': '(xã Hợp Đồng', 'pos': [111, 123]}\n",
            "new_entity:     {'text': 'xã Hợp Đồng', 'pos': [112, 123]}\n",
            "\n",
            "original:       {'text': '(Công an tỉnh Nam Định', 'pos': [96, 118]}\n",
            "new_entity:     {'text': 'Công an tỉnh Nam Định', 'pos': [97, 118]}\n",
            "\n",
            "original:       {'text': '-TPHCM', 'pos': [13, 19]}\n",
            "new_entity:     {'text': 'TPHCM', 'pos': [14, 19]}\n",
            "\n",
            "original:       {'text': '\\ufeffAT&T', 'pos': [0, 5]}\n",
            "new_entity:     {'text': 'AT&T', 'pos': [1, 5]}\n",
            "\n",
            "original:       {'text': '“Ma Bell', 'pos': [92, 100]}\n",
            "new_entity:     {'text': 'Ma Bell', 'pos': [93, 100]}\n",
            "\n",
            "original:       {'text': '\\ufeffVũ Dino', 'pos': [0, 8]}\n",
            "new_entity:     {'text': 'Vũ Dino', 'pos': [1, 8]}\n",
            "\n",
            "original:       {'text': '/VOV1', 'pos': [12, 17]}\n",
            "new_entity:     {'text': 'VOV1', 'pos': [13, 17]}\n",
            "\n",
            "original:       {'text': '\\ufeffHoa Sen Group', 'pos': [0, 14]}\n",
            "new_entity:     {'text': 'Hoa Sen Group', 'pos': [1, 14]}\n",
            "\n",
            "original:       {'text': '(Hoa Sen Group', 'pos': [60, 74]}\n",
            "new_entity:     {'text': 'Hoa Sen Group', 'pos': [61, 74]}\n",
            "\n",
            "original:       {'text': '\\ufeffToyota', 'pos': [0, 7]}\n",
            "new_entity:     {'text': 'Toyota', 'pos': [1, 7]}\n",
            "\n",
            "original:       {'text': '(Các Tiểu vương quốc Ả Rập Thống nhất', 'pos': [76, 113]}\n",
            "new_entity:     {'text': 'Các Tiểu vương quốc Ả Rập Thống nhất', 'pos': [77, 113]}\n",
            "\n",
            "original:       {'text': '(Bách Long', 'pos': [68, 78]}\n",
            "new_entity:     {'text': 'Bách Long', 'pos': [69, 78]}\n",
            "\n",
            "original:       {'text': '(Ấn Độ', 'pos': [114, 120]}\n",
            "new_entity:     {'text': 'Ấn Độ', 'pos': [115, 120]}\n",
            "\n",
            "original:       {'text': '(Thái Lan', 'pos': [99, 108]}\n",
            "new_entity:     {'text': 'Thái Lan', 'pos': [100, 108]}\n",
            "\n",
            "original:       {'text': '(VCCI', 'pos': [208, 213]}\n",
            "new_entity:     {'text': 'VCCI', 'pos': [209, 213]}\n",
            "\n",
            "original:       {'text': '(TAND tỉnh Bình Phước', 'pos': [26, 47]}\n",
            "new_entity:     {'text': 'TAND tỉnh Bình Phước', 'pos': [27, 47]}\n",
            "\n",
            "original:       {'text': \"\\ufeff'Federer\", 'pos': [0, 9]}\n",
            "new_entity:     {'text': 'Federer', 'pos': [2, 9]}\n",
            "\n",
            "original:       {'text': '(huyện Cần Giờ', 'pos': [139, 153]}\n",
            "new_entity:     {'text': 'huyện Cần Giờ', 'pos': [140, 153]}\n",
            "\n",
            "original:       {'text': '(Sóc Trăng', 'pos': [59, 69]}\n",
            "new_entity:     {'text': 'Sóc Trăng', 'pos': [60, 69]}\n",
            "\n",
            "original:       {'text': '(xã An Thạnh Đông', 'pos': [40, 57]}\n",
            "new_entity:     {'text': 'xã An Thạnh Đông', 'pos': [41, 57]}\n",
            "\n",
            "original:       {'text': '(Võ Thanh Quang', 'pos': [24, 39]}\n",
            "new_entity:     {'text': 'Võ Thanh Quang', 'pos': [25, 39]}\n",
            "\n",
            "original:       {'text': '(Trà Vinh', 'pos': [71, 80]}\n",
            "new_entity:     {'text': 'Trà Vinh', 'pos': [72, 80]}\n",
            "\n",
            "original:       {'text': '(Lê Minh Đương', 'pos': [14, 28]}\n",
            "new_entity:     {'text': 'Lê Minh Đương', 'pos': [15, 28]}\n",
            "\n",
            "original:       {'text': '\\ufeffEVNNPT', 'pos': [0, 7]}\n",
            "new_entity:     {'text': 'EVNNPT', 'pos': [1, 7]}\n",
            "\n",
            "original:       {'text': '(EVNNPT', 'pos': [38, 45]}\n",
            "new_entity:     {'text': 'EVNNPT', 'pos': [39, 45]}\n",
            "\n",
            "original:       {'text': '-BTC', 'pos': [282, 286]}\n",
            "new_entity:     {'text': 'BTC', 'pos': [283, 286]}\n",
            "\n",
            "original:       {'text': '(Trần Tế Xương', 'pos': [17, 31]}\n",
            "new_entity:     {'text': 'Trần Tế Xương', 'pos': [18, 31]}\n",
            "\n",
            "original:       {'text': '(Nam Định', 'pos': [53, 62]}\n",
            "new_entity:     {'text': 'Nam Định', 'pos': [54, 62]}\n",
            "\n",
            "original:       {'text': '(Lữ', 'pos': [34, 37]}\n",
            "new_entity:     {'text': 'Lữ', 'pos': [35, 37]}\n",
            "\n",
            "original:       {'text': '(Đào Uyên Minh', 'pos': [24, 38]}\n",
            "new_entity:     {'text': 'Đào Uyên Minh', 'pos': [25, 38]}\n",
            "\n",
            "original:       {'text': '(Bắc Ninh', 'pos': [48, 57]}\n",
            "new_entity:     {'text': 'Bắc Ninh', 'pos': [49, 57]}\n",
            "\n",
            "original:       {'text': '\\ufeffVũ Duy Khánh', 'pos': [0, 13]}\n",
            "new_entity:     {'text': 'Vũ Duy Khánh', 'pos': [1, 13]}\n",
            "\n",
            "original:       {'text': '(Đức', 'pos': [50, 54]}\n",
            "new_entity:     {'text': 'Đức', 'pos': [51, 54]}\n",
            "\n",
            "original:       {'text': '(Đan Mạch', 'pos': [46, 55]}\n",
            "new_entity:     {'text': 'Đan Mạch', 'pos': [47, 55]}\n",
            "\n",
            "original:       {'text': '(Hà Lan', 'pos': [81, 88]}\n",
            "new_entity:     {'text': 'Hà Lan', 'pos': [82, 88]}\n",
            "\n",
            "original:       {'text': '(GOT7', 'pos': [149, 154]}\n",
            "new_entity:     {'text': 'GOT7', 'pos': [150, 154]}\n",
            "\n",
            "original:       {'text': '(Báo Nhân Dân', 'pos': [184, 197]}\n",
            "new_entity:     {'text': 'Báo Nhân Dân', 'pos': [185, 197]}\n",
            "\n",
            "original:       {'text': '-Quốc Khánh', 'pos': [94, 105]}\n",
            "new_entity:     {'text': 'Quốc Khánh', 'pos': [95, 105]}\n",
            "\n",
            "original:       {'text': '(Báo Sài Gòn Giải Phóng', 'pos': [106, 129]}\n",
            "new_entity:     {'text': 'Báo Sài Gòn Giải Phóng', 'pos': [107, 129]}\n",
            "\n",
            "original:       {'text': '-Nhật Nam', 'pos': [113, 122]}\n",
            "new_entity:     {'text': 'Nhật Nam', 'pos': [114, 122]}\n",
            "\n",
            "original:       {'text': '(Báo điện tử Chính phủ', 'pos': [123, 145]}\n",
            "new_entity:     {'text': 'Báo điện tử Chính phủ', 'pos': [124, 145]}\n",
            "\n",
            "original:       {'text': '(Đài Tiếng nói Việt Nam', 'pos': [87, 110]}\n",
            "new_entity:     {'text': 'Đài Tiếng nói Việt Nam', 'pos': [88, 110]}\n",
            "\n",
            "original:       {'text': '-Mỹ Hạnh', 'pos': [80, 88]}\n",
            "new_entity:     {'text': 'Mỹ Hạnh', 'pos': [81, 88]}\n",
            "\n",
            "original:       {'text': '-Hồng Quân', 'pos': [89, 99]}\n",
            "new_entity:     {'text': 'Hồng Quân', 'pos': [90, 99]}\n",
            "\n",
            "original:       {'text': '-Tuấn Tú', 'pos': [100, 108]}\n",
            "new_entity:     {'text': 'Tuấn Tú', 'pos': [101, 108]}\n",
            "\n",
            "original:       {'text': '(Trung tâm Truyền hình Nhân Dân', 'pos': [109, 140]}\n",
            "new_entity:     {'text': 'Trung tâm Truyền hình Nhân Dân', 'pos': [110, 140]}\n",
            "\n",
            "original:       {'text': '(Thông tấn xã Việt Nam', 'pos': [111, 133]}\n",
            "new_entity:     {'text': 'Thông tấn xã Việt Nam', 'pos': [112, 133]}\n",
            "\n",
            "original:       {'text': '(Tạp chí Nội chính', 'pos': [111, 129]}\n",
            "new_entity:     {'text': 'Tạp chí Nội chính', 'pos': [112, 129]}\n",
            "\n",
            "original:       {'text': '-Nguyễn Minh Phong', 'pos': [139, 157]}\n",
            "new_entity:     {'text': 'Nguyễn Minh Phong', 'pos': [140, 157]}\n",
            "\n",
            "original:       {'text': '-Hoàng Gia Minh', 'pos': [158, 173]}\n",
            "new_entity:     {'text': 'Hoàng Gia Minh', 'pos': [159, 173]}\n",
            "\n",
            "original:       {'text': '-Hồ Quang Phương', 'pos': [174, 190]}\n",
            "new_entity:     {'text': 'Hồ Quang Phương', 'pos': [175, 190]}\n",
            "\n",
            "original:       {'text': '(Báo Quân đội nhân dân', 'pos': [191, 213]}\n",
            "new_entity:     {'text': 'Báo Quân đội nhân dân', 'pos': [192, 213]}\n",
            "\n",
            "original:       {'text': '(Báo Cần Thơ', 'pos': [103, 115]}\n",
            "new_entity:     {'text': 'Báo Cần Thơ', 'pos': [104, 115]}\n",
            "\n",
            "original:       {'text': '(Báo Bạc Liêu', 'pos': [66, 79]}\n",
            "new_entity:     {'text': 'Báo Bạc Liêu', 'pos': [67, 79]}\n",
            "\n",
            "original:       {'text': '\"Xã Cổ Đô', 'pos': [12, 21]}\n",
            "new_entity:     {'text': 'Xã Cổ Đô', 'pos': [13, 21]}\n",
            "\n",
            "original:       {'text': '-Thanh Nhất', 'pos': [25, 36]}\n",
            "new_entity:     {'text': 'Thanh Nhất', 'pos': [26, 36]}\n",
            "\n",
            "original:       {'text': '-Đức Lựu', 'pos': [37, 45]}\n",
            "new_entity:     {'text': 'Đức Lựu', 'pos': [38, 45]}\n",
            "\n",
            "original:       {'text': '-Sỹ Toàn', 'pos': [46, 54]}\n",
            "new_entity:     {'text': 'Sỹ Toàn', 'pos': [47, 54]}\n",
            "\n",
            "original:       {'text': '-Thùy Dương', 'pos': [55, 66]}\n",
            "new_entity:     {'text': 'Thùy Dương', 'pos': [56, 66]}\n",
            "\n",
            "original:       {'text': '-Phúc Lâm', 'pos': [67, 76]}\n",
            "new_entity:     {'text': 'Phúc Lâm', 'pos': [68, 76]}\n",
            "\n",
            "original:       {'text': '(Đài Phát thanh-Truyền hình Quảng Nam', 'pos': [77, 114]}\n",
            "new_entity:     {'text': 'Đài Phát thanh-Truyền hình Quảng Nam', 'pos': [78, 114]}\n",
            "\n",
            "original:       {'text': '-Anh Tuấn', 'pos': [116, 125]}\n",
            "new_entity:     {'text': 'Anh Tuấn', 'pos': [117, 125]}\n",
            "\n",
            "original:       {'text': '-Đức Hiệp', 'pos': [126, 135]}\n",
            "new_entity:     {'text': 'Đức Hiệp', 'pos': [127, 135]}\n",
            "\n",
            "original:       {'text': '-Đức Phong', 'pos': [136, 146]}\n",
            "new_entity:     {'text': 'Đức Phong', 'pos': [137, 146]}\n",
            "\n",
            "original:       {'text': '-Xuân Hoàng', 'pos': [147, 158]}\n",
            "new_entity:     {'text': 'Xuân Hoàng', 'pos': [148, 158]}\n",
            "\n",
            "original:       {'text': '(Đài Phát thanh-Truyền hình Quảng Ninh', 'pos': [159, 197]}\n",
            "new_entity:     {'text': 'Đài Phát thanh-Truyền hình Quảng Ninh', 'pos': [160, 197]}\n",
            "\n",
            "original:       {'text': '(Tạp chí Cộng sản', 'pos': [150, 167]}\n",
            "new_entity:     {'text': 'Tạp chí Cộng sản', 'pos': [151, 167]}\n",
            "\n",
            "original:       {'text': '(Tạp chí Kiểm tra', 'pos': [98, 115]}\n",
            "new_entity:     {'text': 'Tạp chí Kiểm tra', 'pos': [99, 115]}\n",
            "\n",
            "original:       {'text': '-Thái Sơn', 'pos': [79, 88]}\n",
            "new_entity:     {'text': 'Thái Sơn', 'pos': [80, 88]}\n",
            "\n",
            "original:       {'text': '-Ái Châu', 'pos': [89, 97]}\n",
            "new_entity:     {'text': 'Ái Châu', 'pos': [90, 97]}\n",
            "\n",
            "original:       {'text': '-Ngọc Minh', 'pos': [98, 108]}\n",
            "new_entity:     {'text': 'Ngọc Minh', 'pos': [99, 108]}\n",
            "\n",
            "original:       {'text': '(Báo Thanh Niên', 'pos': [109, 124]}\n",
            "new_entity:     {'text': 'Báo Thanh Niên', 'pos': [110, 124]}\n",
            "\n",
            "original:       {'text': '-Vũ Ngọc Hà', 'pos': [100, 111]}\n",
            "new_entity:     {'text': 'Vũ Ngọc Hà', 'pos': [101, 111]}\n",
            "\n",
            "original:       {'text': '-Nguyễn Thành Tâm', 'pos': [112, 129]}\n",
            "new_entity:     {'text': 'Nguyễn Thành Tâm', 'pos': [113, 129]}\n",
            "\n",
            "original:       {'text': '(Báo Hà Nội Mới', 'pos': [130, 145]}\n",
            "new_entity:     {'text': 'Báo Hà Nội Mới', 'pos': [131, 145]}\n",
            "\n",
            "original:       {'text': '-Nguyễn Thị Lý', 'pos': [90, 104]}\n",
            "new_entity:     {'text': 'Nguyễn Thị Lý', 'pos': [91, 104]}\n",
            "\n",
            "original:       {'text': '-Nguyễn Phương Triều', 'pos': [105, 125]}\n",
            "new_entity:     {'text': 'Nguyễn Phương Triều', 'pos': [106, 125]}\n",
            "\n",
            "original:       {'text': '(Báo Quảng Ngãi', 'pos': [126, 141]}\n",
            "new_entity:     {'text': 'Báo Quảng Ngãi', 'pos': [127, 141]}\n",
            "\n",
            "original:       {'text': '-Minh Châm', 'pos': [173, 183]}\n",
            "new_entity:     {'text': 'Minh Châm', 'pos': [174, 183]}\n",
            "\n",
            "original:       {'text': '(Báo Hải Phòng', 'pos': [184, 198]}\n",
            "new_entity:     {'text': 'Báo Hải Phòng', 'pos': [185, 198]}\n",
            "\n",
            "original:       {'text': '(Báo Công an nhân dân', 'pos': [94, 115]}\n",
            "new_entity:     {'text': 'Báo Công an nhân dân', 'pos': [95, 115]}\n",
            "\n",
            "original:       {'text': '(Báo điện tử VietnamPlus', 'pos': [102, 126]}\n",
            "new_entity:     {'text': 'Báo điện tử VietnamPlus', 'pos': [103, 126]}\n",
            "\n",
            "original:       {'text': '-Thu Hà', 'pos': [85, 92]}\n",
            "new_entity:     {'text': 'Thu Hà', 'pos': [86, 92]}\n",
            "\n",
            "original:       {'text': '(Báo điện tử Đảng Cộng sản Việt Nam', 'pos': [93, 128]}\n",
            "new_entity:     {'text': 'Báo điện tử Đảng Cộng sản Việt Nam', 'pos': [94, 128]}\n",
            "\n",
            "original:       {'text': '(Báo An Giang', 'pos': [75, 88]}\n",
            "new_entity:     {'text': 'Báo An Giang', 'pos': [76, 88]}\n",
            "\n",
            "original:       {'text': '-Đình Dương', 'pos': [46, 57]}\n",
            "new_entity:     {'text': 'Đình Dương', 'pos': [47, 57]}\n",
            "\n",
            "original:       {'text': '-Hữu Thành', 'pos': [58, 68]}\n",
            "new_entity:     {'text': 'Hữu Thành', 'pos': [59, 68]}\n",
            "\n",
            "original:       {'text': '-Đình Lộc', 'pos': [69, 78]}\n",
            "new_entity:     {'text': 'Đình Lộc', 'pos': [70, 78]}\n",
            "\n",
            "original:       {'text': '(Chi hội Nhà báo VTV9', 'pos': [79, 100]}\n",
            "new_entity:     {'text': 'Chi hội Nhà báo VTV9', 'pos': [80, 100]}\n",
            "\n",
            "original:       {'text': '-Song Lâm', 'pos': [95, 104]}\n",
            "new_entity:     {'text': 'Song Lâm', 'pos': [96, 104]}\n",
            "\n",
            "original:       {'text': '-Hoàng Bạc', 'pos': [105, 115]}\n",
            "new_entity:     {'text': 'Hoàng Bạc', 'pos': [106, 115]}\n",
            "\n",
            "original:       {'text': '-Thanh Miền', 'pos': [85, 96]}\n",
            "new_entity:     {'text': 'Thanh Miền', 'pos': [86, 96]}\n",
            "\n",
            "original:       {'text': '(Báo Yên Bái', 'pos': [97, 109]}\n",
            "new_entity:     {'text': 'Báo Yên Bái', 'pos': [98, 109]}\n",
            "\n",
            "original:       {'text': '(Đoàn luật sư TP Hà Nội', 'pos': [56, 79]}\n",
            "new_entity:     {'text': 'Đoàn luật sư TP Hà Nội', 'pos': [57, 79]}\n",
            "\n",
            "original:       {'text': '(huyện Hóc Môn', 'pos': [40, 54]}\n",
            "new_entity:     {'text': 'huyện Hóc Môn', 'pos': [41, 54]}\n",
            "\n",
            "original:       {'text': '(TP Hà Nội', 'pos': [94, 104]}\n",
            "new_entity:     {'text': 'TP Hà Nội', 'pos': [95, 104]}\n",
            "\n",
            "original:       {'text': '(Phường Dịch Vọng Hậu', 'pos': [66, 87]}\n",
            "new_entity:     {'text': 'Phường Dịch Vọng Hậu', 'pos': [67, 87]}\n",
            "\n",
            "original:       {'text': '(Nam', 'pos': [24, 28]}\n",
            "new_entity:     {'text': 'Nam', 'pos': [25, 28]}\n",
            "\n",
            "original:       {'text': '(Kitô giáo', 'pos': [106, 116]}\n",
            "new_entity:     {'text': 'Kitô giáo', 'pos': [107, 116]}\n",
            "\n",
            "original:       {'text': '“John Fitzgerald Kennedy', 'pos': [133, 157]}\n",
            "new_entity:     {'text': 'John Fitzgerald Kennedy', 'pos': [134, 157]}\n",
            "\n",
            "original:       {'text': '(AFP', 'pos': [31, 35]}\n",
            "new_entity:     {'text': 'AFP', 'pos': [32, 35]}\n",
            "\n",
            "original:       {'text': '\\ufeffTừ Hoảng', 'pos': [0, 9]}\n",
            "new_entity:     {'text': 'Từ Hoảng', 'pos': [1, 9]}\n",
            "\n",
            "original:       {'text': '(Vũ', 'pos': [19, 22]}\n",
            "new_entity:     {'text': 'Vũ', 'pos': [20, 22]}\n",
            "\n",
            "original:       {'text': '\"Từ Hoảng', 'pos': [56, 65]}\n",
            "new_entity:     {'text': 'Từ Hoảng', 'pos': [57, 65]}\n",
            "\n",
            "original:       {'text': '(Tào Tháo', 'pos': [112, 121]}\n",
            "new_entity:     {'text': 'Tào Tháo', 'pos': [113, 121]}\n",
            "\n",
            "original:       {'text': '\"Từ', 'pos': [40, 43]}\n",
            "new_entity:     {'text': 'Từ', 'pos': [41, 43]}\n",
            "\n",
            "original:       {'text': '(Từ Hoảng', 'pos': [123, 132]}\n",
            "new_entity:     {'text': 'Từ Hoảng', 'pos': [124, 132]}\n",
            "\n",
            "original:       {'text': '\\ufeffLâm Vinh Hải', 'pos': [0, 13]}\n",
            "new_entity:     {'text': 'Lâm Vinh Hải', 'pos': [1, 13]}\n",
            "\n",
            "original:       {'text': '“Hải', 'pos': [0, 4]}\n",
            "new_entity:     {'text': 'Hải', 'pos': [1, 4]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcwMQApng86X"
      },
      "source": [
        "# Install Pytorch/XLA for using TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44bbgzl_g_2h"
      },
      "source": [
        "###import os\n",
        "###assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnJK-U0OhCJg"
      },
      "source": [
        "###!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhLi-foUnFyS"
      },
      "source": [
        "###os.environ['XLA_USE_BF16'] = \"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrfoTuISvWoh"
      },
      "source": [
        "###!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "###!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ5JPjBUvq1g"
      },
      "source": [
        "###os.environ['XLA_USE_BF16'] = \"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYgSRt5lcdsA"
      },
      "source": [
        "# Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7COIdSRHZww2"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPco5igPEJfC"
      },
      "source": [
        "\"\"\" All model config \"\"\"\n",
        "\n",
        "flags = {\n",
        "\n",
        "    'vnlib': 'underthesea',  # underthesea, vncorenlp (code bên dưới chỉ dùng được cho underthesea, thư viện này có vẻ tốt hơn vncorenlp)\n",
        "\n",
        "\t'model_save_folder': 'rec_save_model',\n",
        "\n",
        "    'use_phobert': True,   # True, False   (có dùng phobert hay không)\n",
        "    'phobert_model': 'phobert_base', # phobert_base, phobert_large\n",
        "    'pb_entity_handle_type': 'average_pooling',       # max_pooling, average_pooling, sum, random.  lấy max hay average pooling, ... word_piece\n",
        "    'pb_emb_layer_lst': [2,3,4,5,6,7,8,9,10,11,12,13],             # danh sách các layer mà ta sẽ lấy embedding. từ 1 -> 13 cho phobert_base, 1 -> 25 cho phobert_large. \n",
        "                                          # layer số 1 là initial embedding\n",
        "    'pb_emb_layer_handle_type': 'sum',   # sum, concat, max_pooling, average_pooling. cách ta xử lý các layer mà ta lấy embedding. \n",
        "                                          # như concat hay lấy sum 4 layer cuối của 1 word_piece\n",
        "    'pb_combine_ents_rule': ['ent_1', 'ent_2', 'mul', 'add', 'abs_sub'], # [h_s, h_t, h_s*h_t, h_s+h_t, |h_s-h_t|]\n",
        "                                                                         # curent not implemented, just use case above.\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t # nếu sau này đổi mà khiến số lượng element của list thay đổi (khác 5)\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t # thì cần phải cập nhật chiều dài trong calculate_len_sent_embedding\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t # sent_emb_len = entity_emb_len * 5\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t # đổi 5 thành chiều dài mới của list.\n",
        "\n",
        "\n",
        "\n",
        "    'use_xlmr': True,   # True, False   (có dùng XLM-RoBERTa hay không)\n",
        "    'xlmr_model': 'xlmr_large', # xlmr_base, xlmr_large\n",
        "    'xlmr_entity_handle_type': 'average_pooling',       # max_pooling, average_pooling, sum, random.  lấy max hay average pooling, ... word_piece\n",
        "    'xlmr_emb_layer_lst': [22,23,24,25],             # danh sách các layer mà ta sẽ lấy embedding. từ 1 -> 13 cho xlmr_base, 1 -> 25 cho xlmr_large. \n",
        "                                            # layer số 1 là initial embedding\n",
        "    'xlmr_emb_layer_handle_type': 'sum',   # sum, concat, max_pooling, average_pooling. cách ta xử lý các layer mà ta lấy embedding. \n",
        "                                            # như concat hay lấy sum 4 layer cuối của 1 word_piece\n",
        "    'xlmr_combine_ents_rule': ['ent_1', 'ent_2', 'mul', 'add', 'abs_sub'], # [h_s, h_t, h_s*h_t, h_s+h_t, |h_s-h_t|]\n",
        "                                                                           # curent not implemented, just use case above.\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\t\n",
        "    'total_epochs': 100,       # \n",
        "\t\n",
        "\t# we will huggingface adamw as optimizer for phobert and xlmr\n",
        "    'phobert_num_epochs': 4,    # num epoch of fintune phobert. after this epoch, we will freeze phobert layer. 0 mean no finetune.\n",
        "    'phobert_lr': 0.001,\n",
        "\t\n",
        "\t# chắc k chỉnh mấy cái dưới cơ mà cứ thêm để có gì chỉnh đỡ phải sửa vào code\n",
        "\t'phobert_weight_decay': 0,\n",
        "\t'phobert_betas': (0.9, 0.999),\n",
        "\t'phobert_eps': 1e-6,\n",
        "\t\n",
        "\t\n",
        "\t\n",
        "\t'xlmr_num_epochs': 0,    # num epoch of fintune xlmr. after this epoch, we will freeze xlmr layer. 0 mean no finetune.\n",
        "    'xlmr_lr': 0.001,\n",
        "\t\n",
        "\t# chắc k chỉnh mấy cái dưới cơ mà cứ thêm để có gì chỉnh đỡ phải sửa vào code\n",
        "\t'xlmr_weight_decay': 0,\n",
        "\t'xlmr_betas': (0.9, 0.999),\n",
        "\t'xlmr_eps': 1e-6,\n",
        "\t\n",
        "\t\n",
        "\t\n",
        "\t'batch_size': 32,        # 8, 16, 64\n",
        "\t\n",
        "\t'dropout1_rate': 0.65,\n",
        "\t'out_linear1': 1024,\n",
        "\t'dropout2_rate': 0.25,\n",
        "\n",
        "\t'clip_grad_norm_rate': 5.5,\n",
        "\t\n",
        "\t# linear\n",
        "\t'linear_lr': 0.00001,\n",
        "\t'linear_lr_schedule_epoch': 5,\n",
        "\t'linear_lr_schedule_rate': 0.9,\n",
        "\t\n",
        "\t# chắc k chỉnh mấy cái dưới cơ mà cứ thêm để có gì chỉnh đỡ phải sửa vào code\n",
        "\t'linear_weight_decay': 0.5,\n",
        "\t'linear_betas': (0.9, 0.999),\n",
        "\t'linear_eps': 1e-6,\n",
        "\t\n",
        "\n",
        "    'seed': 6,\n",
        "\t'log_batch': 30,\n",
        "\n",
        "\n",
        "\n",
        "    'num_workers': 8         # Number of tpu core. Colab has TPU V2-8 which have 8 cores.\n",
        "\n",
        "\n",
        "\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb5kMJ-VC2Iz"
      },
      "source": [
        "## Import, turn on TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLGpzWTJMY7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2392bc9-98d6-4550-e506-0aeb74a1ecd1"
      },
      "source": [
        "import os, time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, Sampler, TensorDataset, random_split\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer, XLMRobertaModel, XLMRobertaTokenizer, AdamW\n",
        "\n",
        "'''\n",
        "# TPU\n",
        "# imports the torch_xla package\n",
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# TPU\\n# imports the torch_xla package\\nimport torch\\nimport torch_xla\\nimport torch_xla.core.xla_model as xm\\nimport torch_xla.distributed.xla_multiprocessing as xmp\\n\\nimport torch_xla.distributed.parallel_loader as pl\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OWIkCz9aYyV"
      },
      "source": [
        "## Generate model and tokenier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "3b4914b965714f99af3c84f41c557e66",
            "1a2dbf0a41084d14b21517d7f7306b73",
            "1389102cb5f14ae9b9af203ec5039e9e",
            "1b1da9e4470c44c0a21ab79ed89556ed",
            "c3e8916e15084fe0bd0dd96504ec6a3b",
            "2109fc3dac3646b28bb9020f18e81964",
            "0ad3661d994a4f99bd823bca42b71c1b",
            "c07e7b5d6d9140c98bbf0efd578d6783",
            "7f6f1a73374b4ec689b24732c38d5a2e",
            "692521ef3c59480da0a1fc822e37aecf",
            "b2c1f316dead466bb0b2a355c171a095",
            "a81aa834510f47bda6cf9eb148352227",
            "2c06913a935c42528c255546a26c2f05",
            "6dbc2545ee13463095f9ab150b46ab29",
            "4f7c1a5ce5bb44f2989918102982a5cd",
            "9713508a9e204469929c3e139dd413c5",
            "66522d14d090439bbe2feb6a1292a554",
            "d91789c4a44341509837c3b85084b75a",
            "09af6a98c4b547d8af96651865c54036",
            "01773a58409e487e94a8133e1b3a8078",
            "bb75464a31434356ba8e753e568455f8",
            "b7d9f7eba2b8449caff7635cdbefc9df",
            "325916b51bb1482d9b0d57b471e80731",
            "c6fe7852aa334d2586e93d3b523a9bfa",
            "c786160f313442fba41d1a1e14161294",
            "37c74f19e47840ae914fdcb88b52b500",
            "634d88480adc4ae48146d0e4f4b2b716",
            "958076a125154442ab3ac83281bad961",
            "efb71f459f614454951fe40689ab69b6",
            "77b601f2cb4b441d80925a50f9f7eb72",
            "499c8787b8be463d9e0ec3587e99a3fa",
            "0fa2070b54a142fd9b7f53044e360f57",
            "e80b886cea5e4c9db3379578295e0cb1",
            "84b5ddeeb4ef458c811dbdb9ef883e50",
            "9a7527ed660f4825a124ff3fdceb2efc",
            "ccf88d2944a94cae9d6c458a6f698cf3",
            "235ff5f78c4f40a1bde12cc78fa1ae1b",
            "a908a9cc874b429eb062f05a716102e1",
            "5215bca1d1614145aee3902a15a03b83",
            "ba0bdf6881a64ea4adee7d98006719bd",
            "2ef2129e42764d3e8428aa6809b62cd5",
            "65a27361ab054ed9a77327bf9c386500",
            "68e1612d2fa244dd9be0335c51eb345b",
            "5e7ef50b4b294fba9c647f84532d9e3a",
            "b97cd71315c24580997aabcae6ee8e92",
            "07c685d035824b42b947b3612c359ab3",
            "00d577cb2e584112beb93888e60d02da",
            "878a6a271e9b42f59377487a8a9224ba",
            "cd60906f1baf464c8274b68700d21c1f",
            "795ac19284be47fbb287aa9f41d38cc8",
            "ce82f626aaab41ce9f2ea3b4fdd4569d",
            "4c7500d70a564b7b916accf239b5dd0a",
            "a1ebb8452e0c4786940aafcb890603b6",
            "1eb2b370959f4f3eaf9aecf5b123dfa6",
            "16ea42b255434efda55d312216fbcf0b",
            "37f057d05e1a4c8895ada124e454bd5f",
            "be06c71b047946f0badc4bfb917c0fa3",
            "6299a9ef76244965af5d0a6322514ede",
            "9e46b6e3b9a74d6c883d10bd06028503",
            "961fecb7da8c4b79a656a518be131d8b",
            "b80f9cdbdee44c66804e8d35f6da4084",
            "7475d11c72f943e6b42bf2ed3c7762b2",
            "eac68e038ea64c26b4ed2b8a3dce6277",
            "1c7e860bce3d48d5a02a8e37f71fef31"
          ]
        },
        "id": "bozPQIurTSii",
        "outputId": "3f662a73-6ec8-4495-8b24-a6f276046281"
      },
      "source": [
        "#pb_base_model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "pb_base_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "#pb_large_model = AutoModel.from_pretrained(\"vinai/phobert-large\")\n",
        "pb_large_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-large\")\n",
        "\n",
        "#xlmr_base_model = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
        "xlmr_base_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "#xlmr_large_model = XLMRobertaModel.from_pretrained('xlm-roberta-large')\n",
        "xlmr_large_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b4914b965714f99af3c84f41c557e66",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=557.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f6f1a73374b4ec689b24732c38d5a2e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=895321.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66522d14d090439bbe2feb6a1292a554",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1135173.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c786160f313442fba41d1a1e14161294",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=558.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e80b886cea5e4c9db3379578295e0cb1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=895321.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ef2129e42764d3e8428aa6809b62cd5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1135173.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd60906f1baf464c8274b68700d21c1f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be06c71b047946f0badc4bfb917c0fa3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5gepmeCzx1H",
        "outputId": "bbfe8468-ea4f-461b-abaa-db426594d150"
      },
      "source": [
        "'''\n",
        "if flags['use_phobert'] == True:\n",
        "    if flags['phobert_model'] == 'phobert_base':\n",
        "        print('Using phobert_base.')\n",
        "        pb_model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "        pb_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "    elif flags['phobert_model'] == 'phobert_large':\n",
        "        print('Using phobert_large.')\n",
        "        pb_model = AutoModel.from_pretrained(\"vinai/phobert-large\")\n",
        "        pb_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-large\")\n",
        "\n",
        "    else:\n",
        "        assert False, str('Unknown phobert model: ' + flags['phobert_model'] + '. Allowed: phobert_base and phobert_large')\n",
        "\n",
        "if flags['use_xlmr'] == True:\n",
        "    if flags['xlmr_model'] == 'xlmr_base':\n",
        "        print('Using xlmr_base.')\n",
        "        xlmr_model = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
        "        xlmr_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "    elif flags['xlmr_model'] == 'xlmr_large':\n",
        "        print('Using xlmr_large.')\n",
        "        xlmr_model = XLMRobertaModel.from_pretrained('xlm-roberta-large')\n",
        "        xlmr_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n",
        "\n",
        "    else:\n",
        "        assert False, str('Unknown phobert model: ' + flags['xlmr_model'] + '. Allowed: xlmr_base and xlmr_large')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nif flags[\\'use_phobert\\'] == True:\\n    if flags[\\'phobert_model\\'] == \\'phobert_base\\':\\n        print(\\'Using phobert_base.\\')\\n        pb_model = AutoModel.from_pretrained(\"vinai/phobert-base\")\\n        pb_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\\n\\n    elif flags[\\'phobert_model\\'] == \\'phobert_large\\':\\n        print(\\'Using phobert_large.\\')\\n        pb_model = AutoModel.from_pretrained(\"vinai/phobert-large\")\\n        pb_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-large\")\\n\\n    else:\\n        assert False, str(\\'Unknown phobert model: \\' + flags[\\'phobert_model\\'] + \\'. Allowed: phobert_base and phobert_large\\')\\n\\nif flags[\\'use_xlmr\\'] == True:\\n    if flags[\\'xlmr_model\\'] == \\'xlmr_base\\':\\n        print(\\'Using xlmr_base.\\')\\n        xlmr_model = XLMRobertaModel.from_pretrained(\\'xlm-roberta-base\\')\\n        xlmr_tokenizer = XLMRobertaTokenizer.from_pretrained(\\'xlm-roberta-base\\')\\n\\n    elif flags[\\'xlmr_model\\'] == \\'xlmr_large\\':\\n        print(\\'Using xlmr_large.\\')\\n        xlmr_model = XLMRobertaModel.from_pretrained(\\'xlm-roberta-large\\')\\n        xlmr_tokenizer = XLMRobertaTokenizer.from_pretrained(\\'xlm-roberta-large\\')\\n\\n    else:\\n        assert False, str(\\'Unknown phobert model: \\' + flags[\\'xlmr_model\\'] + \\'. Allowed: xlmr_base and xlmr_large\\')\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMsPwfmbCx_v"
      },
      "source": [
        "## GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia9OejTRI52-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9555de9d-c35f-4778-bf6b-41e55bbef082"
      },
      "source": [
        "\n",
        "# GPU\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IADdZhElb4D4"
      },
      "source": [
        "## Create model input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DVZCoLqGFNh"
      },
      "source": [
        "Một điểm cần chú ý:\n",
        "- PhoBERT cần câu input được word tokenize sẵn (dùng VNCoreNLP hoặc Underthesea). Có vẻ là để tạo N-gram\n",
        "- XLMR thì không cần, ta cứ truyền trực tiếp câu thô vào làm input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc2MsmAt4zDF"
      },
      "source": [
        "### Get entity's word_tokenize index (PhoBERT need this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWb98FjG46cV"
      },
      "source": [
        "#### Readme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyVtt9BSy2ji"
      },
      "source": [
        "Một entity có thể có nhiều từ, và các từ này lại được tokenize thành nhiều word_piece. Nên một entity có thể có rất nhiều vector word_piece. Ta có thể chọn đại diện 1 vector để làm đại diện cho entity, hoặc là pooling các vector word_piece để ra 1 vector đại diện duy nhất cho entity.\n",
        "\n",
        "Để có thể pooling thì ta cần biết các word_piece nào thuộc entity. hay nói cách khác là ta cần lấy được index của các entity word_piece trong danh sách word_piece của cả câu.\n",
        "\n",
        "\n",
        "**Với Phobert**:\n",
        "- Do đầu vào cần là câu được Underthesea nên xảy ra vấn đề là word_tokenize của Underthesea cho ra kết quả không khớp với NER của entity trong dữ liệu. Ví dụ:\n",
        "  + \"Hôm nay, là ngày Quốc Tế Lao Động.\"\n",
        "  + entity: \"Quốc Tế Lao Động\"\n",
        "  + word_tokenize: [\"Hôm nay\", \"là\", \"ngày Quốc Tế\", \"Lao Động\"]\n",
        "  + tức là entity lại word_tokenize ra kết quả không trùng với entity. Nếu mà ra là: [\"Hôm nay\", \"là\", \"ngày\", \"Quốc Tế\", \"Lao Động\"] thì đẹp.\n",
        "  + Trên chỉ là một ví dụ, còn nhiều trường hợp Underthesea ra xấu hơn và phi lý vô cùng khi đọc.\n",
        "  + entity: B C D\n",
        "  + nhưng underthesea có thể ra là: [..., 'A B C D E F', ...], [..., 'A B', 'C', 'D', ...], [..., 'A B', 'C', 'D E F K'], ...\n",
        "- Nhưng tổng quát vấn đề này có các khả năng:\n",
        "  + Đầu entity trùng đầu 1 token. [..., 'B C', 'D E F',...]\n",
        "  + Đầu entity là cuối 1 token:  [..., 'A B C', 'D',...]\n",
        "  \n",
        "  + Cuối entity trùng cuối 1 token. [..., 'A B C', 'D',...]\n",
        "  + Cuối entity là đầu 1 token:  [..., 'B C', 'D E F',...]\n",
        "\n",
        "\n",
        "- Ý tưởng: biến [\"Hôm nay\", \"là\", \"ngày Quốc Tế\", \"Lao Động\"] thành [\"Hôm nay\", \"là\", \"ngày\", \"Quốc Tế\", \"Lao Động\"]. Tức là nếu đầu hay cuối entity bị lẫn, là một phần con trong một token thì ta sẽ cắt token đấy ra. Tức là ta sẽ tạo ra phiên bản word_tokenize mới dựa trên word_tokenize của Underthesea\n",
        "- Nên để lấy index của PhoBERT cần qua 2 bước:\n",
        "  + tạo word_tokenize mới và lấy index token trong entity trong word_tokenize này\n",
        "  + lấy index của word_piece các token trong câu word_tokenize mới.\n",
        "  + entity -> token -> word_piece : index_word_piece -> token <-> index token -> entity\n",
        "- ta bắt buộc phải trải qua 2 bước trên vì ta không thể tách token dựa trên word_piece của phobert được.ta phải tách trước thì word_piece của Phobert mới đẹp theo.\n",
        "\n",
        "\n",
        "Với **XLMR BERT**:\n",
        "- thì mọi thứ đơn giản hơn, ta chỉ việc fed câu gốc vào mà không cần qua Underthesea nên dễ dàng hơn không cần sửa trước như trên.\n",
        "- Nên để lấy index của XLMR thì ta chỉ việc từ index word_piece ra thẳng entity mà thôi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKLlYYYY4-r4"
      },
      "source": [
        "#### Func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anngy6FArdp1"
      },
      "source": [
        "def my_word_tokenize(word_tokenize_lst, tokens_non_space_pos_lst, split_token_index, split_pos, sentence):\n",
        "\n",
        "    '''\n",
        "    word_tokenize_lst: word_tokenize_lst cần được sửa\n",
        "    tokens_non_space_pos_lst: vị trí pos indice của mọi token trong word_tokenize_lst đối với câu không có space\n",
        "    split_token_index: index của token cần được tách trong word_tokenize_lst\n",
        "    sentence: sentence gốc có space\n",
        "    split_pos: vị trí (NON SPACE) đối với câu (tại vị trí này ta sẽ tách token cần tách thành 2 phần, từ đầu token tới vị trí này,\n",
        "    và từ vị trí này đến hết token)\n",
        "    vị trí này sẽ nằm giữa đầu vs cuối của token)\n",
        "    ..... vitri_dau_token  vitri_tach_token   vitri_cuoi_token ...\n",
        "    '''\n",
        "\n",
        "    new_word_tokenize_lst = []\n",
        "    new_tokens_non_space_pos_lst = []\n",
        "\n",
        "    for itk, tk in enumerate(word_tokenize_lst):\n",
        "\n",
        "        # nếu không phải token cần tách thì cứ thêm vào list word_tokenize mới\n",
        "        if itk != split_token_index:\n",
        "            # in 'else' part below, I create same (duplicate) variable (name) for debugging easier\n",
        "            sent_non_space = ''.join(sentence.split())   # remove all space from original sentence\n",
        "            token_non_space = ''.join(tk.split())  # remove all space from current tk\n",
        "            token_non_space_pos = tokens_non_space_pos_lst[itk]\n",
        "\n",
        "            assert (sent_non_space[token_non_space_pos[0]:token_non_space_pos[1]] == token_non_space), \\\n",
        "            str('Token_non_space not match with non_space_pos.')\n",
        "                \n",
        "\n",
        "            new_word_tokenize_lst.append(tk)  # tk with space\n",
        "            new_tokens_non_space_pos_lst.append(copy.deepcopy(tokens_non_space_pos_lst[itk]))  # non space pos\n",
        "\n",
        "        \n",
        "        \n",
        "        else:   # token cần tách\n",
        "            # token: [A B C D] [E F], entity: C D E F\n",
        "            # ...      C D   E F  ...\n",
        "            # ... [A B C D] [E F] ...\n",
        "\n",
        "            #     C D E F\n",
        "            # A B C D E F\n",
        "\n",
        "            #   CDEF\n",
        "            # ABCDEF\n",
        "\n",
        "            token_space = copy.deepcopy(word_tokenize_lst[split_token_index])   # A B C D\n",
        "\n",
        "            sent_non_space = ''.join(sentence.split())   # ...ABCDEF...\n",
        "            token_non_space = ''.join(token_space.split())  # ABCD\n",
        "\n",
        "            token_non_space_pos = tokens_non_space_pos_lst[split_token_index]  # <----- pos trong sent_non_space\n",
        "                                                                               # pos ABCD trong ...ABCDEF...\n",
        "\n",
        "            # just a check\n",
        "            assert (sent_non_space[token_non_space_pos[0]:token_non_space_pos[1]] == token_non_space), \\\n",
        "            str('Token non space not match with token non space pos.')\n",
        "\n",
        "\n",
        "            token_head_non_space = sent_non_space[token_non_space_pos[0]:split_pos]   # AB\n",
        "            token_tail_non_space = sent_non_space[split_pos:token_non_space_pos[1]]   # CD\n",
        "\n",
        "            # just check even no necessary: AB+CD = ABCD\n",
        "            assert ((token_head_non_space + token_tail_non_space) == token_non_space), \\\n",
        "            str('AB+CD != ABCD')  # -.- so tired\n",
        "\n",
        "\n",
        "            subtoken_lst = token_space.split() # ['A', 'B', 'C', 'D']\n",
        "\n",
        "            \n",
        "            num_space_in_token_space = 0\n",
        "            for ctk in token_space:\n",
        "                if ctk.isspace():\n",
        "                    num_space_in_token_space += 1\n",
        "            \n",
        "\n",
        "            # 'A B C D'  ->  4 - 1 = 3 space \n",
        "            # Note: Nếu mà không bằng chính tỏ là giữa hai từ đơn trong một token (cụm từ) không chỉ có 1 dấu cách\n",
        "            # lúc này thì có thể tạo ra một list chứ số dấu cách trong token. \n",
        "            # 'A  B C   D'  ->  [2, 1, 3] \n",
        "            # giữa 'A  B' có 2 dấu cách. giữa 'B C' có 1 dấu cách. giữa 'C   D' có 3 cách. \n",
        "            assert (len(subtoken_lst) == (num_space_in_token_space + 1)), \\\n",
        "            str('Seem that underthesea word tokenize does not contain only one space between words')\n",
        "\n",
        "\n",
        "\n",
        "            sub_token_head_lst = []\n",
        "\n",
        "            for subtoken in subtoken_lst:          \n",
        "                sub_token_head_lst.append(subtoken)  # iter:  0        1\n",
        "                                                     #        ['A'] -> ['A', 'B']\n",
        "\n",
        "                if ''.join(sub_token_head_lst) == token_head_non_space:    # ''.join['A', 'B'] == 'AB'\n",
        "                    break\n",
        "                \n",
        "            # [A, B] < [A, B, C, D]   (nếu có LỖI tức là k chạy vào break ở vòng for trên thì sub_token_head_lst sẽ giống hệt subtoken_lst)\n",
        "            assert (len(sub_token_head_lst) < len(subtoken_lst)), str('[A, B] not < [A, B, C, D]')\n",
        "\n",
        "            # [A, B, C, D] - [A, B] = [C, D]\n",
        "            sub_token_tail_lst = subtoken_lst[len(sub_token_head_lst):]   # [C, D]\n",
        "\n",
        "            # ''.join(['C', 'D']) == 'CD'\n",
        "            assert (''.join(sub_token_tail_lst) == token_tail_non_space), str(\"''.join(['C', 'D']) != 'CD'\")\n",
        "\n",
        "            # just anotherrrrr checkkkkkkkkkkkkkkkkkkkkkkkk\n",
        "            # chỉ đúng nếu giữa các từ trong một token có đúng 1 dấu cách. nếu có nhiều hơn 1 thì phải dùng lst space như NOTE bên trên\n",
        "            assert (str(' '.join(sub_token_head_lst) + ' ' + ' '.join(sub_token_tail_lst)) == token_space), \\\n",
        "            str(\"' '.join(['A', 'B']) + ' ' + ' '.join(['C', 'D']) != 'A B C D'\")\n",
        "\n",
        "            # append 'A B'\n",
        "            new_word_tokenize_lst.append(' '.join(sub_token_head_lst))\n",
        "\n",
        "            token_head_non_space_pos_start = token_non_space_pos[0]\n",
        "            token_head_non_space_pos_end = token_head_non_space_pos_start + sum([len(tmp) for tmp in sub_token_head_lst])\n",
        "\n",
        "            # just check even maybe not necessary\n",
        "            assert (token_head_non_space_pos_end == split_pos), str('token_head end pos not equal to split pos.')\n",
        "            assert (sent_non_space[token_head_non_space_pos_start:token_head_non_space_pos_end] == token_head_non_space), \\\n",
        "            str('token_head_non_space not match with pos found in sent_no_space.')\n",
        "\n",
        "            new_tokens_non_space_pos_lst.append([token_head_non_space_pos_start, token_head_non_space_pos_end])\n",
        "\n",
        "                \n",
        "            # append 'C D'\n",
        "            new_word_tokenize_lst.append(' '.join(sub_token_tail_lst))\n",
        "\n",
        "            token_tail_non_space_pos_start = split_pos\n",
        "            token_tail_non_space_pos_end = token_tail_non_space_pos_start + sum([len(tmp) for tmp in sub_token_tail_lst]) # = token_non_space_pos[1]\n",
        "\n",
        "            # just check even maybe not necessary\n",
        "            assert (token_tail_non_space_pos_end == token_non_space_pos[1]), str('token_tail end pos not equal to originial token_non_space end pos.')\n",
        "            assert (sent_non_space[token_tail_non_space_pos_start:token_tail_non_space_pos_end] == token_tail_non_space), \\\n",
        "            str('token_tail_non_space not match with pos found in sent_no_space.')\n",
        "\n",
        "            new_tokens_non_space_pos_lst.append([token_tail_non_space_pos_start, token_tail_non_space_pos_end])\n",
        "                \n",
        "\n",
        "    return new_word_tokenize_lst, new_tokens_non_space_pos_lst\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCBFXtDOcytv"
      },
      "source": [
        "def get_entity_index_in_underthesea_word_tokenize(sent_id, sentence, word_tokenize_lst, entity):\n",
        "    \n",
        "    entity_text = entity['text']\n",
        "    entity_pos = entity['pos']\n",
        "\n",
        "    ##### CHECK\n",
        "\n",
        "    assert (sentence[entity_pos[0]:entity_pos[1]] == entity_text), str('\\nEntity_pos not match entity. Entity: ' + entity + '\\nsent-id: ' + str(sent_id))\n",
        "\n",
        "    assert (type(word_tokenize_lst) is list), str('word_tokenize_lst must be a list. Use Underthesea\\'s word_tokenize with out format=\"text\" option.')\n",
        "\n",
        "    # vì ta sẽ không tính đến dấu cách nên nếu kí tự đầu hoặc cuối entity là khoảng cách thì sẽ có vấn đề\n",
        "    # tức là vì không tính đến khoảng cách nên nếu mà start pos hay end pos của entity là vị trí khoảng trắng thừa thì sẽ không bao giờ match được\n",
        "    # bên trên trong phần clean data đã loại bỏ rồi nhưng cứ check lại cho chắc\n",
        "    if (entity_text[0].isspace()) or (entity_text[-1].isspace()):\n",
        "        assert False, str('\\nFirst or last character in entity is space. Entity: ' + repr(entity) + '\\nsent-id: ' + str(sent_id))\n",
        "        #print(str('\\nFirst or last character in entity is space. Entity: ' + repr(entity) + '\\nsent-id: ' + str(sent_id)))\n",
        "    \n",
        "    ##### END CHECK\n",
        "\n",
        "\n",
        "    # đếm khoảng trắng từ đầu câu đến entity start_pos\n",
        "    count_to_estart = 0\n",
        "    for c in sentence[:entity_pos[0]]:\n",
        "        if c.isspace():\n",
        "            count_to_estart += 1\n",
        "    new_estart_pos = entity_pos[0] - count_to_estart\n",
        "\n",
        "    # đếm khoảng trắng từ đầu câu đến entity end_pos\n",
        "    count_to_eend = 0\n",
        "    for c in sentence[:entity_pos[1]]:\n",
        "        if c.isspace():\n",
        "            count_to_eend += 1\n",
        "    new_eend_pos = entity_pos[1] - count_to_eend\n",
        "\n",
        "    # đếm khoảng trắng trong entity\n",
        "    '''\n",
        "    count_in_e = 0\n",
        "    for i in entity:\n",
        "        if i.isspace():\n",
        "            count_in_e += 1\n",
        "    '''\n",
        "\n",
        "    sent_no_space = ''.join(sentence.split())\n",
        "    entity_text_no_space = ''.join(entity_text.split())\n",
        "\n",
        "    assert (sent_no_space[new_estart_pos:new_eend_pos] == entity_text_no_space), \\\n",
        "    str('\\nEntity_text without space not match entity_pos without space. ' + sent_no_space + ' ' + entity_text_no_space)\n",
        "\n",
        "    #\n",
        "    pre_tkend_pos = 0\n",
        "\n",
        "    found_start = False\n",
        "    found_end = False\n",
        "\n",
        "    start_index = None\n",
        "    end_index = None\n",
        "    \n",
        "    \n",
        "    sent_non_space = ''.join(sentence.split())   # <----- câu không có khoảng trắng\n",
        "\n",
        "    tokens_non_space_pos_lst = []   # <----- pos token không khoảng trắng trong câu không có khoảng trắng\n",
        "\n",
        "\n",
        "    for itk, tk in enumerate(word_tokenize_lst):\n",
        "        # tìm pos của từng token trong câu segment\n",
        "        tkstart_pos = pre_tkend_pos   # do không tính khoảng trắng nên end của 1 token sẽ là start của token ngay sau nó (nếu có)\n",
        "        tkend_pos = tkstart_pos + sum([len(tmp) for tmp in tk.split()])\n",
        "\n",
        "        token_non_space = ''.join(tk.split())\n",
        "\n",
        "        assert (sent_non_space[tkstart_pos:tkend_pos] == token_non_space), \\\n",
        "        str('Token non space not match with token non space pos.')\n",
        "\n",
        "\n",
        "        tokens_non_space_pos_lst.append([tkstart_pos, tkend_pos])\n",
        "\n",
        "        # cập nhật pre_tkend_pos\n",
        "        pre_tkend_pos = tkend_pos\n",
        "\n",
        "\n",
        "        if tkstart_pos == new_estart_pos:\n",
        "            found_start = True\n",
        "            start_index = copy.deepcopy(itk)\n",
        "\n",
        "        if tkend_pos == new_eend_pos:\n",
        "            found_end = True\n",
        "            end_index = copy.deepcopy(itk)\n",
        "\n",
        "        #  token này sẽ phải chứa đoạn đầu của entity\n",
        "        #  ví dụ: \n",
        "        #  token: ... trưởng_phòng Cảnh_sát   ...\n",
        "        # entity:     ...    phòng Cảnh sát   ...\n",
        "        if (tkstart_pos < new_estart_pos) and (new_estart_pos < tkend_pos):\n",
        "            start_index = copy.deepcopy(itk)    # <----- có start_index nhưng found_start vẫn là false\n",
        "\n",
        "\n",
        "        #  token này sẽ phải chứa đoạn đầu của entity\n",
        "        #  ví dụ: \n",
        "        #  token: ... Nguyễn_Hiền_đỗ ...   <-----   lỗi có thật khi dùng underthesea word_tokenize\n",
        "        # entity: ... Nguyễn Hiền    ...\n",
        "        if (tkstart_pos < new_eend_pos) and (new_eend_pos < tkend_pos):\n",
        "            end_index = copy.deepcopy(itk)    # <----- có end_index nhưng found_end vẫn là false\n",
        "\n",
        "    \n",
        "    entity_eids_lst = []\n",
        "    new_word_tokenize_lst = []\n",
        "    new_tokens_non_space_pos_lst = []\n",
        "\n",
        "    if (found_start == True):\n",
        "        new_word_tokenize_lst = copy.deepcopy(word_tokenize_lst)\n",
        "        new_tokens_non_space_pos_lst = copy.deepcopy(tokens_non_space_pos_lst)\n",
        "\n",
        "    if found_start == False:\n",
        "        \n",
        "        # ta sẽ cắt token chứa phần đầu entity ra thành 2 token\n",
        "        #  token: ... [trưởng phòng] [Cảnh sát]   ...\n",
        "        # entity:     ...     phòng Cảnh sát   ... \n",
        "        #  token: ... [trưởng] [phòng] [Cảnh sát]   ...\n",
        "        # entity:     ...       phòng Cảnh sát   ... \n",
        "        \n",
        "\n",
        "        # Vì found_start = False nên phần đầu entity (một từ hoặc một vài từ đầu trong entity) sẽ nằm trong token tại vị trí start_index\n",
        "        # và phần đầu entity sẽ là phần cuối token này. và phần đầu token này (một, hay vài từ khác) sẽ phải được ngăn cách với\n",
        "        # phần đầu entity bằng dấu cách. \n",
        "        #  token:     |          |\n",
        "        #  token: ... trưởng phòng Cảnh sát   ...\n",
        "        # entity:     ...    phòng Cảnh sát   ... \n",
        "        #             |    | |   |\n",
        "        # nếu mà không có dấu cách trong token nghĩa là kiểu lỗi: token (phòng) entity (òng Cảng Sát)\n",
        "        \n",
        "        token_space = word_tokenize_lst[start_index]\n",
        "\n",
        "        num_space_in_token_space = 0\n",
        "        for ctk in token_space:\n",
        "            if ctk.isspace():\n",
        "                num_space_in_token_space += 1\n",
        "\n",
        "        assert (num_space_in_token_space > 0), \\\n",
        "        str('Found start FALSE. Token at start_index does not has space. token: ' + word_tokenize_lst[start_index] + ' entity: ' + entity_text)\n",
        "\n",
        "        new_word_tokenize_lst, new_tokens_non_space_pos_lst = \\\n",
        "        my_word_tokenize(word_tokenize_lst, tokens_non_space_pos_lst, start_index, new_estart_pos, sentence)\n",
        "        \n",
        "        # old_start_index: osi\n",
        "        #      osi      osi+1\n",
        "        # ... [A B C D] [E F]\n",
        "\n",
        "        # sau khi tách:\n",
        "        #      osi  osi+1 osi+2\n",
        "        # ... [A B] [C D] [E F]\n",
        "\n",
        "        start_index = start_index + 1  # osi + 1\n",
        "        end_index = end_index + 1\n",
        "\n",
        "\n",
        "                \n",
        "    if found_end == True:\n",
        "        # no need run two below commented line because we dont change anything\n",
        "        # new_word_tokenize_lst = copy.deepcopy(new_word_tokenize_lst)\n",
        "        # new_tokens_non_space_pos_lst = copy.deepcopy(new_tokens_non_space_pos_lst)\n",
        "\n",
        "        entity_eids_lst = list(range(start_index, (end_index+1)))\n",
        "\n",
        "\n",
        "    if found_end == False:\n",
        "\n",
        "        new_word_tokenize_lst, new_tokens_non_space_pos_lst = \\\n",
        "        my_word_tokenize(copy.deepcopy(new_word_tokenize_lst), copy.deepcopy(new_tokens_non_space_pos_lst), end_index, new_eend_pos, sentence)\n",
        "\n",
        "        entity_eids_lst = list(range(start_index, (end_index+1)))\n",
        "\n",
        "\n",
        "\n",
        "    entity_text_no_space = ''.join(entity_text.split()) # <- tren co roi nhung ke cu tao lai cho de theo doi\n",
        "    entity_subtoken_lst = new_word_tokenize_lst[entity_eids_lst[0]:(entity_eids_lst[-1]+1)]\n",
        "\n",
        "    entity_singleword_lst = []\n",
        "    for entity_subtoken in entity_subtoken_lst:\n",
        "        entity_singleword_lst.extend(entity_subtoken.split())\n",
        "\n",
        "    assert (entity_text_no_space == ''.join(entity_singleword_lst)), \\\n",
        "    str('FOUND ENTITY INDEX NOT MATCH WITH ENTITY TEXT')\n",
        "\n",
        "    '''\n",
        "    # k check cai nay. vi split co the khac do co dau cau nhu / hay ,\n",
        "    # DOUBLE CHECKKKKKKKKKK\n",
        "    assert (entity_singleword_lst == entity_text.split()), \\\n",
        "    str('FOUND ENTITY INDEX NOT MATCH WITH ENTITY TEXT')\n",
        "    '''\n",
        "\n",
        "    assert (sent_non_space == ''.join([itm.replace(' ', '') for itm in new_word_tokenize_lst])), \\\n",
        "    str('sent_non_space not match new_word_tokenize_lst')\n",
        "\n",
        "    assert (new_tokens_non_space_pos_lst[entity_eids_lst[0]][0] == new_estart_pos) \\\n",
        "    and (new_tokens_non_space_pos_lst[entity_eids_lst[-1]][1] == new_eend_pos), \\\n",
        "    str('TWO ENITY NON SPACE POS NOT MATCH.')\n",
        "\n",
        "    entity_pos_no_space = [new_estart_pos, new_eend_pos]\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return new_word_tokenize_lst, entity_eids_lst, entity_pos_no_space\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CuCMaFHwNuV"
      },
      "source": [
        "#### Add entity index and word_tokenize to data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6LFPE13ceDo"
      },
      "source": [
        "def add_word_tokenize_and_entity_index(jdata, train_or_dev, print_output='True'):\n",
        "\n",
        "    new_jdata = []\n",
        "\n",
        "    for sentif in jdata:\n",
        "\n",
        "        word_tokenize_lst = word_tokenize(sentif['new_sentence'])\n",
        "\n",
        "        # Có duy nhất 1 câu trong tập train, chứa token bên dưới, tức là có 1 trường hợp duy nhất mà 2 từ trong 1 token nối với nhau bằng - thay vì khoảng trắng\n",
        "        # và 'Tập' là một từ trong entity\n",
        "        # có thể code để chạy được cho trường hợp dấu - này, tuy nhiên, chỉ có 1 câu duy nhất bị. và còn chưa biết các khả năng có thể xảy khi từ\n",
        "        # được ngăn bởi dấu - thay vì khoảng trắng. nhỡ một phần entity lại là ẹ-Tập thay vì Tập thì sao\n",
        "        # nên để hiểu rõ hơn về dữ liệu chỉ sửa tạm như dưới, có nhiều hơn 1 thì mới tính đến thêm vào func bên trên\n",
        "        # bên dưới ta tạo word_tokenize_lst mới bằng tay để code trong func không lỗi \n",
        "        if ('mẹ-Tập' in word_tokenize_lst) and (train_or_dev == 'train'):\n",
        "            specia_index = word_tokenize_lst.index('mẹ-Tập')\n",
        "            word_tokenize_lst.insert((specia_index+1), 'Tập')\n",
        "            word_tokenize_lst[specia_index] = 'mẹ-'\n",
        "\n",
        "        \n",
        "\n",
        "        new_word_tokenize_lst_1, entity_1_eids_lst, entity_1_pos_no_space = \\\n",
        "        get_entity_index_in_underthesea_word_tokenize(sentif['sent_id'], sentif['new_sentence'], word_tokenize_lst, sentif['new_entity_1'])\n",
        "\n",
        "        new_word_tokenize_lst_2, entity_2_eids_lst, entity_2_pos_no_space = \\\n",
        "        get_entity_index_in_underthesea_word_tokenize(sentif['sent_id'], sentif['new_sentence'], copy.deepcopy(new_word_tokenize_lst_1), sentif['new_entity_2'])\n",
        "        \n",
        "\n",
        "        ######## HOT FIX\n",
        "        # trong câu có sent_id là 1216 , có cụm từ 'CĐ Sư phạm Mỹ thuật'\n",
        "        # trong new_word_tokenize_lst_2 thì sẽ là: 'CĐ Sư phạm', 'Mỹ thuật'\n",
        "        # có vẻ ổn, nhìn vào thì thấy k có vấn đề gì nhưng khi fed vào phobert thì CĐ_Sư_phạm phobert word piece thành 'CĐ', '_S', 'ư_phạm'\n",
        "        # dẫn tới việc 'ư_phạm' k có trong vocab và bị thành <UNK> và k tìm được index word piece cho entity này\n",
        "        # nên ta cần sửa lại trong new_word_tokenize_lst_2, biến 'CĐ Sư phạm' thành 'CĐ', 'Sư phạm' thì k bị lỗi nữa\n",
        "        # và cũng cần đổi entity wtk_index.\n",
        "        # chỉ duy nhất 1 câu bị lỗi này trong tập train nên ta chỉ cần tách thủ công\n",
        "\n",
        "        if (sentif['sent_id'] in list(range(4466, 4481))) and (train_or_dev == 'test'):\n",
        "            nfix_tk_id = new_word_tokenize_lst_2.index('Bành Sơn')\n",
        "\n",
        "            new_word_tokenize_lst_2.insert((nfix_tk_id+1), unicodedata.normalize(\"NFC\", 'Sơn'))\n",
        "            new_word_tokenize_lst_2[nfix_tk_id] = copy.deepcopy(unicodedata.normalize(\"NFC\", 'Bành'))\n",
        "\n",
        "            # chi co entity_2_eids_lst bi anh huong boi viec tach tren\n",
        "            entity_2_eids_lst.append(entity_2_eids_lst[-1] + 1)\n",
        "        \n",
        "        if (sentif['sent_id'] in list(range(4635, 4650))) and (train_or_dev == 'test'):\n",
        "            nfix_tk_id = new_word_tokenize_lst_2.index('Ma túy')\n",
        "\n",
        "            new_word_tokenize_lst_2.insert((nfix_tk_id+1), unicodedata.normalize(\"NFC\", 'túy'))\n",
        "            new_word_tokenize_lst_2[nfix_tk_id] = copy.deepcopy(unicodedata.normalize(\"NFC\", 'Ma'))\n",
        "\n",
        "            # chi co entity_2_eids_lst bi anh huong boi viec tach tren\n",
        "            entity_2_eids_lst.append(entity_2_eids_lst[-1] + 1)\n",
        "\n",
        "        ########\n",
        "\n",
        "\n",
        "\n",
        "        new_sentif = copy.deepcopy(sentif)\n",
        "\n",
        "        # from underthesea: https://github.com/undertheseanlp/underthesea/blob/master/underthesea/word_tokenize/__init__.py#L45\n",
        "        # new_sentif['word_tokenize_sentence'] = copy.deepcopy(u\" \".join([item.replace(\" \", \"_\") for item in new_word_tokenize_lst_2]))\n",
        "        new_sentif['word_tokenize_lst'] = copy.deepcopy(new_word_tokenize_lst_2)\n",
        "\n",
        "        new_sentif['new_entity_1']['wtk_index_lst'] = copy.deepcopy(entity_1_eids_lst)\n",
        "        new_sentif['new_entity_2']['wtk_index_lst'] = copy.deepcopy(entity_2_eids_lst)\n",
        "\n",
        "        new_sentif['new_entity_1']['pos_no_space'] = copy.deepcopy(entity_1_pos_no_space)\n",
        "        new_sentif['new_entity_2']['pos_no_space'] = copy.deepcopy(entity_2_pos_no_space)\n",
        "\n",
        "\n",
        "        new_jdata.append(copy.deepcopy(new_sentif))\n",
        "\n",
        "        # phần cũ vẫn phải y nguyên. bên trên chỉ thêm 'word_tokenize_sentence', và thêm 'wtk_index_lst' vào 'new_entity_1' và 'new_entity_2'\n",
        "        assert (new_jdata[-1]['sent_id'] == sentif['sent_id']), str('FAILED TO COPY sent_id. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['doc_id'] == sentif['doc_id']), str('FAILED TO COPY doc_id. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['sentence'] == sentif['sentence']), str('FAILED TO COPY sentence. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['entity_1'] == sentif['entity_1']), str('FAILED TO COPY entity_1. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['entity_2'] == sentif['entity_2']), str('FAILED TO COPY SENT_ID. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['label'] == sentif['label']), str('FAILED TO COPY SENT_ID. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['spos'] == sentif['spos']), str('FAILED TO COPY spos. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['new_sentence'] == sentif['new_sentence']), str('FAILED TO COPY new_sentence. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['new_entity_1']['text'] == sentif['new_entity_1']['text']), str('FAILED TO COPY new_entity_1 text. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['new_entity_2']['text'] == sentif['new_entity_2']['text']), str('FAILED TO COPY new_entity_2 text. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['new_entity_1']['pos'] == sentif['new_entity_1']['pos']), str('FAILED TO COPY new_entity_1 pos. sent_id: ' + sentif['sent_id'])\n",
        "        assert (new_jdata[-1]['new_entity_2']['pos'] == sentif['new_entity_2']['pos']), str('FAILED TO COPY new_entity_2 pos. sent_id: ' + sentif['sent_id'])\n",
        "        \n",
        "\n",
        "        assert (new_jdata[-1]['new_entity_1']['wtk_index_lst'] == entity_1_eids_lst), str('Fail to copy or add entity_1_eids_lst')\n",
        "        assert (new_jdata[-1]['new_entity_2']['wtk_index_lst'] == entity_2_eids_lst), str('Fail to copy or add entity_2_eids_lst')\n",
        "\n",
        "        if print_output == True:\n",
        "            if (new_word_tokenize_lst_1 != word_tokenize_lst) or (new_word_tokenize_lst_2 != word_tokenize_lst):\n",
        "\n",
        "                print('\\n\\n---------- sent_id: ', sentif['sent_id'])\n",
        "                print('sentence: ', sentif['sentence'])\n",
        "\n",
        "                if (new_word_tokenize_lst_1 != word_tokenize_lst):\n",
        "                    print('\\nentity: ', sentif['new_entity_1'])\n",
        "                    print('entity index list: ', entity_1_eids_lst)\n",
        "                    print(new_word_tokenize_lst_1[entity_1_eids_lst[0]:(entity_1_eids_lst[-1]+1)])\n",
        "                    print(entity_1_pos_no_space)\n",
        "                    print('Underthesea word_tokenize: ', word_tokenize_lst)\n",
        "                    print('My word_tokenize 1:        ', new_word_tokenize_lst_1)\n",
        "\n",
        "                if (new_word_tokenize_lst_2 != new_word_tokenize_lst_1):\n",
        "                    print('\\nentity: ', sentif['new_entity_2'])\n",
        "                    print('entity index list: ', entity_2_eids_lst)\n",
        "                    print(entity_2_pos_no_space)\n",
        "                    print(new_word_tokenize_lst_2[entity_2_eids_lst[0]:(entity_2_eids_lst[-1]+1)])\n",
        "                    print('My word_tokenize 1:        ', new_word_tokenize_lst_1)\n",
        "                    print('My word_tokenize 2:        ', new_word_tokenize_lst_2)\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    print('Done adding new_word_tokenize_lst and entity eids in new_word_tokenize_lst to jdata.')\n",
        "\n",
        "    return new_jdata\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pQSd9DT2nAR"
      },
      "source": [
        "### Get entity's wordpice index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRwH8NmK2tRp"
      },
      "source": [
        "def get_entity_word_piece_index(sent_id, bert_tokenize_lst, entity, sentence, model_name, input_ids, train_or_dev):\n",
        "\n",
        "    '''\n",
        "    bert_tokenize_lst là list thu đuọc từ tokenizer.tokenize(sentence)\n",
        "    tức là chưa có special_token. nên start, end index sẽ phải +1 vì sau này khi encode sẽ có thêm 1 special token ở đầu\n",
        "\n",
        "    '''\n",
        "\n",
        "    entity_pos_no_space = entity['pos_no_space']\n",
        "\n",
        "    sent_non_space = ''.join(sentence.split())\n",
        "\n",
        "    found_start, found_end = None, None\n",
        "    start_index, end_index = None, None\n",
        "\n",
        "    pre_wpiend_pos = 0\n",
        "\n",
        "    # có 1 vài câu có là kí tự bắt đầu câu, nhưng xlmr tokenize sẽ bỏ nó đi, k coi n là 1 token\n",
        "    # nên token đầu tiên trong xlmr tokenize là từ thứ 2 trong câu và bắt đầu từ pos là 1.\n",
        "    # phobert k bỏ cái này nên k cần sửa\n",
        "    if ('\\ufeff' in sent_non_space) and (model_name == 'xlmr_base') and (train_or_dev == 'test'):\n",
        "        assert ('\\ufeff' == sent_non_space[0]), str('\\\\ufeff in sentence but not the first character of sentence.')\n",
        "\n",
        "        pre_wpiend_pos = 1\n",
        "\n",
        "    if ('\\ufeff' in sent_non_space) and (model_name == 'xlmr_large') and (train_or_dev == 'test'):\n",
        "        assert ('\\ufeff' == sent_non_space[0]), str('\\\\ufeff in sentence but not the first character of sentence.')\n",
        "\n",
        "        pre_wpiend_pos = 1\n",
        "\n",
        "\n",
        "    if ('\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b' in sent_non_space) and (model_name == 'xlmr') and (train_or_dev == 'dev'):\n",
        "        assert (sent_non_space.count('\\u200b') == 7), str('sent_non_space count \\\\u200b not equal 7.')\n",
        "\n",
        "        pre_wpiend_pos = 7\n",
        "    \n",
        "    if ('\\ufeff' in sent_non_space) and (model_name == 'xlmr') and (train_or_dev == 'dev'):\n",
        "        assert ('\\ufeff' == sent_non_space[0]), str('\\\\ufeff in sentence but not the first character of sentence.')\n",
        "\n",
        "        pre_wpiend_pos = 1\n",
        "\n",
        "\n",
        "    for iwpi, wpi in enumerate(bert_tokenize_lst):\n",
        "\n",
        "        assert ((' ' not in wpi) and ('\\xa0' not in wpi)), str(' there is space in word piece.')\n",
        "\n",
        "        # phobert\n",
        "        clean_wpi = wpi.replace('_', '')\n",
        "        clean_wpi = clean_wpi.replace('@@', '')\n",
        "        # xlmr\n",
        "        clean_wpi = clean_wpi.replace('▁', '')\n",
        "\n",
        "        wpi_start_pos = copy.deepcopy(pre_wpiend_pos)\n",
        "        wpi_end_pos = wpi_start_pos + len(clean_wpi)\n",
        "\n",
        "        pre_wpiend_pos = copy.deepcopy(wpi_end_pos)\n",
        "\n",
        "        #assert (sent_non_space[wpi_start_pos:wpi_end_pos] == clean_wpi), str('Word_piece not match with non space pos.')\n",
        "\n",
        "        if sent_non_space[wpi_start_pos:wpi_end_pos] != clean_wpi:\n",
        "\n",
        "            # xlmr biến … thành ... khi tokenize nên cần sửa lại wpi_start_pos, wpi_end_pos\n",
        "            # và với cái wpi này thì chấp nhận sent_non_space[wpi_start_pos:wpi_end_pos] != clean_wpi\n",
        "            # tương tự với ½ thành 1⁄2\n",
        "            if (clean_wpi == '...') and (sent_non_space[wpi_start_pos] == '…') and (model_name == 'xlmr_base') and (train_or_dev == 'test'):\n",
        "                wpi_end_pos = copy.deepcopy(wpi_start_pos + 1)\n",
        "                pre_wpiend_pos = copy.deepcopy(wpi_end_pos)\n",
        "\n",
        "            elif (clean_wpi == '...') and (sent_non_space[wpi_start_pos] == '…') and (model_name == 'xlmr_large') and (train_or_dev == 'test'):\n",
        "                wpi_end_pos = copy.deepcopy(wpi_start_pos + 1)\n",
        "                pre_wpiend_pos = copy.deepcopy(wpi_end_pos)\n",
        "            \n",
        "            elif (clean_wpi == '1⁄2') and (sent_non_space[wpi_start_pos] == '½') and (model_name == 'xlmr') and (train_or_dev == 'train'):\n",
        "                wpi_end_pos = copy.deepcopy(wpi_start_pos + 1)\n",
        "                pre_wpiend_pos = copy.deepcopy(wpi_end_pos)\n",
        "            \n",
        "            # trường hợp bên dưới thì chỉ đơn giản là so sánh thì k giống nhau nhưng pos k đổi nên k k cần làm gì hết\n",
        "            elif (clean_wpi == '2') and (sent_non_space[wpi_start_pos] == '²') and (model_name == 'xlmr') and (train_or_dev == 'train'):\n",
        "                #wpi_end_pos = copy.deepcopy(wpi_start_pos + 1)\n",
        "                #pre_wpiend_pos = copy.deepcopy(wpi_end_pos)\n",
        "                pass\n",
        "\n",
        "            \n",
        "            elif (clean_wpi == '...') and (sent_non_space[wpi_start_pos] == '…') and (model_name == 'xlmr') and (train_or_dev == 'dev'):\n",
        "                wpi_end_pos = copy.deepcopy(wpi_start_pos + 1)\n",
        "                pre_wpiend_pos = copy.deepcopy(wpi_end_pos)\n",
        "\n",
        "            elif (clean_wpi == 'một') and (sent_non_space[wpi_start_pos] == '\\u200b') and (model_name == 'xlmr') and (train_or_dev == 'dev'):\n",
        "                wpi_start_pos = copy.deepcopy(wpi_start_pos + 1)\n",
        "                wpi_end_pos = copy.deepcopy(wpi_start_pos + len(clean_wpi))\n",
        "                pre_wpiend_pos = copy.deepcopy(wpi_end_pos)\n",
        "\n",
        "                assert (sent_non_space[wpi_start_pos:wpi_end_pos] == clean_wpi), str('AFTER FIX: Word_piece not match with non space pos.')\n",
        "\n",
        "\n",
        "\n",
        "            else:\n",
        "                print('-----sent_id: ', str(sent_id))\n",
        "                print('sentence:       ', repr(sentence))\n",
        "                print('sent_non_space: ', repr(sent_non_space))\n",
        "                print('cut from sent_non_space: ', repr(sent_non_space[wpi_start_pos:wpi_end_pos]))\n",
        "                print('wpi: ', repr(wpi))\n",
        "                print('clean_wpi: ', repr(clean_wpi))\n",
        "                print('wpi_start_pos: ', wpi_start_pos)\n",
        "                print('wpi_end_pos: ', wpi_end_pos)\n",
        "                print(bert_tokenize_lst)\n",
        "\n",
        "                assert False, str('Word_piece not match with non space pos.')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if wpi_start_pos == entity_pos_no_space[0]:\n",
        "            start_index = iwpi\n",
        "            found_start = True\n",
        "\n",
        "        if wpi_end_pos == entity_pos_no_space[1]:\n",
        "            end_index = iwpi\n",
        "            found_end = True\n",
        "            break # k duoc xoa break, vi trong xlmr co truong hop wpi la '▁' ngay sau entity nen bi lan vao entity\n",
        "\n",
        "\n",
        "    # check\n",
        "    assert ((found_start == True) and (found_end == True)), str('No word piece non space pos match with entity non space pos.')\n",
        "\n",
        "    found_entity_text = ''.join(bert_tokenize_lst[start_index:(end_index+1)])\n",
        "    found_entity_text = found_entity_text.replace('_', '')\n",
        "    found_entity_text = found_entity_text.replace('@@', '')\n",
        "    found_entity_text = found_entity_text.replace('▁', '')\n",
        "\n",
        "\n",
        "    entity_text_no_space = ''.join(entity['text'].split())\n",
        "\n",
        "    assert (found_entity_text == entity_text_no_space), str('found entity text not match entity text.' + found_entity_text + ' ' + entity_text_no_space)\n",
        "\n",
        "    # start_index, end_index đều được tăng lên 1 vì sau này sẽ có thêm 1 special token ở đầu input_ids\n",
        "    entity_wpi_ids_lst = list(range((start_index+1), (end_index+2)))\n",
        "\n",
        "\n",
        "    tmp_ent_txt = ''\n",
        "\n",
        "    for entity_wpi_id in entity_wpi_ids_lst:\n",
        "        if model_name == 'phobert_base':\n",
        "            tmp_ent_txt += pb_base_tokenizer.decode([input_ids[entity_wpi_id]])\n",
        "        elif model_name == 'xlmr_base':\n",
        "            tmp_ent_txt += xlmr_base_tokenizer.decode([input_ids[entity_wpi_id]])\n",
        "        \n",
        "        elif model_name == 'phobert_large':\n",
        "            tmp_ent_txt += pb_large_tokenizer.decode([input_ids[entity_wpi_id]])\n",
        "        elif model_name == 'xlmr_large':\n",
        "            tmp_ent_txt += xlmr_large_tokenizer.decode([input_ids[entity_wpi_id]])\n",
        "        \n",
        "        else:\n",
        "            assert False, str('Unknown model_name. Alow: phobert, xlmr')\n",
        "    \n",
        "    tmp_ent_txt = tmp_ent_txt.replace(' ', '')\n",
        "    tmp_ent_txt = tmp_ent_txt.replace('_', '')\n",
        "    tmp_ent_txt = tmp_ent_txt.replace('@@', '')\n",
        "    tmp_ent_txt = tmp_ent_txt.replace('▁', '')\n",
        "\n",
        "    assert (tmp_ent_txt == entity_text_no_space), str('entity text decode not match entity text. ' + str(sent_id) + ' ' + tmp_ent_txt + ' ' + entity_text_no_space)\n",
        "    \n",
        "\n",
        "    return entity_wpi_ids_lst\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAeHqITIgWdI"
      },
      "source": [
        "### Encode label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vspAwSYXgeuB"
      },
      "source": [
        "def encode_label(sentence_label):\n",
        "    label = -1\n",
        "    if sentence_label == \"LOCATED\":\n",
        "        label = 0\n",
        "    elif sentence_label == \"PART_WHOLE\":\n",
        "        label = 1\n",
        "    elif sentence_label == \"PERSONAL_SOCIAL\":\n",
        "        label = 2\n",
        "    elif sentence_label == \"AFFILIATION\":\n",
        "        label = 3\n",
        "    elif sentence_label == \"IS_LOCATED\":\n",
        "        label = 4\n",
        "    elif sentence_label == \"WHOLE_PART\":\n",
        "        label = 5\n",
        "    elif sentence_label == \"AFFILIATION_TO\":\n",
        "        label = 6\n",
        "    elif sentence_label == \"OTHERS\":\n",
        "        label = 7\n",
        "    else:\n",
        "        assert False, \"UNKNOWN LABEL\"\n",
        "    \n",
        "    return label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nFiNJVV5DTf"
      },
      "source": [
        "### test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Pwkh5W6wZ4"
      },
      "source": [
        "##### Add entity index and word_tokenize to traindata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Howkrjf35HV5",
        "outputId": "9ccfae87-a69d-4480-b272-e75015e76330"
      },
      "source": [
        "# sẽ in ra việc những câu mà underthesea word_tokenize bị thay đổi cho đúng với entity_text\n",
        "jtest_data_v3 = copy.deepcopy(add_word_tokenize_and_entity_index(jtest_data_v2, 'test', print_output=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done adding new_word_tokenize_lst and entity eids in new_word_tokenize_lst to jdata.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDOao2awGe9L",
        "outputId": "66c20630-845e-446e-cfc3-926df49ed580"
      },
      "source": [
        "print(*jtest_data_v3[0:20], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'doc_id': '23352901', 'sent_id': 1, 'sentence': \"Đây là lý do khiến Yoon Ah quyết định cắt mái tóc dài 'nữ thần' Mái tóc cũ của thành viên SNSD bị hư hỏng nặng nề và Yoon Ah thậm chí không muốn nuôi tóc lại.\", 'spos': [0, 158], 'entity_1': {'text': 'Yoon Ah', 'pos': [19, 26]}, 'entity_2': {'text': 'SNSD', 'pos': [90, 94]}, 'entity_1_info': {'first_token': [5, 6, 1, 'PERSON'], 'last_token': [6, 7, 1, 'PERSON']}, 'entity_2_info': {'first_token': [21, 22, 0, 'ORGANIZATION'], 'last_token': [21, 22, 0, 'ORGANIZATION']}, 'label': 'OTHERS', 'new_sentence': \"Đây là lý do khiến Yoon Ah quyết định cắt mái tóc dài 'nữ thần' Mái tóc cũ của thành viên SNSD bị hư hỏng nặng nề và Yoon Ah thậm chí không muốn nuôi tóc lại.\", 'new_entity_1': {'text': 'Yoon Ah', 'pos': [19, 26], 'wtk_index_lst': [4], 'pos_no_space': [14, 20]}, 'new_entity_2': {'text': 'SNSD', 'pos': [90, 94], 'wtk_index_lst': [17], 'pos_no_space': [69, 73]}, 'word_tokenize_lst': ['Đây', 'là', 'lý do', 'khiến', 'Yoon Ah', 'quyết định', 'cắt', 'mái tóc', 'dài', \"'\", 'nữ', 'thần', \"'\", 'Mái tóc', 'cũ', 'của', 'thành viên', 'SNSD', 'bị', 'hư hỏng', 'nặng nề', 'và', 'Yoon Ah', 'thậm chí', 'không', 'muốn', 'nuôi', 'tóc', 'lại', '.']}\n",
            "{'doc_id': '23352901', 'sent_id': 2, 'sentence': \"Đây là lý do khiến Yoon Ah quyết định cắt mái tóc dài 'nữ thần' Mái tóc cũ của thành viên SNSD bị hư hỏng nặng nề và Yoon Ah thậm chí không muốn nuôi tóc lại.\", 'spos': [0, 158], 'entity_1': {'text': 'Yoon Ah', 'pos': [19, 26]}, 'entity_2': {'text': 'Yoon Ah', 'pos': [117, 124]}, 'entity_1_info': {'first_token': [5, 6, 1, 'PERSON'], 'last_token': [6, 7, 1, 'PERSON']}, 'entity_2_info': {'first_token': [28, 29, 2, 'PERSON'], 'last_token': [29, 30, 2, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': \"Đây là lý do khiến Yoon Ah quyết định cắt mái tóc dài 'nữ thần' Mái tóc cũ của thành viên SNSD bị hư hỏng nặng nề và Yoon Ah thậm chí không muốn nuôi tóc lại.\", 'new_entity_1': {'text': 'Yoon Ah', 'pos': [19, 26], 'wtk_index_lst': [4], 'pos_no_space': [14, 20]}, 'new_entity_2': {'text': 'Yoon Ah', 'pos': [117, 124], 'wtk_index_lst': [22], 'pos_no_space': [89, 95]}, 'word_tokenize_lst': ['Đây', 'là', 'lý do', 'khiến', 'Yoon Ah', 'quyết định', 'cắt', 'mái tóc', 'dài', \"'\", 'nữ', 'thần', \"'\", 'Mái tóc', 'cũ', 'của', 'thành viên', 'SNSD', 'bị', 'hư hỏng', 'nặng nề', 'và', 'Yoon Ah', 'thậm chí', 'không', 'muốn', 'nuôi', 'tóc', 'lại', '.']}\n",
            "{'doc_id': '23352901', 'sent_id': 3, 'sentence': \"Đây là lý do khiến Yoon Ah quyết định cắt mái tóc dài 'nữ thần' Mái tóc cũ của thành viên SNSD bị hư hỏng nặng nề và Yoon Ah thậm chí không muốn nuôi tóc lại.\", 'spos': [0, 158], 'entity_1': {'text': 'SNSD', 'pos': [90, 94]}, 'entity_2': {'text': 'Yoon Ah', 'pos': [117, 124]}, 'entity_1_info': {'first_token': [21, 22, 0, 'ORGANIZATION'], 'last_token': [21, 22, 0, 'ORGANIZATION']}, 'entity_2_info': {'first_token': [28, 29, 2, 'PERSON'], 'last_token': [29, 30, 2, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': \"Đây là lý do khiến Yoon Ah quyết định cắt mái tóc dài 'nữ thần' Mái tóc cũ của thành viên SNSD bị hư hỏng nặng nề và Yoon Ah thậm chí không muốn nuôi tóc lại.\", 'new_entity_1': {'text': 'SNSD', 'pos': [90, 94], 'wtk_index_lst': [17], 'pos_no_space': [69, 73]}, 'new_entity_2': {'text': 'Yoon Ah', 'pos': [117, 124], 'wtk_index_lst': [22], 'pos_no_space': [89, 95]}, 'word_tokenize_lst': ['Đây', 'là', 'lý do', 'khiến', 'Yoon Ah', 'quyết định', 'cắt', 'mái tóc', 'dài', \"'\", 'nữ', 'thần', \"'\", 'Mái tóc', 'cũ', 'của', 'thành viên', 'SNSD', 'bị', 'hư hỏng', 'nặng nề', 'và', 'Yoon Ah', 'thậm chí', 'không', 'muốn', 'nuôi', 'tóc', 'lại', '.']}\n",
            "{'doc_id': '23352901', 'sent_id': 4, 'sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Soo Young', 'pos': [0, 9]}, 'entity_2': {'text': 'Seo Hyun', 'pos': [13, 21]}, 'entity_1_info': {'first_token': [258, 259, 8, 'PERSON'], 'last_token': [259, 260, 8, 'PERSON']}, 'entity_2_info': {'first_token': [261, 262, 9, 'PERSON'], 'last_token': [262, 263, 9, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'new_entity_1': {'text': 'Soo Young', 'pos': [0, 9], 'wtk_index_lst': [0], 'pos_no_space': [0, 8]}, 'new_entity_2': {'text': 'Seo Hyun', 'pos': [13, 21], 'wtk_index_lst': [2], 'pos_no_space': [10, 17]}, 'word_tokenize_lst': ['Soo Young', 'và', 'Seo Hyun', 'cắt', 'để', 'đóng phim', ',', 'Yoon Ah', 'và', 'Sunny', 'muốn', 'thay đổi', 'bản thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang vai', 'trẻ trung', '.']}\n",
            "{'doc_id': '23352901', 'sent_id': 5, 'sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Soo Young', 'pos': [0, 9]}, 'entity_2': {'text': 'Yoon Ah', 'pos': [40, 47]}, 'entity_1_info': {'first_token': [258, 259, 8, 'PERSON'], 'last_token': [259, 260, 8, 'PERSON']}, 'entity_2_info': {'first_token': [267, 268, 10, 'PERSON'], 'last_token': [268, 269, 10, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'new_entity_1': {'text': 'Soo Young', 'pos': [0, 9], 'wtk_index_lst': [0], 'pos_no_space': [0, 8]}, 'new_entity_2': {'text': 'Yoon Ah', 'pos': [40, 47], 'wtk_index_lst': [7], 'pos_no_space': [31, 37]}, 'word_tokenize_lst': ['Soo Young', 'và', 'Seo Hyun', 'cắt', 'để', 'đóng phim', ',', 'Yoon Ah', 'và', 'Sunny', 'muốn', 'thay đổi', 'bản thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang vai', 'trẻ trung', '.']}\n",
            "{'doc_id': '23352901', 'sent_id': 6, 'sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Soo Young', 'pos': [0, 9]}, 'entity_2': {'text': 'Sunny', 'pos': [51, 56]}, 'entity_1_info': {'first_token': [258, 259, 8, 'PERSON'], 'last_token': [259, 260, 8, 'PERSON']}, 'entity_2_info': {'first_token': [270, 271, 0, 'PERSON'], 'last_token': [270, 271, 0, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'new_entity_1': {'text': 'Soo Young', 'pos': [0, 9], 'wtk_index_lst': [0], 'pos_no_space': [0, 8]}, 'new_entity_2': {'text': 'Sunny', 'pos': [51, 56], 'wtk_index_lst': [9], 'pos_no_space': [39, 44]}, 'word_tokenize_lst': ['Soo Young', 'và', 'Seo Hyun', 'cắt', 'để', 'đóng phim', ',', 'Yoon Ah', 'và', 'Sunny', 'muốn', 'thay đổi', 'bản thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang vai', 'trẻ trung', '.']}\n",
            "{'doc_id': '23352901', 'sent_id': 7, 'sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Seo Hyun', 'pos': [13, 21]}, 'entity_2': {'text': 'Yoon Ah', 'pos': [40, 47]}, 'entity_1_info': {'first_token': [261, 262, 9, 'PERSON'], 'last_token': [262, 263, 9, 'PERSON']}, 'entity_2_info': {'first_token': [267, 268, 10, 'PERSON'], 'last_token': [268, 269, 10, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'new_entity_1': {'text': 'Seo Hyun', 'pos': [13, 21], 'wtk_index_lst': [2], 'pos_no_space': [10, 17]}, 'new_entity_2': {'text': 'Yoon Ah', 'pos': [40, 47], 'wtk_index_lst': [7], 'pos_no_space': [31, 37]}, 'word_tokenize_lst': ['Soo Young', 'và', 'Seo Hyun', 'cắt', 'để', 'đóng phim', ',', 'Yoon Ah', 'và', 'Sunny', 'muốn', 'thay đổi', 'bản thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang vai', 'trẻ trung', '.']}\n",
            "{'doc_id': '23352901', 'sent_id': 8, 'sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Seo Hyun', 'pos': [13, 21]}, 'entity_2': {'text': 'Sunny', 'pos': [51, 56]}, 'entity_1_info': {'first_token': [261, 262, 9, 'PERSON'], 'last_token': [262, 263, 9, 'PERSON']}, 'entity_2_info': {'first_token': [270, 271, 0, 'PERSON'], 'last_token': [270, 271, 0, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'new_entity_1': {'text': 'Seo Hyun', 'pos': [13, 21], 'wtk_index_lst': [2], 'pos_no_space': [10, 17]}, 'new_entity_2': {'text': 'Sunny', 'pos': [51, 56], 'wtk_index_lst': [9], 'pos_no_space': [39, 44]}, 'word_tokenize_lst': ['Soo Young', 'và', 'Seo Hyun', 'cắt', 'để', 'đóng phim', ',', 'Yoon Ah', 'và', 'Sunny', 'muốn', 'thay đổi', 'bản thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang vai', 'trẻ trung', '.']}\n",
            "{'doc_id': '23352901', 'sent_id': 9, 'sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'spos': [1128, 1264], 'entity_1': {'text': 'Yoon Ah', 'pos': [40, 47]}, 'entity_2': {'text': 'Sunny', 'pos': [51, 56]}, 'entity_1_info': {'first_token': [267, 268, 10, 'PERSON'], 'last_token': [268, 269, 10, 'PERSON']}, 'entity_2_info': {'first_token': [270, 271, 0, 'PERSON'], 'last_token': [270, 271, 0, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': 'Soo Young và Seo Hyun cắt để đóng phim, Yoon Ah và Sunny muốn thay đổi bản thân và Yuri cũng mới tỉa thành kiểu tóc ngang vai trẻ trung.', 'new_entity_1': {'text': 'Yoon Ah', 'pos': [40, 47], 'wtk_index_lst': [7], 'pos_no_space': [31, 37]}, 'new_entity_2': {'text': 'Sunny', 'pos': [51, 56], 'wtk_index_lst': [9], 'pos_no_space': [39, 44]}, 'word_tokenize_lst': ['Soo Young', 'và', 'Seo Hyun', 'cắt', 'để', 'đóng phim', ',', 'Yoon Ah', 'và', 'Sunny', 'muốn', 'thay đổi', 'bản thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang vai', 'trẻ trung', '.']}\n",
            "{'doc_id': '23352906', 'sent_id': 10, 'sentence': 'Bày tỏ quan điểm cũng bực tức như Tu Dinh Huong , thành viên Pymini tiếp lời:\"Nhà mình mà phần cho người sau thì còn chọn toàn cái ngon để phần, chứ không có chuyện ăn uống bày bừa, mặc kệ người ăn sau như thế này”.', 'spos': [1110, 1325], 'entity_1': {'text': 'Tu Dinh Huong', 'pos': [34, 47]}, 'entity_2': {'text': 'Pymini', 'pos': [61, 67]}, 'entity_1_info': {'first_token': [261, 262, 2, 'PERSON'], 'last_token': [263, 264, 2, 'PERSON']}, 'entity_2_info': {'first_token': [267, 268, 0, 'PERSON'], 'last_token': [267, 268, 0, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': 'Bày tỏ quan điểm cũng bực tức như Tu Dinh Huong , thành viên Pymini tiếp lời:\"Nhà mình mà phần cho người sau thì còn chọn toàn cái ngon để phần, chứ không có chuyện ăn uống bày bừa, mặc kệ người ăn sau như thế này”.', 'new_entity_1': {'text': 'Tu Dinh Huong', 'pos': [34, 47], 'wtk_index_lst': [4], 'pos_no_space': [26, 37]}, 'new_entity_2': {'text': 'Pymini', 'pos': [61, 67], 'wtk_index_lst': [7], 'pos_no_space': [47, 53]}, 'word_tokenize_lst': ['Bày tỏ', 'quan điểm', 'cũng', 'bực tức như', 'Tu Dinh Huong', ',', 'thành viên', 'Pymini', 'tiếp lời', ':', '\"', 'Nhà', 'mình', 'mà', 'phần', 'cho', 'người', 'sau', 'thì', 'còn', 'chọn', 'toàn', 'cái', 'ngon', 'để phần', ',', 'chứ', 'không', 'có', 'chuyện', 'ăn uống', 'bày', 'bừa', ',', 'mặc kệ', 'người', 'ăn', 'sau', 'như', 'thế này', '”', '.']}\n",
            "{'doc_id': '23352911', 'sent_id': 11, 'sentence': \"Ca sĩ Đăng Dương chơi đàn bầu trong liveshow 'Mặt trời của tôi' Nhân kỷ niệm 20 năm ca hát, ca sĩ Đăng Dương sẽ tổ chức liveshow có tựa đề “Mặt trời của tôi” được diễn ra trong 2 ngày 14, 15/10 tại Hà Nội .\", 'spos': [0, 206], 'entity_1': {'text': 'Đăng Dương', 'pos': [6, 16]}, 'entity_2': {'text': 'Đăng Dương', 'pos': [98, 108]}, 'entity_1_info': {'first_token': [2, 3, 1, 'PERSON'], 'last_token': [3, 4, 1, 'PERSON']}, 'entity_2_info': {'first_token': [22, 23, 2, 'PERSON'], 'last_token': [23, 24, 2, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': \"Ca sĩ Đăng Dương chơi đàn bầu trong liveshow 'Mặt trời của tôi' Nhân kỷ niệm 20 năm ca hát, ca sĩ Đăng Dương sẽ tổ chức liveshow có tựa đề “Mặt trời của tôi” được diễn ra trong 2 ngày 14, 15/10 tại Hà Nội .\", 'new_entity_1': {'text': 'Đăng Dương', 'pos': [6, 16], 'wtk_index_lst': [1], 'pos_no_space': [4, 13]}, 'new_entity_2': {'text': 'Đăng Dương', 'pos': [98, 108], 'wtk_index_lst': [18], 'pos_no_space': [76, 85]}, 'word_tokenize_lst': ['Ca sĩ', 'Đăng Dương', 'chơi', 'đàn bầu', 'trong', 'liveshow', \"'\", 'Mặt trời', 'của', 'tôi', \"'\", 'Nhân', 'kỷ niệm', '20', 'năm', 'ca hát', ',', 'ca sĩ', 'Đăng Dương', 'sẽ', 'tổ chức', 'liveshow', 'có', 'tựa đề', '“', 'Mặt trời', 'của', 'tôi', '”', 'được', 'diễn', 'ra', 'trong', '2', 'ngày', '14', ',', '15/10', 'tại', 'Hà Nội', '.']}\n",
            "{'doc_id': '23352911', 'sent_id': 12, 'sentence': \"Ca sĩ Đăng Dương chơi đàn bầu trong liveshow 'Mặt trời của tôi' Nhân kỷ niệm 20 năm ca hát, ca sĩ Đăng Dương sẽ tổ chức liveshow có tựa đề “Mặt trời của tôi” được diễn ra trong 2 ngày 14, 15/10 tại Hà Nội .\", 'spos': [0, 206], 'entity_1': {'text': 'Đăng Dương', 'pos': [6, 16]}, 'entity_2': {'text': 'Hà Nội', 'pos': [198, 204]}, 'entity_1_info': {'first_token': [2, 3, 1, 'PERSON'], 'last_token': [3, 4, 1, 'PERSON']}, 'entity_2_info': {'first_token': [44, 45, 3, 'LOCATION'], 'last_token': [45, 46, 3, 'LOCATION']}, 'label': 'OTHERS', 'new_sentence': \"Ca sĩ Đăng Dương chơi đàn bầu trong liveshow 'Mặt trời của tôi' Nhân kỷ niệm 20 năm ca hát, ca sĩ Đăng Dương sẽ tổ chức liveshow có tựa đề “Mặt trời của tôi” được diễn ra trong 2 ngày 14, 15/10 tại Hà Nội .\", 'new_entity_1': {'text': 'Đăng Dương', 'pos': [6, 16], 'wtk_index_lst': [1], 'pos_no_space': [4, 13]}, 'new_entity_2': {'text': 'Hà Nội', 'pos': [198, 204], 'wtk_index_lst': [39], 'pos_no_space': [154, 159]}, 'word_tokenize_lst': ['Ca sĩ', 'Đăng Dương', 'chơi', 'đàn bầu', 'trong', 'liveshow', \"'\", 'Mặt trời', 'của', 'tôi', \"'\", 'Nhân', 'kỷ niệm', '20', 'năm', 'ca hát', ',', 'ca sĩ', 'Đăng Dương', 'sẽ', 'tổ chức', 'liveshow', 'có', 'tựa đề', '“', 'Mặt trời', 'của', 'tôi', '”', 'được', 'diễn', 'ra', 'trong', '2', 'ngày', '14', ',', '15/10', 'tại', 'Hà Nội', '.']}\n",
            "{'doc_id': '23352911', 'sent_id': 13, 'sentence': \"Ca sĩ Đăng Dương chơi đàn bầu trong liveshow 'Mặt trời của tôi' Nhân kỷ niệm 20 năm ca hát, ca sĩ Đăng Dương sẽ tổ chức liveshow có tựa đề “Mặt trời của tôi” được diễn ra trong 2 ngày 14, 15/10 tại Hà Nội .\", 'spos': [0, 206], 'entity_1': {'text': 'Đăng Dương', 'pos': [98, 108]}, 'entity_2': {'text': 'Hà Nội', 'pos': [198, 204]}, 'entity_1_info': {'first_token': [22, 23, 2, 'PERSON'], 'last_token': [23, 24, 2, 'PERSON']}, 'entity_2_info': {'first_token': [44, 45, 3, 'LOCATION'], 'last_token': [45, 46, 3, 'LOCATION']}, 'label': 'OTHERS', 'new_sentence': \"Ca sĩ Đăng Dương chơi đàn bầu trong liveshow 'Mặt trời của tôi' Nhân kỷ niệm 20 năm ca hát, ca sĩ Đăng Dương sẽ tổ chức liveshow có tựa đề “Mặt trời của tôi” được diễn ra trong 2 ngày 14, 15/10 tại Hà Nội .\", 'new_entity_1': {'text': 'Đăng Dương', 'pos': [98, 108], 'wtk_index_lst': [17], 'pos_no_space': [76, 85]}, 'new_entity_2': {'text': 'Hà Nội', 'pos': [198, 204], 'wtk_index_lst': [38], 'pos_no_space': [154, 159]}, 'word_tokenize_lst': ['Ca sĩ', 'Đăng Dương chơi', 'đàn bầu', 'trong', 'liveshow', \"'\", 'Mặt trời', 'của', 'tôi', \"'\", 'Nhân', 'kỷ niệm', '20', 'năm', 'ca hát', ',', 'ca sĩ', 'Đăng Dương', 'sẽ', 'tổ chức', 'liveshow', 'có', 'tựa đề', '“', 'Mặt trời', 'của', 'tôi', '”', 'được', 'diễn', 'ra', 'trong', '2', 'ngày', '14', ',', '15/10', 'tại', 'Hà Nội', '.']}\n",
            "{'doc_id': '23352911', 'sent_id': 14, 'sentence': 'Trước khi theo học thanh nhạc và trở thành giọng ca chính thống hàng đầu Việt Nam , ca sĩ Đăng Dương từng có gần 10 năm theo học đàn bầu của nghệ sĩ Thanh Tâm (mẹ nhạc sĩ Hồ Hoài Anh ).', 'spos': [207, 392], 'entity_1': {'text': 'Việt Nam', 'pos': [73, 81]}, 'entity_2': {'text': 'Đăng Dương', 'pos': [90, 100]}, 'entity_1_info': {'first_token': [62, 63, 4, 'LOCATION'], 'last_token': [63, 64, 4, 'LOCATION']}, 'entity_2_info': {'first_token': [67, 68, 5, 'PERSON'], 'last_token': [68, 69, 5, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': 'Trước khi theo học thanh nhạc và trở thành giọng ca chính thống hàng đầu Việt Nam , ca sĩ Đăng Dương từng có gần 10 năm theo học đàn bầu của nghệ sĩ Thanh Tâm (mẹ nhạc sĩ Hồ Hoài Anh ).', 'new_entity_1': {'text': 'Việt Nam', 'pos': [73, 81], 'wtk_index_lst': [11], 'pos_no_space': [58, 65]}, 'new_entity_2': {'text': 'Đăng Dương', 'pos': [90, 100], 'wtk_index_lst': [14], 'pos_no_space': [70, 79]}, 'word_tokenize_lst': ['Trước', 'khi', 'theo', 'học', 'thanh nhạc', 'và', 'trở thành', 'giọng', 'ca', 'chính thống', 'hàng đầu', 'Việt Nam', ',', 'ca sĩ', 'Đăng Dương', 'từng', 'có', 'gần', '10', 'năm', 'theo', 'học', 'đàn bầu', 'của', 'nghệ sĩ', 'Thanh Tâm', '(', 'mẹ', 'nhạc sĩ', 'Hồ Hoài Anh', ')', '.']}\n",
            "{'doc_id': '23352911', 'sent_id': 15, 'sentence': 'Trước khi theo học thanh nhạc và trở thành giọng ca chính thống hàng đầu Việt Nam , ca sĩ Đăng Dương từng có gần 10 năm theo học đàn bầu của nghệ sĩ Thanh Tâm (mẹ nhạc sĩ Hồ Hoài Anh ).', 'spos': [207, 392], 'entity_1': {'text': 'Việt Nam', 'pos': [73, 81]}, 'entity_2': {'text': 'Thanh Tâm', 'pos': [149, 158]}, 'entity_1_info': {'first_token': [62, 63, 4, 'LOCATION'], 'last_token': [63, 64, 4, 'LOCATION']}, 'entity_2_info': {'first_token': [81, 82, 6, 'PERSON'], 'last_token': [82, 83, 6, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': 'Trước khi theo học thanh nhạc và trở thành giọng ca chính thống hàng đầu Việt Nam , ca sĩ Đăng Dương từng có gần 10 năm theo học đàn bầu của nghệ sĩ Thanh Tâm (mẹ nhạc sĩ Hồ Hoài Anh ).', 'new_entity_1': {'text': 'Việt Nam', 'pos': [73, 81], 'wtk_index_lst': [11], 'pos_no_space': [58, 65]}, 'new_entity_2': {'text': 'Thanh Tâm', 'pos': [149, 158], 'wtk_index_lst': [25], 'pos_no_space': [115, 123]}, 'word_tokenize_lst': ['Trước', 'khi', 'theo', 'học', 'thanh nhạc', 'và', 'trở thành', 'giọng', 'ca', 'chính thống', 'hàng đầu', 'Việt Nam', ',', 'ca sĩ', 'Đăng Dương', 'từng', 'có', 'gần', '10', 'năm', 'theo', 'học', 'đàn bầu', 'của', 'nghệ sĩ', 'Thanh Tâm', '(', 'mẹ', 'nhạc sĩ', 'Hồ Hoài Anh', ')', '.']}\n",
            "{'doc_id': '23352911', 'sent_id': 16, 'sentence': 'Trước khi theo học thanh nhạc và trở thành giọng ca chính thống hàng đầu Việt Nam , ca sĩ Đăng Dương từng có gần 10 năm theo học đàn bầu của nghệ sĩ Thanh Tâm (mẹ nhạc sĩ Hồ Hoài Anh ).', 'spos': [207, 392], 'entity_1': {'text': 'Việt Nam', 'pos': [73, 81]}, 'entity_2': {'text': 'Hồ Hoài Anh', 'pos': [171, 182]}, 'entity_1_info': {'first_token': [62, 63, 4, 'LOCATION'], 'last_token': [63, 64, 4, 'LOCATION']}, 'entity_2_info': {'first_token': [86, 87, 7, 'PERSON'], 'last_token': [88, 89, 7, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': 'Trước khi theo học thanh nhạc và trở thành giọng ca chính thống hàng đầu Việt Nam , ca sĩ Đăng Dương từng có gần 10 năm theo học đàn bầu của nghệ sĩ Thanh Tâm (mẹ nhạc sĩ Hồ Hoài Anh ).', 'new_entity_1': {'text': 'Việt Nam', 'pos': [73, 81], 'wtk_index_lst': [11], 'pos_no_space': [58, 65]}, 'new_entity_2': {'text': 'Hồ Hoài Anh', 'pos': [171, 182], 'wtk_index_lst': [29], 'pos_no_space': [132, 141]}, 'word_tokenize_lst': ['Trước', 'khi', 'theo', 'học', 'thanh nhạc', 'và', 'trở thành', 'giọng', 'ca', 'chính thống', 'hàng đầu', 'Việt Nam', ',', 'ca sĩ', 'Đăng Dương', 'từng', 'có', 'gần', '10', 'năm', 'theo', 'học', 'đàn bầu', 'của', 'nghệ sĩ', 'Thanh Tâm', '(', 'mẹ', 'nhạc sĩ', 'Hồ Hoài Anh', ')', '.']}\n",
            "{'doc_id': '23352911', 'sent_id': 17, 'sentence': 'Trước khi theo học thanh nhạc và trở thành giọng ca chính thống hàng đầu Việt Nam , ca sĩ Đăng Dương từng có gần 10 năm theo học đàn bầu của nghệ sĩ Thanh Tâm (mẹ nhạc sĩ Hồ Hoài Anh ).', 'spos': [207, 392], 'entity_1': {'text': 'Đăng Dương', 'pos': [90, 100]}, 'entity_2': {'text': 'Thanh Tâm', 'pos': [149, 158]}, 'entity_1_info': {'first_token': [67, 68, 5, 'PERSON'], 'last_token': [68, 69, 5, 'PERSON']}, 'entity_2_info': {'first_token': [81, 82, 6, 'PERSON'], 'last_token': [82, 83, 6, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': 'Trước khi theo học thanh nhạc và trở thành giọng ca chính thống hàng đầu Việt Nam , ca sĩ Đăng Dương từng có gần 10 năm theo học đàn bầu của nghệ sĩ Thanh Tâm (mẹ nhạc sĩ Hồ Hoài Anh ).', 'new_entity_1': {'text': 'Đăng Dương', 'pos': [90, 100], 'wtk_index_lst': [14], 'pos_no_space': [70, 79]}, 'new_entity_2': {'text': 'Thanh Tâm', 'pos': [149, 158], 'wtk_index_lst': [25], 'pos_no_space': [115, 123]}, 'word_tokenize_lst': ['Trước', 'khi', 'theo', 'học', 'thanh nhạc', 'và', 'trở thành', 'giọng', 'ca', 'chính thống', 'hàng đầu', 'Việt Nam', ',', 'ca sĩ', 'Đăng Dương', 'từng', 'có', 'gần', '10', 'năm', 'theo', 'học', 'đàn bầu', 'của', 'nghệ sĩ', 'Thanh Tâm', '(', 'mẹ', 'nhạc sĩ', 'Hồ Hoài Anh', ')', '.']}\n",
            "{'doc_id': '23352911', 'sent_id': 18, 'sentence': 'Trước khi theo học thanh nhạc và trở thành giọng ca chính thống hàng đầu Việt Nam , ca sĩ Đăng Dương từng có gần 10 năm theo học đàn bầu của nghệ sĩ Thanh Tâm (mẹ nhạc sĩ Hồ Hoài Anh ).', 'spos': [207, 392], 'entity_1': {'text': 'Đăng Dương', 'pos': [90, 100]}, 'entity_2': {'text': 'Hồ Hoài Anh', 'pos': [171, 182]}, 'entity_1_info': {'first_token': [67, 68, 5, 'PERSON'], 'last_token': [68, 69, 5, 'PERSON']}, 'entity_2_info': {'first_token': [86, 87, 7, 'PERSON'], 'last_token': [88, 89, 7, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': 'Trước khi theo học thanh nhạc và trở thành giọng ca chính thống hàng đầu Việt Nam , ca sĩ Đăng Dương từng có gần 10 năm theo học đàn bầu của nghệ sĩ Thanh Tâm (mẹ nhạc sĩ Hồ Hoài Anh ).', 'new_entity_1': {'text': 'Đăng Dương', 'pos': [90, 100], 'wtk_index_lst': [14], 'pos_no_space': [70, 79]}, 'new_entity_2': {'text': 'Hồ Hoài Anh', 'pos': [171, 182], 'wtk_index_lst': [29], 'pos_no_space': [132, 141]}, 'word_tokenize_lst': ['Trước', 'khi', 'theo', 'học', 'thanh nhạc', 'và', 'trở thành', 'giọng', 'ca', 'chính thống', 'hàng đầu', 'Việt Nam', ',', 'ca sĩ', 'Đăng Dương', 'từng', 'có', 'gần', '10', 'năm', 'theo', 'học', 'đàn bầu', 'của', 'nghệ sĩ', 'Thanh Tâm', '(', 'mẹ', 'nhạc sĩ', 'Hồ Hoài Anh', ')', '.']}\n",
            "{'doc_id': '23352911', 'sent_id': 19, 'sentence': 'Trước khi theo học thanh nhạc và trở thành giọng ca chính thống hàng đầu Việt Nam , ca sĩ Đăng Dương từng có gần 10 năm theo học đàn bầu của nghệ sĩ Thanh Tâm (mẹ nhạc sĩ Hồ Hoài Anh ).', 'spos': [207, 392], 'entity_1': {'text': 'Thanh Tâm', 'pos': [149, 158]}, 'entity_2': {'text': 'Hồ Hoài Anh', 'pos': [171, 182]}, 'entity_1_info': {'first_token': [81, 82, 6, 'PERSON'], 'last_token': [82, 83, 6, 'PERSON']}, 'entity_2_info': {'first_token': [86, 87, 7, 'PERSON'], 'last_token': [88, 89, 7, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': 'Trước khi theo học thanh nhạc và trở thành giọng ca chính thống hàng đầu Việt Nam , ca sĩ Đăng Dương từng có gần 10 năm theo học đàn bầu của nghệ sĩ Thanh Tâm (mẹ nhạc sĩ Hồ Hoài Anh ).', 'new_entity_1': {'text': 'Thanh Tâm', 'pos': [149, 158], 'wtk_index_lst': [25], 'pos_no_space': [115, 123]}, 'new_entity_2': {'text': 'Hồ Hoài Anh', 'pos': [171, 182], 'wtk_index_lst': [29], 'pos_no_space': [132, 141]}, 'word_tokenize_lst': ['Trước', 'khi', 'theo', 'học', 'thanh nhạc', 'và', 'trở thành', 'giọng', 'ca', 'chính thống', 'hàng đầu', 'Việt Nam', ',', 'ca sĩ', 'Đăng Dương', 'từng', 'có', 'gần', '10', 'năm', 'theo', 'học', 'đàn bầu', 'của', 'nghệ sĩ', 'Thanh Tâm', '(', 'mẹ', 'nhạc sĩ', 'Hồ Hoài Anh', ')', '.']}\n",
            "{'doc_id': '23352911', 'sent_id': 20, 'sentence': 'Nói về sư chậm trễ thực hiện đêm nhạc riêng trong khi những đồng nghiệp cùng trang lứa như ca sĩ Trọng Tấn , Việt Hoàn , Anh Thơ …', 'spos': [1155, 1285], 'entity_1': {'text': 'Trọng Tấn', 'pos': [97, 106]}, 'entity_2': {'text': 'Việt Hoàn', 'pos': [109, 118]}, 'entity_1_info': {'first_token': [281, 282, 10, 'PERSON'], 'last_token': [282, 283, 10, 'PERSON']}, 'entity_2_info': {'first_token': [284, 285, 11, 'PERSON'], 'last_token': [285, 286, 11, 'PERSON']}, 'label': 'OTHERS', 'new_sentence': 'Nói về sư chậm trễ thực hiện đêm nhạc riêng trong khi những đồng nghiệp cùng trang lứa như ca sĩ Trọng Tấn , Việt Hoàn , Anh Thơ …', 'new_entity_1': {'text': 'Trọng Tấn', 'pos': [97, 106], 'wtk_index_lst': [16], 'pos_no_space': [76, 84]}, 'new_entity_2': {'text': 'Việt Hoàn', 'pos': [109, 118], 'wtk_index_lst': [18], 'pos_no_space': [85, 93]}, 'word_tokenize_lst': ['Nói', 'về', 'sư', 'chậm trễ', 'thực hiện', 'đêm', 'nhạc', 'riêng', 'trong', 'khi', 'những', 'đồng nghiệp', 'cùng', 'trang lứa', 'như', 'ca sĩ', 'Trọng Tấn', ',', 'Việt Hoàn', ',', 'Anh', 'Thơ', '…']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoKdd-zY66Jc"
      },
      "source": [
        "##### Create test input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xfu-7oPFyIA"
      },
      "source": [
        "###### max len"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzFYaBbn5N80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd571ff4-b215-4c1f-ffa1-edbfe086fa3d"
      },
      "source": [
        "print(jtest_data_v3[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'doc_id': '23352901', 'sent_id': 1, 'sentence': \"Đây là lý do khiến Yoon Ah quyết định cắt mái tóc dài 'nữ thần' Mái tóc cũ của thành viên SNSD bị hư hỏng nặng nề và Yoon Ah thậm chí không muốn nuôi tóc lại.\", 'spos': [0, 158], 'entity_1': {'text': 'Yoon Ah', 'pos': [19, 26]}, 'entity_2': {'text': 'SNSD', 'pos': [90, 94]}, 'entity_1_info': {'first_token': [5, 6, 1, 'PERSON'], 'last_token': [6, 7, 1, 'PERSON']}, 'entity_2_info': {'first_token': [21, 22, 0, 'ORGANIZATION'], 'last_token': [21, 22, 0, 'ORGANIZATION']}, 'label': 'OTHERS', 'new_sentence': \"Đây là lý do khiến Yoon Ah quyết định cắt mái tóc dài 'nữ thần' Mái tóc cũ của thành viên SNSD bị hư hỏng nặng nề và Yoon Ah thậm chí không muốn nuôi tóc lại.\", 'new_entity_1': {'text': 'Yoon Ah', 'pos': [19, 26], 'wtk_index_lst': [4], 'pos_no_space': [14, 20]}, 'new_entity_2': {'text': 'SNSD', 'pos': [90, 94], 'wtk_index_lst': [17], 'pos_no_space': [69, 73]}, 'word_tokenize_lst': ['Đây', 'là', 'lý do', 'khiến', 'Yoon Ah', 'quyết định', 'cắt', 'mái tóc', 'dài', \"'\", 'nữ', 'thần', \"'\", 'Mái tóc', 'cũ', 'của', 'thành viên', 'SNSD', 'bị', 'hư hỏng', 'nặng nề', 'và', 'Yoon Ah', 'thậm chí', 'không', 'muốn', 'nuôi', 'tóc', 'lại', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOCfi8DmzHDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8914ad5-6893-4525-80cf-390a246496c8"
      },
      "source": [
        "pb_base_sent_maxlen = 0\n",
        "xlmr_base_sent_maxlen = 0\n",
        "\n",
        "pb_large_sent_maxlen = 0\n",
        "xlmr_large_sent_maxlen = 0\n",
        "\n",
        "pb_base_sent_len_lst = []\n",
        "xlmr_base_sent_len_lst = []\n",
        "\n",
        "pb_large_sent_len_lst = []\n",
        "xlmr_large_sent_len_lst = []\n",
        "\n",
        "for sentif in jtest_data_v3:\n",
        "\n",
        "    assert (sentif['new_sentence'] == unicodedata.normalize(\"NFC\", sentif['new_sentence'])), str('sentence not seem to be normalized.')\n",
        "    assert (''.join(sentif['new_sentence'].split()) == u''.join([tk.replace(\" \", \"\") for tk in sentif['word_tokenize_lst']])), \\\n",
        "    str('word_tokenize_lst not match new_sentence.')\n",
        "\n",
        "   \n",
        "    \n",
        "    pb_base_sent = u\" \".join([tk.replace(\" \", \"_\") for tk in sentif['word_tokenize_lst']])\n",
        "    pb_base_sent_tokenize = pb_base_tokenizer.tokenize(pb_base_sent)\n",
        "\n",
        "    pb_base_sent_maxlen = max(pb_base_sent_maxlen, len(pb_base_sent_tokenize))\n",
        "\n",
        "    pb_base_sent_len_lst.append(len(pb_base_sent_tokenize))\n",
        "\n",
        "    \n",
        "    xlmr_base_sent = u\" \".join(copy.deepcopy(sentif['word_tokenize_lst']))\n",
        "    xlmr_base_sent_tokenize = xlmr_base_tokenizer.tokenize(xlmr_base_sent)\n",
        "\n",
        "    xlmr_base_sent_maxlen = max(xlmr_base_sent_maxlen, len(xlmr_base_sent_tokenize))\n",
        "    xlmr_base_sent_len_lst.append(len(xlmr_base_sent_tokenize))\n",
        "\n",
        "\n",
        "\n",
        "    pb_large_sent = u\" \".join([tk.replace(\" \", \"_\") for tk in sentif['word_tokenize_lst']])\n",
        "    pb_large_sent_tokenize = pb_large_tokenizer.tokenize(pb_large_sent)\n",
        "\n",
        "    pb_large_sent_maxlen = max(pb_large_sent_maxlen, len(pb_large_sent_tokenize))\n",
        "\n",
        "    pb_large_sent_len_lst.append(len(pb_large_sent_tokenize))\n",
        "\n",
        "    \n",
        "    xlmr_large_sent = u\" \".join(copy.deepcopy(sentif['word_tokenize_lst']))\n",
        "    xlmr_large_sent_tokenize = xlmr_large_tokenizer.tokenize(xlmr_large_sent)\n",
        "\n",
        "    xlmr_large_sent_maxlen = max(xlmr_large_sent_maxlen, len(xlmr_large_sent_tokenize))\n",
        "    xlmr_large_sent_len_lst.append(len(xlmr_large_sent_tokenize))\n",
        "\n",
        "\n",
        "\n",
        "print('pb_base_sent_maxlen: ', pb_base_sent_maxlen)\n",
        "print('xlmr_base_sent_maxlen: ', xlmr_base_sent_maxlen)\n",
        "print('pb_large_sent_maxlen: ', pb_large_sent_maxlen)\n",
        "print('xlmr_large_sent_maxlen: ', xlmr_large_sent_maxlen)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pb_base_sent_maxlen:  159\n",
            "xlmr_base_sent_maxlen:  260\n",
            "pb_large_sent_maxlen:  159\n",
            "xlmr_large_sent_maxlen:  260\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JurhkcDLa7xH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad57f792-41a8-4b5b-c275-e5b0cd5a3ab3"
      },
      "source": [
        "print(pb_base_sent_len_lst == pb_large_sent_len_lst)\n",
        "print(xlmr_base_sent_len_lst == xlmr_large_sent_len_lst)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEdxfgcBARDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5591357-0fce-43c1-f56c-a870a1569d24"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "from collections import Counter \n",
        "\n",
        "# phobert\n",
        "\n",
        "print('--phobert')\n",
        "print(pd.Series(pb_base_sent_len_lst).describe())\n",
        "most_common_len = Counter(pb_base_sent_len_lst).most_common(1)[0]\n",
        "print('most common sentence len: ', most_common_len[0] , ' appear ' , most_common_len[1], ' times.')\n",
        "# xlmr\n",
        "\n",
        "print('\\n--XLM-RoBERTa')\n",
        "print(pd.Series(xlmr_base_sent_len_lst).describe())\n",
        "most_common_len = Counter(xlmr_base_sent_len_lst).most_common(1)[0]\n",
        "print('most common sentence len: ', most_common_len[0] , ' appear ' , most_common_len[1], ' times.')\n",
        "\n",
        "\n",
        "print('--phobert')\n",
        "print(pd.Series(pb_large_sent_len_lst).describe())\n",
        "most_common_len = Counter(pb_large_sent_len_lst).most_common(1)[0]\n",
        "print('most common sentence len: ', most_common_len[0] , ' appear ' , most_common_len[1], ' times.')\n",
        "# xlmr\n",
        "\n",
        "print('\\n--XLM-RoBERTa')\n",
        "print(pd.Series(xlmr_large_sent_len_lst).describe())\n",
        "most_common_len = Counter(xlmr_large_sent_len_lst).most_common(1)[0]\n",
        "print('most common sentence len: ', most_common_len[0] , ' appear ' , most_common_len[1], ' times.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--phobert\n",
            "count    9567.000000\n",
            "mean       46.005958\n",
            "std        22.731124\n",
            "min         4.000000\n",
            "25%        31.000000\n",
            "50%        41.000000\n",
            "75%        54.000000\n",
            "max       159.000000\n",
            "dtype: float64\n",
            "most common sentence len:  40  appear  308  times.\n",
            "\n",
            "--XLM-RoBERTa\n",
            "count    9567.000000\n",
            "mean       61.574684\n",
            "std        30.440656\n",
            "min         5.000000\n",
            "25%        42.000000\n",
            "50%        55.000000\n",
            "75%        74.000000\n",
            "max       260.000000\n",
            "dtype: float64\n",
            "most common sentence len:  49  appear  272  times.\n",
            "--phobert\n",
            "count    9567.000000\n",
            "mean       46.005958\n",
            "std        22.731124\n",
            "min         4.000000\n",
            "25%        31.000000\n",
            "50%        41.000000\n",
            "75%        54.000000\n",
            "max       159.000000\n",
            "dtype: float64\n",
            "most common sentence len:  40  appear  308  times.\n",
            "\n",
            "--XLM-RoBERTa\n",
            "count    9567.000000\n",
            "mean       61.574684\n",
            "std        30.440656\n",
            "min         5.000000\n",
            "25%        42.000000\n",
            "50%        55.000000\n",
            "75%        74.000000\n",
            "max       260.000000\n",
            "dtype: float64\n",
            "most common sentence len:  49  appear  272  times.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuL9lddpF3eS"
      },
      "source": [
        "###### create input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho30-2avoKm4"
      },
      "source": [
        "for sentif in jtest_data_v3:\n",
        "    if 'Hotspur' in sentif['new_sentence']:\n",
        "        print(sentif)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4Lb-dzlCpWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30daf0d4-803c-4599-a290-f5f1ccd21f6e"
      },
      "source": [
        "'''\n",
        "# thêm token vào phobert để tránh bị <unk> token, token này sẽ có embedding được khởi tạo một cái random\n",
        "# https://github.com/huggingface/transformers/issues/1413\n",
        "# https://huggingface.co/transformers/internal/tokenization_utils.html?highlight=add_token\n",
        "\n",
        "print(len(pb_tokenizer))\n",
        "\n",
        "add_Hotspur_to_pb = pb_tokenizer.add_tokens([unicodedata.normalize(\"NFC\", 'Hotspur')])\n",
        "print('We have added', add_Hotspur_to_pb, 'tokens')\n",
        "# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
        "\n",
        "print(len(pb_tokenizer))\n",
        "\n",
        "pb_model.resize_token_embeddings(len(pb_tokenizer))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# thêm token vào phobert để tránh bị <unk> token, token này sẽ có embedding được khởi tạo một cái random\\n# https://github.com/huggingface/transformers/issues/1413\\n# https://huggingface.co/transformers/internal/tokenization_utils.html?highlight=add_token\\n\\nprint(len(pb_tokenizer))\\n\\nadd_Hotspur_to_pb = pb_tokenizer.add_tokens([unicodedata.normalize(\"NFC\", \\'Hotspur\\')])\\nprint(\\'We have added\\', add_Hotspur_to_pb, \\'tokens\\')\\n# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\\n\\nprint(len(pb_tokenizer))\\n\\npb_model.resize_token_embeddings(len(pb_tokenizer))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpJ2RKTg65ft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2e73aa-4e35-4e22-dc34-99c46d494dd2"
      },
      "source": [
        "test_pb_base_input_ids = []\n",
        "test_pb_base_attention_masks = []\n",
        "test_pb_base_entity_1_eids = []\n",
        "test_pb_base_entity_2_eids = []\n",
        "#test_pb_labels = []\n",
        "\n",
        "test_xlmr_base_input_ids = []\n",
        "test_xlmr_base_attention_masks = []\n",
        "test_xlmr_base_entity_1_eids = []\n",
        "test_xlmr_base_entity_2_eids = []\n",
        "#test_xlmr_labels = []\n",
        "\n",
        "test_pb_large_input_ids = []\n",
        "test_pb_large_attention_masks = []\n",
        "test_pb_large_entity_1_eids = []\n",
        "test_pb_large_entity_2_eids = []\n",
        "#test_pb_labels = []\n",
        "\n",
        "test_xlmr_large_input_ids = []\n",
        "test_xlmr_large_attention_masks = []\n",
        "test_xlmr_large_entity_1_eids = []\n",
        "test_xlmr_large_entity_2_eids = []\n",
        "#test_xlmr_labels = []\n",
        "\n",
        "sent_ids = []\n",
        "\n",
        "test_labels = []\n",
        "\n",
        "# Do entity_eids của các entity trong các câu không cùng chiều dài nên ta cần chuyển về cùng chiều dài thì mới biến thành tensor được\n",
        "# để chuyển về cùng chiều dài ta sẽ pad các eids ngắn hơn bằng -2.\n",
        "# vì eids phải >= 1 nên để số < 0 sẽ không sợ nhầm và ta chỉ thêm vào bên phải. \n",
        "# >= 1 vì lúc tìm eids đã tính <s> nên đã +1 sẵn nên min = 1\n",
        "# giả sử 1 entity_eids chỉ có chiều dài max là 30\n",
        "pad_ent_eid = -2\n",
        "max_len_ent_eid = 30\n",
        "\n",
        "assert (pad_ent_eid < 0), str('pad_ent_eid must < 0')\n",
        "\n",
        "jtest_data_use = copy.deepcopy(jtest_data_v3)\n",
        "\n",
        "for isentif, sentif in enumerate(jtest_data_use):\n",
        "    \n",
        "    \n",
        "    if True:\n",
        "        # xlmr_base\n",
        "        # xlmr_base biến … thành ... khi tokenize nên cần sửa lại wpi_start_pos, wpi_end_pos\n",
        "                # và với cái wpi này thì chấp nhận sent_non_space[wpi_start_pos:wpi_end_pos] != clean_wpi\n",
        "                # tương tự với ½ thành 1⁄2\n",
        "\n",
        "        # Lưu ý: XLM-R có một số kí tự như … bị biến thành ..., ½ thành 1⁄2 dẫn tới việc check, kiểm tra so sánh chuỗi bị khác\n",
        "        # và dù trong câu lẫn word_tokenize_lst có kí tự \\ufeff nhưng khi XLM-R lại loại bỏ khi toknenize nên kí tự cũng bị thay đổi vị trí\n",
        "        # các vấn đề này được fix trong hàm: get_entity_word_piece_index()\n",
        "\n",
        "        #xlmr_base_input_sent = sentif['new_sentence']\n",
        "        xlmr_base_input_sent = u\" \".join(sentif['word_tokenize_lst'])\n",
        "\n",
        "        xlmr_base_encode_dict = xlmr_base_tokenizer(xlmr_base_input_sent, add_special_tokens=True, padding='max_length', max_length=746)\n",
        "\n",
        "        xlmr_base_tokenize_lst = xlmr_base_tokenizer.tokenize(xlmr_base_input_sent)\n",
        "        assert ((len(xlmr_base_tokenize_lst) + 2) <= 746), str('len xlmr_base_tokenize_lst > 746')\n",
        "\n",
        "        xlmr_base_entity_1_eids = get_entity_word_piece_index(sentif['sent_id'], xlmr_base_tokenize_lst, sentif['new_entity_1'], sentif['new_sentence'], 'xlmr_base', xlmr_base_encode_dict['input_ids'], 'test')\n",
        "        xlmr_base_entity_2_eids = get_entity_word_piece_index(sentif['sent_id'], xlmr_base_tokenize_lst, sentif['new_entity_2'], sentif['new_sentence'], 'xlmr_base', xlmr_base_encode_dict['input_ids'], 'test')\n",
        "        \n",
        "        # pad\n",
        "        assert ((len(xlmr_base_entity_1_eids) <= max_len_ent_eid) and (len(xlmr_base_entity_2_eids) <= max_len_ent_eid)), str('max_len_ent_eid must > ' + str(max_len_ent_eid))\n",
        "\n",
        "        xlmr_base_entity_1_eids += [pad_ent_eid] * (max_len_ent_eid - len(xlmr_base_entity_1_eids))\n",
        "        xlmr_base_entity_2_eids += [pad_ent_eid] * (max_len_ent_eid - len(xlmr_base_entity_2_eids))\n",
        "\n",
        "\n",
        "        if isentif < 10:\n",
        "            print('\\n')\n",
        "            print(xlmr_base_tokenize_lst)\n",
        "            print(sentif['new_entity_1']['text'])\n",
        "            print(xlmr_base_entity_1_eids)\n",
        "            print(sentif['new_entity_2']['text'])\n",
        "            print(xlmr_base_entity_2_eids)\n",
        "        \n",
        "\n",
        "        test_xlmr_base_input_ids.append(copy.deepcopy(xlmr_base_encode_dict['input_ids']))\n",
        "        test_xlmr_base_attention_masks.append(copy.deepcopy(xlmr_base_encode_dict['attention_mask']))\n",
        "        test_xlmr_base_entity_1_eids.append(copy.deepcopy(xlmr_base_entity_1_eids))\n",
        "        test_xlmr_base_entity_2_eids.append(copy.deepcopy(xlmr_base_entity_2_eids))\n",
        "        #test_xlmr_base_labels.append(copy.deepcopy(encode_label(sentif['label'])))\n",
        "\n",
        "    \n",
        "\n",
        "    if True:\n",
        "        # xlmr_large\n",
        "        # xlmr_large biến … thành ... khi tokenize nên cần sửa lại wpi_start_pos, wpi_end_pos\n",
        "                # và với cái wpi này thì chấp nhận sent_non_space[wpi_start_pos:wpi_end_pos] != clean_wpi\n",
        "                # tương tự với ½ thành 1⁄2\n",
        "\n",
        "        # Lưu ý: XLM-R có một số kí tự như … bị biến thành ..., ½ thành 1⁄2 dẫn tới việc check, kiểm tra so sánh chuỗi bị khác\n",
        "        # và dù trong câu lẫn word_tokenize_lst có kí tự \\ufeff nhưng khi XLM-R lại loại bỏ khi toknenize nên kí tự cũng bị thay đổi vị trí\n",
        "        # các vấn đề này được fix trong hàm: get_entity_word_piece_index()\n",
        "\n",
        "        #xlmr_large_input_sent = sentif['new_sentence']\n",
        "        xlmr_large_input_sent = u\" \".join(sentif['word_tokenize_lst'])\n",
        "\n",
        "        xlmr_large_encode_dict = xlmr_large_tokenizer(xlmr_large_input_sent, add_special_tokens=True, padding='max_length', max_length=746)\n",
        "\n",
        "        xlmr_large_tokenize_lst = xlmr_large_tokenizer.tokenize(xlmr_large_input_sent)\n",
        "        assert ((len(xlmr_large_tokenize_lst) + 2) <= 746), str('len xlmr_large_tokenize_lst > 746')\n",
        "\n",
        "        xlmr_large_entity_1_eids = get_entity_word_piece_index(sentif['sent_id'], xlmr_large_tokenize_lst, sentif['new_entity_1'], sentif['new_sentence'], 'xlmr_large', xlmr_large_encode_dict['input_ids'], 'test')\n",
        "        xlmr_large_entity_2_eids = get_entity_word_piece_index(sentif['sent_id'], xlmr_large_tokenize_lst, sentif['new_entity_2'], sentif['new_sentence'], 'xlmr_large', xlmr_large_encode_dict['input_ids'], 'test')\n",
        "        \n",
        "        # pad\n",
        "        assert ((len(xlmr_large_entity_1_eids) <= max_len_ent_eid) and (len(xlmr_large_entity_2_eids) <= max_len_ent_eid)), str('max_len_ent_eid must > ' + str(max_len_ent_eid))\n",
        "\n",
        "        xlmr_large_entity_1_eids += [pad_ent_eid] * (max_len_ent_eid - len(xlmr_large_entity_1_eids))\n",
        "        xlmr_large_entity_2_eids += [pad_ent_eid] * (max_len_ent_eid - len(xlmr_large_entity_2_eids))\n",
        "\n",
        "\n",
        "        if isentif < 10:\n",
        "            print('\\n')\n",
        "            print(xlmr_large_tokenize_lst)\n",
        "            print(sentif['new_entity_1']['text'])\n",
        "            print(xlmr_large_entity_1_eids)\n",
        "            print(sentif['new_entity_2']['text'])\n",
        "            print(xlmr_large_entity_2_eids)\n",
        "        \n",
        "\n",
        "        test_xlmr_large_input_ids.append(copy.deepcopy(xlmr_large_encode_dict['input_ids']))\n",
        "        test_xlmr_large_attention_masks.append(copy.deepcopy(xlmr_large_encode_dict['attention_mask']))\n",
        "        test_xlmr_large_entity_1_eids.append(copy.deepcopy(xlmr_large_entity_1_eids))\n",
        "        test_xlmr_large_entity_2_eids.append(copy.deepcopy(xlmr_large_entity_2_eids))\n",
        "        #test_xlmr_large_labels.append(copy.deepcopy(encode_label(sentif['label'])))\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    if True:\n",
        "        # phobert\n",
        "        # lưu ý: phải để phobert ở bên dưới xlmr_base để những thay đổi bên dưới chỉ ảnh hưởng tới phobert chứ không ảnh hưởng tới XLM-R\n",
        "        # trong các câu chứa các cụm dưới, chả hiểu sao trong phobert '’' khi đứng 1 mình thì encode là 1 wpi bthg \n",
        "        # nhưng đứng trong cụm dưới dưới thì lại thành <unk>\n",
        "        # ta sẽ đổi ’ thành ' thì sẽ k bị lỗi nữa\n",
        "\n",
        "        if ('Hanoi Kids ’ Art Center' in sentif['word_tokenize_lst']) or ('Girl’s Day' in sentif['word_tokenize_lst']) or\\\n",
        "        ('Jermaine O’Neal' in sentif['word_tokenize_lst']) or ('O’Neal' in sentif['word_tokenize_lst']) or\\\n",
        "        ('Kon K’tu' in sentif['word_tokenize_lst']) or ('M’nông' in sentif['word_tokenize_lst']) or\\\n",
        "        ('H’rung' in sentif['word_tokenize_lst']) or ('K’Yếu' in sentif['word_tokenize_lst']) or\\\n",
        "        ('Moody’s' in sentif['word_tokenize_lst']) or ('Las Vegas’s Route' in sentif['word_tokenize_lst']) or\\\n",
        "        ('Women’s Summit' in sentif['word_tokenize_lst']) or ('E’Twaun Moore' in sentif['word_tokenize_lst']) or\\\n",
        "        ('King’s Garden' in sentif['word_tokenize_lst']):\n",
        "            assert (sentif['new_sentence'].count('’') == 1), str('count ’ in new_sentence not equal 1.')\n",
        "\n",
        "            for iwtk_item, wtk_item in enumerate(sentif['word_tokenize_lst']):\n",
        "                if '’' in wtk_item:\n",
        "                    sentif['word_tokenize_lst'][iwtk_item] = sentif['word_tokenize_lst'][iwtk_item].replace('’', unicodedata.normalize(\"NFC\", \"\\'\"))\n",
        "                    break\n",
        "            \n",
        "\n",
        "            sentif['new_sentence'] = sentif['new_sentence'].replace('’', unicodedata.normalize(\"NFC\", \"\\'\"))\n",
        "            sentif['new_entity_1']['text'] = sentif['new_entity_1']['text'].replace('’', unicodedata.normalize(\"NFC\", \"\\'\"))\n",
        "            sentif['new_entity_2']['text'] = sentif['new_entity_2']['text'].replace('’', unicodedata.normalize(\"NFC\", \"\\'\"))\n",
        "        \n",
        "        if ('Garbinẽ Muguruza' in sentif['word_tokenize_lst']):\n",
        "            assert (sentif['new_sentence'].count('Garbinẽ Muguruza') == 1), str('count Garbinẽ Muguruza in new_sentence not equal 1.')\n",
        "\n",
        "            for iwtk_item, wtk_item in enumerate(sentif['word_tokenize_lst']):\n",
        "                if 'Garbinẽ Muguruza' in wtk_item:\n",
        "                    sentif['word_tokenize_lst'][iwtk_item] = sentif['word_tokenize_lst'][iwtk_item].replace('Garbinẽ Muguruza', unicodedata.normalize(\"NFC\", \"Garbine Muguruza\"))\n",
        "                    break\n",
        "            \n",
        "\n",
        "            sentif['new_sentence'] = sentif['new_sentence'].replace('Garbinẽ Muguruza', unicodedata.normalize(\"NFC\", \"Garbine Muguruza\"))\n",
        "            sentif['new_entity_1']['text'] = sentif['new_entity_1']['text'].replace('Garbinẽ Muguruza', unicodedata.normalize(\"NFC\", \"Garbine Muguruza\"))\n",
        "            sentif['new_entity_2']['text'] = sentif['new_entity_2']['text'].replace('Garbinẽ Muguruza', unicodedata.normalize(\"NFC\", \"Garbine Muguruza\"))\n",
        "\n",
        "\n",
        "        \n",
        "        pb_base_input_sent = u\" \".join([tk.replace(\" \", \"_\") for tk in sentif['word_tokenize_lst']])\n",
        "\n",
        "        pb_base_encode_dict = pb_base_tokenizer(pb_base_input_sent, add_special_tokens=True, padding='max_length', max_length=507)\n",
        "\n",
        "        pb_base_tokenize_lst = pb_base_tokenizer.tokenize(pb_base_input_sent)\n",
        "        assert ((len(pb_base_tokenize_lst) + 2) <= 507), str('len pb_base_tokenize_lst > 507')\n",
        "\n",
        "        pb_base_entity_1_eids = get_entity_word_piece_index(sentif['sent_id'], pb_base_tokenize_lst, sentif['new_entity_1'], sentif['new_sentence'], 'phobert_base', pb_base_encode_dict['input_ids'], 'test')\n",
        "        pb_base_entity_2_eids = get_entity_word_piece_index(sentif['sent_id'], pb_base_tokenize_lst, sentif['new_entity_2'], sentif['new_sentence'], 'phobert_base', pb_base_encode_dict['input_ids'], 'test')\n",
        "\n",
        "        # pad\n",
        "        assert ((len(pb_base_entity_1_eids) <= max_len_ent_eid) and (len(pb_base_entity_2_eids) <= max_len_ent_eid)), str('max_len_ent_eid must > ' + str(max_len_ent_eid))\n",
        "\n",
        "        pb_base_entity_1_eids += [pad_ent_eid] * (max_len_ent_eid - len(pb_base_entity_1_eids))\n",
        "        pb_base_entity_2_eids += [pad_ent_eid] * (max_len_ent_eid - len(pb_base_entity_2_eids))\n",
        "        \n",
        "\n",
        "\n",
        "        if isentif < 10:\n",
        "            print('\\n')\n",
        "            print(pb_base_tokenize_lst)\n",
        "            print(sentif['new_entity_1']['text'])\n",
        "            print(pb_base_entity_1_eids)\n",
        "            print(sentif['new_entity_2']['text'])\n",
        "            print(pb_base_entity_2_eids)\n",
        "        \n",
        "\n",
        "\n",
        "        test_pb_base_input_ids.append(copy.deepcopy(pb_base_encode_dict['input_ids']))\n",
        "        test_pb_base_attention_masks.append(copy.deepcopy(pb_base_encode_dict['attention_mask']))\n",
        "        test_pb_base_entity_1_eids.append(copy.deepcopy(pb_base_entity_1_eids))\n",
        "        test_pb_base_entity_2_eids.append(copy.deepcopy(pb_base_entity_2_eids))\n",
        "        #test_pb_base_labels.append(copy.deepcopy(encode_label(sentif['label'])))\n",
        "\n",
        "    \n",
        "    if True:\n",
        "        # phobert\n",
        "        # lưu ý: phải để phobert ở bên dưới xlmr_large để những thay đổi bên dưới chỉ ảnh hưởng tới phobert chứ không ảnh hưởng tới XLM-R\n",
        "        # trong các câu chứa các cụm dưới, chả hiểu sao trong phobert '’' khi đứng 1 mình thì encode là 1 wpi bthg \n",
        "        # nhưng đứng trong cụm dưới dưới thì lại thành <unk>\n",
        "        # ta sẽ đổi ’ thành ' thì sẽ k bị lỗi nữa\n",
        "        if ('Hanoi Kids ’ Art Center' in sentif['word_tokenize_lst']) or ('Girl’s Day' in sentif['word_tokenize_lst']) or\\\n",
        "        ('Jermaine O’Neal' in sentif['word_tokenize_lst']) or ('O’Neal' in sentif['word_tokenize_lst']) or\\\n",
        "        ('Kon K’tu' in sentif['word_tokenize_lst']) or ('M’nông' in sentif['word_tokenize_lst']) or\\\n",
        "        ('H’rung' in sentif['word_tokenize_lst']) or ('K’Yếu' in sentif['word_tokenize_lst']) or\\\n",
        "        ('Moody’s' in sentif['word_tokenize_lst']) or ('Las Vegas’s Route' in sentif['word_tokenize_lst']) or\\\n",
        "        ('Women’s Summit' in sentif['word_tokenize_lst']) or ('E’Twaun Moore' in sentif['word_tokenize_lst']) or\\\n",
        "        ('King’s Garden' in sentif['word_tokenize_lst']):\n",
        "            assert (sentif['new_sentence'].count('’') == 1), str('count ’ in new_sentence not equal 1.')\n",
        "\n",
        "            for iwtk_item, wtk_item in enumerate(sentif['word_tokenize_lst']):\n",
        "                if '’' in wtk_item:\n",
        "                    sentif['word_tokenize_lst'][iwtk_item] = sentif['word_tokenize_lst'][iwtk_item].replace('’', unicodedata.normalize(\"NFC\", \"\\'\"))\n",
        "                    break\n",
        "            \n",
        "\n",
        "            sentif['new_sentence'] = sentif['new_sentence'].replace('’', unicodedata.normalize(\"NFC\", \"\\'\"))\n",
        "            sentif['new_entity_1']['text'] = sentif['new_entity_1']['text'].replace('’', unicodedata.normalize(\"NFC\", \"\\'\"))\n",
        "            sentif['new_entity_2']['text'] = sentif['new_entity_2']['text'].replace('’', unicodedata.normalize(\"NFC\", \"\\'\"))\n",
        "        \n",
        "        if ('Garbinẽ Muguruza' in sentif['word_tokenize_lst']):\n",
        "            assert (sentif['new_sentence'].count('Garbinẽ Muguruza') == 1), str('count Garbinẽ Muguruza in new_sentence not equal 1.')\n",
        "\n",
        "            for iwtk_item, wtk_item in enumerate(sentif['word_tokenize_lst']):\n",
        "                if 'Garbinẽ Muguruza' in wtk_item:\n",
        "                    sentif['word_tokenize_lst'][iwtk_item] = sentif['word_tokenize_lst'][iwtk_item].replace('Garbinẽ Muguruza', unicodedata.normalize(\"NFC\", \"Garbine Muguruza\"))\n",
        "                    break\n",
        "            \n",
        "\n",
        "            sentif['new_sentence'] = sentif['new_sentence'].replace('Garbinẽ Muguruza', unicodedata.normalize(\"NFC\", \"Garbine Muguruza\"))\n",
        "            sentif['new_entity_1']['text'] = sentif['new_entity_1']['text'].replace('Garbinẽ Muguruza', unicodedata.normalize(\"NFC\", \"Garbine Muguruza\"))\n",
        "            sentif['new_entity_2']['text'] = sentif['new_entity_2']['text'].replace('Garbinẽ Muguruza', unicodedata.normalize(\"NFC\", \"Garbine Muguruza\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        pb_large_input_sent = u\" \".join([tk.replace(\" \", \"_\") for tk in sentif['word_tokenize_lst']])\n",
        "\n",
        "        pb_large_encode_dict = pb_large_tokenizer(pb_large_input_sent, add_special_tokens=True, padding='max_length', max_length=507)\n",
        "\n",
        "        pb_large_tokenize_lst = pb_large_tokenizer.tokenize(pb_large_input_sent)\n",
        "        assert ((len(pb_large_tokenize_lst) + 2) <= 507), str('len pb_large_tokenize_lst > 507')\n",
        "\n",
        "        pb_large_entity_1_eids = get_entity_word_piece_index(sentif['sent_id'], pb_large_tokenize_lst, sentif['new_entity_1'], sentif['new_sentence'], 'phobert_large', pb_large_encode_dict['input_ids'], 'test')\n",
        "        pb_large_entity_2_eids = get_entity_word_piece_index(sentif['sent_id'], pb_large_tokenize_lst, sentif['new_entity_2'], sentif['new_sentence'], 'phobert_large', pb_large_encode_dict['input_ids'], 'test')\n",
        "\n",
        "        # pad\n",
        "        assert ((len(pb_large_entity_1_eids) <= max_len_ent_eid) and (len(pb_large_entity_2_eids) <= max_len_ent_eid)), str('max_len_ent_eid must > ' + str(max_len_ent_eid))\n",
        "\n",
        "        pb_large_entity_1_eids += [pad_ent_eid] * (max_len_ent_eid - len(pb_large_entity_1_eids))\n",
        "        pb_large_entity_2_eids += [pad_ent_eid] * (max_len_ent_eid - len(pb_large_entity_2_eids))\n",
        "        \n",
        "\n",
        "\n",
        "        if isentif < 10:\n",
        "            print('\\n')\n",
        "            print(pb_large_tokenize_lst)\n",
        "            print(sentif['new_entity_1']['text'])\n",
        "            print(pb_large_entity_1_eids)\n",
        "            print(sentif['new_entity_2']['text'])\n",
        "            print(pb_large_entity_2_eids)\n",
        "        \n",
        "\n",
        "\n",
        "        test_pb_large_input_ids.append(copy.deepcopy(pb_large_encode_dict['input_ids']))\n",
        "        test_pb_large_attention_masks.append(copy.deepcopy(pb_large_encode_dict['attention_mask']))\n",
        "        test_pb_large_entity_1_eids.append(copy.deepcopy(pb_large_entity_1_eids))\n",
        "        test_pb_large_entity_2_eids.append(copy.deepcopy(pb_large_entity_2_eids))\n",
        "        #test_pb_large_labels.append(copy.deepcopy(encode_label(sentif['label'])))\n",
        "\n",
        "\n",
        "    test_labels.append(copy.deepcopy(encode_label(sentif['label'])))\n",
        "    sent_ids.append(copy.deepcopy(sentif['sent_id']))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "['▁Đây', '▁là', '▁lý', '▁do', '▁khiến', '▁Yo', 'on', '▁Ah', '▁quyết', '▁định', '▁cắt', '▁mái', '▁tóc', '▁dài', \"▁'\", '▁nữ', '▁thần', \"▁'\", '▁Má', 'i', '▁tóc', '▁cũ', '▁của', '▁thành', '▁viên', '▁SNS', 'D', '▁bị', '▁hư', '▁hỏng', '▁nặng', '▁n', 'ề', '▁và', '▁Yo', 'on', '▁Ah', '▁thậm', '▁chí', '▁không', '▁muốn', '▁nuôi', '▁tóc', '▁lại', '▁', '.']\n",
            "Yoon Ah\n",
            "[6, 7, 8, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "SNSD\n",
            "[26, 27, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Đây', '▁là', '▁lý', '▁do', '▁khiến', '▁Yo', 'on', '▁Ah', '▁quyết', '▁định', '▁cắt', '▁mái', '▁tóc', '▁dài', \"▁'\", '▁nữ', '▁thần', \"▁'\", '▁Má', 'i', '▁tóc', '▁cũ', '▁của', '▁thành', '▁viên', '▁SNS', 'D', '▁bị', '▁hư', '▁hỏng', '▁nặng', '▁n', 'ề', '▁và', '▁Yo', 'on', '▁Ah', '▁thậm', '▁chí', '▁không', '▁muốn', '▁nuôi', '▁tóc', '▁lại', '▁', '.']\n",
            "Yoon Ah\n",
            "[6, 7, 8, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "SNSD\n",
            "[26, 27, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Đây', 'là', 'lý_do', 'khiến', 'Yoon_@@', 'Ah', 'quyết_định', 'cắt', 'mái_tóc', 'dài', \"'\", 'nữ', 'thần', \"'\", 'Mái_tóc', 'cũ', 'của', 'thành_viên', 'SNSD', 'bị', 'hư_hỏng', 'nặng_nề', 'và', 'Yoon_@@', 'Ah', 'thậm_chí', 'không', 'muốn', 'nuôi', 'tóc', 'lại', '.']\n",
            "Yoon Ah\n",
            "[5, 6, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "SNSD\n",
            "[19, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Đây', 'là', 'lý_do', 'khiến', 'Yoon_@@', 'Ah', 'quyết_định', 'cắt', 'mái_tóc', 'dài', \"'\", 'nữ', 'thần', \"'\", 'Mái_tóc', 'cũ', 'của', 'thành_viên', 'SNSD', 'bị', 'hư_hỏng', 'nặng_nề', 'và', 'Yoon_@@', 'Ah', 'thậm_chí', 'không', 'muốn', 'nuôi', 'tóc', 'lại', '.']\n",
            "Yoon Ah\n",
            "[5, 6, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "SNSD\n",
            "[19, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Đây', '▁là', '▁lý', '▁do', '▁khiến', '▁Yo', 'on', '▁Ah', '▁quyết', '▁định', '▁cắt', '▁mái', '▁tóc', '▁dài', \"▁'\", '▁nữ', '▁thần', \"▁'\", '▁Má', 'i', '▁tóc', '▁cũ', '▁của', '▁thành', '▁viên', '▁SNS', 'D', '▁bị', '▁hư', '▁hỏng', '▁nặng', '▁n', 'ề', '▁và', '▁Yo', 'on', '▁Ah', '▁thậm', '▁chí', '▁không', '▁muốn', '▁nuôi', '▁tóc', '▁lại', '▁', '.']\n",
            "Yoon Ah\n",
            "[6, 7, 8, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[35, 36, 37, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Đây', '▁là', '▁lý', '▁do', '▁khiến', '▁Yo', 'on', '▁Ah', '▁quyết', '▁định', '▁cắt', '▁mái', '▁tóc', '▁dài', \"▁'\", '▁nữ', '▁thần', \"▁'\", '▁Má', 'i', '▁tóc', '▁cũ', '▁của', '▁thành', '▁viên', '▁SNS', 'D', '▁bị', '▁hư', '▁hỏng', '▁nặng', '▁n', 'ề', '▁và', '▁Yo', 'on', '▁Ah', '▁thậm', '▁chí', '▁không', '▁muốn', '▁nuôi', '▁tóc', '▁lại', '▁', '.']\n",
            "Yoon Ah\n",
            "[6, 7, 8, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[35, 36, 37, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Đây', 'là', 'lý_do', 'khiến', 'Yoon_@@', 'Ah', 'quyết_định', 'cắt', 'mái_tóc', 'dài', \"'\", 'nữ', 'thần', \"'\", 'Mái_tóc', 'cũ', 'của', 'thành_viên', 'SNSD', 'bị', 'hư_hỏng', 'nặng_nề', 'và', 'Yoon_@@', 'Ah', 'thậm_chí', 'không', 'muốn', 'nuôi', 'tóc', 'lại', '.']\n",
            "Yoon Ah\n",
            "[5, 6, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[24, 25, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Đây', 'là', 'lý_do', 'khiến', 'Yoon_@@', 'Ah', 'quyết_định', 'cắt', 'mái_tóc', 'dài', \"'\", 'nữ', 'thần', \"'\", 'Mái_tóc', 'cũ', 'của', 'thành_viên', 'SNSD', 'bị', 'hư_hỏng', 'nặng_nề', 'và', 'Yoon_@@', 'Ah', 'thậm_chí', 'không', 'muốn', 'nuôi', 'tóc', 'lại', '.']\n",
            "Yoon Ah\n",
            "[5, 6, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[24, 25, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Đây', '▁là', '▁lý', '▁do', '▁khiến', '▁Yo', 'on', '▁Ah', '▁quyết', '▁định', '▁cắt', '▁mái', '▁tóc', '▁dài', \"▁'\", '▁nữ', '▁thần', \"▁'\", '▁Má', 'i', '▁tóc', '▁cũ', '▁của', '▁thành', '▁viên', '▁SNS', 'D', '▁bị', '▁hư', '▁hỏng', '▁nặng', '▁n', 'ề', '▁và', '▁Yo', 'on', '▁Ah', '▁thậm', '▁chí', '▁không', '▁muốn', '▁nuôi', '▁tóc', '▁lại', '▁', '.']\n",
            "SNSD\n",
            "[26, 27, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[35, 36, 37, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Đây', '▁là', '▁lý', '▁do', '▁khiến', '▁Yo', 'on', '▁Ah', '▁quyết', '▁định', '▁cắt', '▁mái', '▁tóc', '▁dài', \"▁'\", '▁nữ', '▁thần', \"▁'\", '▁Má', 'i', '▁tóc', '▁cũ', '▁của', '▁thành', '▁viên', '▁SNS', 'D', '▁bị', '▁hư', '▁hỏng', '▁nặng', '▁n', 'ề', '▁và', '▁Yo', 'on', '▁Ah', '▁thậm', '▁chí', '▁không', '▁muốn', '▁nuôi', '▁tóc', '▁lại', '▁', '.']\n",
            "SNSD\n",
            "[26, 27, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[35, 36, 37, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Đây', 'là', 'lý_do', 'khiến', 'Yoon_@@', 'Ah', 'quyết_định', 'cắt', 'mái_tóc', 'dài', \"'\", 'nữ', 'thần', \"'\", 'Mái_tóc', 'cũ', 'của', 'thành_viên', 'SNSD', 'bị', 'hư_hỏng', 'nặng_nề', 'và', 'Yoon_@@', 'Ah', 'thậm_chí', 'không', 'muốn', 'nuôi', 'tóc', 'lại', '.']\n",
            "SNSD\n",
            "[19, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[24, 25, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Đây', 'là', 'lý_do', 'khiến', 'Yoon_@@', 'Ah', 'quyết_định', 'cắt', 'mái_tóc', 'dài', \"'\", 'nữ', 'thần', \"'\", 'Mái_tóc', 'cũ', 'của', 'thành_viên', 'SNSD', 'bị', 'hư_hỏng', 'nặng_nề', 'và', 'Yoon_@@', 'Ah', 'thậm_chí', 'không', 'muốn', 'nuôi', 'tóc', 'lại', '.']\n",
            "SNSD\n",
            "[19, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[24, 25, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Soo', '▁Young', '▁và', '▁Seo', '▁Hy', 'un', '▁cắt', '▁để', '▁đóng', '▁phim', '▁', ',', '▁Yo', 'on', '▁Ah', '▁và', '▁Sunny', '▁muốn', '▁thay', '▁đổi', '▁bản', '▁thân', '▁và', '▁Yuri', '▁cũng', '▁mới', '▁tỉ', 'a', '▁thành', '▁kiểu', '▁tóc', '▁ngang', '▁vai', '▁trẻ', '▁trung', '▁', '.']\n",
            "Soo Young\n",
            "[1, 2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Seo Hyun\n",
            "[4, 5, 6, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Soo', '▁Young', '▁và', '▁Seo', '▁Hy', 'un', '▁cắt', '▁để', '▁đóng', '▁phim', '▁', ',', '▁Yo', 'on', '▁Ah', '▁và', '▁Sunny', '▁muốn', '▁thay', '▁đổi', '▁bản', '▁thân', '▁và', '▁Yuri', '▁cũng', '▁mới', '▁tỉ', 'a', '▁thành', '▁kiểu', '▁tóc', '▁ngang', '▁vai', '▁trẻ', '▁trung', '▁', '.']\n",
            "Soo Young\n",
            "[1, 2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Seo Hyun\n",
            "[4, 5, 6, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Soo_@@', 'Young', 'và', 'Seo_@@', 'Hyun', 'cắt', 'để', 'đóng_@@', 'phim', ',', 'Yoon_@@', 'Ah', 'và', 'Sunny', 'muốn', 'thay_đổi', 'bản_thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang_@@', 'vai', 'trẻ_trung', '.']\n",
            "Soo Young\n",
            "[1, 2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Seo Hyun\n",
            "[4, 5, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Soo_@@', 'Young', 'và', 'Seo_@@', 'Hyun', 'cắt', 'để', 'đóng_@@', 'phim', ',', 'Yoon_@@', 'Ah', 'và', 'Sunny', 'muốn', 'thay_đổi', 'bản_thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang_@@', 'vai', 'trẻ_trung', '.']\n",
            "Soo Young\n",
            "[1, 2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Seo Hyun\n",
            "[4, 5, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Soo', '▁Young', '▁và', '▁Seo', '▁Hy', 'un', '▁cắt', '▁để', '▁đóng', '▁phim', '▁', ',', '▁Yo', 'on', '▁Ah', '▁và', '▁Sunny', '▁muốn', '▁thay', '▁đổi', '▁bản', '▁thân', '▁và', '▁Yuri', '▁cũng', '▁mới', '▁tỉ', 'a', '▁thành', '▁kiểu', '▁tóc', '▁ngang', '▁vai', '▁trẻ', '▁trung', '▁', '.']\n",
            "Soo Young\n",
            "[1, 2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[13, 14, 15, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Soo', '▁Young', '▁và', '▁Seo', '▁Hy', 'un', '▁cắt', '▁để', '▁đóng', '▁phim', '▁', ',', '▁Yo', 'on', '▁Ah', '▁và', '▁Sunny', '▁muốn', '▁thay', '▁đổi', '▁bản', '▁thân', '▁và', '▁Yuri', '▁cũng', '▁mới', '▁tỉ', 'a', '▁thành', '▁kiểu', '▁tóc', '▁ngang', '▁vai', '▁trẻ', '▁trung', '▁', '.']\n",
            "Soo Young\n",
            "[1, 2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[13, 14, 15, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Soo_@@', 'Young', 'và', 'Seo_@@', 'Hyun', 'cắt', 'để', 'đóng_@@', 'phim', ',', 'Yoon_@@', 'Ah', 'và', 'Sunny', 'muốn', 'thay_đổi', 'bản_thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang_@@', 'vai', 'trẻ_trung', '.']\n",
            "Soo Young\n",
            "[1, 2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[11, 12, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Soo_@@', 'Young', 'và', 'Seo_@@', 'Hyun', 'cắt', 'để', 'đóng_@@', 'phim', ',', 'Yoon_@@', 'Ah', 'và', 'Sunny', 'muốn', 'thay_đổi', 'bản_thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang_@@', 'vai', 'trẻ_trung', '.']\n",
            "Soo Young\n",
            "[1, 2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[11, 12, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Soo', '▁Young', '▁và', '▁Seo', '▁Hy', 'un', '▁cắt', '▁để', '▁đóng', '▁phim', '▁', ',', '▁Yo', 'on', '▁Ah', '▁và', '▁Sunny', '▁muốn', '▁thay', '▁đổi', '▁bản', '▁thân', '▁và', '▁Yuri', '▁cũng', '▁mới', '▁tỉ', 'a', '▁thành', '▁kiểu', '▁tóc', '▁ngang', '▁vai', '▁trẻ', '▁trung', '▁', '.']\n",
            "Soo Young\n",
            "[1, 2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Sunny\n",
            "[17, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Soo', '▁Young', '▁và', '▁Seo', '▁Hy', 'un', '▁cắt', '▁để', '▁đóng', '▁phim', '▁', ',', '▁Yo', 'on', '▁Ah', '▁và', '▁Sunny', '▁muốn', '▁thay', '▁đổi', '▁bản', '▁thân', '▁và', '▁Yuri', '▁cũng', '▁mới', '▁tỉ', 'a', '▁thành', '▁kiểu', '▁tóc', '▁ngang', '▁vai', '▁trẻ', '▁trung', '▁', '.']\n",
            "Soo Young\n",
            "[1, 2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Sunny\n",
            "[17, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Soo_@@', 'Young', 'và', 'Seo_@@', 'Hyun', 'cắt', 'để', 'đóng_@@', 'phim', ',', 'Yoon_@@', 'Ah', 'và', 'Sunny', 'muốn', 'thay_đổi', 'bản_thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang_@@', 'vai', 'trẻ_trung', '.']\n",
            "Soo Young\n",
            "[1, 2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Sunny\n",
            "[14, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Soo_@@', 'Young', 'và', 'Seo_@@', 'Hyun', 'cắt', 'để', 'đóng_@@', 'phim', ',', 'Yoon_@@', 'Ah', 'và', 'Sunny', 'muốn', 'thay_đổi', 'bản_thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang_@@', 'vai', 'trẻ_trung', '.']\n",
            "Soo Young\n",
            "[1, 2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Sunny\n",
            "[14, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Soo', '▁Young', '▁và', '▁Seo', '▁Hy', 'un', '▁cắt', '▁để', '▁đóng', '▁phim', '▁', ',', '▁Yo', 'on', '▁Ah', '▁và', '▁Sunny', '▁muốn', '▁thay', '▁đổi', '▁bản', '▁thân', '▁và', '▁Yuri', '▁cũng', '▁mới', '▁tỉ', 'a', '▁thành', '▁kiểu', '▁tóc', '▁ngang', '▁vai', '▁trẻ', '▁trung', '▁', '.']\n",
            "Seo Hyun\n",
            "[4, 5, 6, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[13, 14, 15, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Soo', '▁Young', '▁và', '▁Seo', '▁Hy', 'un', '▁cắt', '▁để', '▁đóng', '▁phim', '▁', ',', '▁Yo', 'on', '▁Ah', '▁và', '▁Sunny', '▁muốn', '▁thay', '▁đổi', '▁bản', '▁thân', '▁và', '▁Yuri', '▁cũng', '▁mới', '▁tỉ', 'a', '▁thành', '▁kiểu', '▁tóc', '▁ngang', '▁vai', '▁trẻ', '▁trung', '▁', '.']\n",
            "Seo Hyun\n",
            "[4, 5, 6, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[13, 14, 15, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Soo_@@', 'Young', 'và', 'Seo_@@', 'Hyun', 'cắt', 'để', 'đóng_@@', 'phim', ',', 'Yoon_@@', 'Ah', 'và', 'Sunny', 'muốn', 'thay_đổi', 'bản_thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang_@@', 'vai', 'trẻ_trung', '.']\n",
            "Seo Hyun\n",
            "[4, 5, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[11, 12, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Soo_@@', 'Young', 'và', 'Seo_@@', 'Hyun', 'cắt', 'để', 'đóng_@@', 'phim', ',', 'Yoon_@@', 'Ah', 'và', 'Sunny', 'muốn', 'thay_đổi', 'bản_thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang_@@', 'vai', 'trẻ_trung', '.']\n",
            "Seo Hyun\n",
            "[4, 5, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Yoon Ah\n",
            "[11, 12, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Soo', '▁Young', '▁và', '▁Seo', '▁Hy', 'un', '▁cắt', '▁để', '▁đóng', '▁phim', '▁', ',', '▁Yo', 'on', '▁Ah', '▁và', '▁Sunny', '▁muốn', '▁thay', '▁đổi', '▁bản', '▁thân', '▁và', '▁Yuri', '▁cũng', '▁mới', '▁tỉ', 'a', '▁thành', '▁kiểu', '▁tóc', '▁ngang', '▁vai', '▁trẻ', '▁trung', '▁', '.']\n",
            "Seo Hyun\n",
            "[4, 5, 6, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Sunny\n",
            "[17, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Soo', '▁Young', '▁và', '▁Seo', '▁Hy', 'un', '▁cắt', '▁để', '▁đóng', '▁phim', '▁', ',', '▁Yo', 'on', '▁Ah', '▁và', '▁Sunny', '▁muốn', '▁thay', '▁đổi', '▁bản', '▁thân', '▁và', '▁Yuri', '▁cũng', '▁mới', '▁tỉ', 'a', '▁thành', '▁kiểu', '▁tóc', '▁ngang', '▁vai', '▁trẻ', '▁trung', '▁', '.']\n",
            "Seo Hyun\n",
            "[4, 5, 6, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Sunny\n",
            "[17, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Soo_@@', 'Young', 'và', 'Seo_@@', 'Hyun', 'cắt', 'để', 'đóng_@@', 'phim', ',', 'Yoon_@@', 'Ah', 'và', 'Sunny', 'muốn', 'thay_đổi', 'bản_thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang_@@', 'vai', 'trẻ_trung', '.']\n",
            "Seo Hyun\n",
            "[4, 5, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Sunny\n",
            "[14, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Soo_@@', 'Young', 'và', 'Seo_@@', 'Hyun', 'cắt', 'để', 'đóng_@@', 'phim', ',', 'Yoon_@@', 'Ah', 'và', 'Sunny', 'muốn', 'thay_đổi', 'bản_thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang_@@', 'vai', 'trẻ_trung', '.']\n",
            "Seo Hyun\n",
            "[4, 5, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Sunny\n",
            "[14, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Soo', '▁Young', '▁và', '▁Seo', '▁Hy', 'un', '▁cắt', '▁để', '▁đóng', '▁phim', '▁', ',', '▁Yo', 'on', '▁Ah', '▁và', '▁Sunny', '▁muốn', '▁thay', '▁đổi', '▁bản', '▁thân', '▁và', '▁Yuri', '▁cũng', '▁mới', '▁tỉ', 'a', '▁thành', '▁kiểu', '▁tóc', '▁ngang', '▁vai', '▁trẻ', '▁trung', '▁', '.']\n",
            "Yoon Ah\n",
            "[13, 14, 15, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Sunny\n",
            "[17, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Soo', '▁Young', '▁và', '▁Seo', '▁Hy', 'un', '▁cắt', '▁để', '▁đóng', '▁phim', '▁', ',', '▁Yo', 'on', '▁Ah', '▁và', '▁Sunny', '▁muốn', '▁thay', '▁đổi', '▁bản', '▁thân', '▁và', '▁Yuri', '▁cũng', '▁mới', '▁tỉ', 'a', '▁thành', '▁kiểu', '▁tóc', '▁ngang', '▁vai', '▁trẻ', '▁trung', '▁', '.']\n",
            "Yoon Ah\n",
            "[13, 14, 15, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Sunny\n",
            "[17, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Soo_@@', 'Young', 'và', 'Seo_@@', 'Hyun', 'cắt', 'để', 'đóng_@@', 'phim', ',', 'Yoon_@@', 'Ah', 'và', 'Sunny', 'muốn', 'thay_đổi', 'bản_thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang_@@', 'vai', 'trẻ_trung', '.']\n",
            "Yoon Ah\n",
            "[11, 12, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Sunny\n",
            "[14, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Soo_@@', 'Young', 'và', 'Seo_@@', 'Hyun', 'cắt', 'để', 'đóng_@@', 'phim', ',', 'Yoon_@@', 'Ah', 'và', 'Sunny', 'muốn', 'thay_đổi', 'bản_thân', 'và', 'Yuri', 'cũng', 'mới', 'tỉa', 'thành', 'kiểu', 'tóc', 'ngang_@@', 'vai', 'trẻ_trung', '.']\n",
            "Yoon Ah\n",
            "[11, 12, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Sunny\n",
            "[14, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Bà', 'y', '▁tỏ', '▁quan', '▁điểm', '▁cũng', '▁b', 'ực', '▁tức', '▁như', '▁Tu', '▁Din', 'h', '▁Hu', 'ong', '▁', ',', '▁thành', '▁viên', '▁Py', 'mini', '▁tiếp', '▁lời', '▁:', '▁\"', '▁Nhà', '▁mình', '▁mà', '▁phần', '▁cho', '▁người', '▁sau', '▁thì', '▁còn', '▁chọn', '▁toàn', '▁cái', '▁ngon', '▁để', '▁phần', '▁', ',', '▁chứ', '▁không', '▁có', '▁chuyện', '▁ăn', '▁uống', '▁bày', '▁b', 'ừ', 'a', '▁', ',', '▁mặc', '▁k', 'ệ', '▁người', '▁ăn', '▁sau', '▁như', '▁thế', '▁này', '▁”', '▁', '.']\n",
            "Tu Dinh Huong\n",
            "[11, 12, 13, 14, 15, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Pymini\n",
            "[20, 21, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['▁Bà', 'y', '▁tỏ', '▁quan', '▁điểm', '▁cũng', '▁b', 'ực', '▁tức', '▁như', '▁Tu', '▁Din', 'h', '▁Hu', 'ong', '▁', ',', '▁thành', '▁viên', '▁Py', 'mini', '▁tiếp', '▁lời', '▁:', '▁\"', '▁Nhà', '▁mình', '▁mà', '▁phần', '▁cho', '▁người', '▁sau', '▁thì', '▁còn', '▁chọn', '▁toàn', '▁cái', '▁ngon', '▁để', '▁phần', '▁', ',', '▁chứ', '▁không', '▁có', '▁chuyện', '▁ăn', '▁uống', '▁bày', '▁b', 'ừ', 'a', '▁', ',', '▁mặc', '▁k', 'ệ', '▁người', '▁ăn', '▁sau', '▁như', '▁thế', '▁này', '▁”', '▁', '.']\n",
            "Tu Dinh Huong\n",
            "[11, 12, 13, 14, 15, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Pymini\n",
            "[20, 21, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Bày_tỏ', 'quan_điểm', 'cũng', 'b@@', 'ực_t@@', 'ức_@@', 'như', 'Tu_@@', 'Dinh_@@', 'Hu@@', 'ong', ',', 'thành_viên', 'Py@@', 'mini', 'tiếp_lời', ':', '\"', 'Nhà', 'mình', 'mà', 'phần', 'cho', 'người', 'sau', 'thì', 'còn', 'chọn', 'toàn', 'cái', 'ngon', 'để_phần', ',', 'chứ', 'không', 'có', 'chuyện', 'ăn_uống', 'bày', 'bừa', ',', 'mặc_kệ', 'người', 'ăn', 'sau', 'như', 'thế_này', '”', '.']\n",
            "Tu Dinh Huong\n",
            "[8, 9, 10, 11, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Pymini\n",
            "[14, 15, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "\n",
            "\n",
            "['Bày_tỏ', 'quan_điểm', 'cũng', 'b@@', 'ực_t@@', 'ức_@@', 'như', 'Tu_@@', 'Dinh_@@', 'Hu@@', 'ong', ',', 'thành_viên', 'Py@@', 'mini', 'tiếp_lời', ':', '\"', 'Nhà', 'mình', 'mà', 'phần', 'cho', 'người', 'sau', 'thì', 'còn', 'chọn', 'toàn', 'cái', 'ngon', 'để_phần', ',', 'chứ', 'không', 'có', 'chuyện', 'ăn_uống', 'bày', 'bừa', ',', 'mặc_kệ', 'người', 'ăn', 'sau', 'như', 'thế_này', '”', '.']\n",
            "Tu Dinh Huong\n",
            "[8, 9, 10, 11, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n",
            "Pymini\n",
            "[14, 15, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-yzwOtCqi13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48de91b-fc8c-41c9-d7dd-51443a5ea90c"
      },
      "source": [
        "# convert to tensor\n",
        "\n",
        "pad_eid = -2\n",
        "\n",
        "\n",
        "test_pb_base_input_ids = torch.tensor(test_pb_base_input_ids)\n",
        "test_pb_base_attention_masks = torch.tensor(test_pb_base_attention_masks)\n",
        "test_pb_base_entity_1_eids = torch.tensor(test_pb_base_entity_1_eids)\n",
        "test_pb_base_entity_2_eids = torch.tensor(test_pb_base_entity_2_eids)\n",
        "#test_pb_labels = torch.tensor(test_pb_labels)\n",
        "\n",
        "test_xlmr_base_input_ids = torch.tensor(test_xlmr_base_input_ids)\n",
        "test_xlmr_base_attention_masks = torch.tensor(test_xlmr_base_attention_masks)\n",
        "test_xlmr_base_entity_1_eids = torch.tensor(test_xlmr_base_entity_1_eids)\n",
        "test_xlmr_base_entity_2_eids = torch.tensor(test_xlmr_base_entity_2_eids)\n",
        "#test_xlmr_labels = torch.tensor(test_xlmr_labels)\n",
        "\n",
        "\n",
        "\n",
        "test_pb_large_input_ids = torch.tensor(test_pb_large_input_ids)\n",
        "test_pb_large_attention_masks = torch.tensor(test_pb_large_attention_masks)\n",
        "test_pb_large_entity_1_eids = torch.tensor(test_pb_large_entity_1_eids)\n",
        "test_pb_large_entity_2_eids = torch.tensor(test_pb_large_entity_2_eids)\n",
        "#test_pb_labels = torch.tensor(test_pb_labels)\n",
        "\n",
        "\n",
        "\n",
        "test_xlmr_large_input_ids = torch.tensor(test_xlmr_large_input_ids)\n",
        "test_xlmr_large_attention_masks = torch.tensor(test_xlmr_large_attention_masks)\n",
        "test_xlmr_large_entity_1_eids = torch.tensor(test_xlmr_large_entity_1_eids)\n",
        "test_xlmr_large_entity_2_eids = torch.tensor(test_xlmr_large_entity_2_eids)\n",
        "#test_xlmr_labels = torch.tensor(test_xlmr_labels)\n",
        "\n",
        "sent_ids = torch.tensor(sent_ids)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "print('DONE')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXbHGBay7ZhL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b45e14c4-ec9f-485a-dacc-02be2b68dc92"
      },
      "source": [
        "print(torch.all(torch.eq(test_pb_base_input_ids, test_pb_large_input_ids)))\n",
        "print(torch.all(torch.eq(test_pb_base_attention_masks, test_pb_large_attention_masks)))\n",
        "print(torch.all(torch.eq(test_pb_base_entity_1_eids, test_pb_large_entity_1_eids)))\n",
        "print(torch.all(torch.eq(test_pb_base_entity_2_eids, test_pb_large_entity_2_eids)))\n",
        "\n",
        "print(torch.all(torch.eq(test_xlmr_base_input_ids, test_xlmr_large_input_ids)))\n",
        "print(torch.all(torch.eq(test_xlmr_base_attention_masks, test_xlmr_large_attention_masks)))\n",
        "print(torch.all(torch.eq(test_xlmr_base_entity_1_eids, test_xlmr_large_entity_1_eids)))\n",
        "print(torch.all(torch.eq(test_xlmr_base_entity_2_eids, test_xlmr_large_entity_2_eids)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOlIRV0x8suz"
      },
      "source": [
        "test_pb_input_ids = copy.deepcopy(test_pb_base_input_ids)\n",
        "test_pb_attention_masks = copy.deepcopy(test_pb_base_attention_masks)\n",
        "test_pb_entity_1_eids = copy.deepcopy(test_pb_base_entity_1_eids)\n",
        "test_pb_entity_2_eids = copy.deepcopy(test_pb_base_entity_2_eids)\n",
        "#test_pb_labels = torch.tensor(test_pb_labels)\n",
        "\n",
        "test_xlmr_input_ids = copy.deepcopy(test_xlmr_base_input_ids)\n",
        "test_xlmr_attention_masks = copy.deepcopy(test_xlmr_base_attention_masks)\n",
        "test_xlmr_entity_1_eids = copy.deepcopy(test_xlmr_base_entity_1_eids)\n",
        "test_xlmr_entity_2_eids = copy.deepcopy(test_xlmr_base_entity_2_eids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQnGMcvkDajP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6625911e-bb38-4c3a-a045-bf3f7fbf8940"
      },
      "source": [
        "'''\n",
        "if (flags['use_phobert'] == True) and (flags['use_xlmr'] == True):\n",
        "    train_dataset = TensorDataset(train_pb_input_ids, train_pb_attention_masks, train_pb_entity_1_eids, train_pb_entity_2_eids, \\\n",
        "                                  train_xlmr_input_ids, train_xlmr_attention_masks, train_xlmr_entity_1_eids, train_xlmr_entity_2_eids, \\\n",
        "                                  train_labels)\n",
        "\n",
        "elif (flags['use_phobert'] == True) and (flags['use_xlmr'] == False):\n",
        "    train_dataset = TensorDataset(train_pb_input_ids, train_pb_attention_masks, train_pb_entity_1_eids, train_pb_entity_2_eids, train_labels)\n",
        "\n",
        "elif (flags['use_phobert'] == False) and (flags['use_xlmr'] == True):\n",
        "    train_dataset = TensorDataset(train_xlmr_input_ids, train_xlmr_attention_masks, train_xlmr_entity_1_eids, train_xlmr_entity_2_eids, train_labels)\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nif (flags['use_phobert'] == True) and (flags['use_xlmr'] == True):\\n    train_dataset = TensorDataset(train_pb_input_ids, train_pb_attention_masks, train_pb_entity_1_eids, train_pb_entity_2_eids,                                   train_xlmr_input_ids, train_xlmr_attention_masks, train_xlmr_entity_1_eids, train_xlmr_entity_2_eids,                                   train_labels)\\n\\nelif (flags['use_phobert'] == True) and (flags['use_xlmr'] == False):\\n    train_dataset = TensorDataset(train_pb_input_ids, train_pb_attention_masks, train_pb_entity_1_eids, train_pb_entity_2_eids, train_labels)\\n\\nelif (flags['use_phobert'] == False) and (flags['use_xlmr'] == True):\\n    train_dataset = TensorDataset(train_xlmr_input_ids, train_xlmr_attention_masks, train_xlmr_entity_1_eids, train_xlmr_entity_2_eids, train_labels)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYAF_MORQfQi"
      },
      "source": [
        "#train_dataloader = DataLoader(train_dataset, batch_size=flags['batch_size'], shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2w44XoqKy_P"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP4BFfO0cVk1"
      },
      "source": [
        "class BERTModel(nn.Module):\n",
        "    def __init__(self, model_type, entity_handle_type, emb_layer_lst, emb_layer_handle_type):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model_type = model_type\n",
        "\n",
        "        if self.model_type == 'phobert_base':\n",
        "            print('Using ', self.model_type, '.')\n",
        "            self.bert_model = AutoModel.from_pretrained(\"vinai/phobert-base\", output_hidden_states=True)\n",
        "\n",
        "        elif self.model_type == 'phobert_large':\n",
        "            print('Using ', self.model_type, '.')\n",
        "            self.bert_model = AutoModel.from_pretrained(\"vinai/phobert-large\", output_hidden_states=True)\n",
        "            \n",
        "        elif self.model_type == 'xlmr_base':\n",
        "            print('Using ', self.model_type, '.')\n",
        "            self.bert_model = XLMRobertaModel.from_pretrained('xlm-roberta-base', output_hidden_states=True)\n",
        "\n",
        "        elif self.model_type == 'xlmr_large':\n",
        "            print('Using ', self.model_type, '.')\n",
        "            self.bert_model = XLMRobertaModel.from_pretrained('xlm-roberta-large', output_hidden_states=True)\n",
        "        \n",
        "        else:\n",
        "            assert False, str('Unkown model name: ' + self.model_type + '. Allow: phobert_base, phobert_large, xlmr_base, xlmr_large')\n",
        "\n",
        "\n",
        "        self.entity_handle_type = entity_handle_type\n",
        "        self.emb_layer_lst = emb_layer_lst\n",
        "        self.emb_layer_handle_type = emb_layer_handle_type\n",
        "\n",
        "        # dùng để kiểm tra 1 phần xem code có chạy ổn không\n",
        "        self.sent_emb_len, self.wpi_emb_len = self.calculate_len_embedding()\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, b_input_ids, b_attention_mask, b_entity_1_eids, b_entity_2_eids):\n",
        "\n",
        "        outputs = self.bert_model(b_input_ids, b_attention_mask)\n",
        "        \n",
        "        # num_layer (13) * batch_size * max_sent_len * emb_size\n",
        "        hidden_states = outputs[2]\n",
        "\n",
        "        b_sent_final_embedding = self.get_sent_final_vector(hidden_states, b_entity_1_eids, b_entity_2_eids)\n",
        "\n",
        "        return b_sent_final_embedding\n",
        "\n",
        "\n",
        "    '''\n",
        "\n",
        "    hàm dưới sẽ duyệt từng cặp enitty trong 1 câu\n",
        "    sau đó tìm vector đại diện cho từng entity trong cặp này -> 2 vector embedding đại diện cho 2 entity\n",
        "    từ 2 vector này ta sẽ kết hợp lại theo luật dưới, rồi concat lại với nhau\n",
        "    [h_s,h_t,h_s*h_t,h_s+h_t,|h_s-h_t|]\n",
        "    cuối cùng sẽ thu được 1 vector duy nhất đại diện cho câu, và vector này có thể dùng cho lớp linear,... để phân loại\n",
        "\n",
        "    Lưu ý: hàm này sẽ trả về batch vector đại diện cho batch câu\n",
        "    '''\n",
        "    def get_sent_final_vector(self, hidden_states, b_entity_1_eids, b_entity_2_eids):\n",
        "\n",
        "        assert (len(b_entity_1_eids) == len(b_entity_2_eids)), str('len(b_entity_1_eids) != len(b_entity_2_eids)')\n",
        "            \n",
        "        sent_final_embedding_lst = []\n",
        "\n",
        "        for isent in range(len(b_entity_1_eids)):   # từng câu 1 trong batch\n",
        "\n",
        "            \n",
        "            entity_1_final_vector = self.get_entity_embedding_vector(hidden_states, b_entity_1_eids[isent], isent)\n",
        "            entity_2_final_vector = self.get_entity_embedding_vector(hidden_states, b_entity_2_eids[isent], isent)\n",
        "\n",
        "            assert (entity_1_final_vector.size() == entity_2_final_vector.size()), str('entity_1_final_vector size != entity_2_final_vector size')\n",
        "\n",
        "            # not implement custom emb stack yet \n",
        "            entity_sum_vector = torch.add(entity_1_final_vector, entity_2_final_vector)\n",
        "            entity_mul_vector = torch.mul(entity_1_final_vector, entity_2_final_vector)\n",
        "            entity_abs_sub_vector = torch.abs(torch.sub(entity_1_final_vector, entity_2_final_vector))\n",
        "\n",
        "            # [h_s,h_t,h_s*h_t,h_s+h_t,|h_s-h_t|]\n",
        "            sent_final_vector = torch.cat((entity_1_final_vector, entity_2_final_vector, entity_mul_vector, entity_sum_vector, entity_abs_sub_vector))\n",
        "\n",
        "            assert (len(sent_final_vector.size()) == 1), str('sent_final_vector is not a vector')\n",
        "\n",
        "            assert (sent_final_vector.size()[0] == self.sent_emb_len), str('sent_emb_len is not: ' + str(self.sent_emb_len))\n",
        "\n",
        "\n",
        "            sent_final_embedding_lst.append(sent_final_vector)\n",
        "\n",
        "\n",
        "        assert (len(sent_final_embedding_lst) == len(b_entity_1_eids)), str(self.model_type + ': len batch sent embedding not qual batch_size.')\n",
        "        sent_final_embedding_lst = torch.stack(sent_final_embedding_lst)  # convert from 'python list of tensors' to 'pytorch tensor of tensers'\n",
        "\n",
        "\n",
        "        return sent_final_embedding_lst\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "    hàm dưới dùng để lấy ra 1 vector embedding đại diện duy nhất cho 1 entity\n",
        "    đầu tiên, duyệt từng word pice trong danh sách cách word piece của entity\n",
        "\n",
        "    với mỗi word pice, ta sẽ lấy embedding của word piece này ở các layer mà ta muốn sau đó gộp thành 1 vector đại diện cho word pice duy nhất\n",
        "    tức là ví dụ: với mỗi word piece ta sẽ lấy các embedding của word piece này trong 4 layer cuối -> 4 vector từ 4 layer cho 1 word pice,\n",
        "    sau đó ta có thể sum element wise 4 vector này để ra 1 vector duy nhất đại diện cho word piece\n",
        "\n",
        "    cuối cùng ta sẽ có 1 list các embedding (đại diện) của các word piece trong 1 entity\n",
        "    từ các embedding này ta có thể chọn ngẫu nhiên 1 cái hoăc lấy max pooling các embedding này để ra 1 vector duy nhất cho 1 entity\n",
        "    '''\n",
        "    def get_entity_embedding_vector(self, hidden_states, entity_eids, isent):\n",
        "\n",
        "        entity_wpi_embedding_lst = []\n",
        "        for ient_eid, entity_eid in enumerate(entity_eids):                    # từng word piece của entity\n",
        "            \n",
        "            if entity_eid < 0:  # nếu gặp padding wpi (-2) thì dừng\n",
        "                if not (ient_eid > 0):\n",
        "                    print('EORRRRRRRRRRRRRRR')\n",
        "                    print(entity_eids)\n",
        "                    print(isent)\n",
        "                    assert False, str('No wpi id')\n",
        "                break\n",
        "             \n",
        "            wpi_final_vector = None\n",
        "            wpi_embedding_lst = []\n",
        "            # thu thập mọi vector trong các layer muốn lấy của 1 word piece\n",
        "            for emb_layer in self.emb_layer_lst:          # từng layer mà ta muốn lấy embedding\n",
        "                wpi_embedding_lst.append(hidden_states[emb_layer-1][isent][entity_eid])\n",
        "                \n",
        "            # xử lý embedding thuộc các layer khác nhau của 1 word piece\n",
        "            if self.emb_layer_handle_type == 'sum':\n",
        "                wpi_embedding_lst = torch.stack(wpi_embedding_lst)  # convert from 'python list of tensors' to 'pytorch tensor of tensers'\n",
        "                wpi_final_vector = torch.sum(wpi_embedding_lst, dim=0)\n",
        "\n",
        "            elif self.emb_layer_handle_type == 'concat':\n",
        "                wpi_final_vector = torch.cat(wpi_embedding_lst)\n",
        "\n",
        "            elif self.emb_layer_handle_type == 'max_pooling':\n",
        "                wpi_embedding_lst = torch.stack(wpi_embedding_lst)  # convert from 'python list of tensors' to 'pytorch tensor of tensers'\n",
        "                wpi_final_vector = torch.max(wpi_embedding_lst, dim=0).values\n",
        "\n",
        "            elif self.emb_layer_handle_type == 'average_pooling':\n",
        "                wpi_embedding_lst = torch.stack(wpi_embedding_lst)  # convert from 'python list of tensors' to 'pytorch tensor of tensers'\n",
        "                wpi_final_vector = torch.mean(wpi_embedding_lst, dim=0)\n",
        "\n",
        "            else:\n",
        "                assert False, \\\n",
        "                str(self.model_type + ': Unknow emb_layer_handle_type: ' + self.emb_layer_handle_type + '. Allow: sum, concat, max_pooling, average_pooling.')\n",
        "\n",
        "            assert (len(wpi_final_vector.size()) == 1), str('entity_final_vector is not a vector.')\n",
        "            assert (wpi_final_vector.size()[0] == self.wpi_emb_len), str('wpi_emb_len is not: ' + str(self.wpi_emb_len))\n",
        "\n",
        "            entity_wpi_embedding_lst.append(wpi_final_vector)\n",
        "            \n",
        "\n",
        "        assert (len(entity_wpi_embedding_lst) > 0), str('entity_wpi_embedding_lst is empty.')\n",
        "\n",
        "        # xử lý embedding của mọi word piece trong 1 entity\n",
        "        if self.entity_handle_type == 'max_pooling':\n",
        "            entity_wpi_embedding_lst = torch.stack(entity_wpi_embedding_lst)  # convert from 'python list of tensors' to 'pytorch tensor of tensers'\n",
        "            entity_final_vector = torch.max(entity_wpi_embedding_lst, dim=0).values\n",
        "\n",
        "        elif self.entity_handle_type == 'average_pooling':\n",
        "            entity_wpi_embedding_lst = torch.stack(entity_wpi_embedding_lst)  # convert from 'python list of tensors' to 'pytorch tensor of tensers'\n",
        "            entity_final_vector = torch.mean(entity_wpi_embedding_lst, dim=0)\n",
        "\n",
        "        elif self.entity_handle_type == 'sum':\n",
        "            entity_wpi_embedding_lst = torch.stack(entity_wpi_embedding_lst)  # convert from 'python list of tensors' to 'pytorch tensor of tensers'\n",
        "            entity_final_vector = torch.sum(entity_wpi_embedding_lst, dim=0)\n",
        "            \n",
        "        elif self.entity_handle_type == 'random':\n",
        "            rand_index = torch.randint(len(entity_wpi_embedding_lst), (1,))\n",
        "            entity_wpi_embedding_lst = torch.stack(entity_wpi_embedding_lst)  # convert from 'python list of tensors' to 'pytorch tensor of tensers'\n",
        "            entity_final_vector = entity_wpi_embedding_lst[rand_index][0]\n",
        "                \n",
        "        else:\n",
        "            assert False, \\\n",
        "            str(self.model_type + ': Unknow entity_handle_type: ' + self.entity_handle_type + '. Allow: max_pooling, average_pooling, sum, random.')\n",
        "\n",
        "\n",
        "        assert (len(entity_final_vector.size()) == 1), str('entity_final_vector is not a vector.')\n",
        "        \n",
        "        assert (entity_final_vector.size()[0] == self.wpi_emb_len), str('entity_final_vector is not equal wpi_emb_len: ' + str(self.wpi_emb_len))\n",
        "\n",
        "\n",
        "        return entity_final_vector\n",
        "\n",
        "\n",
        "\n",
        "    #  tính len của vector đại diện cho câu\n",
        "    def calculate_len_embedding(self):\n",
        "\n",
        "        if (self.model_type == 'phobert_base') or (self.model_type == 'xlmr_base'):\n",
        "            wpi_emb_len = 768\n",
        "        elif self.model_type == 'phobert_large' or (self.model_type == 'xlmr_large'):\n",
        "            wpi_emb_len = 1024\n",
        "        else:\n",
        "            assert False, str('Unkown model name: ' + self.model_type + '. Allow: phobert_base, phobert_large, xlmr_base, xlmr_large')\n",
        "\n",
        "        # do entity thì ta chỉ max, average pooling hoặc lấy sum các word piece nên độ dài sẽ bằng luôn độ dài vector đại diện word piece\n",
        "        # nếu không phải concat thì chiều vector đại diện wordpiece sẽ giữ nguyên\n",
        "        if (self.emb_layer_handle_type == 'sum') or (self.emb_layer_handle_type == 'max_pooling') \\\n",
        "        or (self.emb_layer_handle_type == 'average_pooling'):\n",
        "            entity_emb_len = wpi_emb_len\n",
        "\n",
        "        # nếu là concat thì chiều vector đại diện wordpiece sẽ nhân với số layer concat\n",
        "        elif self.emb_layer_handle_type == 'concat':\n",
        "            wpi_emb_len = wpi_emb_len * len(self.emb_layer_lst)\n",
        "            entity_emb_len = wpi_emb_len\n",
        "        else:\n",
        "            assert False, \\\n",
        "            str(self.model_type + ': Unknow emb_layer_handle_type: ' + self.emb_layer_handle_type + '. Allow: sum, concat, max_pooling, average_pooling.')\n",
        "\n",
        "        \n",
        "\n",
        "        # [h_s, h_t, h_s*h_t, h_s+h_t, |h_s-h_t|]\n",
        "        # sau này nếu đổi \n",
        "        sent_emb_len = entity_emb_len * 5\n",
        "\n",
        "\n",
        "        return sent_emb_len, wpi_emb_len\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMBOFVoQYLQv"
      },
      "source": [
        "class REClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, flags):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        if flags['use_phobert'] == True:\n",
        "            self.pb_model = BERTModel(flags['phobert_model'], flags['pb_entity_handle_type'], flags['pb_emb_layer_lst'], flags['pb_emb_layer_handle_type'])\n",
        "            pb_sent_emb_len = self.calculate_len_sent_embedding(flags['phobert_model'], flags['pb_emb_layer_lst'], flags['pb_emb_layer_handle_type'])\n",
        "\n",
        "        if flags['use_xlmr'] == True:\n",
        "            self.xlmr_model = BERTModel(flags['xlmr_model'], flags['xlmr_entity_handle_type'], flags['xlmr_emb_layer_lst'], flags['xlmr_emb_layer_handle_type'])\n",
        "            xlmr_sent_emb_len = self.calculate_len_sent_embedding(flags['xlmr_model'], flags['xlmr_emb_layer_lst'], flags['xlmr_emb_layer_handle_type'])\n",
        "\n",
        "        if (flags['use_phobert'] == True) and (flags['use_xlmr'] == True):\n",
        "            self.final_sent_emb_len = pb_sent_emb_len + xlmr_sent_emb_len\n",
        "\n",
        "        elif (flags['use_phobert'] == True) and (flags['use_xlmr'] == False):\n",
        "            self.final_sent_emb_len = pb_sent_emb_len\n",
        "        \n",
        "        elif (flags['use_phobert'] == False) and (flags['use_xlmr'] == True):\n",
        "            self.final_sent_emb_len = xlmr_sent_emb_len\n",
        "\n",
        "\n",
        "        self.flags = flags\n",
        "\n",
        "        self.dropout1 = nn.Dropout(p=flags['dropout1_rate'])\n",
        "        self.linear1 = nn.Linear(self.final_sent_emb_len, flags['out_linear1'])\n",
        "        self.dropout2 = nn.Dropout(p=flags['dropout2_rate'])\n",
        "        self.linear2 = nn.Linear(flags['out_linear1'], 8)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, pb_input_ids, pb_attention_masks, pb_entity_1_eids, pb_entity_2_eids, \\\n",
        "                xlmr_input_ids, xlmr_attention_masks, xlmr_entity_1_eids, xlmr_entity_2_eids):\n",
        "\n",
        "        if (self.flags['use_phobert'] == True) and (self.flags['use_xlmr'] == True):\n",
        "            \n",
        "\n",
        "            pb_sent_final_embedding = self.pb_model(pb_input_ids, pb_attention_masks, pb_entity_1_eids, pb_entity_2_eids)\n",
        "            xlmr_sent_final_embedding = self.xlmr_model(xlmr_input_ids, xlmr_attention_masks, xlmr_entity_1_eids, xlmr_entity_2_eids)\n",
        "            \n",
        "            # concat two embedding\n",
        "            bert_sent_final_embedding = torch.cat((pb_sent_final_embedding, xlmr_sent_final_embedding), dim=1)\n",
        "\n",
        "            assert (len(bert_sent_final_embedding.size()) == 2) and (bert_sent_final_embedding.size()[0] == pb_sent_final_embedding.size()[0]) \\\n",
        "            and (bert_sent_final_embedding.size()[0] == xlmr_sent_final_embedding.size()[0]) \\\n",
        "            and (bert_sent_final_embedding.size()[1] == self.final_sent_emb_len), \\\n",
        "            str('REClassifier: PROBLEM WITH sent_final_embedding len.')\n",
        "\n",
        "        elif (self.flags['use_phobert'] == True) and (self.flags['use_xlmr'] == False):\n",
        "            \n",
        "\n",
        "            bert_sent_final_embedding = self.pb_model(pb_input_ids, pb_attention_masks, pb_entity_1_eids, pb_entity_2_eids)\n",
        "\n",
        "            assert (bert_sent_final_embedding.size()[1] == self.final_sent_emb_len), \\\n",
        "            str('REClassifier: PROBLEM WITH sent_final_embedding len.')\n",
        "            \n",
        "        \n",
        "        elif (self.flags['use_phobert'] == False) and (self.flags['use_xlmr'] == True):\n",
        "            \n",
        "\n",
        "            bert_sent_final_embedding = self.xlmr_model(xlmr_input_ids, xlmr_attention_masks, xlmr_entity_1_eids, xlmr_entity_2_eids)\n",
        "            \n",
        "            assert (bert_sent_final_embedding.size()[1] == self.final_sent_emb_len), \\\n",
        "            str('REClassifier: PROBLEM WITH sent_final_embedding len.')\n",
        "\n",
        "\n",
        "        x = self.dropout1(bert_sent_final_embedding)\n",
        "        x = self.linear1(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "    #  tính len của vector đại diện cho câu\n",
        "    def calculate_len_sent_embedding(self, model_type, emb_layer_lst, emb_layer_handle_type):\n",
        "\n",
        "        if (model_type == 'phobert_base') or (model_type == 'xlmr_base'):\n",
        "            wpi_emb_len = 768\n",
        "        elif (model_type == 'phobert_large') or (model_type == 'xlmr_large'):\n",
        "            wpi_emb_len = 1024\n",
        "        else:\n",
        "            assert False, str('Unkown model name: ' + model_type + '. Allow: phobert_base, phobert_large, xlmr_base, xlmr_large')\n",
        "\n",
        "        # do entity thì ta chỉ max, average pooling hoặc lấy sum các word piece nên độ dài sẽ bằng luôn độ dài vector đại diện word piece\n",
        "        # nếu không phải concat thì chiều vector đại diện wordpiece sẽ giữ nguyên\n",
        "        if (emb_layer_handle_type == 'sum') or (emb_layer_handle_type == 'max_pooling') \\\n",
        "        or (emb_layer_handle_type == 'average_pooling'):\n",
        "            entity_emb_len = wpi_emb_len\n",
        "\n",
        "        # nếu là concat thì chiều vector đại diện wordpiece sẽ nhân với số layer concat\n",
        "        elif emb_layer_handle_type == 'concat':\n",
        "            wpi_emb_len = wpi_emb_len * len(emb_layer_lst)\n",
        "            entity_emb_len = wpi_emb_len\n",
        "        else:\n",
        "            assert False, \\\n",
        "            str(model_type + ': Unknow emb_layer_handle_type: ' + emb_layer_handle_type + '. Allow: sum, concat, max_pooling, average_pooling.')\n",
        "\n",
        "        \n",
        "\n",
        "        # [h_s, h_t, h_s*h_t, h_s+h_t, |h_s-h_t|]\n",
        "        # sau này nếu đổi \n",
        "        sent_emb_len = entity_emb_len * 5\n",
        "\n",
        "\n",
        "        return sent_emb_len\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TJjwwBgu32B"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7SbTS2ol2bC"
      },
      "source": [
        "### test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDc6lLJGlijU"
      },
      "source": [
        "test_dataset = TensorDataset(test_pb_input_ids, test_pb_attention_masks, test_pb_entity_1_eids, test_pb_entity_2_eids, \\\n",
        "                             test_xlmr_input_ids, test_xlmr_attention_masks, test_xlmr_entity_1_eids, test_xlmr_entity_2_eids, \\\n",
        "                             test_labels, sent_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ulUGZfMIjh"
      },
      "source": [
        "### Main\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxdvAntREnO1"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9HUHFe0QRKc"
      },
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7epI-VsS9QF"
      },
      "source": [
        "**YOU NEED TO CHANGE THE PATH TO YOUR MODEL.BIN AND FLAGS.TXT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ8AL8A9AFdi"
      },
      "source": [
        "model_1_path = 'PATH_TO_MODEL/MODEL_NAME.bin'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kzez-ChQqWm"
      },
      "source": [
        "with open('PATH_TO_MODEL/flags.txt', 'r') as f:\n",
        "    model_1_flags = json.load(f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMwCnvoNxVCo",
        "outputId": "f76f103a-021d-48b1-f9e3-241d1836fe8e"
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwM2q4CifcO7"
      },
      "source": [
        "re_model_1 = REClassifier(model_1_flags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob3FyMLQw9TN"
      },
      "source": [
        "re_model_1.load_state_dict(torch.load(model_1_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlYSevWpTZfc"
      },
      "source": [
        "def decode_label(sentence_label):\n",
        "    label = -1\n",
        "    if sentence_label == 0:\n",
        "        label = \"LOCATED\"\n",
        "    elif sentence_label == 1:\n",
        "        label = \"PART_WHOLE\"\n",
        "    elif sentence_label == 2:\n",
        "        label = \"PERSONAL_SOCIAL\"\n",
        "    elif sentence_label == 3:\n",
        "        label = \"AFFILIATION\"\n",
        "    elif sentence_label == 4:\n",
        "        label = \"IS_LOCATED\"\n",
        "    elif sentence_label == 5:\n",
        "        label = \"WHOLE_PART\"\n",
        "    elif sentence_label == 6:\n",
        "        label = \"AFFILIATION_TO\"\n",
        "    elif sentence_label == 7:\n",
        "        label = \"OTHERS\"\n",
        "    else:\n",
        "        assert False, \"UNKNOWN LABEL\"\n",
        "    \n",
        "    return label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I77dIVAa4XYZ"
      },
      "source": [
        "re_model_1.to(device)\n",
        "\n",
        "re_model_1.eval()\n",
        "\n",
        "model_1_outputs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        sampler = torch.utils.data.SequentialSampler(test_dataset),\n",
        "        batch_size=32)\n",
        "    \n",
        "    for batch_num, batch in enumerate(test_loader):\n",
        "        if batch_num % 30 == 0:\n",
        "            print(batch_num)\n",
        "            \n",
        "        pb_input_ids, pb_attention_masks, pb_entity_1_eids, pb_entity_2_eids, \\\n",
        "        xlmr_input_ids, xlmr_attention_masks, xlmr_entity_1_eids, xlmr_entity_2_eids, targets, sent_id = \\\n",
        "        batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), \\\n",
        "        batch[4].to(device), batch[5].to(device), batch[6].to(device), batch[7].to(device), batch[8].to(device), batch[9].to(device)\n",
        "\n",
        "            \n",
        "        # Acquires the network's best guesses at each class\n",
        "        output = re_model_1(pb_input_ids, pb_attention_masks, pb_entity_1_eids, pb_entity_2_eids, \\\n",
        "                                xlmr_input_ids, xlmr_attention_masks, xlmr_entity_1_eids, xlmr_entity_2_eids)\n",
        "\n",
        "            \n",
        "        labels = torch.argmax(output, dim=1)\n",
        "\n",
        "        labels_decoded = [decode_label(label) for label in labels]\n",
        "\n",
        "        model_1_outputs.extend(copy.deepcopy(labels_decoded))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NG-9bMJ5r53"
      },
      "source": [
        "model_1_results = copy.deepcopy(jtest_data_v3)\n",
        "\n",
        "for i in range(len(jtest_data_v3)):\n",
        "    model_1_results[i]['label'] = copy.deepcopy(model_1_outputs[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BhdS2eARwPe"
      },
      "source": [
        "# review\n",
        "for i in range(len(model_1_results)):\n",
        "    if model_1_results[i]['label'] != 'OTHERS':\n",
        "        print(model_1_results[i])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
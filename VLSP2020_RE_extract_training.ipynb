{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VLSP2020_RE_extract_training_V4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "H3ICier0JyzI",
        "PcS3CiYZxv8a",
        "jhOuiCNgaVqk",
        "chNxSZ0mx5wS",
        "-wzBGIIgKYPD",
        "EYMqFdP5eTPL",
        "RQKOe59wfEcS",
        "yNIBK7jecGMV",
        "kBr6Gx-Y2oj2",
        "oGR4QQfBauVE",
        "PQt6fgmG7foW",
        "8crlbk4MJTlx",
        "L0mBtqdZthaa"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQDbz_q5irs0"
      },
      "source": [
        "# Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxh7foXeJtrf"
      },
      "source": [
        "## Unrar dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etKZzzHSF6_i"
      },
      "source": [
        "Please upload VLSP2020_RE_training.rar to Colab then */content* folder then unrar it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXjtAzVJHNI_",
        "outputId": "108b2f62-ff97-40ce-df63-264f53757142"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6x3Nl_tfBf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "181068c2-84ff-4bf7-cfd6-96e1dac85ff5"
      },
      "source": [
        "!pip install unrar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unrar\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/0b/53130ccd483e3db8c8a460cb579bdb21b458d5494d67a261e1a5b273fbb9/unrar-0.4-py3-none-any.whl\n",
            "Installing collected packages: unrar\n",
            "Successfully installed unrar-0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnlpfnYxiSiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "569fd1ab-e486-4a9e-c8d0-770194ea4630"
      },
      "source": [
        "!unrar x VLSP2020_RE_training.rar"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from VLSP2020_RE_training.rar\n",
            "\n",
            "Creating    VLSP2020_RE_training                                      OK\n",
            "Creating    VLSP2020_RE_training/23351113.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351113.conll/CURATION_USER.tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351164.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351164.conll/CURATION_USER.tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351190.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351190.conll/CURATION_USER.tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351214.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351214.conll/CURATION_USER.tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351225.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351225.conll/CURATION_USER.tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351260.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351260.conll/CURATION_USER.tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351307.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351307.conll/CURATION_USER.tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351316.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351316.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351318.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351318.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351385.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351385.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351391.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351391.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351392.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351392.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351393.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351393.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351394.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351394.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351416.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351416.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351422.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351422.conll/CURATION_USER.tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351424.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351424.conll/CURATION_USER.tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351425.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351425.conll/CURATION_USER.tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351426.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351426.conll/CURATION_USER.tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351427.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351427.conll/CURATION_USER.tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351430.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351430.conll/CURATION_USER.tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351431.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351431.conll/CURATION_USER.tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351432.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351432.conll/CURATION_USER.tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351433.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351433.conll/CURATION_USER.tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351434.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351434.conll/CURATION_USER.tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351435.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351435.conll/CURATION_USER.tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351436.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351436.conll/CURATION_USER.tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351437.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351437.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351438.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351438.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351440.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351440.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351460.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351460.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351489.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351489.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351493.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351493.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351494.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351494.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351510.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351510.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351511.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351511.conll/CURATION_USER.tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351514.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351514.conll/CURATION_USER.tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351515.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351515.conll/CURATION_USER.tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351516.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351516.conll/CURATION_USER.tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351518.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351518.conll/CURATION_USER.tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351519.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351519.conll/CURATION_USER.tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351521.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351521.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351522.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351522.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351524.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351524.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351542.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351542.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351543.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351543.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351549.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351549.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351554.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351554.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351555.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351555.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351556.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351556.conll/CURATION_USER.tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351561.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351561.conll/CURATION_USER.tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351562.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351562.conll/CURATION_USER.tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351563.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351563.conll/CURATION_USER.tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351564.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351564.conll/CURATION_USER.tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351566.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351566.conll/CURATION_USER.tsv        \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351567.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351567.conll/CURATION_USER.tsv        \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351569.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351569.conll/CURATION_USER.tsv        \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351571.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351571.conll/CURATION_USER.tsv        \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351574.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351574.conll/CURATION_USER.tsv        \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351576.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351576.conll/CURATION_USER.tsv        \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351578.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351578.conll/CURATION_USER.tsv        \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351579.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351579.conll/CURATION_USER.tsv        \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351581.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351581.conll/CURATION_USER.tsv        \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351582.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351582.conll/CURATION_USER.tsv        \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351595.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351595.conll/CURATION_USER.tsv        \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351607.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351607.conll/CURATION_USER.tsv        \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351610.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351610.conll/CURATION_USER.tsv        \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351611.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351611.conll/CURATION_USER.tsv        \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351612.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351612.conll/CURATION_USER.tsv        \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351615.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351615.conll/CURATION_USER.tsv        \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351617.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351617.conll/CURATION_USER.tsv        \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351627.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351627.conll/CURATION_USER.tsv        \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351632.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351632.conll/CURATION_USER.tsv        \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351635.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351635.conll/CURATION_USER.tsv        \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351636.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351636.conll/CURATION_USER.tsv        \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351642.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351642.conll/CURATION_USER.tsv        \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351645.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351645.conll/CURATION_USER.tsv        \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351647.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351647.conll/CURATION_USER.tsv        \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351649.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351649.conll/CURATION_USER.tsv        \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351650.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351650.conll/CURATION_USER.tsv        \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351651.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351651.conll/CURATION_USER.tsv        \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351652.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351652.conll/CURATION_USER.tsv        \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351653.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351653.conll/CURATION_USER.tsv        \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351672.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351672.conll/CURATION_USER.tsv        \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351700.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351700.conll/CURATION_USER.tsv        \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351719.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351719.conll/CURATION_USER.tsv        \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351749.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351749.conll/CURATION_USER.tsv        \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351778.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351778.conll/CURATION_USER.tsv        \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351809.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351809.conll/CURATION_USER.tsv        \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351814.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351814.conll/CURATION_USER.tsv        \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351815.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351815.conll/CURATION_USER.tsv        \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351817.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351817.conll/CURATION_USER.tsv        \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351820.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351820.conll/CURATION_USER.tsv        \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351831.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351831.conll/CURATION_USER.tsv        \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351834.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351834.conll/CURATION_USER.tsv        \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351837.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351837.conll/CURATION_USER.tsv        \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351839.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351839.conll/CURATION_USER.tsv        \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351841.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351841.conll/CURATION_USER.tsv        \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351846.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351846.conll/CURATION_USER.tsv        \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351848.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351848.conll/CURATION_USER.tsv        \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351849.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351849.conll/CURATION_USER.tsv        \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351851.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351851.conll/CURATION_USER.tsv        \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351852.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351852.conll/CURATION_USER.tsv        \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351853.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351853.conll/CURATION_USER.tsv        \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351856.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351856.conll/CURATION_USER.tsv        \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351858.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351858.conll/CURATION_USER.tsv        \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351861.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351861.conll/CURATION_USER.tsv        \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351864.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351864.conll/CURATION_USER.tsv        \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351887.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351887.conll/CURATION_USER.tsv        \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351888.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351888.conll/CURATION_USER.tsv        \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351923.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351923.conll/CURATION_USER.tsv        \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351931.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv        \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351933.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351933.conll/CURATION_USER.tsv        \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351937.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351937.conll/CURATION_USER.tsv        \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351939.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351939.conll/CURATION_USER.tsv        \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351941.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351941.conll/CURATION_USER.tsv        \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351943.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351943.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351945.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351945.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351946.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351946.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351947.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351947.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351948.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351948.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351949.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351949.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351950.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351950.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351951.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351951.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351952.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351952.conll/CURATION_USER.tsv        \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351956.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351956.conll/CURATION_USER.tsv        \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351959.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351959.conll/CURATION_USER.tsv        \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351960.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351960.conll/CURATION_USER.tsv        \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351961.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351961.conll/CURATION_USER.tsv        \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351963.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351963.conll/CURATION_USER.tsv        \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351965.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351965.conll/CURATION_USER.tsv        \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351967.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351967.conll/CURATION_USER.tsv        \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351969.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351969.conll/CURATION_USER.tsv        \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351970.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351970.conll/CURATION_USER.tsv        \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351971.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351971.conll/CURATION_USER.tsv        \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351974.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351974.conll/CURATION_USER.tsv        \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351976.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351976.conll/CURATION_USER.tsv        \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351978.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351978.conll/CURATION_USER.tsv        \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351979.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351979.conll/CURATION_USER.tsv        \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351981.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351981.conll/CURATION_USER.tsv        \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351982.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351982.conll/CURATION_USER.tsv        \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351983.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351983.conll/CURATION_USER.tsv        \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351984.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351984.conll/CURATION_USER.tsv        \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351985.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351985.conll/CURATION_USER.tsv        \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351987.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351987.conll/CURATION_USER.tsv        \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351988.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351988.conll/CURATION_USER.tsv        \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351990.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351990.conll/CURATION_USER.tsv        \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351992.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351992.conll/CURATION_USER.tsv        \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351994.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351994.conll/CURATION_USER.tsv        \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351995.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351995.conll/CURATION_USER.tsv        \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352656.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352656.conll/CURATION_USER.tsv        \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352659.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352659.conll/CURATION_USER.tsv        \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352662.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352662.conll/CURATION_USER.tsv        \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352663.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352663.conll/CURATION_USER.tsv        \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352665.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352665.conll/CURATION_USER.tsv        \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352671.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352671.conll/CURATION_USER.tsv        \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352674.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352674.conll/CURATION_USER.tsv        \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352675.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352675.conll/CURATION_USER.tsv        \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352676.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352676.conll/CURATION_USER.tsv        \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352677.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352677.conll/CURATION_USER.tsv        \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352681.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352681.conll/CURATION_USER.tsv        \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352682.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352682.conll/CURATION_USER.tsv        \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352683.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352683.conll/CURATION_USER.tsv        \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352684.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352684.conll/CURATION_USER.tsv        \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352686.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352686.conll/CURATION_USER.tsv        \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352687.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352687.conll/CURATION_USER.tsv        \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352690.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352690.conll/CURATION_USER.tsv        \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352693.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352693.conll/CURATION_USER.tsv        \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352695.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352695.conll/CURATION_USER.tsv        \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352696.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352696.conll/CURATION_USER.tsv        \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352701.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352701.conll/CURATION_USER.tsv        \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352702.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352702.conll/CURATION_USER.tsv        \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352704.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352704.conll/CURATION_USER.tsv        \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352706.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352706.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352707.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352707.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352708.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352708.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352710.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352710.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352713.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352713.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352715.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352715.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352717.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352717.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352718.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352718.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352719.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352719.conll/CURATION_USER.tsv        \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352720.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352720.conll/CURATION_USER.tsv        \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352725.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352725.conll/CURATION_USER.tsv        \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352730.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352730.conll/CURATION_USER.tsv        \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352731.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352731.conll/CURATION_USER.tsv        \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352738.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352738.conll/CURATION_USER (1).tsv     \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352739.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352739.conll/CURATION_USER (1).tsv     \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352743.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352743.conll/CURATION_USER (1).tsv     \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352746.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352746.conll/CURATION_USER (1).tsv     \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352747.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352747.conll/CURATION_USER (1).tsv     \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352748.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv     \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352750.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352750.conll/CURATION_USER (1).tsv     \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352751.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352751.conll/CURATION_USER (1).tsv     \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352752.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352752.conll/CURATION_USER (1).tsv     \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352753.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352753.conll/CURATION_USER (1).tsv     \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352754.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352754.conll/CURATION_USER (1).tsv     \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352755.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352755.conll/CURATION_USER (1).tsv     \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352757.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352757.conll/CURATION_USER (1).tsv     \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352761.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352761.conll/CURATION_USER (1).tsv     \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352765.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352765.conll/CURATION_USER (1).tsv     \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352769.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352769.conll/CURATION_USER (1).tsv     \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352774.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352774.conll/CURATION_USER (1).tsv     \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352777.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352777.conll/CURATION_USER (1).tsv     \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352778.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352778.conll/CURATION_USER (1).tsv     \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352781.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352781.conll/CURATION_USER (1).tsv     \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352785.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352785.conll/CURATION_USER (1).tsv     \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352787.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352787.conll/CURATION_USER (1).tsv     \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352792.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352792.conll/CURATION_USER (1).tsv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352795.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352795.conll/CURATION_USER (1).tsv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352800.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352800.conll/CURATION_USER (1).tsv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352802.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352802.conll/CURATION_USER (1).tsv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352804.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352804.conll/CURATION_USER (1).tsv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352806.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352806.conll/CURATION_USER (1).tsv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352807.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352807.conll/CURATION_USER (1).tsv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352814.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352814.conll/CURATION_USER (1).tsv     \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352816.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv        \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352820.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352820.conll/CURATION_USER (1).tsv     \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352821.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352821.conll/CURATION_USER (1).tsv     \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352822.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352822.conll/CURATION_USER (1).tsv     \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352824.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352824.conll/CURATION_USER (1).tsv     \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352825.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352825.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352829.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352829.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352830.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352830.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352831.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352831.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352844.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352844.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352845.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352845.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352849.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352849.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352853.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352853.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352856.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352856.conll/CURATION_USER (1).tsv     \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352857.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352857.conll/CURATION_USER (1).tsv     \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352870.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352870.conll/CURATION_USER (1).tsv     \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352871.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352871.conll/CURATION_USER (1).tsv     \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352872.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352872.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352874.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352874.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352876.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352876.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352878.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352878.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352880.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352880.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352883.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352883.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352886.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352886.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352887.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352887.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352892.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352892.conll/CURATION_USER (1).tsv     \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352894.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352894.conll/CURATION_USER (1).tsv     \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352896.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352896.conll/CURATION_USER (1).tsv     \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352899.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352899.conll/CURATION_USER (1).tsv     \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352900.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352900.conll/CURATION_USER (1).tsv     \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353721.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353721.conll/CURATION_USER.tsv        \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353723.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353723.conll/CURATION_USER.tsv        \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353727.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353727.conll/CURATION_USER.tsv        \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353732.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353732.conll/CURATION_USER.tsv        \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353739.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353739.conll/CURATION_USER.tsv        \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353755.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353755.conll/CURATION_USER.tsv        \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353757.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353757.conll/CURATION_USER.tsv        \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353760.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353760.conll/CURATION_USER.tsv        \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353763.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353763.conll/CURATION_USER.tsv        \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353773.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353773.conll/CURATION_USER.tsv        \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353779.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353779.conll/CURATION_USER.tsv        \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353780.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353780.conll/CURATION_USER.tsv        \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353785.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353785.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353786.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353786.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353787.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353787.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353791.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353791.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353794.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353794.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353799.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353799.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353801.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353801.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353824.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353824.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353825.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353825.conll/CURATION_USER.tsv        \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353830.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353830.conll/CURATION_USER.tsv        \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353834.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353834.conll/CURATION_USER.tsv        \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353838.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353838.conll/CURATION_USER.tsv        \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353840.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353840.conll/CURATION_USER.tsv        \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353841.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353841.conll/CURATION_USER.tsv        \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353842.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353842.conll/CURATION_USER.tsv        \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353846.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353846.conll/CURATION_USER.tsv        \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353849.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353849.conll/CURATION_USER.tsv        \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353857.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353857.conll/CURATION_USER.tsv        \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353860.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353860.conll/CURATION_USER.tsv        \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353861.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353861.conll/CURATION_USER.tsv        \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353863.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353863.conll/CURATION_USER.tsv        \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353864.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353864.conll/CURATION_USER.tsv        \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353867.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353867.conll/CURATION_USER.tsv        \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353872.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353872.conll/CURATION_USER.tsv        \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353874.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353874.conll/CURATION_USER.tsv        \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353878.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353878.conll/CURATION_USER.tsv        \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353891.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353891.conll/CURATION_USER.tsv        \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353901.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353901.conll/CURATION_USER.tsv        \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353904.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353904.conll/CURATION_USER.tsv        \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353913.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv        \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353916.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353916.conll/CURATION_USER.tsv        \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353931.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353931.conll/CURATION_USER.tsv        \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353944.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353944.conll/CURATION_USER.tsv        \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353945.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353945.conll/CURATION_USER.tsv        \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353950.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353950.conll/CURATION_USER.tsv        \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353954.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353954.conll/CURATION_USER.tsv        \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353967.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353967.conll/CURATION_USER.tsv        \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353973.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353973.conll/CURATION_USER.tsv        \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353975.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353975.conll/CURATION_USER.tsv        \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353976.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353976.conll/CURATION_USER.tsv        \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353995.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353995.conll/CURATION_USER.tsv        \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353996.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353996.conll/CURATION_USER.tsv        \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354010.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354010.conll/CURATION_USER.tsv        \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354027.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354027.conll/CURATION_USER.tsv        \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354028.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354028.conll/CURATION_USER.tsv        \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354030.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354030.conll/CURATION_USER.tsv        \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354032.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354032.conll/CURATION_USER.tsv        \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354034.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354034.conll/CURATION_USER.tsv        \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354042.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354042.conll/CURATION_USER.tsv        \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354045.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354045.conll/CURATION_USER.tsv        \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354055.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354055.conll/CURATION_USER.tsv        \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354065.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354065.conll/CURATION_USER.tsv        \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354082.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354082.conll/CURATION_USER.tsv        \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354085.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354085.conll/CURATION_USER.tsv        \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354088.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354088.conll/CURATION_USER.tsv        \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354089.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354089.conll/CURATION_USER.tsv        \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354091.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354091.conll/CURATION_USER.tsv        \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354092.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354092.conll/CURATION_USER.tsv        \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354093.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354093.conll/CURATION_USER.tsv        \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354098.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354098.conll/CURATION_USER.tsv        \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354103.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354103.conll/CURATION_USER.tsv        \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354126.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354126.conll/CURATION_USER.tsv        \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354130.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354130.conll/CURATION_USER.tsv        \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354166.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354166.conll/CURATION_USER.tsv        \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354202.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354202.conll/CURATION_USER.tsv        \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354219.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354219.conll/CURATION_USER.tsv        \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354244.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354244.conll/CURATION_USER.tsv        \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354253.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354253.conll/CURATION_USER.tsv        \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354265.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354265.conll/CURATION_USER.tsv        \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354285.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354285.conll/CURATION_USER.tsv        \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354288.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354288.conll/CURATION_USER.tsv        \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354310.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354310.conll/CURATION_USER.tsv        \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354318.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354318.conll/CURATION_USER.tsv        \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354320.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354320.conll/CURATION_USER.tsv        \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354336.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354336.conll/CURATION_USER.tsv        \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354396.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354396.conll/CURATION_USER.tsv        \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354400.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354400.conll/CURATION_USER.tsv        \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354442.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354442.conll/CURATION_USER.tsv        \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354450.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354450.conll/CURATION_USER.tsv        \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354460.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354460.conll/CURATION_USER.tsv        \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354474.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354474.conll/CURATION_USER.tsv        \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354516.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354516.conll/CURATION_USER.tsv        \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354536.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354536.conll/CURATION_USER.tsv        \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354538.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354538.conll/CURATION_USER.tsv        \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354544.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354544.conll/CURATION_USER.tsv        \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354545.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354545.conll/CURATION_USER.tsv        \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354575.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354575.conll/CURATION_USER.tsv        \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354619.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354619.conll/CURATION_USER.tsv        \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354627.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354627.conll/CURATION_USER.tsv        \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354648.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354648.conll/CURATION_USER.tsv        \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354656.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354656.conll/CURATION_USER.tsv        \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354695.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354695.conll/CURATION_USER.tsv        \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354697.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354697.conll/CURATION_USER.tsv        \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354698.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354698.conll/CURATION_USER.tsv        \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354699.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354699.conll/CURATION_USER.tsv        \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354717.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354717.conll/CURATION_USER.tsv        \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354718.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354718.conll/CURATION_USER.tsv        \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354719.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354719.conll/CURATION_USER.tsv        \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354738.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354738.conll/CURATION_USER.tsv        \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354739.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354739.conll/CURATION_USER.tsv        \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354751.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354751.conll/CURATION_USER.tsv        \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354780.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354780.conll/CURATION_USER.tsv        \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354793.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354793.conll/CURATION_USER.tsv        \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354796.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354796.conll/CURATION_USER.tsv        \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354803.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354803.conll/CURATION_USER.tsv        \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354816.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354816.conll/CURATION_USER.tsv        \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354831.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354831.conll/CURATION_USER.tsv        \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354879.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354879.conll/CURATION_USER.tsv        \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354880.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354880.conll/CURATION_USER.tsv        \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354881.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354881.conll/CURATION_USER.tsv        \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354910.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354910.conll/CURATION_USER.tsv        \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354912.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354912.conll/CURATION_USER.tsv        \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354916.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354916.conll/CURATION_USER.tsv        \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354920.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354920.conll/CURATION_USER.tsv        \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354935.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354935.conll/CURATION_USER.tsv        \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354944.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354944.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354946.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354946.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354953.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354953.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354956.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354956.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354977.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354977.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354982.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354982.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355001.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355001.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355040.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355040.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355061.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355061.conll/CURATION_USER.tsv        \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355064.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355064.conll/CURATION_USER.tsv        \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355095.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355095.conll/CURATION_USER.tsv        \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355132.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355132.conll/CURATION_USER.tsv        \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355228.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355228.conll/CURATION_USER.tsv        \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355250.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355250.conll/CURATION_USER.tsv        \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355254.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355254.conll/CURATION_USER.tsv        \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355290.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv        \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355416.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355416.conll/CURATION_USER.tsv        \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355434.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355434.conll/CURATION_USER.tsv        \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355470.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355470.conll/CURATION_USER.tsv        \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355557.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355557.conll/CURATION_USER.tsv        \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355571.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355571.conll/CURATION_USER.tsv        \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355626.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355626.conll/CURATION_USER.tsv        \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355656.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355656.conll/CURATION_USER.tsv        \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355773.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355773.conll/CURATION_USER.tsv        \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355817.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355817.conll/CURATION_USER.tsv        \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355858.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355858.conll/CURATION_USER.tsv        \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355917.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355917.conll/CURATION_USER.tsv        \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355935.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355935.conll/CURATION_USER.tsv        \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355988.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355988.conll/CURATION_USER.tsv        \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356093.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356093.conll/CURATION_USER.tsv        \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356193.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356193.conll/CURATION_USER.tsv        \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356205.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356205.conll/CURATION_USER.tsv        \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356221.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356221.conll/CURATION_USER.tsv        \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356245.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv        \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356247.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356247.conll/CURATION_USER.tsv        \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356295.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356295.conll/CURATION_USER.tsv        \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356299.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356299.conll/CURATION_USER.tsv        \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356314.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356314.conll/CURATION_USER.tsv        \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356315.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356315.conll/CURATION_USER.tsv        \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356329.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356329.conll/CURATION_USER.tsv        \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356339.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356339.conll/CURATION_USER.tsv        \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356494.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356494.conll/CURATION_USER.tsv        \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356505.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356505.conll/CURATION_USER.tsv        \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356511.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356511.conll/CURATION_USER.tsv        \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356574.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356574.conll/CURATION_USER.tsv        \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356604.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356604.conll/CURATION_USER.tsv        \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356622.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356622.conll/CURATION_USER.tsv        \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356624.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356624.conll/CURATION_USER.tsv        \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356638.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356638.conll/CURATION_USER.tsv        \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356715.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356715.conll/CURATION_USER.tsv        \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356716.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356716.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356724.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356724.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356731.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356731.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356745.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356745.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356765.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356765.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356767.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356767.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356771.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356771.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356782.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356782.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356793.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356793.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356798.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356798.conll/CURATION_USER.tsv        \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356858.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356858.conll/CURATION_USER.tsv        \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356874.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356874.conll/CURATION_USER.tsv        \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356885.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356885.conll/CURATION_USER.tsv        \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356887.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356887.conll/CURATION_USER.tsv        \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356902.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356902.conll/CURATION_USER.tsv        \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356906.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356906.conll/CURATION_USER.tsv        \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356907.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356907.conll/CURATION_USER.tsv        \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356915.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356915.conll/CURATION_USER.tsv        \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356918.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356918.conll/CURATION_USER.tsv        \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356933.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356933.conll/CURATION_USER.tsv        \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356960.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356960.conll/CURATION_USER.tsv        \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356961.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356961.conll/CURATION_USER.tsv        \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356992.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356992.conll/CURATION_USER.tsv        \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356993.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356993.conll/CURATION_USER.tsv        \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356998.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356998.conll/CURATION_USER.tsv        \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357000.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357000.conll/CURATION_USER.tsv        \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357028.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357028.conll/CURATION_USER.tsv        \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357037.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357037.conll/CURATION_USER.tsv        \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357062.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357062.conll/CURATION_USER.tsv        \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357063.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357063.conll/CURATION_USER.tsv        \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357081.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357081.conll/CURATION_USER.tsv        \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357094.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357094.conll/CURATION_USER.tsv        \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357095.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357095.conll/CURATION_USER.tsv        \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357097.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357097.conll/CURATION_USER.tsv        \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357120.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357120.conll/CURATION_USER.tsv        \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357135.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357135.conll/CURATION_USER.tsv        \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357151.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357151.conll/CURATION_USER.tsv        \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357167.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357167.conll/CURATION_USER.tsv        \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357190.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357190.conll/CURATION_USER.tsv        \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357233.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357233.conll/CURATION_USER.tsv        \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357240.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357240.conll/CURATION_USER.tsv        \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357258.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357258.conll/CURATION_USER.tsv        \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357263.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357263.conll/CURATION_USER.tsv        \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357266.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357266.conll/CURATION_USER.tsv        \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357288.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357288.conll/CURATION_USER.tsv        \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357308.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357308.conll/CURATION_USER.tsv        \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357309.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357309.conll/CURATION_USER.tsv        \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357329.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357329.conll/CURATION_USER.tsv        \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357336.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357336.conll/CURATION_USER.tsv        \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357341.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357341.conll/CURATION_USER.tsv        \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357344.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357344.conll/CURATION_USER.tsv        \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357389.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357389.conll/CURATION_USER.tsv        \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357394.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357394.conll/CURATION_USER.tsv        \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357396.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357396.conll/CURATION_USER.tsv        \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357443.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357443.conll/CURATION_USER.tsv        \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357457.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357457.conll/CURATION_USER.tsv        \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357471.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357471.conll/CURATION_USER.tsv        \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357489.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357489.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357491.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357491.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357534.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357534.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357544.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357544.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357550.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357550.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357652.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357652.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357711.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357711.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357741.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357741.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357752.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357752.conll/CURATION_USER.tsv        \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357765.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357765.conll/CURATION_USER.tsv        \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357779.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357779.conll/CURATION_USER.tsv        \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357809.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357809.conll/CURATION_USER.tsv        \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357851.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357851.conll/CURATION_USER.tsv        \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357897.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357897.conll/CURATION_USER.tsv        \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357937.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357937.conll/CURATION_USER.tsv        \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357994.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357994.conll/CURATION_USER.tsv        \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23358011.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23358011.conll/CURATION_USER.tsv        \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23358086.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23358086.conll/CURATION_USER.tsv        \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23358097.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23358097.conll/CURATION_USER.tsv        \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23358104.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23358104.conll/CURATION_USER.tsv        \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23358261.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23358261.conll/CURATION_USER.tsv        \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23366716.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23366716.conll/CURATION_USER.tsv        \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23366722.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23366722.conll/CURATION_USER.tsv        \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23366740.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23366740.conll/CURATION_USER.tsv        \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23366751.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23366751.conll/CURATION_USER.tsv        \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23366765.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23366765.conll/CURATION_USER.tsv        \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3ICier0JyzI"
      },
      "source": [
        "## Install Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcS3CiYZxv8a"
      },
      "source": [
        "### Install VNCoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzsgsAb4uET6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26b6824-9310-448e-c064-870c27997d9f"
      },
      "source": [
        "# Install the vncorenlp python wrapper\n",
        "!pip install vncorenlp==1.0.3"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vncorenlp==1.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n",
            "\r\u001b[K     |▏                               | 10kB 16.4MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 21.6MB/s eta 0:00:01\r\u001b[K     |▍                               | 30kB 25.5MB/s eta 0:00:01\r\u001b[K     |▌                               | 40kB 23.4MB/s eta 0:00:01\r\u001b[K     |▋                               | 51kB 14.2MB/s eta 0:00:01\r\u001b[K     |▊                               | 61kB 16.1MB/s eta 0:00:01\r\u001b[K     |▉                               | 71kB 13.4MB/s eta 0:00:01\r\u001b[K     |█                               | 81kB 14.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 92kB 12.3MB/s eta 0:00:01\r\u001b[K     |█▎                              | 102kB 11.8MB/s eta 0:00:01\r\u001b[K     |█▍                              | 112kB 11.8MB/s eta 0:00:01\r\u001b[K     |█▌                              | 122kB 11.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 133kB 11.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 143kB 11.8MB/s eta 0:00:01\r\u001b[K     |█▉                              | 153kB 11.8MB/s eta 0:00:01\r\u001b[K     |██                              | 163kB 11.8MB/s eta 0:00:01\r\u001b[K     |██                              | 174kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▎                             | 184kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 194kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 204kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 215kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 225kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▉                             | 235kB 11.8MB/s eta 0:00:01\r\u001b[K     |███                             | 245kB 11.8MB/s eta 0:00:01\r\u001b[K     |███                             | 256kB 11.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 266kB 11.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 276kB 11.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 286kB 11.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 296kB 11.8MB/s eta 0:00:01\r\u001b[K     |███▊                            | 307kB 11.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 317kB 11.8MB/s eta 0:00:01\r\u001b[K     |████                            | 327kB 11.8MB/s eta 0:00:01\r\u001b[K     |████                            | 337kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▏                           | 348kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 358kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▌                           | 368kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 378kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 389kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 399kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 409kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 419kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 430kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 440kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 450kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 460kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 471kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 481kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 491kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 501kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 512kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 522kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 532kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 542kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 552kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 563kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 573kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 583kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 593kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 604kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 614kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 624kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 634kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 645kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 655kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 665kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 675kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 686kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 696kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 706kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 716kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 727kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 737kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 747kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 757kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 768kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 778kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 788kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 798kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 808kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 819kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 829kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 839kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 849kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 860kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 870kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 880kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 890kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 901kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 911kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 921kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 931kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 942kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 952kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 962kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 972kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 983kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 993kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 1.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 1.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 1.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 1.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 1.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.7MB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp==1.0.3) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (2.10)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp37-none-any.whl size=2645936 sha256=ecd7fe32877ad0d0958286383980044d627959154b0ba1c030c8becba66c6fbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxVE9cR6yZ3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e20bd25-ff1d-45ce-9fcb-88e4d32ceb3d"
      },
      "source": [
        "# Download VnCoreNLP-1.1.1.jar & all of its  component (i.e. RDRSegmenter, pos, ner, deprel) \n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!mkdir -p vncorenlp/models/dep\n",
        "!mkdir -p vncorenlp/models/ner\n",
        "!mkdir -p vncorenlp/models/postagger\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/dep/vi-dep.xz\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger\n",
        "\n",
        "\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "\n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/\n",
        "\n",
        "!mv vi-dep.xz vncorenlp/models/dep/\n",
        "\n",
        "!mv vi-500brownclusters.xz vncorenlp/models/ner/\n",
        "!mv vi-ner.xz vncorenlp/models/ner/\n",
        "!mv vi-pretrainedembeddings.xz vncorenlp/models/ner/\n",
        "\n",
        "!mv vi-tagger vncorenlp/models/postagger/\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-09 12:15:00--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-09 12:15:01 (260 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2021-05-09 12:15:01--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-05-09 12:15:01 (28.5 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2021-05-09 12:15:01--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2021-05-09 12:15:01 (38.1 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n",
            "--2021-05-09 12:15:01--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/dep/vi-dep.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16048864 (15M) [application/octet-stream]\n",
            "Saving to: ‘vi-dep.xz’\n",
            "\n",
            "vi-dep.xz           100%[===================>]  15.30M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-09 12:15:02 (123 MB/s) - ‘vi-dep.xz’ saved [16048864/16048864]\n",
            "\n",
            "--2021-05-09 12:15:02--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5599844 (5.3M) [application/octet-stream]\n",
            "Saving to: ‘vi-500brownclusters.xz’\n",
            "\n",
            "vi-500brownclusters 100%[===================>]   5.34M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-05-09 12:15:02 (78.0 MB/s) - ‘vi-500brownclusters.xz’ saved [5599844/5599844]\n",
            "\n",
            "--2021-05-09 12:15:02--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9956876 (9.5M) [application/octet-stream]\n",
            "Saving to: ‘vi-ner.xz’\n",
            "\n",
            "vi-ner.xz           100%[===================>]   9.50M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-05-09 12:15:02 (121 MB/s) - ‘vi-ner.xz’ saved [9956876/9956876]\n",
            "\n",
            "--2021-05-09 12:15:02--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 57313672 (55M) [application/octet-stream]\n",
            "Saving to: ‘vi-pretrainedembeddings.xz’\n",
            "\n",
            "vi-pretrainedembedd 100%[===================>]  54.66M   231MB/s    in 0.2s    \n",
            "\n",
            "2021-05-09 12:15:03 (231 MB/s) - ‘vi-pretrainedembeddings.xz’ saved [57313672/57313672]\n",
            "\n",
            "--2021-05-09 12:15:03--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29709468 (28M) [application/octet-stream]\n",
            "Saving to: ‘vi-tagger’\n",
            "\n",
            "vi-tagger           100%[===================>]  28.33M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-09 12:15:03 (192 MB/s) - ‘vi-tagger’ saved [29709468/29709468]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXfVgT46BB-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e57f7d40-232b-4bef-a935-4e9b153420d7"
      },
      "source": [
        "import unicodedata\n",
        "from vncorenlp import VnCoreNLP\n",
        "\n",
        "# To perform word segmentation, POS tagging, NER and then dependency parsing\n",
        "annotator1 = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx2g') \n",
        "\n",
        "# To perform word segmentation, POS tagging and then NER\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg,pos,ner\", max_heap_size='-Xmx2g') \n",
        "# To perform word segmentation and then POS tagging\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg,pos\", max_heap_size='-Xmx2g') \n",
        "# To perform word segmentation only\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
        "# Input \n",
        "text = unicodedata.normalize(\"NFD\", \"Thanh Thủy\")\n",
        "\n",
        "\n",
        "# To perform word segmentation only\n",
        "word_segmented_text = annotator1.tokenize(text) \n",
        "\n",
        "print(*word_segmented_text, sep=\"\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Thanh', 'Thủy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhOuiCNgaVqk"
      },
      "source": [
        "### Install Underthesea"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IujpzlPaKfo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b34da190-e565-491c-b165-3819972bcfe0"
      },
      "source": [
        "!pip install underthesea==1.2.3"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting underthesea==1.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/46/1acb7e83092bbcbc9082afe3901ec51e98a303a19c8152655c43bd51583f/underthesea-1.2.3-py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (3.2.5)\n",
            "Collecting scikit-learn<0.22,>=0.20\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/c5/e5267eb84994e9a92a2c6a6ee768514f255d036f3c8378acfa694e9f2c99/scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 46.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (3.13)\n",
            "Collecting python-crfsuite>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (7.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (0.8.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (4.41.1)\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 54.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea==1.2.3) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.22,>=0.20->underthesea==1.2.3) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.22,>=0.20->underthesea==1.2.3) (1.19.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (3.0.4)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=1e38e7c80ddbbf8c109a484b0986a592fa91290fd9036d45f5e0e2df28213688\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: scikit-learn, python-crfsuite, seqeval, unidecode, underthesea\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed python-crfsuite-0.9.7 scikit-learn-0.21.3 seqeval-1.2.2 underthesea-1.2.3 unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjsyeDFRKpfF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d50b9f-89f2-4d3c-82be-e67d853fa017"
      },
      "source": [
        "from underthesea import sent_tokenize\n",
        "text = 'Quảng Bình : Cát tặc lộng hành, hiểm họa rình rập cầu Long Đại và dòng sông? Hiện tượng khai thác cát “lậu” trên sông cách cầu Long Đại vài trăm mét về phía hạ lưu, khiến cầu và sông Long Đại đang đứng trước hiểm họa khó lường? Vừa qua Pháp luật Plus nhận phản ánh của những người dân sống ở xã Xuân Ninh , huyện Quảng Ninh , tỉnh Quảng Bình về việc hiện nay ở xã này, cụ thể là tại thôn Xuân Dục 1 khu vực ven sông Long Đại lâu này xuất hiện những bãi tập kết cát trái phép và hiện tượng khai thác cát “lậu” trên sông cả ngày lẫn đêm gây ảnh hưởng đến đến cuộc sống thường nhật của người dân nơi đây. Từ những nguồn tin nêu trên sáng ngày 21/9, PV đã tiếp cận hiện trường đoạn sông Long Đại thuộc thôn Xuân Dụ 1 , xã Xuân Ninh , huyện Quảng Ninh , tỉnh Quảng Bình nơi người dân phản ánh để củng cố thông tin. Tại đây, PV nhận thấy nhiều bãi tập kết cát gần khu vực dân cư sinh sống, hàng ngày nhiều tàu chở cát vào đây để tập kết cát, gây ra tiếng ồn khó chịu ảnh hưởng không nhỏ đến sinh hoạt của người dân. Những bãi tập kết cát trái phép. Hơn thế nữa theo tìm hiểu của PV được biết, những vị trí có bãi tập kết cát kể trên không đủ tiêu chuẩn để tàu cập bến tập kết?. Trưa cùng ngày PV đã đi theo hướng thượng nguồn sông Long Đại mà theo phản ánh là xảy ra tình trạng khai thác cát trái phép thường xuyên diễn ra. PV nhận thấy một chiếc thuyền đang neo đậu cách bờ chừng vài chục mét và cách móng cầu Long Đại chừng vài trăm mét theo hướng hạ nguồn đang hút cát lên thuyền. Chiếc thuyền (ô đỏ) đang khai thác cát trái phép cách cầu Long Đại không xa. Tiếp tục ghi nhận và theo dõi vụ việc khoảng chừng hơn 30 phút, chiếc thuyền đã đầy cát đã được đi chuyển đi tập kết. Chiếc thuyền sau khi hút cát trái phép di chuyển về bãi tập kết. Qua tìm hiểu của PV được biết, cát ở khu vực gần cầu Long Đại là cát nhiễm mặn nếu dùng vào việc thi công công trình sẽ ảnh hưởng đến chất lượng của công trình đó… Và cát này được chủ thuyền bán lại cho người sử dụng với giá rẻ hơn so với cát được khai thác ở mỏ được cấp phép gây nên sự cạnh tranh không lành mạnh về giá cát. Tuy nhiên nhiều người dân chưa nhận thấy đến việc chất lượng của công trình sau này khi sử dụng cát nhiễm mặn này. Điều đáng nói là việc khai thác cát trái phép lại diễn ra khu vực gần móng cầu Long Đại (cả đường sắt lẫn đường bộ) nguy cơ sạc lở đất khu vực móng cầu, khiến cầu Long Đại đứng trước hiểm họa khó lường?. Liên quan đến vấn đề này, trao đổi với PV ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh cho biết về phía xã cũng đã nhiều lần xử lý nhắc nhở người dân trong vấn đề tập kết cát đúng nơi quy định. “Ngoài ra, xã đang hướng dẫn và hoàn thành các thủ tục nhằm đưa các điểm tập kết trái phép này đúng vào nơi quy định trong thời gian sớm nhất”, ông Tiến cho biết thêm. Ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh (bên phải) tại buổi làm việc với PV. Khi được PV cũng cấp bằng chứng về việc thuyền khai thác cát trái phép ngay giữa ban ngày gần khu vực móng cầu Long Đại , ông Tiến đã hết sức bất ngờ nói “Như vậy là không được rồi, không được rồi… sẽ cho xử lý ngay” Tiếp đó PV liên lạc qua điện thoại với ông Phạm Trung Đông – Chủ tịch UBND huyện Quảng Ninh để phản ánh sự việc thì ông Đông cho biết, đang bận và hướng dẫn PV liên hệ với ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh để làm việc. Tại buổi là việc với ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh PV đã cung cấp clip về việc nạn khai thác trái phép diễn ra ngay trên sông Long Đại đoạn gần móng cầu ông Giai cũng đã kiên quyết và hứa sẽ đấu tranh xử lý, đồng thời phối hợp với các cơ quan chức năng khác thường xuyên kiểm tra để chấm dứt tình trạng này. Ông Nguyễn Viết Giai cho biết sẽ đấu tranh xử lý Còn về việc các bãi tập kết trái phép, ông Giai cho biết sẽ xử lý dứt điểm trong thời gian sớm nhất để không ảnh hưởng tới cuộc sống người dân xung quanh. Khi được PV hỏi thời gian sớm nhất là bao lâu ông Giai cho biết: “ở đây đang còn vướng một khâu thủ tục. thời gian giải quyết sớm nhất cũng phải mất chừng 7 đến 10 ngày.” Việc khai thác cát trái phép gần cầu Long Đại (cả đường sắt lẫn đường bộ) nguy cơ sạt lở đất khu vực móng cầu, khiến cầu Long Đại đứng trước hiểm họa khó lường? Tuy là vậy nhưng trong sáng 22/9, PV một lần nữa đến tại hiện trường chiếc thuyền khai thác trái phép thì nhận thấy tình hình khai thác cát trái phép vẫn không hề thay đổi. Một lần nữa PV đã gọi điện thoại cho ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh và ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh để phản ánh thì lại được 2 vị hứa “sẽ xử lý”. Trong sáng 22/9 việc khai thác cát trái phép vẫn diễn ra mà không có sự can thiệp của cơ quyan chức năng? Từ những việc nêu trên, dư luận không thể không đặt ra câu hỏi liệu những việc xảy ra ở đây có phải là có sự “bảo kê” hoặc có sự tiếp tay của lực lượng chức năng có thẩm thẩm quyền hay không? Pháp luật Plus sẽ tiếp tục thông tin vụ việc đến bạn đọc.'\n",
        "sent_tokenize(text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Quảng Bình : Cát tặc lộng hành, hiểm họa rình rập cầu Long Đại và dòng sông?',\n",
              " 'Hiện tượng khai thác cát “lậu” trên sông cách cầu Long Đại vài trăm mét về phía hạ lưu, khiến cầu và sông Long Đại đang đứng trước hiểm họa khó lường?',\n",
              " 'Vừa qua Pháp luật Plus nhận phản ánh của những người dân sống ở xã Xuân Ninh , huyện Quảng Ninh , tỉnh Quảng Bình về việc hiện nay ở xã này, cụ thể là tại thôn Xuân Dục 1 khu vực ven sông Long Đại lâu này xuất hiện những bãi tập kết cát trái phép và hiện tượng khai thác cát “lậu” trên sông cả ngày lẫn đêm gây ảnh hưởng đến đến cuộc sống thường nhật của người dân nơi đây.',\n",
              " 'Từ những nguồn tin nêu trên sáng ngày 21/9, PV đã tiếp cận hiện trường đoạn sông Long Đại thuộc thôn Xuân Dụ 1 , xã Xuân Ninh , huyện Quảng Ninh , tỉnh Quảng Bình nơi người dân phản ánh để củng cố thông tin.',\n",
              " 'Tại đây, PV nhận thấy nhiều bãi tập kết cát gần khu vực dân cư sinh sống, hàng ngày nhiều tàu chở cát vào đây để tập kết cát, gây ra tiếng ồn khó chịu ảnh hưởng không nhỏ đến sinh hoạt của người dân.',\n",
              " 'Những bãi tập kết cát trái phép.',\n",
              " 'Hơn thế nữa theo tìm hiểu của PV được biết, những vị trí có bãi tập kết cát kể trên không đủ tiêu chuẩn để tàu cập bến tập kết?.',\n",
              " 'Trưa cùng ngày PV đã đi theo hướng thượng nguồn sông Long Đại mà theo phản ánh là xảy ra tình trạng khai thác cát trái phép thường xuyên diễn ra.',\n",
              " 'PV nhận thấy một chiếc thuyền đang neo đậu cách bờ chừng vài chục mét và cách móng cầu Long Đại chừng vài trăm mét theo hướng hạ nguồn đang hút cát lên thuyền.',\n",
              " 'Chiếc thuyền (ô đỏ) đang khai thác cát trái phép cách cầu Long Đại không xa.',\n",
              " 'Tiếp tục ghi nhận và theo dõi vụ việc khoảng chừng hơn 30 phút, chiếc thuyền đã đầy cát đã được đi chuyển đi tập kết.',\n",
              " 'Chiếc thuyền sau khi hút cát trái phép di chuyển về bãi tập kết.',\n",
              " 'Qua tìm hiểu của PV được biết, cát ở khu vực gần cầu Long Đại là cát nhiễm mặn nếu dùng vào việc thi công công trình sẽ ảnh hưởng đến chất lượng của công trình đó… Và cát này được chủ thuyền bán lại cho người sử dụng với giá rẻ hơn so với cát được khai thác ở mỏ được cấp phép gây nên sự cạnh tranh không lành mạnh về giá cát.',\n",
              " 'Tuy nhiên nhiều người dân chưa nhận thấy đến việc chất lượng của công trình sau này khi sử dụng cát nhiễm mặn này.',\n",
              " 'Điều đáng nói là việc khai thác cát trái phép lại diễn ra khu vực gần móng cầu Long Đại (cả đường sắt lẫn đường bộ) nguy cơ sạc lở đất khu vực móng cầu, khiến cầu Long Đại đứng trước hiểm họa khó lường?.',\n",
              " 'Liên quan đến vấn đề này, trao đổi với PV ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh cho biết về phía xã cũng đã nhiều lần xử lý nhắc nhở người dân trong vấn đề tập kết cát đúng nơi quy định.',\n",
              " '“Ngoài ra, xã đang hướng dẫn và hoàn thành các thủ tục nhằm đưa các điểm tập kết trái phép này đúng vào nơi quy định trong thời gian sớm nhất”, ông Tiến cho biết thêm.',\n",
              " 'Ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh (bên phải) tại buổi làm việc với PV.',\n",
              " 'Khi được PV cũng cấp bằng chứng về việc thuyền khai thác cát trái phép ngay giữa ban ngày gần khu vực móng cầu Long Đại , ông Tiến đã hết sức bất ngờ nói “Như vậy là không được rồi, không được rồi… sẽ cho xử lý ngay” Tiếp đó PV liên lạc qua điện thoại với ông Phạm Trung Đông – Chủ tịch UBND huyện Quảng Ninh để phản ánh sự việc thì ông Đông cho biết, đang bận và hướng dẫn PV liên hệ với ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh để làm việc.',\n",
              " 'Tại buổi là việc với ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh PV đã cung cấp clip về việc nạn khai thác trái phép diễn ra ngay trên sông Long Đại đoạn gần móng cầu ông Giai cũng đã kiên quyết và hứa sẽ đấu tranh xử lý, đồng thời phối hợp với các cơ quan chức năng khác thường xuyên kiểm tra để chấm dứt tình trạng này.',\n",
              " 'Ông Nguyễn Viết Giai cho biết sẽ đấu tranh xử lý Còn về việc các bãi tập kết trái phép, ông Giai cho biết sẽ xử lý dứt điểm trong thời gian sớm nhất để không ảnh hưởng tới cuộc sống người dân xung quanh.',\n",
              " 'Khi được PV hỏi thời gian sớm nhất là bao lâu ông Giai cho biết: “ở đây đang còn vướng một khâu thủ tục.',\n",
              " 'thời gian giải quyết sớm nhất cũng phải mất chừng 7 đến 10 ngày.” Việc khai thác cát trái phép gần cầu Long Đại (cả đường sắt lẫn đường bộ) nguy cơ sạt lở đất khu vực móng cầu, khiến cầu Long Đại đứng trước hiểm họa khó lường?',\n",
              " 'Tuy là vậy nhưng trong sáng 22/9, PV một lần nữa đến tại hiện trường chiếc thuyền khai thác trái phép thì nhận thấy tình hình khai thác cát trái phép vẫn không hề thay đổi.',\n",
              " 'Một lần nữa PV đã gọi điện thoại cho ông Nguyễn Trường Tiến – Chủ tịch xã Xuân Ninh và ông Nguyễn Viết Giai - Trưởng Phòng Tài nguyên môi trường huyện Quảng Ninh để phản ánh thì lại được 2 vị hứa “sẽ xử lý”.',\n",
              " 'Trong sáng 22/9 việc khai thác cát trái phép vẫn diễn ra mà không có sự can thiệp của cơ quyan chức năng?',\n",
              " 'Từ những việc nêu trên, dư luận không thể không đặt ra câu hỏi liệu những việc xảy ra ở đây có phải là có sự “bảo kê” hoặc có sự tiếp tay của lực lượng chức năng có thẩm thẩm quyền hay không?',\n",
              " 'Pháp luật Plus sẽ tiếp tục thông tin vụ việc đến bạn đọc.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chNxSZ0mx5wS"
      },
      "source": [
        "# Extract raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phjF0edvNGpc"
      },
      "source": [
        "import os\n",
        "import re"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pTphYOiMYho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e786bb7-f88c-43ad-f81f-32e3bfae7f0a"
      },
      "source": [
        "# get all subfolers and files in subfolers\n",
        "# [(\"top_subfolders\", [subfolders_in_top_subfolders], [files_in_top_subfolders])]\n",
        "sub_folders = [f for f in os.walk(\"VLSP2020_RE_training\")][1:]\n",
        "sub_folders = sorted(sub_folders, key=lambda x: x[0])   # sort by top_subfolder name\n",
        "\n",
        "## top subfolder contain only 1 single file.\n",
        "check = False\n",
        "for i in sub_folders:\n",
        "    if i[1] or len(i[-1])!= 1:\n",
        "        print(\"ALERT!!!\")\n",
        "        check = True\n",
        "\n",
        "if not check:\n",
        "    print(\"There is \", len(sub_folders), \" subfolders. All subfolders contain only 1 file.\",\n",
        "          \" So that we have \", len(sub_folders), \" files.\")\n",
        "\n",
        "# generate data files name\n",
        "files_path = [os.path.join(i[0], i[-1][0]) for i in sub_folders]\n",
        "\n",
        "# print(*files_path, sep=\"\\n\")\n",
        "\n",
        "# print(files_path)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is  506  subfolders. All subfolders contain only 1 file.  So that we have  506  files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LJk2k0vD0dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37164169-9fca-4519-8202-c15cd5298267"
      },
      "source": [
        "# Xem trong bộ dữ liệu có những character gì\n",
        "\n",
        "character_lst = []\n",
        "for file in files_path:\n",
        "    with open(file, mode='r') as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "        # find line start with \"#Text=\"\n",
        "        textline_id = []\n",
        "        for i, text in enumerate(lines):\n",
        "            if (\"#Text=\" == text[0:6]):\n",
        "                textline_id.append(i)\n",
        "\n",
        "        # every data file has only one line that start with \"#Text=\"\"\n",
        "        assert (len(textline_id) == 1), str(\"1 is not number of line start with #Text=. \\nDoc: \" + file)\n",
        "\n",
        "        for c in lines[textline_id[0]][6:]:\n",
        "            if c not in character_lst:\n",
        "                character_lst.append(c)\n",
        "\n",
        "\n",
        "# Print all of the single characters, 30 per row.\n",
        "# For every batch of 30 tokens...\n",
        "for i in range(0, len(character_lst), 30):\n",
        "    \n",
        "    # Limit the end index so we don't go past the end of the list.\n",
        "    end = min(i + 30, len(character_lst) + 1)\n",
        "    \n",
        "    # Print out the tokens, separated by a space.\n",
        "    print(repr(' '.join(character_lst[i:end])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'C â u   c h y ệ n l ạ m t p ả i đ ư ợ g q ế ừ 3 í a X ủ B d'\n",
            "'ẹ ọ s ậ ắ ớ ầ , ề à á o r ằ ù ó v ò ị k ô ũ ê ể ì . Ả T ứ ở'\n",
            "'ộ G D & Đ N ễ ĩ ã ý ấ ố ữ ờ ụ x e é b ú ă “ ơ ử ồ H ” Ô L S'\n",
            "'ẽ ỉ ự ổ ỏ V – P ẻ : ỗ ẫ 2 ; ặ è ( ) ẩ Q ? 1 / 9 õ 0 … K U -'\n",
            "'7 M ỹ R W w ỳ O I 5 % 6 4 ! ỡ 8 A f ỷ \" F Á E z ẳ Ý Â ẵ J j'\n",
            "\"' \\ufeff Z Ế \\xa0 _ Y Ủ Ư Ú Ễ Ở Ă Ấ ’ À Ê + Ơ Ạ Ứ ̣ ́ ̀ Ầ Í | ỵ Ắ Ọ\"\n",
            "'Ỗ > Ậ Ờ Ồ @ ̉ ̃ ‘ Ì $ * ö < = # Ố ″ ½ Ð Ũ ² [ ] Ớ Ẩ Õ ñ ï Ệ'\n",
            "'Ị Ề ¾ × ƒ ‑ — Ó ® ™ Ể'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugs-iTcldECs"
      },
      "source": [
        "constant = {\"entity_name\": [\"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"MISCELLANEOUS\"],\n",
        "          \"relation_name\": [\"LOCATED\", \"PART – WHOLE\", \"AFFILIATION\", \"PERSONAL - SOCIAL\"]\n",
        "           }"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rqndrEtimOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b4786bd-5bbb-460e-dc59-437295ccabd8"
      },
      "source": [
        "\"\"\"\n",
        "            dict = {\"doc_id\": id of folder contain doc, \n",
        "                      \"text\": doc, \n",
        "                 \"token_ids\": [                                        tokens_id, ...], \n",
        "              \"subtoken_ids\": [                                   None or sub-id, ...],\n",
        "                       \"pos\": [                             [start pos, end pos], ...],\n",
        "                    \"tokens\": [                                      tokens_text, ...],\n",
        "                    \"entity\": [                        [entity_ids, entity_name], ...],\n",
        "                  \"relation\": [    [relation, stoken_id, ssubtoken_id direction], ...]\n",
        "                   }\n",
        "\n",
        "\n",
        "                      doc_id: id of folder contain doc                                            (str)\n",
        "                         doc: doc. line start with \"#Text=\"                                       (str)\n",
        "                   token_ids: ids of tokens.                                    first column       (list int)\n",
        "                subtoken_ids: int if crr token is a subtoken, otherwise None    first column      (list int, None)\n",
        "                         pos: posittion of tokens.                              second column     (list list int) \n",
        "                              [\n",
        "                                  [start pos, end pos],\n",
        "                                  ...\n",
        "                              ]\n",
        "                       token: tokens.                                           third column      (list str)\n",
        "                      entity: entity infor if token is entity, else None.       4th, 5th column   (list list, None)\n",
        "                              [\n",
        "                                  [entity_id, entity_name],\n",
        "                                  ...\n",
        "                              ]\n",
        "                    relation: relation if token is in a relation, else None.    other column      (list list, None)\n",
        "                              [\n",
        "                                  [[relation1_name, relation1_start_tokenID, relation1_start_subtokenID, [start_entity_id, end_entity_id]], ...],  \n",
        "                                                                         --> relation1_start_subtokenID may be None if start token is not a sub token\n",
        "                                  [[relation1_name, relation1_start_tokenID, relation1_start_subtokenID,                             None], ...],  <-- dataset has mistake. Don't have direction.\n",
        "                                                                                 \n",
        "                                  \n",
        "                                  ....\n",
        "                              ]\n",
        "                    \n",
        "\"\"\"\n",
        "\n",
        "raw_tdata = []\n",
        "\n",
        "for file in files_path:\n",
        "    docif = {}\n",
        "    with open(file, mode='r') as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "        # example: VLSP2020_RE_training/23351113.conll/CURATION_USER.tsv -> 23351113\n",
        "        docif[\"doc_id\"] = file[(file.find(\"/\") + 1): file.find(\".\")]\n",
        "\n",
        "        # find line start with \"#Text=\"\n",
        "        textline_id = []\n",
        "        for i, text in enumerate(lines):\n",
        "            if (\"#Text=\" == text[0:6]):\n",
        "                textline_id.append(i)\n",
        "\n",
        "        # every data file has only one line that start with \"#Text=\"\"\n",
        "        assert (len(textline_id) == 1), str(\"1 is not number of line start with #Text=. \\nDoc: \" + file)\n",
        "\n",
        "        docif[\"text\"] = lines[textline_id[0]][6:]\n",
        "\n",
        "        first_cline = lines[(textline_id[0] + 1)].rstrip(\"\\t\").split(\"\\t\")   # first column_line\n",
        "        assert (len(first_cline) in [3, 5, 7, 8]), str(\"Doc has problem. doc: \" + file)\n",
        "\n",
        "\n",
        "        token_ids, subtoken_ids, pos, tokens = [], [], [], []\n",
        "        entity = []\n",
        "        relation = []\n",
        "\n",
        "        pretk_id = 0\n",
        "\n",
        "        for tk_id, line in enumerate(lines[(textline_id[0] + 1):]):\n",
        "            lineif = line.rstrip(\"\\t\").split(\"\\t\")   # seperate by one \\t between columns: [abc\\txyz\\t]\n",
        "\n",
        "            # check if columns is seperated by only one single Tab character '\\t'\n",
        "            lineif1 = re.split(r'\\t+', line.rstrip('\\t'))   # seperate by all \\t between column: [abc\\t\\t\\txyz\\t]\n",
        "            assert (lineif == lineif1), str(\"Columns is not seperated by only one single TAB '\\\\t'. doc: \" + file + \" line: \" + line)\n",
        "\n",
        "            # check if inside a doc, only exist one number of (no) columns\n",
        "            # above we check if len(lineif) in [3, 5, 7, 8], too. so we can make sure that\n",
        "            # in a doc, number of columns only in [3, 5, 7, 8]\n",
        "            # and all line in a doc has same no columns\n",
        "            assert len(lineif) == len(first_cline), str(\"Number of columns in doc is not consistent. \\nDoc: \" + file + \" line: \" + line)\n",
        "\n",
        "\n",
        "            # remove all \"_\" in lineif because we don't need it\n",
        "            # [3, 5, 7, 8] -> [3, 4, 5, 7]\n",
        "            # and all data has first three column. (4th and 5th) is a pair, (6th and 7th) is a pair\n",
        "            # after removing all \"_\", if:\n",
        "            # len(lineif) = 3 -> token_ids, pos, no entity, no relation\n",
        "            # len(lineif) = 5 -> token_ids, pos, entity, no relation\n",
        "            # len(lineif) = 7 -> token_ids, pos, entity, relation\n",
        "\n",
        "            # len(lineif) = 4 --> token_ids, pos, no entity, no relation (this is a mistake in dataset, in data file has 8 columns)\n",
        "\n",
        "            lineif = [col for col in lineif if col != \"_\"]\n",
        "\n",
        "            assert (len(lineif) in [3, 4, 5, 7]), str(\"Problem with number of columns after remove \\'_\\'.\\nIn doc: \" + file + \" line \" + line)\n",
        "\n",
        "            # match first column format\n",
        "            # startwith (\"1-\") then (number) end:   1-id\n",
        "            pattern_token_ids = re.compile(\"^(1-)([\\d]+)$\")\n",
        "\n",
        "            # a token may has many subtokens\n",
        "            # startwith (\"1-\") then (number) then (. char) then (number) end:   1-id.subid \n",
        "            # pattern_subtoken_ids = re.compile(\"^(1-)([\\d]+)(\\.)([\\d]+)$\")\n",
        "\n",
        "            # Currently in train dataset, number of subtoken of a token is 0 or 1\n",
        "            # startwith (\"1-\") then (number) then (.1) end:   1-id.1 \n",
        "            pattern_subtoken_ids = re.compile(\"^(1-)([\\d]+)(\\.1)$\")\n",
        "\n",
        "            assert (pattern_token_ids.match(lineif[0]) or pattern_subtoken_ids.match(lineif[0])), \\\n",
        "            str(\"Unexpected first column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "            if pattern_token_ids.match(lineif[0]):\n",
        "                # 1-id\n",
        "                # Check if token id is increased by one in each line or not.\n",
        "                assert (int(lineif[0][2:]) == (pretk_id + 1)), str(\"First column, Token_ID is not increased by one in each line. \\nIn doc: \" + file + \" \\nline: \" + line)\n",
        "\n",
        "                token_ids.append(int(lineif[0][2:]))\n",
        "                subtoken_ids.append(None)   # Not a subtoken\n",
        "\n",
        "                pretk_id += 1\n",
        "            \n",
        "            else:\n",
        "                # 1-id.1\n",
        "                # 1-id.subid\n",
        "                tmp = lineif[0].find(\".\")\n",
        "                tokenID = int(lineif[0][2:tmp])\n",
        "                subtokenID = int(lineif[0][(tmp+1):])\n",
        "                \n",
        "                assert (tokenID == token_ids[-1]), str(\"Exist subtoken without a token before it. \\nIn doc: \" + file + \" \\nline\" + line)\n",
        "\n",
        "                token_ids.append(tokenID)\n",
        "                subtoken_ids.append(subtokenID)\n",
        "\n",
        "                print(\"\\nTHERE IS A SUBTOKEN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "\n",
        "            # match second column format\n",
        "            # startwith (number) then (\"-\" char) then (number) end\n",
        "            pattern_pos = re.compile(\"^([\\d]+)(\\-)([\\d]+)$\")\n",
        "            assert (pattern_pos.match(lineif[1])), str(\"Unexpected second column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "            pos.append([int(ele) for ele in lineif[1].split(\"-\")])    # example: \"3-6\" -> [3, 6]\n",
        "            \n",
        "            # if current token is a subtoken, check if pos subtoken is inside father token or not.\n",
        "            if pattern_subtoken_ids.match(lineif[0]):\n",
        "                father_token = token_ids.index(token_ids[-1])\n",
        "\n",
        "                assert (pos[father_token][0] <= pos[-1][0]) and (pos[-1][1] <= pos[father_token][1]), \\\n",
        "                str(\"Subtoken\\'s position is not inside father token\\'s position. \\nIndoc: \" + file + \"\\Line: \" + line)\n",
        "\n",
        "\n",
        "            # third column\n",
        "            #check if token is matched with pos (second column) or not\n",
        "            crr_token_pos = [int(ele) for ele in lineif[1].split(\"-\")]\n",
        "            if lineif[2] == lines[textline_id[0]][6:][crr_token_pos[0]:crr_token_pos[1]]:\n",
        "                tokens.append(lineif[2])\n",
        "            else:\n",
        "                assert False, str(\"Token in 3th column not match with position at 2th column. \\nIn doc: \" + file + \" \\nline: \" + line)\n",
        "            \n",
        "\n",
        "            if (len(lineif) == 3) or (len(lineif) == 4):\n",
        "                entity.append(None)\n",
        "                relation.append(None)\n",
        "\n",
        "                if len(lineif) == 4:\n",
        "                    print(\"\\n4 COLUMNS.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "\n",
        "            # because we removed all \"_\", \n",
        "            # so when len(lineif) = 5 or len(lineif) = 7, this line must has: token_ids, pos, tokens and entity.\n",
        "            # (we don't have to check if 4th, 5th column is \"_\" anymore, since we removed all \"_\")\n",
        "            if (len(lineif) == 5) or (len(lineif) == 7):\n",
        "                # 4th column now only have two posibilities: \"*\" or \"*[number]\"\n",
        "                pattern_entity_id = re.compile(\"^(\\*)(\\[)([\\d]+)(\\])$\")\n",
        "                assert ((lineif[3] == \"*\") or pattern_entity_id.match(lineif[3])), str(\"Unexpected fourth column's format. In doc: \" + file + \" line \" + line)\n",
        "\n",
        "                # in doc: 23352816\n",
        "                # line: 1-23\t126-136\t</ENAMEX>)\t*\t*\t_\t_\t_\t\n",
        "                # there is a mistake in 5th column. Unknow enity name\n",
        "                # I will let this token entity is None.\n",
        "\n",
        "                # We can just let all token entity is None\n",
        "                # if 5th column is not in constant[\"entity_name\"]\n",
        "                # but below, I just code for this specific case\n",
        "                # because I want to know more about dataset\n",
        "\n",
        "                if (lineif[3] == \"*\"):\n",
        "\n",
        "                    if (lineif[4] == \"*\"):   # specific mistake case\n",
        "                        entity.append(None)\n",
        "                        print(\"\\nENTITY NAME MISTAKE IN 5TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                    else:\n",
        "                        assert (lineif[4] in constant[\"entity_name\"]), str(\"Unknown entity name. \\nDoc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "                        entity_id = 0\n",
        "                        entity_n = lineif[4]\n",
        "\n",
        "                        entity.append([entity_id, entity_n])\n",
        "                \n",
        "                elif pattern_entity_id.match(lineif[3]):\n",
        "                    # *[number]: *[26] -> 26\n",
        "                    entity_id = int(lineif[3][2:-1])\n",
        "                    \n",
        "                    # PERSON[26]\n",
        "                    tmp = lineif[4].find(\"[\")\n",
        "                    \n",
        "                    assert (entity_id == int(lineif[4][(tmp+1):-1])), str(\"Entity ID in 4th and 5th column are not the same. In doc: \" + file + \" line \" + line)\n",
        "                    \n",
        "                    assert (lineif[4][:tmp] in constant[\"entity_name\"]), str(\"Unknown entity name in doc: \" + file + \" line \" + line)\n",
        "                    \n",
        "                    entity_n = lineif[4][:tmp]\n",
        "\n",
        "                    entity.append([entity_id, entity_n])\n",
        "\n",
        "                # may be we dont need this last else because we use regex above\n",
        "                else:\n",
        "                    assert False, str(\"4th, 5th column has UNKNOWN MISTAKE. In Doc: \" + file + \"\\nline: \" + line)\n",
        "\n",
        "\n",
        "\n",
        "            if len(lineif) == 5:\n",
        "                relation.append(None)\n",
        "            \n",
        "\n",
        "            if len(lineif) == 7:\n",
        "                # example:\n",
        "                # AFFILIATION\t1-593[13_14]\n",
        "                # PART – WHOLE\t1-42[1_2]\n",
        "                # PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-80[3_8]|1-105[7_8]\n",
        "\n",
        "                # PART – WHOLE\t1-42    (an error in dataset that need to be handled)\n",
        "\n",
        "                rel_names = lineif[5].split(\"|\")    # PERSONAL - SOCIAL|PERSONAL - SOCIAL --> [\"PERSONAL - SOCIAL\", \"PERSONAL - SOCIAL\"]\n",
        "                rel_oifs = lineif[6].split(\"|\")     # 1-80[3_8]|1-105[7_8] --> [\"1-80[3_8]\", \"1-105[7_8]\"]\n",
        "\n",
        "                # in doc: 23351515\n",
        "                # line: 1-318\n",
        "                # 6th column: PART – WHOLE|LOCATED|PART – WHOLE|*\n",
        "                # last relation name is: *  -> mistake\n",
        "                # We can read data and change it to right one \n",
        "                # but I will remove this \"*\" relation in 6th and 7th column\n",
        "\n",
        "                if '*' in rel_names:\n",
        "                    tmp = rel_names.index('*')\n",
        "\n",
        "                    del rel_names[tmp]\n",
        "                    del rel_oifs[tmp]\n",
        "\n",
        "                    print(\"\\nRELATION MISTAKE IN 6TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                # MISTAKE In doc: 23351856\n",
        "                # line: 1-185\t807-812\tTriều\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
        "                # assert ((len(rel_names) == len(rel_oifs)) and (len(rel_names) >= 1)), str(\"Number of relations in 6th and 7th columns is different to each other. In doc: \" + file + \" line \" + line )\n",
        "                # handle later\n",
        "\n",
        "                assert (len(rel_names) == len(rel_oifs)), str(\"Number of relations in 6th and 7th columns is different to each other. \\nIn doc: \" + file + \" \\nline \" + line )\n",
        "\n",
        "\n",
        "                rels = []\n",
        "                for i in range(len(rel_names)):\n",
        "                    assert (rel_names[i] in constant[\"relation_name\"]), \\\n",
        "                    str(\"Unknown relation_name in doc: \" + file + \" \\nline \" + line)\n",
        "                    \n",
        "                    relation_n = rel_names[i]\n",
        "\n",
        "                    # (startwith \"1-\") then (number) then ([ char) then (number) then (_ char) then (number) then (] char) end\n",
        "                    #             1-         26            [             3             _             0             ]   \n",
        "                    pattern_relation_oif = re.compile(\"^(1-)([\\d]+)(\\[)([\\d]+)(\\_)([\\d]+)(\\])$\")\n",
        "\n",
        "                    # (startwith \"1-\") then (number) then (.1) then ([ char) then (number) then (_ char) then (number) then (] char) end\n",
        "                    #             1-         26            .1        [             3             _             0             ]   \n",
        "                    pattern_relation_oif_1 = re.compile(\"^(1-)([\\d]+)(\\.1)(\\[)([\\d]+)(\\_)([\\d]+)(\\])$\")\n",
        "                    \n",
        "                    # below is a mistake in dataset\n",
        "                    # but currently, this type mistake has only below form (only has token id).\n",
        "                    # (don't have subtoken id mistake type, yet)\n",
        "                    # (startwith \"1-\") then (number)  end\n",
        "                    #             1-         26          \n",
        "                    pattern_relation_oif_mistake_1 = re.compile(\"^(1-)([\\d]+)$\")\n",
        "\n",
        "\n",
        "\n",
        "                    assert (pattern_relation_oif.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_1.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_mistake_1.match(rel_oifs[i])), \\\n",
        "                            str(\"Unexpected seventh column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "                    \n",
        "\n",
        "                    # NOTICE:\n",
        "                    # IN BELOW CODE, I DONT CHECK IF ONE OF TWO ENTITIES OF A RELATION\n",
        "                    # IS \"MISCELLANEOUS\" OR NOT. \n",
        "                    # MISCELLANEOUS IS A LEGIT ENTITY NAME, BUT IT IS NOT USED IN ANY RELATION TYPE.\n",
        "                    # I WONDER IF DATASET HAS THIS MISTAKE OR NOT.\n",
        "                    # I WILL CHECK IT WHEN I CREATE SENTENCES AS INPUT OF BERT.\n",
        "\n",
        "\n",
        "                    if pattern_relation_oif.match(rel_oifs[i]):\n",
        "                        # 1-id[id_id]\n",
        "\n",
        "                        tmp_stkid = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_etkid = rel_oifs[i].find(\"[\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp_stkid:tmp_etkid]\n",
        "\n",
        "                        # start subtoken id\n",
        "                        sstoken_id = None\n",
        "\n",
        "                        direction = rel_oifs[i][(tmp_etkid+1):-1]\n",
        "\n",
        "                        direction = direction.split(\"_\")   # [sentity_id, eentity_id]\n",
        "\n",
        "                        rels.append([relation_n, int(stoken_id), sstoken_id, [int(direction[0]), int(direction[1])]])\n",
        "\n",
        "                    elif pattern_relation_oif_1.match(rel_oifs[i]):\n",
        "                        # 1-id.subid[id_id]\n",
        "\n",
        "                        tmp_sid = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_eid = rel_oifs[i].find(\".\")\n",
        "\n",
        "                        tmp_ssid = rel_oifs[i].find(\".\") + 1\n",
        "                        tmp_esid = rel_oifs[i].find(\"[\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp_sid:tmp_eid]\n",
        "\n",
        "                        sstoken_id = rel_oifs[i][tmp_ssid:tmp_esid]\n",
        "\n",
        "\n",
        "                        direction = rel_oifs[i][(tmp_esid+1):-1]\n",
        "                        direction = direction.split(\"_\")\n",
        "\n",
        "                        rels.append([relation_n, int(stoken_id), int(sstoken_id), [int(direction[0]), int(direction[1])]])\n",
        "\n",
        "                        print(\"\\nSPECIAL SUBTOKEN IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "                    \n",
        "                    else:\n",
        "                        # 1-id\n",
        "                        tmp = rel_oifs[i].find(\"-\") + 1\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp:]\n",
        "                        \n",
        "                        sstoken_id = None\n",
        "                        direction = None\n",
        "\n",
        "                        # rels.append([relation_n, stoken_id, sstoken_id, direction])\n",
        "                        rels.append([relation_n, int(stoken_id), None, None])\n",
        "\n",
        "                        print(\"\\nMISTAKE IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                \n",
        "                if len(rels) == 0:\n",
        "                    # MISTAKE In doc: 23351856\n",
        "                    # line: 1-185\t807-812\tTriều\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
        "                    relation.append(None)\n",
        "                    print(\"\\nREALTION NAME MISTAKE IN 6TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                else:\n",
        "                    relation.append(rels)\n",
        "\n",
        "\n",
        "        docif[\"token_ids\"] = token_ids\n",
        "        docif[\"subtoken_ids\"] = subtoken_ids\n",
        "        docif[\"pos\"] = pos\n",
        "        docif[\"tokens\"] = tokens\n",
        "        docif[\"entity\"] = entity\n",
        "        docif[\"relation\"] = relation\n",
        "\n",
        "    raw_tdata.append(docif)\n",
        "\n",
        "\n",
        "print(len(raw_tdata))           \n",
        "                \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351316.conll/CURATION_USER.tsv \n",
            "Line:  1-651\t2931-2939\tAlphabet\t*\tORGANIZATION\tPART – WHOLE\t1-649\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351316.conll/CURATION_USER.tsv \n",
            "Line:  1-687\t3105-3109\tLyft\t*\tORGANIZATION\tAFFILIATION\t1-683\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351316.conll/CURATION_USER.tsv \n",
            "Line:  1-726\t3275-3276\tÝ\t*\tLOCATION\tLOCATED\t1-728\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351394.conll/CURATION_USER.tsv \n",
            "Line:  1-103\t459-464\tValve\t*\tORGANIZATION\tAFFILIATION\t1-106\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351426.conll/CURATION_USER.tsv \n",
            "Line:  1-299\t1355-1361\tH.T.H.\t*\tPERSON\tPERSONAL - SOCIAL\t1-291\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351426.conll/CURATION_USER.tsv \n",
            "Line:  1-318\t1438-1444\tNguyên\t*\tPERSON\tPERSONAL - SOCIAL\t1-313\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351430.conll/CURATION_USER.tsv \n",
            "Line:  1-762\t3550-3555\tTPHCM\t*\tLOCATION\tLOCATED\t1-766\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23351433.conll/CURATION_USER.tsv \n",
            "Line:  1-583.1\t2570-2573\tTập\t*[19]\tORGANIZATION[19]\t_\t_\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351515.conll/CURATION_USER.tsv \n",
            "Line:  1-318\t1372-1376\ttỉnh\t*[19]\tLOCATION[19]\tPART – WHOLE|LOCATED|PART – WHOLE|*\t1-315[18_19]|1-301[15_19]|1-307[16_19]|1-310[17_19]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351516.conll/CURATION_USER.tsv \n",
            "Line:  1-423\t1851-1853\tMỹ\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-421|1-418[17_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351519.conll/CURATION_USER.tsv \n",
            "Line:  1-221\t972-979\t(TP.HCM\t*\tLOCATION\tLOCATED|LOCATED\t1-212[12_0]|1-189\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351519.conll/CURATION_USER.tsv \n",
            "Line:  1-231\t1015-1019\tHùng\t*\tPERSON\tPERSONAL - SOCIAL\t1-229\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351519.conll/CURATION_USER.tsv \n",
            "Line:  1-349\t1543-1546\tHải\t*\tPERSON\tPERSONAL - SOCIAL\t1-347\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351519.conll/CURATION_USER.tsv \n",
            "Line:  1-485\t2141-2144\tHải\t*\tPERSON\tPERSONAL - SOCIAL\t1-476\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351519.conll/CURATION_USER.tsv \n",
            "Line:  1-662\t2946-2949\tHải\t*\tPERSON\tPERSONAL - SOCIAL\t1-659\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351522.conll/CURATION_USER.tsv \n",
            "Line:  1-40\t186-190\tReal\t*\tORGANIZATION\tAFFILIATION\t1-33\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351556.conll/CURATION_USER.tsv \n",
            "Line:  1-119\t534-540\tLondon\t*\tLOCATION\tLOCATED|LOCATED\t1-121|1-123[8_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351556.conll/CURATION_USER.tsv \n",
            "Line:  1-495\t2256-2263\tWembley\t*\tLOCATION\tLOCATED\t1-488\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351564.conll/CURATION_USER.tsv \n",
            "Line:  1-567\t2526-2528\tH.\t*\tPERSON\tPERSONAL - SOCIAL\t1-579\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351566.conll/CURATION_USER.tsv \n",
            "Line:  1-461\t2163-2174\tTechcombank\t*\tORGANIZATION\tAFFILIATION\t1-455\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351566.conll/CURATION_USER.tsv \n",
            "Line:  1-488\t2295-2306\tTechcombank\t*\tORGANIZATION\tAFFILIATION\t1-478\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351566.conll/CURATION_USER.tsv \n",
            "Line:  1-583\t2734-2740\tTP.HCM\t*\tLOCATION\tLOCATED\t1-579\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23351610.conll/CURATION_USER.tsv \n",
            "Line:  1-201.1\t906-912\ttrường\t*[10]\tORGANIZATION[10]\t_\t_\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351612.conll/CURATION_USER.tsv \n",
            "Line:  1-39\t197-202\t(Pháp\t*\tLOCATION\tLOCATED\t1-38\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351627.conll/CURATION_USER.tsv \n",
            "Line:  1-88\t409-413\tMack\t*\tPERSON\tPERSONAL - SOCIAL\t1-79\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351651.conll/CURATION_USER.tsv \n",
            "Line:  1-214\t984-986\tMỹ\t*\tLOCATION\tAFFILIATION\t1-215\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351672.conll/CURATION_USER.tsv \n",
            "Line:  1-465\t2123-2130\tChelsea\t*\tORGANIZATION\tAFFILIATION|AFFILIATION|AFFILIATION\t1-462|1-460|1-457[13_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351672.conll/CURATION_USER.tsv \n",
            "Line:  1-465\t2123-2130\tChelsea\t*\tORGANIZATION\tAFFILIATION|AFFILIATION|AFFILIATION\t1-462|1-460|1-457[13_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351841.conll/CURATION_USER.tsv \n",
            "Line:  1-108\t473-479\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED\t1-106|1-101[2_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351856.conll/CURATION_USER.tsv \n",
            "Line:  1-142\t611-616\tSeoul\t*\tLOCATION\tAFFILIATION\t1-121\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351856.conll/CURATION_USER.tsv \n",
            "Line:  1-185\t807-812\tTriều\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351856.conll/CURATION_USER.tsv \n",
            "Line:  1-185\t807-812\tTriều\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351856.conll/CURATION_USER.tsv \n",
            "Line:  1-201\t883-887\tBình\t*[14]\tLOCATION[14]\t*\t1-198[0_14]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351856.conll/CURATION_USER.tsv \n",
            "Line:  1-201\t883-887\tBình\t*[14]\tLOCATION[14]\t*\t1-198[0_14]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351887.conll/CURATION_USER.tsv \n",
            "Line:  1-514\t2332-2337\tASEAN\t*\tORGANIZATION\tAFFILIATION\t1-510\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv \n",
            "Line:  1-624\t2833-2842\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-632\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv \n",
            "Line:  1-648\t2941-2950\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-646\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv \n",
            "Line:  1-669\t3040-3043\tPVN\t*\tORGANIZATION\tAFFILIATION\t1-671\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv \n",
            "Line:  1-698\t3170-3179\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-692\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv \n",
            "Line:  1-739\t3347-3350\tPVN\t*\tORGANIZATION\tAFFILIATION\t1-734\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv \n",
            "Line:  1-836\t3779-3788\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-830\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv \n",
            "Line:  1-844\t3817-3820\tPVN\t*\tORGANIZATION\tAFFILIATION\t1-830\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351945.conll/CURATION_USER.tsv \n",
            "Line:  1-67\t310-316\tTP.HCM\t*\tLOCATION\tAFFILIATION\t1-58\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351945.conll/CURATION_USER.tsv \n",
            "Line:  1-101\t465-471\tPrague\t*\tLOCATION\tLOCATED\t1-91\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23351945.conll/CURATION_USER.tsv \n",
            "Line:  1-466.1\t2183-2188\tChris\t*\tPERSON\t_\t_\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351951.conll/CURATION_USER.tsv \n",
            "Line:  1-49\t263-268\tGabon\t*\tLOCATION\tAFFILIATION\t1-55\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351965.conll/CURATION_USER.tsv \n",
            "Line:  1-320\t1419-1427\tBrussels\t*\tLOCATION\tPART – WHOLE\t1-321\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351970.conll/CURATION_USER.tsv \n",
            "Line:  1-118\t537-542\tASEAN\t*\tORGANIZATION\tAFFILIATION|PART – WHOLE\t1-102|1-99[8_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351978.conll/CURATION_USER.tsv \n",
            "Line:  1-406\t1763-1767\tThảo\t*\tPERSON\tPERSONAL - SOCIAL\t1-404\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351981.conll/CURATION_USER.tsv \n",
            "Line:  1-168\t782-784\tMỹ\t*\tLOCATION\tAFFILIATION\t1-145\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23351984.conll/CURATION_USER.tsv \n",
            "Line:  1-610.1\t2837-2839\tÚc\t*\tLOCATION\tLOCATED\t1-599\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351984.conll/CURATION_USER.tsv \n",
            "Line:  1-610.1\t2837-2839\tÚc\t*\tLOCATION\tLOCATED\t1-599\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351985.conll/CURATION_USER.tsv \n",
            "Line:  1-331\t1448-1451\tMai\t*\tPERSON\tPERSONAL - SOCIAL\t1-323\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351988.conll/CURATION_USER.tsv \n",
            "Line:  1-309\t1340-1344\tThảo\t*\tPERSON\tPERSONAL - SOCIAL\t1-297\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351992.conll/CURATION_USER.tsv \n",
            "Line:  1-296\t1313-1317\tNHNN\t*\tORGANIZATION\tPART – WHOLE\t1-288\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351992.conll/CURATION_USER.tsv \n",
            "Line:  1-471\t2111-2120\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-479\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351992.conll/CURATION_USER.tsv \n",
            "Line:  1-542\t2427-2436\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-538\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351992.conll/CURATION_USER.tsv \n",
            "Line:  1-548\t2455-2458\tPVN\t*\tORGANIZATION\tAFFILIATION\t1-538\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351994.conll/CURATION_USER.tsv \n",
            "Line:  1-393\t1708-1713\tTISCO\t*\tORGANIZATION\tAFFILIATION\t1-391\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352662.conll/CURATION_USER.tsv \n",
            "Line:  1-432\t1954-1959\tBarca\t*\tORGANIZATION\tAFFILIATION\t1-430\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352671.conll/CURATION_USER.tsv \n",
            "Line:  1-384\t1818-1821\tUAE\t*\tLOCATION\tPART – WHOLE\t1-374\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352690.conll/CURATION_USER.tsv \n",
            "Line:  1-112\t515-520\tSyria\t*\tLOCATION\tLOCATED\t1-111\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352690.conll/CURATION_USER.tsv \n",
            "Line:  1-807\t3604-3609\tSyria\t*\tLOCATION\tLOCATED|LOCATED\t1-805|1-803\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352690.conll/CURATION_USER.tsv \n",
            "Line:  1-807\t3604-3609\tSyria\t*\tLOCATION\tLOCATED|LOCATED\t1-805|1-803\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352696.conll/CURATION_USER.tsv \n",
            "Line:  1-129\t592-600\tAtletico\t*\tORGANIZATION\tAFFILIATION\t1-137\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352696.conll/CURATION_USER.tsv \n",
            "Line:  1-480\t2138-2146\tValencia\t*\tORGANIZATION\tAFFILIATION\t1-495\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352696.conll/CURATION_USER.tsv \n",
            "Line:  1-541\t2424-2429\tBarca\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-535|1-533\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352696.conll/CURATION_USER.tsv \n",
            "Line:  1-541\t2424-2429\tBarca\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-535|1-533\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352696.conll/CURATION_USER.tsv \n",
            "Line:  1-547\t2449-2456\tSevilla\t*\tORGANIZATION\tAFFILIATION\t1-543\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352696.conll/CURATION_USER.tsv \n",
            "Line:  1-549\t2459-2467\tValencia\t*\tORGANIZATION\tAFFILIATION\t1-552\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352701.conll/CURATION_USER.tsv \n",
            "Line:  1-162\t744-754\t(Indonesia\t*\tLOCATION\tPART – WHOLE\t1-161\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352701.conll/CURATION_USER.tsv \n",
            "Line:  1-329\t1519-1523\tHAGL\t*\tORGANIZATION\tAFFILIATION\t1-324\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352702.conll/CURATION_USER.tsv \n",
            "Line:  1-197\t928-931\tVân\t*\tPERSON\tPERSONAL - SOCIAL\t1-193\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352702.conll/CURATION_USER.tsv \n",
            "Line:  1-252\t1171-1174\tLâm\t*\tPERSON\tPERSONAL - SOCIAL\t1-250\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352719.conll/CURATION_USER.tsv \n",
            "Line:  1-585\t2580-2586\tDalton\t*\tPERSON\tPERSONAL - SOCIAL\t1-582\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352730.conll/CURATION_USER.tsv \n",
            "Line:  1-129\t563-565\tÁo\t*\tLOCATION\tPART – WHOLE\t1-127\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352730.conll/CURATION_USER.tsv \n",
            "Line:  1-164\t715-722\tNamibia\t*\tLOCATION\tPART – WHOLE\t1-162\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352730.conll/CURATION_USER.tsv \n",
            "Line:  1-333\t1491-1503\tPennsylvania\t*\tLOCATION\tPART – WHOLE\t1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352730.conll/CURATION_USER.tsv \n",
            "Line:  1-335\t1506-1508\tMỹ\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-333|1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352730.conll/CURATION_USER.tsv \n",
            "Line:  1-335\t1506-1508\tMỹ\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-333|1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352730.conll/CURATION_USER.tsv \n",
            "Line:  1-509\t2334-2338\tPháp\t*\tLOCATION\tPART – WHOLE\t1-507\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352738.conll/CURATION_USER (1).tsv \n",
            "Line:  1-222\t1062-1070\tDamascus\t*\tLOCATION\tLOCATED|LOCATED\t1-238|1-236[3_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352738.conll/CURATION_USER (1).tsv \n",
            "Line:  1-547\t2598-2606\tDamascus\t*\tLOCATION\tLOCATED|LOCATED\t1-561[7_0]|1-563\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-545\t2518-2521\tAnh\t*\tLOCATION\tLOCATED\t1-547\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-680\t3150-3159\tBarcelona\t*\tORGANIZATION\tAFFILIATION\t1-673\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-686\t3181-3190\tLiverpool\t*\tORGANIZATION\tAFFILIATION\t1-673\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-688\t3193-3201\tValencia\t*\tORGANIZATION\tAFFILIATION\t1-673\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-772\t3607-3622\tgồm Estudiantes\t*\tORGANIZATION\tAFFILIATION\t1-756\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-774\t3625-3638\tIndependiente\t*\tORGANIZATION\tAFFILIATION\t1-756\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-775\t3639-3648\tvà Alavés\t*\tORGANIZATION\tAFFILIATION\t1-756\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-838\t3945-3948\tAnh\t*\tLOCATION\tLOCATED\t1-822\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352751.conll/CURATION_USER (1).tsv \n",
            "Line:  1-173\t778-787\tCampuchia\t*\tLOCATION\tPART – WHOLE\t1-181\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352753.conll/CURATION_USER (1).tsv \n",
            "Line:  1-815\t3578-3581\tPVN\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-822|1-824\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352753.conll/CURATION_USER (1).tsv \n",
            "Line:  1-815\t3578-3581\tPVN\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-822|1-824\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352753.conll/CURATION_USER (1).tsv \n",
            "Line:  1-884\t3867-3870\tPVN\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE\t1-885|1-887|1-889\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352753.conll/CURATION_USER (1).tsv \n",
            "Line:  1-884\t3867-3870\tPVN\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE\t1-885|1-887|1-889\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352753.conll/CURATION_USER (1).tsv \n",
            "Line:  1-884\t3867-3870\tPVN\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE\t1-885|1-887|1-889\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352753.conll/CURATION_USER (1).tsv \n",
            "Line:  1-1500\t6612-6617\tTISCO\t*\tORGANIZATION\tAFFILIATION\t1-1489\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352769.conll/CURATION_USER (1).tsv \n",
            "Line:  1-420\t1842-1846\t(TTI\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-385[23_0]|1-393\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352800.conll/CURATION_USER (1).tsv \n",
            "Line:  1-113\t527-529\tMỹ\t*\tLOCATION\tPART – WHOLE\t1-94\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352802.conll/CURATION_USER (1).tsv \n",
            "Line:  1-261\t1128-1131\tAnh\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-271|1-276\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352802.conll/CURATION_USER (1).tsv \n",
            "Line:  1-261\t1128-1131\tAnh\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-271|1-276\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352804.conll/CURATION_USER (1).tsv \n",
            "Line:  1-116\t538-545\tKashmir\t*\tLOCATION\tPART – WHOLE\t1-113\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352804.conll/CURATION_USER (1).tsv \n",
            "Line:  1-240\t1104-1112\tPakistan\t*\tLOCATION\tPART – WHOLE\t1-230\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352814.conll/CURATION_USER (1).tsv \n",
            "Line:  1-603\t2705-2707\tMỹ\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-623|1-625\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352814.conll/CURATION_USER (1).tsv \n",
            "Line:  1-603\t2705-2707\tMỹ\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-623|1-625\t\n",
            "\n",
            "4 COLUMNS.\n",
            "DOC:  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv \n",
            "Line:  1-12\t53-61\t(<ENAMEX\t_\t_\t*\t_\t_\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv \n",
            "Line:  1-13.1\t82-85\tHội\t*[2]\tORGANIZATION[2]\t_\tAFFILIATION\t1-11[0_2]\t\n",
            "\n",
            "ENTITY NAME MISTAKE IN 5TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv \n",
            "Line:  1-23\t126-136\t</ENAMEX>)\t*\t*\t_\t_\t_\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv \n",
            "Line:  1-23.1\t126-135\t</ENAMEX>\t*\t*\t_\t_\t_\t\n",
            "\n",
            "ENTITY NAME MISTAKE IN 5TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv \n",
            "Line:  1-23.1\t126-135\t</ENAMEX>\t*\t*\t_\t_\t_\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv \n",
            "Line:  1-71.1\t377-380\tHội\t*[7]\tORGANIZATION[7]\t_\t_\t_\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv \n",
            "Line:  1-77\t404-407\tđại\t*[8]\tORGANIZATION[8]\t_\tPART – WHOLE\t1-71.1[7_8]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352825.conll/CURATION_USER (1).tsv \n",
            "Line:  1-7\t25-31\tLondon\t*\tLOCATION\tLOCATED\t1-1\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352825.conll/CURATION_USER (1).tsv \n",
            "Line:  1-138\t608-614\tLondon\t*\tLOCATION\tLOCATED\t1-136\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352825.conll/CURATION_USER (1).tsv \n",
            "Line:  1-238\t1041-1043\tMỹ\t*\tLOCATION\tLOCATED\t1-204\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352849.conll/CURATION_USER (1).tsv \n",
            "Line:  1-87\t403-414\tal-Bavitieh\t*\tLOCATION\tAFFILIATION\t1-79\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352849.conll/CURATION_USER (1).tsv \n",
            "Line:  1-181\t845-852\tAl-Hawi\t*\tLOCATION\tAFFILIATION\t1-168\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352849.conll/CURATION_USER (1).tsv \n",
            "Line:  1-183\t856-863\tAl-Hamd\t*\tLOCATION\tAFFILIATION\t1-168\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352857.conll/CURATION_USER (1).tsv \n",
            "Line:  1-77\t353-360\tL'Oréal\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-68|1-83[2_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352857.conll/CURATION_USER (1).tsv \n",
            "Line:  1-435\t2054-2061\tL'Oréal\t*\tORGANIZATION\tAFFILIATION\t1-426\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352857.conll/CURATION_USER (1).tsv \n",
            "Line:  1-449\t2123-2129\tNestlé\t*\tORGANIZATION\tAFFILIATION\t1-445\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352857.conll/CURATION_USER (1).tsv \n",
            "Line:  1-469\t2210-2216\tNestlé\t*\tORGANIZATION\tAFFILIATION\t1-445\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352857.conll/CURATION_USER (1).tsv \n",
            "Line:  1-498\t2338-2345\tL'Oréal\t*\tORGANIZATION\tAFFILIATION\t1-490\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352892.conll/CURATION_USER (1).tsv \n",
            "Line:  1-51\t240-243\tNga\t*\tLOCATION\tPART – WHOLE\t1-45\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352899.conll/CURATION_USER (1).tsv \n",
            "Line:  1-440\t1993-2000\tFrancia\t*\tPERSON\tPERSONAL - SOCIAL\t1-428\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352899.conll/CURATION_USER (1).tsv \n",
            "Line:  1-508\t2311-2317\tSelena\t*\tPERSON\tPERSONAL - SOCIAL\t1-498\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353785.conll/CURATION_USER.tsv \n",
            "Line:  1-191\t859-864\tLibya\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-187[5_0]|1-183\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353786.conll/CURATION_USER.tsv \n",
            "Line:  1-42\t201-206\t(Pháp\t*\tLOCATION\tPART – WHOLE\t1-41\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353786.conll/CURATION_USER.tsv \n",
            "Line:  1-71\t344-348\tPháp\t*\tLOCATION\tLOCATED\t1-72\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353864.conll/CURATION_USER.tsv \n",
            "Line:  1-285\t1257-1264\tTimothy\t*\tPERSON\tPERSONAL - SOCIAL\t1-278\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353864.conll/CURATION_USER.tsv \n",
            "Line:  1-287\t1268-1275\tSolomon\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-285|1-278\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353864.conll/CURATION_USER.tsv \n",
            "Line:  1-287\t1268-1275\tSolomon\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-285|1-278\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353864.conll/CURATION_USER.tsv \n",
            "Line:  1-317\t1412-1419\tTimothy\t*\tPERSON\tPERSONAL - SOCIAL\t1-306\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353867.conll/CURATION_USER.tsv \n",
            "Line:  1-592\t2632-2635\tMai\t*\tPERSON\tPERSONAL - SOCIAL\t1-582\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353867.conll/CURATION_USER.tsv \n",
            "Line:  1-601\t2667-2670\tSơn\t*\tPERSON\tPERSONAL - SOCIAL\t1-582\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353874.conll/CURATION_USER.tsv \n",
            "Line:  1-29\t132-138\tTP.HCM\t*\tLOCATION\tLOCATED|LOCATED\t1-32|1-24[1_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353874.conll/CURATION_USER.tsv \n",
            "Line:  1-41\t183-192\tSingapore\t*\tLOCATION\tLOCATED\t1-32\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353874.conll/CURATION_USER.tsv \n",
            "Line:  1-142\t640-649\tSingapore\t*\tLOCATION\tLOCATED\t1-128\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353891.conll/CURATION_USER.tsv \n",
            "Line:  1-72\t319-323\t(Anh\t*\tLOCATION\tLOCATED\t1-71\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353891.conll/CURATION_USER.tsv \n",
            "Line:  1-391\t1786-1793\t(Italia\t*\tLOCATION\tPART – WHOLE\t1-390\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-213\t969-972\tAnh\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-215\t975-981\tAi-len\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-220\t995-1001\tCanada\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-225\t1014-1016\tBỉ\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-230\t1036-1046\t(Mauritius\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-232\t1050-1052\tÚc\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-234\t1055-1057\tÁo\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-239\t1069-1077\tMê-hi-cô\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-241\t1080-1086\tMa-rốc\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353931.conll/CURATION_USER.tsv \n",
            "Line:  1-200\t880-884\tVườn\t*[15]\tLOCATION[15]\t*\t1-190[14_15]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353931.conll/CURATION_USER.tsv \n",
            "Line:  1-200\t880-884\tVườn\t*[15]\tLOCATION[15]\t*\t1-190[14_15]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353954.conll/CURATION_USER.tsv \n",
            "Line:  1-617\t2791-2796\tAnbar\t*\tLOCATION\tPART – WHOLE|LOCATED|LOCATED|LOCATED\t1-615[28_0]|1-631[29_0]|1-635|1-635\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353954.conll/CURATION_USER.tsv \n",
            "Line:  1-617\t2791-2796\tAnbar\t*\tLOCATION\tPART – WHOLE|LOCATED|LOCATED|LOCATED\t1-615[28_0]|1-631[29_0]|1-635|1-635\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353973.conll/CURATION_USER.tsv \n",
            "Line:  1-192\t882-884\tMỹ\t*\tLOCATION\tAFFILIATION\t1-193\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353976.conll/CURATION_USER.tsv \n",
            "Line:  1-93\t389-392\t(Mỹ\t*\tLOCATION\tLOCATED\t1-92\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353976.conll/CURATION_USER.tsv \n",
            "Line:  1-1012\t4532-4540\t(Lebanon\t*\tLOCATION\tLOCATED\t1-1011\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354055.conll/CURATION_USER.tsv \n",
            "Line:  1-57\t238-242\tHằng\t*\tPERSON\tPERSONAL - SOCIAL\t1-33\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354055.conll/CURATION_USER.tsv \n",
            "Line:  1-151\t640-644\tNgọc\t*\tPERSON\tPERSONAL - SOCIAL\t1-156\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354285.conll/CURATION_USER.tsv \n",
            "Line:  1-1263\t5606-5609\tAnh\t*\tLOCATION\tLOCATED|PART – WHOLE\t1-1256[13_0]|1-1261\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354285.conll/CURATION_USER.tsv \n",
            "Line:  1-1598\t7094-7096\tÚc\t*\tLOCATION\tLOCATED|PART – WHOLE\t1-1594[16_0]|1-1596\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354285.conll/CURATION_USER.tsv \n",
            "Line:  1-1833\t8159-8169\t(Cambridge\t*\tLOCATION\tLOCATED\t1-1832\t\n",
            "\n",
            "ENTITY NAME MISTAKE IN 5TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354320.conll/CURATION_USER.tsv \n",
            "Line:  1-547\t2478-2482\tMông\t*\t*\t_\t_\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354336.conll/CURATION_USER.tsv \n",
            "Line:  1-305\t1317-1323\tMexico\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-299[16_0]|1-303\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354336.conll/CURATION_USER.tsv \n",
            "Line:  1-333\t1441-1448\tIceland\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-327[17_0]|1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354400.conll/CURATION_USER.tsv \n",
            "Line:  1-30\t145-154\tSingapore\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-27[3_0]|1-17|1-19[1_0]|1-23[2_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354400.conll/CURATION_USER.tsv \n",
            "Line:  1-194\t934-943\tSingapore\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-181|1-183[7_0]|1-187[8_0]|1-191[9_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354619.conll/CURATION_USER.tsv \n",
            "Line:  1-351\t1663-1668\t(Pháp\t*\tLOCATION\tPART – WHOLE\t1-350\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354619.conll/CURATION_USER.tsv \n",
            "Line:  1-424\t2018-2023\t(Pháp\t*\tLOCATION\tPART – WHOLE\t1-423\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354619.conll/CURATION_USER.tsv \n",
            "Line:  1-1147\t5372-5375\tĐức\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-1138|1-1136[13_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354619.conll/CURATION_USER.tsv \n",
            "Line:  1-1243\t5820-5823\tĐức\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-1234|1-1232[14_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354880.conll/CURATION_USER.tsv \n",
            "Line:  1-1376\t6085-6088\t(Bỉ\t*\tLOCATION\tPART – WHOLE\t1-1375\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354880.conll/CURATION_USER.tsv \n",
            "Line:  1-1472\t6506-6510\tViệt\t*[118]\tLOCATION[118]\tPART – WHOLE|*\t1-1479[119_118]|1-1482[120_118]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354982.conll/CURATION_USER.tsv \n",
            "Line:  1-113\t513-519\tOregon\t*\tLOCATION\tPART – WHOLE|LOCATED\t1-111|1-106[5_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354982.conll/CURATION_USER.tsv \n",
            "Line:  1-115\t522-524\tMỹ\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|LOCATED\t1-113|1-111|1-106[5_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354982.conll/CURATION_USER.tsv \n",
            "Line:  1-115\t522-524\tMỹ\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|LOCATED\t1-113|1-111|1-106[5_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-78\t366-372\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED\t1-76|1-69[7_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-101\t477-480\tQ.7\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE\t1-98[11_0]|1-92[10_0]|1-96\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-103\t483-489\tTP.HCM\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE\t1-101|1-92[10_0]|1-96\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-103\t483-489\tTP.HCM\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE\t1-101|1-92[10_0]|1-96\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-111\t522-525\tQ.3\t*\tLOCATION\tPART – WHOLE\t1-109\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-113\t528-534\tTP.HCM\t*\tLOCATION\t*|PART – WHOLE\t1-111|1-109\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-113\t528-534\tTP.HCM\t*\tLOCATION\t*|PART – WHOLE\t1-111|1-109\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-121\t567-573\tTP.HCM\t*\tLOCATION\tPART – WHOLE\t1-119\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-136\t633-639\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED\t1-134|1-126[12_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-150\t703-706\tQ.3\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE\t1-148|1-140[14_0]|1-143[15_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-152\t709-715\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE|PART – WHOLE\t1-150|1-140[14_0]|1-143[15_0]|1-148\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-152\t709-715\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE|PART – WHOLE\t1-150|1-140[14_0]|1-143[15_0]|1-148\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-166\t780-786\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE\t1-164|1-156[16_0]|1-159[17_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-177\t831-838\t(TP.HCM\t*\tLOCATION\tLOCATED|LOCATED|LOCATED\t1-176|1-172|1-174\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-177\t831-838\t(TP.HCM\t*\tLOCATION\tLOCATED|LOCATED|LOCATED\t1-176|1-172|1-174\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-177\t831-838\t(TP.HCM\t*\tLOCATION\tLOCATED|LOCATED|LOCATED\t1-176|1-172|1-174\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-186\t879-885\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED\t1-184|1-181[18_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-196\t928-934\tTP.HCM\t*\tLOCATION\tLOCATED|PART – WHOLE\t1-191[19_0]|1-194\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-206\t981-987\tTP.HCM\t*\tLOCATION\tLOCATED|PART – WHOLE\t1-201[20_0]|1-204\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-218\t1037-1043\tTP.HCM\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|LOCATED\t1-216|1-213[22_0]|1-210[21_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-226\t1077-1083\tTP.HCM\t*\tLOCATION\tPART – WHOLE\t1-224\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-242\t1159-1165\tTP.HCM\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE\t1-239[24_0]|1-232[23_0]|1-237\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-255\t1220-1223\tQ.3\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-253|1-248[25_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-257\t1226-1232\tTP.HCM\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE\t1-255|1-248[25_0]|1-253\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-257\t1226-1232\tTP.HCM\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE\t1-255|1-248[25_0]|1-253\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-273\t1302-1308\tTP.HCM\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE\t1-270[27_0]|1-263[26_0]|1-268\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-288\t1377-1383\tTP.HCM\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|LOCATED\t1-286|1-281[29_0]|1-277[28_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-302\t1442-1445\tQ.3\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE\t1-300|1-292[30_0]|1-295[31_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-304\t1448-1454\tTP.HCM\t*\tLOCATION\tLOCATED|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-292[30_0]|1-295[31_0]|1-300|1-302\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-304\t1448-1454\tTP.HCM\t*\tLOCATION\tLOCATED|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-292[30_0]|1-295[31_0]|1-300|1-302\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-317\t1509-1512\tQ.6\t*\tLOCATION\tLOCATED|PART – WHOLE|PART – WHOLE\t1-308[32_0]|1-311[33_0]|1-315\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-319\t1515-1521\tTP.HCM\t*\tLOCATION\tLOCATED|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-308[32_0]|1-311[33_0]|1-315|1-317\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-319\t1515-1521\tTP.HCM\t*\tLOCATION\tLOCATED|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-308[32_0]|1-311[33_0]|1-315|1-317\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-356\t1701-1707\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED\t1-354|1-350[40_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-364\t1740-1746\tTP.HCM\t*\tLOCATION\tPART – WHOLE\t1-362\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-380\t1820-1826\tTP.HCM\t*\tLOCATION\tLOCATED|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-368[41_0]|1-377[43_0]|1-371[42_0]|1-375\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-398\t1913-1919\tTP.HCM\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE|LOCATED\t1-395[46_0]|1-388[45_0]|1-393|1-384[44_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-409\t1969-1972\tQ.1\t*\tLOCATION\tPART – WHOLE|LOCATED\t1-404[47_0]|1-403\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-411\t1975-1981\tTP.HCM\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|LOCATED\t1-409|1-404[47_0]|1-403\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-411\t1975-1981\tTP.HCM\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|LOCATED\t1-409|1-404[47_0]|1-403\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-424\t2041-2047\tTP.HCM\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-422|1-417[48_0]\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-457\t2210-2214\ttỉnh\t*[54]\tLOCATION[54]\tLOCATED|*|PART – WHOLE|PART – WHOLE\t1-446[51_54]|1-454[53_54]|1-449[52_54]|1-449[52_54]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-473\t2288-2291\tQ.3\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|LOCATED\t1-471|1-466[56_0]|1-463[55_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-475\t2294-2300\tTP.HCM\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE|LOCATED\t1-473|1-466[56_0]|1-471|1-463[55_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-475\t2294-2300\tTP.HCM\t*\tLOCATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE|LOCATED\t1-473|1-466[56_0]|1-471|1-463[55_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-483\t2337-2340\tQ.3\t*\tLOCATION\tLOCATED|PART – WHOLE\t1-479[57_0]|1-481\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-485\t2343-2349\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE\t1-483|1-479[57_0]|1-481\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-485\t2343-2349\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE\t1-483|1-479[57_0]|1-481\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-499\t2409-2415\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE\t1-497|1-489[58_0]|1-493[59_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-525\t2534-2540\tTP.HCM\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-523|1-518[62_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-531\t2568-2572\t(P.1\t*\tLOCATION\tLOCATED\t1-530\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-533\t2575-2578\tQ.3\t*\tLOCATION\tLOCATED|PART – WHOLE\t1-530|1-531\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-533\t2575-2578\tQ.3\t*\tLOCATION\tLOCATED|PART – WHOLE\t1-530|1-531\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-535\t2581-2587\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE\t1-533|1-530|1-531\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-535\t2581-2587\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE\t1-533|1-530|1-531\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-535\t2581-2587\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE\t1-533|1-530|1-531\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-552\t2661-2667\tTP.HCM\t*\tLOCATION\tLOCATED|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-539[63_0]|1-542[64_0]|1-547|1-549[65_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-564\t2722-2728\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE\t1-562|1-556[66_0]|1-559[67_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-570\t2755-2759\t(Q.3\t*\tLOCATION\tLOCATED\t1-569\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-572\t2762-2768\tTP.HCM\t*\tLOCATION\tPART – WHOLE|*\t1-570|1-569\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-572\t2762-2768\tTP.HCM\t*\tLOCATION\tPART – WHOLE|*\t1-570|1-569\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-604\t2909-2915\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED\t1-602|1-599[73_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-613\t2955-2958\tQ.3\t*\tLOCATION\tPART – WHOLE|LOCATED\t1-611|1-608[74_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-615\t2961-2967\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE\t1-613|1-608[74_0]|1-611\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-615\t2961-2967\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE\t1-613|1-608[74_0]|1-611\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-954\t4754-4760\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED|PART – WHOLE\t1-952|1-944[122_0]|1-948[123_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355434.conll/CURATION_USER.tsv \n",
            "Line:  1-370\t1724-1728\tPC67\t*\tORGANIZATION\tAFFILIATION\t1-360\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355817.conll/CURATION_USER.tsv \n",
            "Line:  1-92\t445-451\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED\t1-90|1-85[4_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355817.conll/CURATION_USER.tsv \n",
            "Line:  1-339\t1564-1570\tTP.HCM\t*\tLOCATION\tPART – WHOLE|LOCATED\t1-337|1-331[7_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-482\t2151-2156\tASEAN\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-486|1-488|1-490|1-492|1-494|1-496[13_0]|1-499\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-482\t2151-2156\tASEAN\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-486|1-488|1-490|1-492|1-494|1-496[13_0]|1-499\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-482\t2151-2156\tASEAN\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-486|1-488|1-490|1-492|1-494|1-496[13_0]|1-499\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-482\t2151-2156\tASEAN\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-486|1-488|1-490|1-492|1-494|1-496[13_0]|1-499\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-482\t2151-2156\tASEAN\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-486|1-488|1-490|1-492|1-494|1-496[13_0]|1-499\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-482\t2151-2156\tASEAN\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE|PART – WHOLE\t1-486|1-488|1-490|1-492|1-494|1-496[13_0]|1-499\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-869\t3908-3913\tASEAN\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE\t1-871|1-873|1-875[28_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-869\t3908-3913\tASEAN\t*\tORGANIZATION\tPART – WHOLE|PART – WHOLE|PART – WHOLE\t1-871|1-873|1-875[28_0]\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356511.conll/CURATION_USER.tsv \n",
            "Line:  1-22\t92-95\t(Bộ\t*[3]\tORGANIZATION[3]\tPART – WHOLE|*\t1-17[2_3]|1-10[1_3]\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23356574.conll/CURATION_USER.tsv \n",
            "Line:  1-743.1\t3347-3353\tnghiệp\t*[18]\tORGANIZATION[18]\tPART – WHOLE\t1-733[17_18]\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356574.conll/CURATION_USER.tsv \n",
            "Line:  1-744\t3355-3357\tSở\t*[19]\tORGANIZATION[19]\tAFFILIATION|PART – WHOLE\t1-733[17_19]|1-743.1[18_19]\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356771.conll/CURATION_USER.tsv \n",
            "Line:  1-513\t2282-2286\tHiệp\t*[24]\tORGANIZATION[24]\t*\t1-506[23_24]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356771.conll/CURATION_USER.tsv \n",
            "Line:  1-513\t2282-2286\tHiệp\t*[24]\tORGANIZATION[24]\t*\t1-506[23_24]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356858.conll/CURATION_USER.tsv \n",
            "Line:  1-267\t1169-1171\tHà\t*\tPERSON\tPERSONAL - SOCIAL\t1-270\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356858.conll/CURATION_USER.tsv \n",
            "Line:  1-456\t2054-2057\tLơn\t*\tPERSON\tPERSONAL - SOCIAL\t1-454\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356858.conll/CURATION_USER.tsv \n",
            "Line:  1-516\t2308-2312\tLĩnh\t*\tPERSON\tPERSONAL - SOCIAL\t1-483\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356874.conll/CURATION_USER.tsv \n",
            "Line:  1-59\t281-288\t(Trường\t*[6]\tORGANIZATION[6]\tAFFILIATION|*|AFFILIATION|AFFILIATION|AFFILIATION\t1-56[5_6]|1-40[1_6]|1-44[2_6]|1-48[3_6]|1-52[4_6]\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23357000.conll/CURATION_USER.tsv \n",
            "Line:  1-177.1\t827-832\t1+1>2\t*[8]\tORGANIZATION[8]\tAFFILIATION\t1-166[7_8]\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23357000.conll/CURATION_USER.tsv \n",
            "Line:  1-263.1\t1223-1228\t1+1>2\t*[12]\tORGANIZATION[12]\tAFFILIATION\t1-252[11_12]\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23357063.conll/CURATION_USER.tsv \n",
            "Line:  1-101.1\t456-460\t4006\t*[1]\tORGANIZATION[1]\t_\t_\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357063.conll/CURATION_USER.tsv \n",
            "Line:  1-102\t462-465\tHải\t*[2]\tORGANIZATION[2]\tPART – WHOLE\t1-101.1[1_2]\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357063.conll/CURATION_USER.tsv \n",
            "Line:  1-106\t476-480\tVùng\t*[3]\tLOCATION[3]\tPART – WHOLE|PART – WHOLE\t1-102[2_3]|1-101.1[1_3]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357095.conll/CURATION_USER.tsv \n",
            "Line:  1-112\t536-546\tCalifornia\t*\tLOCATION\tLOCATED|PART – WHOLE\t1-103|1-109[3_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357095.conll/CURATION_USER.tsv \n",
            "Line:  1-271\t1275-1283\tTanzania\t*\tLOCATION\tLOCATED\t1-251\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357240.conll/CURATION_USER.tsv \n",
            "Line:  1-166\t745-750\tApple\t*\tORGANIZATION\tPART – WHOLE\t1-195\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357329.conll/CURATION_USER.tsv \n",
            "Line:  1-27\t122-131\tIndonesia\t*\tLOCATION\tPART – WHOLE|PART – WHOLE\t1-25|1-21[1_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357329.conll/CURATION_USER.tsv \n",
            "Line:  1-297\t1370-1380\t(Australia\t*\tLOCATION\tLOCATED\t1-296\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357752.conll/CURATION_USER.tsv \n",
            "Line:  1-458\t2025-2031\tTP.HCM\t*\tLOCATION\tLOCATED\t1-456\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23357779.conll/CURATION_USER.tsv \n",
            "Line:  1-27.1\t123-125\tHà\t*[2]\tLOCATION[2]\tLOCATED\t1-18[1_2]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357809.conll/CURATION_USER.tsv \n",
            "Line:  1-603\t2716-2722\tOregon\t*\tLOCATION\tPART – WHOLE\t1-601\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357809.conll/CURATION_USER.tsv \n",
            "Line:  1-744\t3358-3365\tHarvard\t*\tORGANIZATION\tAFFILIATION\t1-742\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23358104.conll/CURATION_USER.tsv \n",
            "Line:  1-15\t69-72\tMai\t*\tPERSON\tPERSONAL - SOCIAL\t1-13\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23366740.conll/CURATION_USER.tsv \n",
            "Line:  1-503\t2299-2307\tLabrador\t*\tLOCATION\tLOCATED\t1-489\t\n",
            "506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wzBGIIgKYPD"
      },
      "source": [
        "# Fix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruagt4UGbqL5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "41372e03-1537-4741-dcc9-59d1645d094d"
      },
      "source": [
        "\"\"\"\n",
        "raw_data: - is a list (len = 506)\n",
        "          - each row is a dict {}, information from a single doc (506 doc)\n",
        "            - doc_id: number id in name of folder that contain doc\n",
        "            - text: text in line start with \"#Text=\"\n",
        "            - token_ids: list of int. (1st column)\n",
        "            - subtoken_ids: list. (1st column)\n",
        "                            an element can be None if token is not a subtoken: 1-id -> 1.26,\n",
        "                                           or int (subtoken_id) if token is a subtoken: 1-id.subid -> 1.26.1.\n",
        "                                           currently in train data, only exist subtoken id 1.\n",
        "                            (in extract raw data code, my code can get any subid, not just specify subid = 1.\n",
        "                             but i check if in data has other subid, it will return error -> to know more about data)\n",
        "            - pos: list of child list. (2st column)\n",
        "                   each child list has two int elements.\n",
        "                   [start_position, end_position]\n",
        "            - tokens: list of strings. (3th column)\n",
        "            - entity: list.\n",
        "                      an element is: None if crr token is not entity\n",
        "                                     a list with: 2 element if crr token is an entity.\n",
        "                                                  [entity_id, entity_name]\n",
        "                                                  entity_id: int, from 4th column\n",
        "                                                  entity_name: string, from 5th column\n",
        "            - relation: list\n",
        "                        an element is: None if there is no relation in 6ht, 7th column.\n",
        "                                       a list of child list. number of child list is number of relation in 6th, 7th column.\n",
        "                                                 each child list has: 4 elemnt\n",
        "                                                 [relation_name, stoken_id, sstoken_id, direction[sentity_id, eentity_id]]\n",
        "                                                 relation_name: string, from 6h column\n",
        "                                                 stoken_id: int, tokenid from 7th column\n",
        "                                                 sstoken_id: from 7th column\n",
        "                                                             None, if entity_1 is a token\n",
        "                                                             else: int, subid if entity_1 is subtoken\n",
        "                                                 direction: from 7th column\n",
        "                                                            None, if there is a mistake in dataset\n",
        "                                                            else: [entity_1_id, entity_2_id]\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nraw_data: - is a list (len = 506)\\n          - each row is a dict {}, information from a single doc (506 doc)\\n            - doc_id: number id in name of folder that contain doc\\n            - text: text in line start with \"#Text=\"\\n            - token_ids: list of int. (1st column)\\n            - subtoken_ids: list. (1st column)\\n                            an element can be None if token is not a subtoken: 1-id -> 1.26,\\n                                           or int (subtoken_id) if token is a subtoken: 1-id.subid -> 1.26.1.\\n                                           currently in train data, only exist subtoken id 1.\\n                            (in extract raw data code, my code can get any subid, not just specify subid = 1.\\n                             but i check if in data has other subid, it will return error -> to know more about data)\\n            - pos: list of child list. (2st column)\\n                   each child list has two int elements.\\n                   [start_position, end_position]\\n            - tokens: list of strings. (3th column)\\n            - entity: list.\\n                      an element is: None if crr token is not entity\\n                                     a list with: 2 element if crr token is an entity.\\n                                                  [entity_id, entity_name]\\n                                                  entity_id: int, from 4th column\\n                                                  entity_name: string, from 5th column\\n            - relation: list\\n                        an element is: None if there is no relation in 6ht, 7th column.\\n                                       a list of child list. number of child list is number of relation in 6th, 7th column.\\n                                                 each child list has: 4 elemnt\\n                                                 [relation_name, stoken_id, sstoken_id, direction[sentity_id, eentity_id]]\\n                                                 relation_name: string, from 6h column\\n                                                 stoken_id: int, tokenid from 7th column\\n                                                 sstoken_id: from 7th column\\n                                                             None, if entity_1 is a token\\n                                                             else: int, subid if entity_1 is subtoken\\n                                                 direction: from 7th column\\n                                                            None, if there is a mistake in dataset\\n                                                            else: [entity_1_id, entity_2_id]\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsVmEapDfdYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67e07049-347c-451b-f1d3-1de00ea6a6c9"
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "a = \"mình”,\\xa0\"\n",
        "b = unicodedata.normalize('NFKD',a)\n",
        "\n",
        "print(repr(a))\n",
        "print(repr(b))\n",
        "\n",
        "space_count = 0\n",
        "for s in a[::-1]:\n",
        "    \n",
        "    if s == ' ':\n",
        "        space_count += 1\n",
        "    if s != ' ':\n",
        "        break\n",
        "                \n",
        "a = a.rstrip()\n",
        "\n",
        "\n",
        "print(space_count)\n",
        "print(repr(a))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'mình”,\\xa0'\n",
            "'mình”, '\n",
            "0\n",
            "'mình”,'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYMqFdP5eTPL"
      },
      "source": [
        "## Fix1: Subtoken to token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdwVXKOHs1Ql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d44e2dc5-28ee-4388-8b8b-2df52ce6cd4c"
      },
      "source": [
        "# Trong data có subtoken. Mặc dù hiện tại trong train data 1 token nếu có subtoken thì chỉ có 1 subtoken \n",
        "# và subtoken này thường là entity nên mới cần tách ra. ngoài ra subtoken thường nằm ở cuối hoặc đầu token nên ta có dễ dàng tách ra được.\n",
        "# nên để giữ ngữ nghĩa tốt nhất cho câu thì có thể chia token thành 2 token: subtoken và phần còn lại từ token gốc\n",
        "\n",
        "# Tuy nhiên, nếu trong tương lai, với bộ dev và test,\n",
        "# - nếu có nhiều hơn 1 subtoken và các subtoken k bị overlap thì cũng chia như trên\n",
        "# - còn nếu xảy ra hiện tượng overlap (subtoken bị đè lên nhau) giữa các subtoken\n",
        "# thì lúc này ta không thể tách token ra được nữa\n",
        "# lúc này ta sẽ chọn cách xử lý sẽ là không tách token gốc ra nữa. chèn thêm các subtoken vào câu và coi chúng như 1 token bình thường\n",
        "\n",
        "\n",
        "# ở đây do có 1 subtoken nên chọn cách tách token gốc ra cho đơn giản và giữ được ngữ nghĩa tốt nhất.\n",
        "# đoạn code bên dưới chỉ cho trường hợp 1 subtoken\n",
        "# trường hợp nhiều subtoken nhưng không overlap thì ý tưởng cũng tương tự, \n",
        "# nhưng cần chú ý gom các subtoken của 1 token lại và chọn thứ tự xử lý\n",
        "# và chú ý tới có dấu bằng hay không khi so sánh pos\n",
        "\n",
        "import copy\n",
        "\n",
        "raw_tdata_new = copy.deepcopy(raw_tdata)\n",
        "\n",
        "for docif in raw_tdata_new:\n",
        "    for i in range(len(docif['subtoken_ids'])):\n",
        "        \n",
        "        if docif['subtoken_ids'][i] != None:\n",
        "            \n",
        "            print('\\n\\n-----Doc: ', docif['doc_id'])\n",
        "            print('Before: ')\n",
        "            for key in docif:\n",
        "                if key not in ['doc_id', 'text']:\n",
        "                    print(docif[key][i-1], end='\\t')\n",
        "            print('', end='\\n')\n",
        "            for key in docif:\n",
        "                if key not in ['doc_id', 'text']:\n",
        "                    print(docif[key][i], end='\\t')\n",
        "\n",
        "\n",
        "\n",
        "            #print(docif['token_ids'][i-1], '\\t', docif['pos'][i-1][0], '-', docif['pos'][i-1][1], '\\t', docif['tokens'][i-1])\n",
        "            #print(docif['token_ids'][i] , '.', docif['subtoken_ids'], '\\t', docif['pos'][i][0], '-', docif['pos'][i][1], '\\t', docif['tokens'][i])\n",
        "\n",
        "            # subtoken là một đoạn đầu của token\n",
        "            # 1-200\t    902-905\tmôi\t        *[10]\tORGANIZATION[10]\t_\t_\t\n",
        "            # 1-201\t    906-913\ttrường,\t     _     _\t                _\t_\t        <--- i-1\n",
        "            # 1-201.1\t906-912\ttrường  \t*[10]\tORGANIZATION[10]\t_\t_           <--- i\n",
        "\n",
        "            if (docif['pos'][i][0] == docif['pos'][i-1][0]) and (docif['pos'][i][1] < docif['pos'][i-1][1]):\n",
        "                \n",
        "                # đổi chỗ 2 dòng, dòng subtoken lên trên, dòng token gốc xuống dưới\n",
        "                # i đang là dòng chứa subtoken sẽ thành dòng chứa token\n",
        "                # i-1 đang là dòng chứa token sẽ thành dòng chứa subtoken\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        tmp = copy.deepcopy(docif[key][i])\n",
        "                        docif[key][i] = copy.deepcopy(docif[key][i-1]) \n",
        "                        docif[key][i-1] = copy.deepcopy(tmp)\n",
        "\n",
        "                assert (docif['subtoken_ids'][i-1] != None), str(\"Swap failed.\")\n",
        "\n",
        "                ## xóa bỏ subtoken ids\n",
        "                docif['subtoken_ids'][i-1] = None\n",
        "\n",
        "                ## thay đổi tokens, bỏ phần subtoken đã tách ra\n",
        "                docif['tokens'][i] = docif['tokens'][i].replace(docif['tokens'][i-1], '')\n",
        "\n",
        "                # tuy nhiên, có trường hợp sau khi bỏ phần subtoken đi bị thừa dấu cách, nên ta cần xử lý\n",
        "                # do subtoken nằm ở đầu token, nên ta chỉ đếm dấu cách ở bên trái phần còn lại thôi\n",
        "                space_count = 0\n",
        "                for s in docif['tokens'][i]:\n",
        "                    if s in [' ', '\\xa0']:\n",
        "                        space_count += 1\n",
        "                    else:\n",
        "                        break\n",
        "                \n",
        "\n",
        "                docif['tokens'][i] = docif['tokens'][i].lstrip()\n",
        "\n",
        "\n",
        "                ## thay đổi pos\n",
        "                docif['pos'][i] = [(docif['pos'][i-1][1] + space_count), docif['pos'][i][1]]\n",
        "                \n",
        "                \n",
        "\n",
        "                assert ((docif['text'][docif['pos'][i][0]:docif['pos'][i][1]]) == docif['tokens'][i]), \\\n",
        "                str('\\nWrong postions \\npos' + str(docif['pos'][i][0]) + '-' + str(docif['pos'][i][1]) + '  token: ' + str(docif['tokens'][i]))\n",
        "\n",
        "                # thay đổi token_ids của toàn bộ phần dưới, nếu có subid trong relation ở đâu thì thay đổi, thay đổi stoken_id trong relation\n",
        "                # trường hợp này stoken_id sẽ không đổi nên không cần thay đổi\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if j >= i:\n",
        "                        docif['token_ids'][j] += 1\n",
        "\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][1] == docif['token_ids'][i-1]:   # tìm xem có relation nào trỏ tới subtoken trước kia không\n",
        "                                docif['relation'][j][k][2] = None   # nếu có thì thay bằng None\n",
        "\n",
        "                            # do ở trên, toàn bộ token_ids phía sau (>= i) sẽ bị thay đổi (cộng thêm 1)\n",
        "                            # nên những relation có stoken_id nằm ở phần phía sau này cũng sẽ cần thay đổi theo (cộng thêm 1)\n",
        "                            elif (docif['relation'][j][k][1] > docif['token_ids'][i-1]):\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1\n",
        "\n",
        "\n",
        "                '''\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][2] != None:\n",
        "                                assert False, str('Failed to replace subid in relation')\n",
        "                '''\n",
        "                            \n",
        "\n",
        "                print('\\n\\nAfter:')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i-1], end='\\t')\n",
        "                print('', end='\\n')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i], end='\\t')\n",
        "\n",
        "\n",
        "            # subtoken là một đoạn cuối của token\n",
        "            # 1-583\t    2567-2573\tmẹ-Tập\t _\t    _\t                _\t_\t<--- i-1\n",
        "            # 1-583.1\t2570-2573\tTập\t    *[19]\tORGANIZATION[19]\t_\t_\t<--- i\n",
        "\n",
        "            elif (docif['pos'][i][1] == docif['pos'][i-1][1]) and (docif['pos'][i][0] > docif['pos'][i-1][0]):\n",
        "                ## thay đổi subid\n",
        "                docif['subtoken_ids'][i] = None\n",
        "\n",
        "                ## thay đổi tokens\n",
        "                docif['tokens'][i-1] = docif['tokens'][i-1].replace(docif['tokens'][i], '')\n",
        "\n",
        "                print(repr(docif['tokens'][i-1]))\n",
        "                # tuy nhiên, có trường hợp sau khi bỏ phần subtoken đi bị thừa dấu cách, nên ta cần xử lý\n",
        "                # do subtoken nằm ở cuối token, nên ta chỉ đếm dấu cách ở bên phải phần còn lại thôi\n",
        "                space_count = 0\n",
        "                for s in docif['tokens'][i-1][::-1]:\n",
        "                    if s in [' ', '\\xa0']:\n",
        "                        space_count += 1\n",
        "                    else:\n",
        "                        break\n",
        "                \n",
        "                \n",
        "                docif['tokens'][i-1] = docif['tokens'][i-1].rstrip()\n",
        "\n",
        "\n",
        "                ## thay đổi pos\n",
        "                docif['pos'][i-1] = [docif['pos'][i-1][0], (docif['pos'][i][0] - space_count)]\n",
        "\n",
        "                \n",
        "\n",
        "                assert ((docif['text'][docif['pos'][i-1][0]:docif['pos'][i-1][1]]) == docif['tokens'][i-1]), \\\n",
        "                str('\\nWrong postions \\npos ' + str(docif['pos'][i-1][0]) + '-' + str(docif['pos'][i-1][1]) + '  token: ' + str(docif['tokens'][i-1]))\n",
        "\n",
        "                # thay đổi token_ids của toàn bộ phần dưới, nếu có subid trong relation ở đâu thì thay đổi, thay đổi stoken_id trong relation\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if j >= i:\n",
        "                        docif['token_ids'][j] += 1\n",
        "\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][1] == docif['token_ids'][i-1]:   # tìm xem có relation nào trỏ tới subtoken trước kia không\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1   # do token_id thay đổi nên relation có stoken_id này cũng phải thay đổi \n",
        "                                docif['relation'][j][k][2] = None   # nếu có thì thay bằng None\n",
        "\n",
        "                            # do ở trên, toàn bộ token_ids phía sau (>= i) sẽ bị thay đổi (cộng thêm 1)\n",
        "                            # nên những relation có stoken_id nằm ở phần phía sau này cũng sẽ cần thay đổi theo (cộng thêm 1)\n",
        "                            elif (docif['relation'][j][k][1] > docif['token_ids'][i-1]):\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1\n",
        "\n",
        "                '''\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][2] != None:\n",
        "                                assert False, str('Failed to replace subid in relation')\n",
        "                            \n",
        "                            stoken_eid = docif['token_ids'].index(docif['relation'][j][k][1])\n",
        "                            if docif['entity'][stoken_eid] == None:\n",
        "                                print(docif['doc_id'])\n",
        "                                print(docif['relation'][j][k][1])\n",
        "                                print(stoken_eid)\n",
        "                                print(docif['token_ids'][stoken_eid])\n",
        "                                assert False, str('Failed to replace stoken_id in relation')\n",
        "                        \n",
        "                '''\n",
        "                        \n",
        "\n",
        "\n",
        "\n",
        "                print('\\n\\nAfter:')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i-1], end='\\t')\n",
        "                print('', end='\\n')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i], end='\\t')\n",
        "\n",
        "            else:\n",
        "                assert False, str(\"\\nExist subtoken in middle of token.\\nDoc: \" + docif['doc_id'] + \"\\ntoken_id\" + str(docif['token_ids'][i]))\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "\n",
        "\n",
        "# kiểm tra xem code trên có lỗi gì không\n",
        "print('\\n\\nCHECKING')\n",
        "for idoc, docif in enumerate(raw_tdata_new):\n",
        "\n",
        "    relation_lst = []\n",
        "    for i in range(len(raw_tdata[idoc]['relation'])):\n",
        "        if raw_tdata[idoc]['relation'][i] != None:\n",
        "            relation_lst.append(raw_tdata[idoc]['relation'][i])\n",
        "\n",
        "    \n",
        "    relation_ith = 0\n",
        "    for i in range(len(docif['token_ids'])):\n",
        "        \n",
        "        # nếu code chạy đúng thì sẽ không còn subid\n",
        "        if (docif['subtoken_ids'][i] != None):\n",
        "            assert False, str('ERROR CODE 1')\n",
        "\n",
        "        if docif['relation'][i] != None:\n",
        "            for j in range(len(docif['relation'][i])):\n",
        "                # nếu code chạy đúng thì sẽ không còn subid\n",
        "                if docif['relation'][i][j][2] != None:\n",
        "                    assert False, str('ERROR CODE 2')\n",
        "\n",
        "                # so sánh xem thay đổi stoken_id có đúng không\n",
        "                # string của token và pos của token sẽ không đổi so với raw_tdata\n",
        "                stoken_new = docif['relation'][i][j][1]\n",
        "                stoken_new_ele_id = docif['token_ids'].index(stoken_new)\n",
        "                \n",
        "                stoken = relation_lst[relation_ith][j][1]\n",
        "                if relation_lst[relation_ith][j][2] == 1:\n",
        "                    stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken) + 1\n",
        "                else:\n",
        "                    stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken)\n",
        "\n",
        "                if docif['pos'][stoken_new_ele_id] != raw_tdata[idoc]['pos'][stoken_ele_id]:\n",
        "                    '''\n",
        "                    print('\\n-----Doc: ', docif['doc_id'])\n",
        "                    print(relation_ith)\n",
        "                    print(docif['relation'][i])\n",
        "                    print(relation_lst[relation_ith])\n",
        "                    print(docif['pos'][stoken_new_ele_id])\n",
        "                    print(raw_tdata[idoc]['pos'][stoken_ele_id])\n",
        "                    '''\n",
        "                    assert False, str('ERROR CODE 3')\n",
        "                \n",
        "            relation_ith += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('DONE. EVERYTHINGS SEEM TO BE CORRECTED :D')\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "   "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "-----Doc:  23351433\n",
            "Before: \n",
            "583\tNone\t[2567, 2573]\tmẹ-Tập\tNone\tNone\t\n",
            "583\t1\t[2570, 2573]\tTập\t[19, 'ORGANIZATION']\tNone\t'mẹ-'\n",
            "\n",
            "\n",
            "After:\n",
            "583\tNone\t[2567, 2570]\tmẹ-\tNone\tNone\t\n",
            "584\tNone\t[2570, 2573]\tTập\t[19, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "-----Doc:  23351610\n",
            "Before: \n",
            "201\tNone\t[906, 913]\ttrường,\tNone\tNone\t\n",
            "201\t1\t[906, 912]\ttrường\t[10, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "After:\n",
            "201\tNone\t[906, 912]\ttrường\t[10, 'ORGANIZATION']\tNone\t\n",
            "202\tNone\t[912, 913]\t,\tNone\tNone\t\n",
            "\n",
            "-----Doc:  23351945\n",
            "Before: \n",
            "466\tNone\t[2176, 2188]\tmình”, Chris\tNone\tNone\t\n",
            "466\t1\t[2183, 2188]\tChris\t[0, 'PERSON']\tNone\t'mình”,\\xa0'\n",
            "\n",
            "\n",
            "After:\n",
            "466\tNone\t[2176, 2182]\tmình”,\tNone\tNone\t\n",
            "467\tNone\t[2183, 2188]\tChris\t[0, 'PERSON']\tNone\t\n",
            "\n",
            "-----Doc:  23351984\n",
            "Before: \n",
            "610\tNone\t[2832, 2839]\tnhất Úc\tNone\tNone\t\n",
            "610\t1\t[2837, 2839]\tÚc\t[0, 'LOCATION']\t[['LOCATED', 599, None, None]]\t'nhất\\xa0'\n",
            "\n",
            "\n",
            "After:\n",
            "610\tNone\t[2832, 2836]\tnhất\tNone\tNone\t\n",
            "611\tNone\t[2837, 2839]\tÚc\t[0, 'LOCATION']\t[['LOCATED', 599, None, None]]\t\n",
            "\n",
            "-----Doc:  23352816\n",
            "Before: \n",
            "13\tNone\t[62, 85]\tTYPE=\"ORGANIZATION\">Hội\tNone\tNone\t\n",
            "13\t1\t[82, 85]\tHội\t[2, 'ORGANIZATION']\t[['AFFILIATION', 11, None, [0, 2]]]\t'TYPE=\"ORGANIZATION\">'\n",
            "\n",
            "\n",
            "After:\n",
            "13\tNone\t[62, 82]\tTYPE=\"ORGANIZATION\">\tNone\tNone\t\n",
            "14\tNone\t[82, 85]\tHội\t[2, 'ORGANIZATION']\t[['AFFILIATION', 11, None, [0, 2]]]\t\n",
            "\n",
            "-----Doc:  23352816\n",
            "Before: \n",
            "24\tNone\t[126, 136]\t</ENAMEX>)\tNone\tNone\t\n",
            "24\t1\t[126, 135]\t</ENAMEX>\tNone\tNone\t\n",
            "\n",
            "After:\n",
            "24\tNone\t[126, 135]\t</ENAMEX>\tNone\tNone\t\n",
            "25\tNone\t[135, 136]\t)\tNone\tNone\t\n",
            "\n",
            "-----Doc:  23352816\n",
            "Before: \n",
            "73\tNone\t[357, 380]\tTYPE=\"ORGANIZATION\">Hội\tNone\tNone\t\n",
            "73\t1\t[377, 380]\tHội\t[7, 'ORGANIZATION']\tNone\t'TYPE=\"ORGANIZATION\">'\n",
            "\n",
            "\n",
            "After:\n",
            "73\tNone\t[357, 377]\tTYPE=\"ORGANIZATION\">\tNone\tNone\t\n",
            "74\tNone\t[377, 380]\tHội\t[7, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "-----Doc:  23356574\n",
            "Before: \n",
            "743\tNone\t[3347, 3354]\tnghiệp,\tNone\tNone\t\n",
            "743\t1\t[3347, 3353]\tnghiệp\t[18, 'ORGANIZATION']\t[['PART – WHOLE', 733, None, [17, 18]]]\t\n",
            "\n",
            "After:\n",
            "743\tNone\t[3347, 3353]\tnghiệp\t[18, 'ORGANIZATION']\t[['PART – WHOLE', 733, None, [17, 18]]]\t\n",
            "744\tNone\t[3353, 3354]\t,\tNone\tNone\t\n",
            "\n",
            "-----Doc:  23357000\n",
            "Before: \n",
            "177\tNone\t[827, 833]\t1+1>2;\tNone\tNone\t\n",
            "177\t1\t[827, 832]\t1+1>2\t[8, 'ORGANIZATION']\t[['AFFILIATION', 166, None, [7, 8]]]\t\n",
            "\n",
            "After:\n",
            "177\tNone\t[827, 832]\t1+1>2\t[8, 'ORGANIZATION']\t[['AFFILIATION', 166, None, [7, 8]]]\t\n",
            "178\tNone\t[832, 833]\t;\tNone\tNone\t\n",
            "\n",
            "-----Doc:  23357000\n",
            "Before: \n",
            "264\tNone\t[1223, 1229]\t1+1>2;\tNone\tNone\t\n",
            "264\t1\t[1223, 1228]\t1+1>2\t[12, 'ORGANIZATION']\t[['AFFILIATION', 253, None, [11, 12]]]\t\n",
            "\n",
            "After:\n",
            "264\tNone\t[1223, 1228]\t1+1>2\t[12, 'ORGANIZATION']\t[['AFFILIATION', 253, None, [11, 12]]]\t\n",
            "265\tNone\t[1228, 1229]\t;\tNone\tNone\t\n",
            "\n",
            "-----Doc:  23357063\n",
            "Before: \n",
            "101\tNone\t[456, 461]\t4006,\tNone\tNone\t\n",
            "101\t1\t[456, 460]\t4006\t[1, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "After:\n",
            "101\tNone\t[456, 460]\t4006\t[1, 'ORGANIZATION']\tNone\t\n",
            "102\tNone\t[460, 461]\t,\tNone\tNone\t\n",
            "\n",
            "-----Doc:  23357779\n",
            "Before: \n",
            "27\tNone\t[122, 125]\t(Hà\tNone\tNone\t\n",
            "27\t1\t[123, 125]\tHà\t[2, 'LOCATION']\t[['LOCATED', 18, None, [1, 2]]]\t'('\n",
            "\n",
            "\n",
            "After:\n",
            "27\tNone\t[122, 123]\t(\tNone\tNone\t\n",
            "28\tNone\t[123, 125]\tHà\t[2, 'LOCATION']\t[['LOCATED', 18, None, [1, 2]]]\t\n",
            "\n",
            "CHECKING\n",
            "DONE. EVERYTHINGS SEEM TO BE CORRECTED :D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQKOe59wfEcS"
      },
      "source": [
        "## Fix2: Others fix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpazvkcLtgm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c7b153-8820-4131-b1ef-defdbfbf4b2b"
      },
      "source": [
        "# doc: 23352816\n",
        "# hai token 152, 153 mặc dù cùng có entity_id = 0 và đứng cạnh nhau\n",
        "# nhưng hai token này không phải là 1 entity mà là 2 entity vì chúng có entity_name khác nhau và chúng thuộc 2 câu khác nhau\n",
        "# ngoài ra token 153: \". VMISS\" dính dấu chấm ở câu trước, và đây cũng là 1 entity\n",
        "# ta cần sửa lỗi trong doc này: tách \". VMISS\" thành 2 token \".\" và \"VMISS\"\n",
        "# có nghĩa là ta sẽ thêm 1 dòng nữa cho \".\" và sửa lại \". VMISS\" thành \"VMISS\"\n",
        "\n",
        "raw_tdata_new_v2 = copy.deepcopy(raw_tdata_new)\n",
        "\n",
        "for docif in raw_tdata_new_v2:\n",
        "    if docif['doc_id'] == '23352816':\n",
        "\n",
        "        for i in range(len(docif['token_ids'])):\n",
        "\n",
        "            if (docif['tokens'][i] == \". VMISS\") and (docif['tokens'][i-1] == \"Úc\"):\n",
        "                \n",
        "                # them 1 dong cho dau '.'\n",
        "                docif['token_ids'].insert(i, docif['token_ids'][i])\n",
        "                docif['pos'].insert(i, [docif['pos'][i][0], (docif['pos'][i][0]+1)])\n",
        "                docif['tokens'].insert(i, '.')   # copy kí tự '.' trong doc rồi paste vào để tránh lỗi unicode\n",
        "                docif['subtoken_ids'].insert(i, None)\n",
        "                docif['entity'].insert(i, None)\n",
        "                docif['relation'].insert(i, None)\n",
        "\n",
        "                # sua lai dong '. VMISS' (do thêm '.' vào trước nên dòng '. VMISS' thành i+1)\n",
        "                docif['tokens'][i+1] = 'VMISS'\n",
        "                docif['pos'][i+1] = [docif['pos'][i+1][0] + 2, docif['pos'][i+1][1]]\n",
        "\n",
        "                # do them 1 dong nen phai sua lai token_ids phia sau va relation link toi token_ids phia sau\n",
        "                # thay đổi token_ids của toàn bộ phần dưới, nếu có subid trong relation ở đâu thì thay đổi, thay đổi stoken_id trong relation\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if j > i:\n",
        "                        docif['token_ids'][j] += 1\n",
        "\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            # do ở trên, toàn bộ token_ids phía sau (> i) sẽ bị thay đổi (cộng thêm 1)\n",
        "                            # nên những relation có stoken_id nằm ở phần phía sau này cũng sẽ cần thay đổi theo (cộng thêm 1)\n",
        "                            if (docif['relation'][j][k][1] > docif['token_ids'][i]):\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1\n",
        "\n",
        "\n",
        "\n",
        "print('\\n\\nCHECKING')\n",
        "for idoc, docif in enumerate(raw_tdata_new_v2):\n",
        "\n",
        "    if docif['doc_id'] == '23352816':\n",
        "\n",
        "        relation_lst = []\n",
        "        for i in range(len(raw_tdata[idoc]['relation'])):\n",
        "            if raw_tdata[idoc]['relation'][i] != None:\n",
        "                relation_lst.append(raw_tdata[idoc]['relation'][i])\n",
        "\n",
        "\n",
        "        relation_ith = 0\n",
        "        for i in range(len(docif['token_ids'])):\n",
        "\n",
        "            assert (docif['tokens'][i] == docif['text'][docif['pos'][i][0]:docif['pos'][i][1]]), \\\n",
        "            str('Wrong position')\n",
        "\n",
        "            if i < (len(docif['token_ids']) - 1):\n",
        "                if (docif['token_ids'][i] + 1) != docif['token_ids'][i+1]:\n",
        "                    assert False, str('Tokens_ids has problem')\n",
        "            \n",
        "            if docif['relation'][i] != None:\n",
        "                for j in range(len(docif['relation'][i])):\n",
        "   \n",
        "                    # so sánh xem thay đổi stoken_id có đúng không\n",
        "                    # string của token và pos của token sẽ không đổi so với raw_tdata\n",
        "                    stoken_new = docif['relation'][i][j][1]\n",
        "                    stoken_new_ele_id = docif['token_ids'].index(stoken_new)\n",
        "                    \n",
        "                    stoken = relation_lst[relation_ith][j][1]\n",
        "                    if relation_lst[relation_ith][j][2] == 1:\n",
        "                        stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken) + 1\n",
        "                    else:\n",
        "                        stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken)\n",
        "\n",
        "                    if docif['pos'][stoken_new_ele_id] != raw_tdata[idoc]['pos'][stoken_ele_id]:\n",
        "                        '''\n",
        "                        print('\\n-----Doc: ', docif['doc_id'])\n",
        "                        print(relation_ith)\n",
        "                        print(docif['relation'][i])\n",
        "                        print(relation_lst[relation_ith])\n",
        "                        print(docif['pos'][stoken_new_ele_id])\n",
        "                        print(raw_tdata[idoc]['pos'][stoken_ele_id])\n",
        "                        '''\n",
        "                        assert False, str('ERROR CODE 3')\n",
        "                    \n",
        "                relation_ith += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('DONE. EVERYTHINGS SEEM TO BE CORRECTED :D')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "CHECKING\n",
            "DONE. EVERYTHINGS SEEM TO BE CORRECTED :D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QeZVNtl9rxT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3eef630d-0ee9-4b82-82a1-8dc5e0d39645"
      },
      "source": [
        "'''\n",
        "for idoc, docif in enumerate(raw_tdata_new_v2):\n",
        "\n",
        "    if docif['doc_id'] == '23352816':\n",
        "        for i in range(len(docif['token_ids'])):\n",
        "            for key in docif:\n",
        "                if key not in ['doc_id', 'text']:\n",
        "                    print(docif[key][i], end='\\t')\n",
        "            print('\\n')\n",
        "'''"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfor idoc, docif in enumerate(raw_tdata_new_v2):\\n\\n    if docif['doc_id'] == '23352816':\\n        for i in range(len(docif['token_ids'])):\\n            for key in docif:\\n                if key not in ['doc_id', 'text']:\\n                    print(docif[key][i], end='\\t')\\n            print('\\n')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0VsUK91_VNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f0fdd6-711d-49fe-bc77-89c8dbc2f726"
      },
      "source": [
        "### doc: 23352687\n",
        "# sent: Văn Mạnh hạng thương tật T37 đã thành công ở nội dung chạy 200m.\n",
        "# trong doc này, trước câu trên có 2 dấu cách, khi dùng Underthesea thì Underthesea tách câu chính xác là bỏ 2 dấu cách đi.\n",
        "# nhưng trong doc thì tokenize bị lỗi, từ \"Văn\" trong doc là \" Văn\" tức là dính dấu 1 cách\n",
        "# dẫn tới start pos của entity khác với start pos của câu (chênh nhau 1 đơn vị cho chênh nhau 1 dấu cách)\n",
        "# phương án: sửa lại pos và token của entity trong trường hợp này, bỏ đi dấu cách thừa. \n",
        "\n",
        "raw_tdata_new_v3 = copy.deepcopy(raw_tdata_new_v2)\n",
        "\n",
        "for docif in raw_tdata_new_v3:\n",
        "    \n",
        "    if docif['doc_id'] == '23352687':\n",
        "        for i in range(len(docif['tokens'])):\n",
        "            if docif['tokens'][i] == \" Văn\":\n",
        "                print(docif[\"pos\"][i][0])\n",
        "                print(repr(docif[\"tokens\"][i]))\n",
        "\n",
        "                docif[\"pos\"][i][0] = docif[\"pos\"][i][0] + 1   # bỏ 1 dấu cách ở đầu thì pos sẽ dịch lên 1 đơn vị\n",
        "                docif[\"tokens\"][i] = docif[\"tokens\"][i].lstrip()   # bỏ dấu cách đi\n",
        "\n",
        "                assert (docif['text'][docif[\"pos\"][i][0]:docif[\"pos\"][i][1]] == docif[\"tokens\"][i]), \\\n",
        "                str(\"Change Failed!\")\n",
        "\n",
        "                print(docif[\"pos\"][i][0])\n",
        "                print(repr(docif[\"tokens\"][i]))\n",
        "\n",
        "                break\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "768\n",
            "'\\xa0Văn'\n",
            "769\n",
            "'Văn'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNIBK7jecGMV"
      },
      "source": [
        "## Fix 3: remove relation between entities belong to different sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT88mgCOg4zS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c18eae-2394-4973-ef89-cf82574ea4fa"
      },
      "source": [
        "a = [[1,2,3], [4,5,6], [8,9]]\n",
        "\n",
        "print(a)\n",
        "\n",
        "del a[1][0]\n",
        "del a[1][0]\n",
        "del a[1][0]\n",
        "\n",
        "print(a)\n",
        "\n",
        "if len(a[1]) == 0:\n",
        "    print('ok')\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 3], [4, 5, 6], [8, 9]]\n",
            "[[1, 2, 3], [], [8, 9]]\n",
            "ok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkQg8RftcfoC"
      },
      "source": [
        "# trong một số doc có trường hợp entity thuộc một câu có link relation tới entity thuộc một câu khác\n",
        "# để sửa lỗi này, ta sẽ bỏ đi relation lỗi giữa hai entity này\n",
        "\n",
        "def remove_error_relation(raw_data, doc_id, relation_name, relation_direction, token, id_entity, n_entity):\n",
        "\n",
        "    for docif in raw_data:\n",
        "        if docif['doc_id'] == doc_id:\n",
        "            for i in range(len(docif['relation'])):\n",
        "                if docif['relation'][i] != None:\n",
        "                    for j in range(len(docif['relation'][i])):\n",
        "\n",
        "                        # relation_name, relation_direction của relation cần xóa\n",
        "                        # token, entity_id, entity_name cùng dòng với relation cần xóa\n",
        "                        \n",
        "                        if (docif['relation'][i][j][0] == relation_name) and (docif['relation'][i][j][3] == relation_direction) \\\n",
        "                        and (docif['tokens'][i] == token) and (docif['entity'][i][0] == id_entity) and ((docif['entity'][i][1] == n_entity)):\n",
        "                            \n",
        "                            print('\\n\\n\\n-----', doc_id)\n",
        "                            print('--Before')\n",
        "                            for key in docif:\n",
        "                                if key not in ['doc_id', 'text']:\n",
        "                                    print(docif[key][i], end='\\t')\n",
        "\n",
        "                            del docif['relation'][i][j]\n",
        "\n",
        "                            print('\\n\\n--After delete error relation')\n",
        "                            for key in docif:\n",
        "                                if key not in ['doc_id', 'text']:\n",
        "                                    print(docif[key][i], end='\\t')\n",
        "                \n",
        "                    \n",
        "                            # nếu sau khi xóa relation bị lỗi mà relation list thành rỗng thì biến relation list thành None\n",
        "                            if (len(docif['relation'][i]) == 0):\n",
        "                                docif['relation'][i] = None\n",
        "\n",
        "                                print('\\n\\n--After removed empty relation')\n",
        "                                for key in docif:\n",
        "                                    if key not in ['doc_id', 'text']:\n",
        "                                        print(docif[key][i], end='\\t')\n",
        "\n",
        "                            break\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6QzF73ElLzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f168eede-8f11-42ff-c999-8fa06ad5ffde"
      },
      "source": [
        "raw_tdata_new_v4 = copy.deepcopy(raw_tdata_new_v3)\n",
        "\n",
        "# đầu vào là các thuộc tính trong dòng chứa relation cần xóa, \n",
        "# cần nhiều như này để chọn chính xác được relation cần xóa\n",
        "# relation_name, relation_direction của relation cần xóa\n",
        "# token, entity_id, entity_name cùng dòng với relation cần xóa\n",
        "# lưu ý là những cái này phải xem trong data rồi copy tay từ data paste vào\n",
        "\n",
        "remove_error_relation(raw_tdata_new_v4, '23352683', 'AFFILIATION', [11, 10], 'U16', 10, 'ORGANIZATION')\n",
        "remove_error_relation(raw_tdata_new_v4, '23353950', 'PART – WHOLE', [6, 0], 'Mỹ', 0, 'LOCATION')\n",
        "remove_error_relation(raw_tdata_new_v4, '23354695', 'LOCATED', [24, 25], 'phủ', 25, 'LOCATION')\n",
        "\n",
        "remove_error_relation(raw_tdata_new_v4, '23357765', 'PART – WHOLE', [15, 14], 'Việt', 14, 'LOCATION')\n",
        "remove_error_relation(raw_tdata_new_v4, '23357765', 'PART – WHOLE', [16, 14], 'Việt', 14, 'LOCATION')\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "----- 23352683\n",
            "--Before\n",
            "117\tNone\t[528, 531]\tU16\t[10, 'ORGANIZATION']\t[['AFFILIATION', 151, None, [11, 10]]]\t\n",
            "\n",
            "--After delete error relation\n",
            "117\tNone\t[528, 531]\tU16\t[10, 'ORGANIZATION']\t[]\t\n",
            "\n",
            "--After removed empty relation\n",
            "117\tNone\t[528, 531]\tU16\t[10, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "----- 23353950\n",
            "--Before\n",
            "56\tNone\t[229, 231]\tMỹ\t[0, 'LOCATION']\t[['PART – WHOLE', 71, None, [6, 0]]]\t\n",
            "\n",
            "--After delete error relation\n",
            "56\tNone\t[229, 231]\tMỹ\t[0, 'LOCATION']\t[]\t\n",
            "\n",
            "--After removed empty relation\n",
            "56\tNone\t[229, 231]\tMỹ\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "\n",
            "----- 23354695\n",
            "--Before\n",
            "365\tNone\t[1655, 1658]\tphủ\t[25, 'LOCATION']\t[['LOCATED', 329, None, [24, 25]]]\t\n",
            "\n",
            "--After delete error relation\n",
            "365\tNone\t[1655, 1658]\tphủ\t[25, 'LOCATION']\t[]\t\n",
            "\n",
            "--After removed empty relation\n",
            "365\tNone\t[1655, 1658]\tphủ\t[25, 'LOCATION']\tNone\t\n",
            "\n",
            "\n",
            "----- 23357765\n",
            "--Before\n",
            "283\tNone\t[1238, 1242]\tViệt\t[14, 'LOCATION']\t[['PART – WHOLE', 329, None, [15, 14]], ['PART – WHOLE', 332, None, [16, 14]]]\t\n",
            "\n",
            "--After delete error relation\n",
            "283\tNone\t[1238, 1242]\tViệt\t[14, 'LOCATION']\t[['PART – WHOLE', 332, None, [16, 14]]]\t\n",
            "\n",
            "\n",
            "----- 23357765\n",
            "--Before\n",
            "283\tNone\t[1238, 1242]\tViệt\t[14, 'LOCATION']\t[['PART – WHOLE', 332, None, [16, 14]]]\t\n",
            "\n",
            "--After delete error relation\n",
            "283\tNone\t[1238, 1242]\tViệt\t[14, 'LOCATION']\t[]\t\n",
            "\n",
            "--After removed empty relation\n",
            "283\tNone\t[1238, 1242]\tViệt\t[14, 'LOCATION']\tNone\t"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBr6Gx-Y2oj2"
      },
      "source": [
        "## Find all entity in a doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVl3FKeND9dk"
      },
      "source": [
        "# Tìm tất cả các entity trong một doc:\n",
        "\n",
        "def find_all_entity_in_doc(raw_data, doc_id):\n",
        "    \n",
        "    for docif in raw_data:\n",
        "\n",
        "        ######## Tìm tất cả các entity trong doc hiện tại:\n",
        "\n",
        "        #                      -                                          entity_1                                              -  ...\n",
        "        #                      | -                 token_1                  -  -                   token_2                -  ...|\n",
        "        ##### doc_entity_lst = [ [ [ele_id, token_id, entity_id, entity_name], [ele_id, token_id, entity_id, , entity_name], ...], ...]\n",
        "\n",
        "        ### Lưu ý: ele_id ở đây là element index của token đấy trong docif[\"entity\"]\n",
        "        ### chứ không phải là token_ids\n",
        "        ### lưu cái ele_id thay vì token_ids để truy cập token bằng index nhanh hơn\n",
        "        ### nếu lưu token_ids thì phải từ từ token_ids tìm xem token này nó nằm vị trí nào thì mới ra index (ele_id) để truy\n",
        "        ### thường thì, vì token_ids bắt đầu = 1, element index bắt đầu = 0 nên: token_ids tương ứng sẽ có: token_ids = ele_id + 1\n",
        "        ### nhưng nếu trong doc có subtoken thì điều trên sẽ không được đảm bảo\n",
        "\n",
        "        # token có entity_id giống nhau mà không đứng cạnh nhau (các dòng chứa token không liên tiếp nhau) thì thuộc 2 entity khác nhau. \n",
        "        # Trường hợp này là do dataset lỗi, bị lặp entity_id (bên trên có entity_id = 0 rồi xuống dưới (token không cạnh nhau) lại thấy entity_id = 0)\n",
        "        # Hiện tại mới thấy có entity_id = 0 là bị lặp lại cho nhiều entity khác nhau\n",
        "\n",
        "        \n",
        "\n",
        "        if docif['doc_id'] == doc_id:\n",
        "\n",
        "            doc_entity_lst = []\n",
        "            tmplst = []\n",
        "\n",
        "            for i in range(len(docif[\"entity\"])):\n",
        "                if docif[\"entity\"][i] != None:\n",
        "                    tmplst.append([i, docif[\"token_ids\"][i], docif[\"entity\"][i][0], docif[\"entity\"][i][1]])\n",
        "\n",
        "                    if (i < (len(docif[\"entity\"]) - 1)):\n",
        "\n",
        "                        if docif[\"entity\"][i+1] == None:\n",
        "                            doc_entity_lst.append(tmplst)\n",
        "                            tmplst = []\n",
        "\n",
        "                        elif (docif[\"entity\"][i][0] != docif[\"entity\"][i+1][0]) or (docif[\"entity\"][i][1] != docif[\"entity\"][i+1][1]):\n",
        "                            doc_entity_lst.append(tmplst)\n",
        "                            tmplst = []\n",
        "\n",
        "                    if i == (len(docif[\"entity\"]) - 1):\n",
        "                        doc_entity_lst.append(tmplst)\n",
        "\n",
        "\n",
        "            ###################### Fix lỗi hai entity khác nhau nhưng đứng cạnh nhau và bị trùng entity_id\n",
        "            # mọi cặp entity_id = 0 đứng cạnh nhau trong các doc trong list bên dưới đều sẽ bị tách ra thành các entity riêng\n",
        "            # tuy nhiên ta sẽ xử lý từng cặp một trong từng lần xử lý, run_times là số cặp trùng trong doc\n",
        "            # nếu có doc nào vừa có cặp entity_0 lỗi vừa có cặp không lỗi thì phải sẽ không được thêm list dưới vào mà phải xử lý riêng\n",
        "            # trừ khi cặp bị lỗi là cặp đầu tiên thì run_times đặt là 1\n",
        "            # data cũng không có quá nhiều những cặp như này\n",
        "            \n",
        "            doc_error_lst = [{'doc_error_id': '23351965', 'ith_er_lst': [8]},\n",
        "                             {'doc_error_id': '23352690', 'ith_er_lst': [51]},\n",
        "                             {'doc_error_id': '23352701', 'ith_er_lst': [16]},\n",
        "                             {'doc_error_id': '23352748', 'ith_er_lst': [76]},\n",
        "                             {'doc_error_id': '23352753', 'ith_er_lst': [37]},\n",
        "                             {'doc_error_id': '23352853', 'ith_er_lst': [15, 17]},\n",
        "                             {'doc_error_id': '23352878', 'ith_er_lst': [17]},\n",
        "                             {'doc_error_id': '23353786', 'ith_er_lst': [2]},\n",
        "                             {'doc_error_id': '23353891', 'ith_er_lst': [8]},\n",
        "                             {'doc_error_id': '23354619', 'ith_er_lst': [4, 6]},\n",
        "                             {'doc_error_id': '23354880', 'ith_er_lst': [113]}\n",
        "                            ]\n",
        "\n",
        "            doc_error_id_lst = [doc_error['doc_error_id'] for doc_error in doc_error_lst]\n",
        "\n",
        "            increase_ids = 0\n",
        "\n",
        "            if doc_id in doc_error_id_lst:\n",
        "                ith_doc_id = doc_error_id_lst.index(doc_id)\n",
        "\n",
        "                assert (doc_error_lst[ith_doc_id]['doc_error_id'] == doc_id), str('PRBOLEM')\n",
        "\n",
        "                #print('\\n\\n------', doc_id)\n",
        "\n",
        "                ith_er_list = sorted(doc_error_lst[ith_doc_id]['ith_er_lst'])\n",
        "\n",
        "                for irun, ith_er_id in enumerate(ith_er_list):\n",
        "                    #print('--', irun)\n",
        "                    doc_entity_lst_copy = None\n",
        "                    doc_entity_lst_copy = copy.deepcopy(doc_entity_lst)\n",
        "\n",
        "                    for ient, ent in enumerate(doc_entity_lst):\n",
        "                        if ient == (ith_er_id + increase_ids): # do bị dịch nên cần cộng với số id bị dịch\n",
        "\n",
        "                            assert ((ent[0][2] == 0) and (len(ent) > 1)), \\\n",
        "                            str('\\nWrong ith_er_lst. \\nDoc: ' + str(doc_id) + '\\nith_er_id: ' + str(ith_er_id))\n",
        "\n",
        "                            #print(doc_entity_lst_copy)\n",
        "\n",
        "                            # ví dụ: 3 token thì chỉ cần chạy 3 - 1 = 2 lần\n",
        "                            # lần đầu (itk = 0) thì lấy token cuối entity (token thứ 3: ent[-1]) chèn vào sau vị trí hiện tại của entity\n",
        "                            # lần hai (itk = 1) thì lấy token ngay trước token cuối (token thứ 2 từ cuối lên: ent[-2]) chèn vào sau vị trí hiện tại của entity\n",
        "                            for itk in range(len(ent) - 1):\n",
        "                                doc_entity_lst_copy.insert((ient+1), copy.deepcopy([ent[(-1 - itk)]]))\n",
        "\n",
        "                            # cuối cùng thì biến entity lỗi hiện tại thành token đầu của entity lỗi hiện tại là xong\n",
        "                            doc_entity_lst_copy[ient] = copy.deepcopy([ent[0]])\n",
        "\n",
        "                            #print(doc_entity_lst_copy)\n",
        "\n",
        "                            # do bên trên ta chèn thêm (len(ent) - 1) entity mới vào entity list\n",
        "                            # nên id của entity bị lỗi phía sau sẽ bị tăng lên lượng tương ứng\n",
        "                            # là tổng (len(ent)-1) của mọi ent bị sửa trước nó\n",
        "                            increase_ids += (len(ent) - 1)\n",
        "\n",
        "                            break\n",
        "                            \n",
        "                    doc_entity_lst = copy.deepcopy(doc_entity_lst_copy)\n",
        "                    #print(doc_entity_lst)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "            \n",
        "\n",
        "\n",
        "            '''\n",
        "            # Doc: 23351965\n",
        "            # hai token cạnh nhau (320, 321), cùng entity_id 0, cùng entity_name location nhưng không phải là 1 entity <-- lỗi data\n",
        "            # mà là hai entity, vì entity này link tới entity kia.\n",
        "\n",
        "            if docif['doc_id'] == '23351965':\n",
        "                doc_entity_lst_copy = copy.deepcopy(doc_entity_lst)\n",
        "                for ient, ent in enumerate(doc_entity_lst):\n",
        "                    if (len(ent) == 2) and (ent[0][2] == 0) and (ent[1][2] == 0):\n",
        "                        #print(doc_entity_lst_copy)\n",
        "                        doc_entity_lst_copy.insert((ient+1), [doc_entity_lst_copy[ient][1]])\n",
        "                        doc_entity_lst_copy[ient] = [doc_entity_lst_copy[ient][0]]\n",
        "                        #print(doc_entity_lst_copy)\n",
        "                        \n",
        "                doc_entity_lst = copy.deepcopy(doc_entity_lst_copy)\n",
        "                #print(doc_entity_lst)\n",
        "\n",
        "\n",
        "            # Doc: 23352753  bị giống bên trên\n",
        "            # hai token cạnh nhau (884, 885), cùng entity_id 0, cùng entity_name location nhưng không phải là 1 entity <-- lỗi data\n",
        "            # mà là hai entity, vì entity này link tới entity kia.\n",
        "\n",
        "            if docif['doc_id'] == '23352753':\n",
        "                doc_entity_lst_copy_2 = copy.deepcopy(doc_entity_lst)\n",
        "                for ient, ent in enumerate(doc_entity_lst):\n",
        "                    if (len(ent) == 2) and (ent[0][2] == 0) and (ent[1][2] == 0):\n",
        "                        #print(doc_entity_lst_copy_2)\n",
        "                        doc_entity_lst_copy_2.insert((ient+1), [doc_entity_lst_copy_2[ient][1]])\n",
        "                        doc_entity_lst_copy_2[ient] = [doc_entity_lst_copy_2[ient][0]]\n",
        "                        #print(doc_entity_lst_copy_2)\n",
        "                        \n",
        "                doc_entity_lst = copy.deepcopy(doc_entity_lst_copy_2)\n",
        "                #print(doc_entity_lst)\n",
        "            \n",
        "            '''\n",
        "\n",
        "\n",
        "            \n",
        "            return doc_entity_lst\n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIA5LQry20nQ"
      },
      "source": [
        "# thường thì những entity cạnh nhau và trùng id khiến ta dễ lầm thành 1 entity\n",
        "# thì những entity này thường có entity_id = 0\n",
        "# nên ta sẽ tìm các cặp entity_id = 0 này và xem cặp nào là lỗi cặp nào không lỗi\n",
        "# sau đó đọc xem trong doc này đây là lỗi hay không phải lỗi (có cặp không phải lỗi)\n",
        "# nếu là cặp lỗi thì sẽ phải thêm trường hợp ở cell code bên trên\n",
        "# để về sau dùng hàm tìm entity bên trên sẽ không bị lỗi\n",
        "\n",
        "# MỘT LƯU Ý LÀ: \n",
        "# VÍ DỤ: \n",
        "# 1-209\t935-944\tFrankfurt\t*\tLOCATION\t\n",
        "# 1-210\t945-949\t(Đức\t*\tLOCATION\t\n",
        "\n",
        "# 1-161\t739-743\tBali\t*\tLOCATION\t_\t_\t\n",
        "# 1-162\t744-754\t(Indonesia\t*\tLOCATION\tPART – WHOLE\t1-161\t\n",
        "               \n",
        "# gán không chính xác, bên trên không có relation nhưng bên dưới lại có\n",
        "# nên nếu có relation như bên dưới thì tách ra làm 2 entity\n",
        "# còn không có relation như bên trên thì để nó là một entity\n",
        "# vì nếu không có relation mà vẫn tách ra làm 2 entity thì label giữa chúng sẽ là others, không chính xác\n",
        "# nếu để là cùng 1 entity thì sẽ hợp lý hơn. miễn là để cùng entity không ảnh hưởng gì\n",
        "# và trong data có rất nhiều label other giữa các location (như Mỹ với Anh có thể là others)\n",
        "# nên nếu ta chia ra thì sau trong test set cũng bị để làm others, tức là không hợp lý\n",
        "# thà để thành 1, thì train data cũng thế mà test data cũng thế\n",
        "# hoặc ta có thể tách nhưng phải thêm nhãn part-whole vào\n",
        "\n",
        "# cũng có thể toàn bộ các entity_id = 0 cạnh nhau đều là các entity khác nha, nhưng nếu không phải thì sẽ không hoàn hảo\n",
        "# nên có thể xét các trường hợp riêng thay vì tự động tách các entity_0 cạnh nhau thành các entity khác nhau\n",
        "\n",
        "def find_all_fault_entity_id_0(raw_data):\n",
        "\n",
        "    for docif in raw_data:\n",
        "        ent_lst = find_all_entity_in_doc(raw_data, docif['doc_id'])\n",
        "        \n",
        "        for i in range(len(ent_lst)):\n",
        "\n",
        "            if (len(ent_lst[i]) > 1) and (ent_lst[i][0][2] == 0):\n",
        "                print('\\n\\n------', docif['doc_id'], ' -ith: ', i)\n",
        "                for j in range(len(ent_lst[i])):\n",
        "                    first_tk_eleid = ent_lst[i][j][0]\n",
        "                    for key in docif:\n",
        "                        if key not in ['doc_id', 'text']:\n",
        "                            print(docif[key][first_tk_eleid], end='\\t')\n",
        "                    \n",
        "                    print('\\n')\n",
        "        \n",
        "\n",
        "\n",
        "                \n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POdyqoDS5dA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063be3a4-562e-4cb7-8539-b4116dde1c1f"
      },
      "source": [
        "# tên hàm hơi gây nhầm\n",
        "# các entity được in ra có thể là lỗi hoặc là không lỗi, đa phần là lỗi\n",
        "# và các enity lỗi bên dưới là quyết định không sửa\n",
        "find_all_fault_entity_id_0(raw_tdata_new_v4)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "------ 23351888  -ith:  11\n",
            "209\tNone\t[935, 944]\tFrankfurt\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "210\tNone\t[945, 949]\t(Đức\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23351994  -ith:  25\n",
            "452\tNone\t[1967, 1972]\tTrung\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "453\tNone\t[1973, 1976]\tVTM\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352746  -ith:  3\n",
            "23\tNone\t[94, 98]\tBali\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "24\tNone\t[99, 109]\t(Indonesia\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352802  -ith:  5\n",
            "75\tNone\t[336, 339]\tAFP\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "76\tNone\t[340, 346]\t/TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352804  -ith:  5\n",
            "58\tNone\t[267, 270]\tAFP\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "59\tNone\t[271, 277]\t/TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352804  -ith:  35\n",
            "385\tNone\t[1771, 1777]\t(TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "386\tNone\t[1778, 1787]\t/Vietnam+\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23353849  -ith:  10\n",
            "72\tNone\t[353, 356]\tAFP\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "73\tNone\t[357, 363]\t/TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23353849  -ith:  43\n",
            "592\tNone\t[2793, 2799]\t(TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "593\tNone\t[2800, 2809]\t/Vietnam+\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23355817  -ith:  15\n",
            "474\tNone\t[2177, 2179]\tÚc\t[0, 'ORGANIZATION']\t[['AFFILIATION', 467, None, [8, 0]]]\t\n",
            "\n",
            "475\tNone\t[2180, 2184]\t(SIC\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGR4QQfBauVE"
      },
      "source": [
        "## Fix 4: Fix lỗi relation không nằm ở dòng chứa token đầu tiên trong entity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRCOfiuJa5LZ"
      },
      "source": [
        "# trong data có thể xảy ra trường hợp, relation không nằm ở token đầu của một entity\n",
        "# lỗi này có thể do data như vậy\n",
        "# ví dụ: relation trỏ tới subtoken\n",
        "# ngoài ra code bên dưới cũng giúp loại bỏ các relation bị lặp, tức là relation không chỉ chỉ nằm ở token đầu entity thì còn nằm ở token khác nữa\n",
        "# lưu ý: hàm bên dưới sẽ không thay đổi raw_data mà sẽ tạo bản copy và thay đổi trên bản copy này, rồi trả về bản copy này\n",
        "\n",
        "def fix_relation_not_in_1st_token(raw_data):\n",
        "\n",
        "    raw_data_new = copy.deepcopy(raw_data)\n",
        "\n",
        "    for docif in raw_data_new:\n",
        "\n",
        "        ent_lst = find_all_entity_in_doc(raw_data, docif['doc_id'])\n",
        "\n",
        "        for i in range(len(docif['relation'])):\n",
        "            if docif['relation'][i] != None:  \n",
        "                found = False\n",
        "                for ient, ent in enumerate(ent_lst):\n",
        "                    for itoken in range(len(ent)):\n",
        "                        if i == ent[itoken][0]:\n",
        "                            found = True\n",
        "\n",
        "                            if itoken != 0:   # neu ma khong nam tai token dau cua entity\n",
        "\n",
        "                                print('\\n\\n------ ', docif['doc_id'])\n",
        "                                print('Before')\n",
        "                                for itoken_1 in range(len(ent)):\n",
        "                                    token_ele_id = ent[itoken_1][0]\n",
        "                                    for key in docif:\n",
        "                                        if key not in ['doc_id', 'text']:\n",
        "                                            print(docif[key][token_ele_id], end='\\t')\n",
        "                                    print('\\n')\n",
        "\n",
        "                                # chuyen relation len token dau\n",
        "                                if docif['relation'][ent[0][0]] != None:\n",
        "                                    for rel in docif['relation'][i]:\n",
        "                                        if rel not in docif['relation'][ent[0][0]]:   # nếu không có thì mới thêm vào, còn có rồi thì chỉ cần xóa lặp thôi\n",
        "                                            docif['relation'][ent[0][0]].append(rel)\n",
        "\n",
        "                                else:\n",
        "                                    docif['relation'][ent[0][0]] = copy.deepcopy(docif['relation'][i])\n",
        "                                \n",
        "                                docif['relation'][i] = None   # chuyen xong thi xoa relation cu di\n",
        "\n",
        "                                print('After')\n",
        "                                for itoken_1 in range(len(ent)):\n",
        "                                    token_ele_id = ent[itoken_1][0]\n",
        "                                    for key in docif:\n",
        "                                        if key not in ['doc_id', 'text']:\n",
        "                                            print(docif[key][token_ele_id], end='\\t')\n",
        "                                    print('\\n')\n",
        "\n",
        "                assert (found == True), str('Not found relation in any entity')\n",
        "\n",
        "    return raw_data_new\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Mnxw1W_qy2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22bc5dad-3cc0-41d2-e03e-9697f79e7a80"
      },
      "source": [
        "raw_tdata_new_v5 = copy.deepcopy(raw_tdata_new_v4)\n",
        "raw_tdata_new_v5 = fix_relation_not_in_1st_token(raw_tdata_new_v5)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "------  23356574\n",
            "Before\n",
            "738\tNone\t[3318, 3324]\tTrưởng\t[18, 'ORGANIZATION']\t[['PART – WHOLE', 733, None, [17, 18]]]\t\n",
            "\n",
            "739\tNone\t[3325, 3330]\tphòng\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "740\tNone\t[3331, 3335]\tGiáo\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "741\tNone\t[3336, 3339]\tdục\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "742\tNone\t[3340, 3346]\tchuyên\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "743\tNone\t[3347, 3353]\tnghiệp\t[18, 'ORGANIZATION']\t[['PART – WHOLE', 733, None, [17, 18]]]\t\n",
            "\n",
            "After\n",
            "738\tNone\t[3318, 3324]\tTrưởng\t[18, 'ORGANIZATION']\t[['PART – WHOLE', 733, None, [17, 18]]]\t\n",
            "\n",
            "739\tNone\t[3325, 3330]\tphòng\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "740\tNone\t[3331, 3335]\tGiáo\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "741\tNone\t[3336, 3339]\tdục\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "742\tNone\t[3340, 3346]\tchuyên\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "743\tNone\t[3347, 3353]\tnghiệp\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------  23357000\n",
            "Before\n",
            "173\tNone\t[807, 810]\tvăn\t[8, 'ORGANIZATION']\t[['AFFILIATION', 166, None, [7, 8]]]\t\n",
            "\n",
            "174\tNone\t[811, 816]\tphòng\t[8, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "175\tNone\t[817, 821]\tkiến\t[8, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "176\tNone\t[822, 826]\ttrúc\t[8, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "177\tNone\t[827, 832]\t1+1>2\t[8, 'ORGANIZATION']\t[['AFFILIATION', 166, None, [7, 8]]]\t\n",
            "\n",
            "After\n",
            "173\tNone\t[807, 810]\tvăn\t[8, 'ORGANIZATION']\t[['AFFILIATION', 166, None, [7, 8]]]\t\n",
            "\n",
            "174\tNone\t[811, 816]\tphòng\t[8, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "175\tNone\t[817, 821]\tkiến\t[8, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "176\tNone\t[822, 826]\ttrúc\t[8, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "177\tNone\t[827, 832]\t1+1>2\t[8, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------  23357000\n",
            "Before\n",
            "260\tNone\t[1203, 1206]\tVăn\t[12, 'ORGANIZATION']\t[['AFFILIATION', 253, None, [11, 12]]]\t\n",
            "\n",
            "261\tNone\t[1207, 1212]\tphòng\t[12, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "262\tNone\t[1213, 1217]\tkiến\t[12, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "263\tNone\t[1218, 1222]\ttrúc\t[12, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "264\tNone\t[1223, 1228]\t1+1>2\t[12, 'ORGANIZATION']\t[['AFFILIATION', 253, None, [11, 12]]]\t\n",
            "\n",
            "After\n",
            "260\tNone\t[1203, 1206]\tVăn\t[12, 'ORGANIZATION']\t[['AFFILIATION', 253, None, [11, 12]]]\t\n",
            "\n",
            "261\tNone\t[1207, 1212]\tphòng\t[12, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "262\tNone\t[1213, 1217]\tkiến\t[12, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "263\tNone\t[1218, 1222]\ttrúc\t[12, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "264\tNone\t[1223, 1228]\t1+1>2\t[12, 'ORGANIZATION']\tNone\t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQt6fgmG7foW"
      },
      "source": [
        "## Fix 5: lỗi relation link tới token không phải token đầu của entity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzIm8E4d7rVq"
      },
      "source": [
        "# trong data, thường những entity có chứa subtoken sẽ bị lỗi này\n",
        "# tức là nếu trong entity ngoài token đầy đủ, còn có subtoken của 1 token nào đó\n",
        "# thì relation link tới entity này sẽ có stoken_id là id của subtoken kia\n",
        "# và subtoken thường đứng cuối entity nên dẫn tới việc relation không link tới token đầu của 1 entity\n",
        "# ta sẽ sửa lại cho link tới đúng token đầu (vì bên trên chúng ta cũng đã tách, bỏ đi subid rồi)\n",
        "\n",
        "# ta cần đưa vào thông số chính xác của relation cần sửa\n",
        "# tức là phải xem doc bị lỗi rồi lấy thông tin điền vào chứ không tự phát hiện được vì link sai có nhiều lý do chứ không phải mỗi lý do này.\n",
        "# tuy nhiên chỉ cần điền thông tin dòng chứ relation lỗi chứ không cần điền id token đầu entity đúng\n",
        "\n",
        "# code bên dưới có thể dùng để đổi relation link từ token giữa entity tới token đầu chứ không chỉ là token cuối tới token đầu\n",
        "\n",
        "def change_relation_link(raw_data, doc_id, relation_name, relation_direction, token, id_entity, n_entity):\n",
        "    \n",
        "    for docif in raw_data:\n",
        "        if docif['doc_id'] == doc_id:\n",
        "            for i in range(len(docif['relation'])):\n",
        "                if docif['relation'][i] != None:\n",
        "                    for j in range(len(docif['relation'][i])):\n",
        "\n",
        "                        if (docif['relation'][i][j][0] == relation_name) and (docif['relation'][i][j][3] == relation_direction) \\\n",
        "                        and (docif['tokens'][i] == token) and (docif['entity'][i][0] == id_entity) and ((docif['entity'][i][1] == n_entity)):\n",
        "                            \n",
        "                            print('\\n\\n\\n-----', doc_id)\n",
        "                            print('--Before change relation')\n",
        "                            for key in docif:\n",
        "                                if key not in ['doc_id', 'text']:\n",
        "                                    print(docif[key][i], end='\\t')\n",
        "\n",
        "                        \n",
        "                            ### thu thập mọi entity trong doc\n",
        "                            entity_lst = find_all_entity_in_doc(raw_data, doc_id)\n",
        "\n",
        "    \n",
        "                            ### tìm xem relation stoken_id nằm trong entity nào\n",
        "                            # thì trỏ relation stoken_id về đầu entity đấy\n",
        "\n",
        "                            found_id = -1\n",
        "                            for ient in range(len(entity_lst)):\n",
        "                                for itoken in range(len(entity_lst[ient])):\n",
        "                                    if docif['relation'][i][j][1] == entity_lst[ient][itoken][1]:   # tìm xem stoken_id thuộc entity nào\n",
        "                                        docif['relation'][i][j][1] = copy.deepcopy(entity_lst[ient][0][1])   # tìm thấy thì cập nhật thành id token đầu entity\n",
        "                                        found_id = ient\n",
        "\n",
        "                            assert (found_id != -1), str('Not found entity has relation stoken_id')\n",
        "\n",
        "                            print('\\n\\nEntity that relation link to')\n",
        "                            for itoken in range(len(entity_lst[found_id])):\n",
        "                                token_ele_id = entity_lst[found_id][itoken][0]\n",
        "                                for key in docif:\n",
        "                                    if key not in ['doc_id', 'text']:\n",
        "                                        print(docif[key][token_ele_id], end='\\t')\n",
        "                                print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "                            print('\\n--After change relation link')\n",
        "                            for key in docif:\n",
        "                                if key not in ['doc_id', 'text']:\n",
        "                                    print(docif[key][i], end='\\t')\n",
        "                \n",
        "                    \n",
        "         \n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZm3ZC_eQ5qz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "711ec6cf-0329-4a5d-d1e0-129b87fcd414"
      },
      "source": [
        "raw_tdata_new_v6 = copy.deepcopy(raw_tdata_new_v5)\n",
        "\n",
        "change_relation_link(raw_tdata_new_v6, '23356574', 'PART – WHOLE', [18, 19], 'Sở', 19, 'ORGANIZATION')\n",
        "change_relation_link(raw_tdata_new_v6, '23357063', 'PART – WHOLE', [1, 2], 'Hải', 2, 'ORGANIZATION')\n",
        "change_relation_link(raw_tdata_new_v6, '23357063', 'PART – WHOLE', [1, 3], 'Vùng', 3, 'LOCATION')\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "----- 23356574\n",
            "--Before change relation\n",
            "745\tNone\t[3355, 3357]\tSở\t[19, 'ORGANIZATION']\t[['AFFILIATION', 733, None, [17, 19]], ['PART – WHOLE', 743, None, [18, 19]]]\t\n",
            "\n",
            "Entity that relation link to\n",
            "738\tNone\t[3318, 3324]\tTrưởng\t[18, 'ORGANIZATION']\t[['PART – WHOLE', 733, None, [17, 18]]]\t\n",
            "\n",
            "739\tNone\t[3325, 3330]\tphòng\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "740\tNone\t[3331, 3335]\tGiáo\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "741\tNone\t[3336, 3339]\tdục\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "742\tNone\t[3340, 3346]\tchuyên\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "743\tNone\t[3347, 3353]\tnghiệp\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "--After change relation link\n",
            "745\tNone\t[3355, 3357]\tSở\t[19, 'ORGANIZATION']\t[['AFFILIATION', 733, None, [17, 19]], ['PART – WHOLE', 738, None, [18, 19]]]\t\n",
            "\n",
            "\n",
            "----- 23357063\n",
            "--Before change relation\n",
            "103\tNone\t[462, 465]\tHải\t[2, 'ORGANIZATION']\t[['PART – WHOLE', 101, None, [1, 2]]]\t\n",
            "\n",
            "Entity that relation link to\n",
            "99\tNone\t[448, 451]\ttàu\t[1, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "100\tNone\t[452, 455]\tCSB\t[1, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "101\tNone\t[456, 460]\t4006\t[1, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "--After change relation link\n",
            "103\tNone\t[462, 465]\tHải\t[2, 'ORGANIZATION']\t[['PART – WHOLE', 99, None, [1, 2]]]\t\n",
            "\n",
            "\n",
            "----- 23357063\n",
            "--Before change relation\n",
            "107\tNone\t[476, 480]\tVùng\t[3, 'LOCATION']\t[['PART – WHOLE', 103, None, [2, 3]], ['PART – WHOLE', 101, None, [1, 3]]]\t\n",
            "\n",
            "Entity that relation link to\n",
            "99\tNone\t[448, 451]\ttàu\t[1, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "100\tNone\t[452, 455]\tCSB\t[1, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "101\tNone\t[456, 460]\t4006\t[1, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "--After change relation link\n",
            "107\tNone\t[476, 480]\tVùng\t[3, 'LOCATION']\t[['PART – WHOLE', 103, None, [2, 3]], ['PART – WHOLE', 99, None, [1, 3]]]\t"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfXerQf-fQSv"
      },
      "source": [
        "# Create train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qce_9RiBsIZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "6caecd16-7d4a-40e7-e430-ebbf56bdab53"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "                    label: \n",
        "                    Dấu x là ký hiệu relation label (docif[\"relation\"]) xuất hiện ở chỗ của entity nào\n",
        "\n",
        "                              Entity:                 entity_1    -    entity_2\n",
        "                    Thứ tự trong câu:                 trước            sau\n",
        "                      \n",
        "                      Relation label:     LOCATED                         x     (per/org - loc)\n",
        "                                       IS_LOCATED         x                     (loc     - per/org)\n",
        "                                       PART–WHOLE\t                      x     (part    - whole)\n",
        "                                       WHOLE-PART         x                     (whole   - part)\n",
        "                                  PERSONAL–SOCIAL                               (Undirected)\n",
        "                                      AFFILIATION\t                      x     \n",
        "                                   AFFILIATION_TO         x\n",
        "                                           OTHERS                               (là nhãn giữ 2 entity cùng 1 câu mà không có relation trong data)\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n                    label: \\n                    Dấu x là ký hiệu relation label (docif[\"relation\"]) xuất hiện ở chỗ của entity nào\\n\\n                              Entity:                 entity_1    -    entity_2\\n                    Thứ tự trong câu:                 trước            sau\\n                      \\n                      Relation label:     LOCATED                         x     (per/org - loc)\\n                                       IS_LOCATED         x                     (loc     - per/org)\\n                                       PART–WHOLE\\t                      x     (part    - whole)\\n                                       WHOLE-PART         x                     (whole   - part)\\n                                  PERSONAL–SOCIAL                               (Undirected)\\n                                      AFFILIATION\\t                      x     \\n                                   AFFILIATION_TO         x\\n                                           OTHERS                               (là nhãn giữ 2 entity cùng 1 câu mà không có relation trong data)\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqEMUOpfqHZp"
      },
      "source": [
        "original_labels = ['LOCATED', 'PART – WHOLE', 'PERSONAL - SOCIAL', 'AFFILIATION']\n",
        "\n",
        "# entity chứa relation nằm ở phía sau thì là label gốc\n",
        "labels = {'LOCATED': 'LOCATED', 'IS_LOCATED': 'IS_LOCATED', \n",
        "         'PART_WHOLE': 'PART_WHOLE', 'WHOLE_PART': 'WHOLE_PART', \n",
        "         'PERSONAL_SOCIAL': 'PERSONAL_SOCIAL', \n",
        "         'AFFILIATION': 'AFFILIATION', 'AFFILIATION_TO': 'AFFILIATION_TO', \n",
        "         'OTHERS': 'OTHERS'\n",
        "         }"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8crlbk4MJTlx"
      },
      "source": [
        "### Func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBWuvOzUJUCP"
      },
      "source": [
        "def split_by_colon_punc(doc_sent_tokenize):\n",
        "\n",
        "    new_doc_sent_tokenize = []\n",
        "    for isent, sent in enumerate(doc_sent_tokenize):\n",
        "        if ':' not in sent:\n",
        "            new_doc_sent_tokenize.append(sent)\n",
        "        \n",
        "        else:\n",
        "            new_sents = sent.split(\":\")\n",
        "                \n",
        "            for inew_sent, new_sent in enumerate(new_sents):\n",
        "                if inew_sent != (len(new_sents) - 1):\n",
        "                    new_doc_sent_tokenize.append(str(new_sent.lstrip() + ':'))\n",
        "                else:\n",
        "                    new_doc_sent_tokenize.append(str(new_sent.strip()))\n",
        "\n",
        "    return new_doc_sent_tokenize\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt1PwbNGJG_W"
      },
      "source": [
        "from underthesea import sent_tokenize, word_tokenize\n",
        "\n",
        "def my_sentences_tokenize(doc_id, text):\n",
        "    ######## split sentence from docif[\"text\"] using Underthesea library\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # check if sum length of all sentence < len(text)\n",
        "    len_sentences = [len(s) for s in sentences]\n",
        "    assert (sum(len_sentences) <= len(text)), str(\"\\nSentence tokenize has problem. \\nDoc: \" + docif[\"doc_id\"])\n",
        "\n",
        "\n",
        "\n",
        "    ######\n",
        "    ### trong doc này việc chia sentence bằng Underthesea bị lỗi dẫn tới việc một entity nằm ở 2 câu.\n",
        "    new_sentences = []\n",
        "    \n",
        "    if doc_id == \"23352748\":\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (sent[-3:] != \"St.\") and (sent[:4] != \"Mary\"):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (sent[-3:] == \"St.\") and (sentences[isent + 1][:4] == \"Mary\"):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "        \n",
        "    ### trong các doc bên dưới việc chia sentence bằng Underthesea bị lỗi dẫn tới việc một relation link tới một entity thuộc câu khác.\n",
        "    ### thường do sau tên người viết tắt có dấu chấm\n",
        "    \n",
        "    elif doc_id == \"23351426\":\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (sent != \"Được biết, Nguyên có quan hệ tình cảm với chị H.T.H.\") and (sent != \"(quê Hà Nội , chủ cửa hàng thời trang).\"):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (sent == \"Được biết, Nguyên có quan hệ tình cảm với chị H.T.H.\") and (sentences[isent + 1] == \"(quê Hà Nội , chủ cửa hàng thời trang).\"):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "\n",
        "    \n",
        "    elif doc_id == \"23351515\":\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (sent != \"Cơ quan này cũng đang khẩn trương điều tra, làm rõ cái chết của chị N.T.T.\") and (sent != \"(SN 1983, ngụ xã Vạn Hưng , huyện Vạn Ninh ), vợ của người đàn ông tử vong trong vụ tai nạn trên.\"):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (sent == \"Cơ quan này cũng đang khẩn trương điều tra, làm rõ cái chết của chị N.T.T.\") and (sentences[isent + 1] == \"(SN 1983, ngụ xã Vạn Hưng , huyện Vạn Ninh ), vợ của người đàn ông tử vong trong vụ tai nạn trên.\"):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "        \n",
        "        new_sentences_1 = []\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"...\" not in sent):\n",
        "                new_sentences_1.append(sent)\n",
        "            \n",
        "            elif (\"...\" in sent):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_1.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences_1.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences_1)\n",
        "        sentences = copy.deepcopy(new_sentences_1)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "    \n",
        "    elif doc_id == \"23353874\":\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"sau thời gian điều trị tại BV, chị Trần Thị Đ.\" not in sent) and (\"(35 tuổi, ngụ quận Thủ Đức )\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"sau thời gian điều trị tại BV, chị Trần Thị Đ.\" in sent) and (\"(35 tuổi, ngụ quận Thủ Đức )\" in sentences[isent + 1]):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    ######\n",
        "    # có một số doc có ...\n",
        "\n",
        "    elif doc_id == '23351164':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"xử lý ngay” Tiếp đó PV\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"xử lý ngay” Tiếp đó PV\" in sent):\n",
        "                tmppp = sent.find(\"” Tiếp đó PV\")\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "        new_sentences_3 = []\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('.”' in sent) and ('.”' != sent[-2:]):\n",
        "                new_sents = sent.split('.”')\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_3.append(str(new_sent.lstrip() + '.”'))\n",
        "                    else:\n",
        "                        new_sentences_3.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ('.”' not in sent) or ('.”' == sent[-2:]):\n",
        "                new_sentences_3.append(sent)\n",
        "            \n",
        "        \n",
        "        #print(new_sentences_3)\n",
        "        sentences = copy.deepcopy(new_sentences_3)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == '23354055':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "\n",
        "            if (\"…\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"…\" in sent):\n",
        "                new_sents = sent.split(\"…\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '…'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "        new_sentences_3 = []\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('.”' in sent) and ('.”' != sent[-2:]):\n",
        "                new_sents = sent.split('.”')\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_3.append(str(new_sent.lstrip() + '.”'))\n",
        "                    else:\n",
        "                        new_sentences_3.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ('.”' not in sent) or ('.”' == sent[-2:]):\n",
        "                new_sentences_3.append(sent)\n",
        "            \n",
        "        \n",
        "        #print(new_sentences_3)\n",
        "        sentences = copy.deepcopy(new_sentences_3)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "    elif doc_id == '23354045':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "\n",
        "            if (\"...\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"...\" in sent):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "        new_sentences_3 = []\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('.”' in sent) and ('.”' != sent[-2:]):\n",
        "                new_sents = sent.split('.”')\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_3.append(str(new_sent.lstrip() + '.”'))\n",
        "                    else:\n",
        "                        new_sentences_3.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ('.”' not in sent) or ('.”' == sent[-2:]):\n",
        "                new_sentences_3.append(sent)\n",
        "            \n",
        "        \n",
        "        #print(new_sentences_3)\n",
        "        sentences = copy.deepcopy(new_sentences_3)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == '23351841':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"ban PH. Bức thư của anh\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"ban PH. Bức thư của anh\" in sent):\n",
        "                tmppp = sent.find(\". Bức thư của anh\")\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    elif doc_id == '23351612':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"Cavani và Neymar tranh nhau\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"Cavani và Neymar tranh nhau\" in sent):\n",
        "                tmppp = sent.find(\"' Vụ Neymar tranh\")\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    elif doc_id == '23354796':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"như vậy… Hơn 1 tháng\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"như vậy… Hơn 1 tháng\" in sent):\n",
        "                tmppp = sent.find(\"… Hơn\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    elif doc_id == '23353976':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"_\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"_\" in sent):\n",
        "                sent = sent.replace('_', '')\n",
        "                sent = sent.strip()\n",
        "                new_sentences.append(sent)\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    ### những doc mà ta chỉ tách theo …\n",
        "    # các doc các câu chỉ có … hoặc một số doc có hai loại nhưng ta chỉ chia câu có …\n",
        "    elif doc_id in ['23351214', '23351260', '23351430', '23351433', '23351578', \\\n",
        "                    '23351607', '23351645', '23351647', '23351719', '23351967', \\\n",
        "                    '23351987', '23351990', '23352659', '23352663', '23352675', \\\n",
        "                    '23352681', '23352702', '23352751', '23352755', '23352774', \\\n",
        "                    '23352781', '23352874', '23354130', '23354460', \\\n",
        "                    '23354803', '23354816', '23354880', '23354935', '23354956', \\\n",
        "                    '23355773', '23355917', '23357489', '23358011', '23356622', \\\n",
        "                    '23352856', '23353830', '23354538', \\\n",
        "                    '23354619', '23356574', '23355228']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "\n",
        "            if ((doc_id != '23352856') and (doc_id != '23353830') \\\n",
        "                and (doc_id != '23354538') and (doc_id != '23354619') \\\n",
        "                and (doc_id != '23356574') and (doc_id != '23355228')):\n",
        "                assert ('...' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            if (\"…\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"…\" in sent):\n",
        "                new_sents = sent.split(\"…\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '…'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "    ### những doc mà ta chỉ tách theo ...\n",
        "    # các doc các câu chỉ có ... hoặc một số doc có hai loại nhưng ta chỉ chia câu có ...\n",
        "    elif doc_id in ['23351435', '23351516', \\\n",
        "                    '23351542', '23351579', '23351581', '23351615', '23351888', \\\n",
        "                    '23351945', '23351956', '23351994', '23352725', '23352754', \\\n",
        "                    '23352857', '23353779', '23353780', '23353857', '23353950', \\\n",
        "                    '23354318', '23354953', '23354982', '23356494', \\\n",
        "                    '23356724', '23357000', \\\n",
        "                    '23351636', '23352753', '23353931', '23354320', '23354699', '23358261', '23351422', '23351427']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ((doc_id != '23351636') and (doc_id != '23352753') and (doc_id != '23353931') \\\n",
        "                and (doc_id != '23354320') and (doc_id != '23354699') and (doc_id != '23358261') \\\n",
        "                and (doc_id != '23351422') and (doc_id != '23351427')):   # doc nay co ca 2 nhung ta chi chia theo ...\n",
        "                assert ('…' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            if (doc_id == '23351422') and ('_' in sent):\n",
        "                #print(sent)\n",
        "                sent = sent.replace('_', '')\n",
        "                sent = sent.strip()\n",
        "                #print(sent)\n",
        "\n",
        "            if (\"...\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"...\" in sent):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "    ### những doc mà ta tách theo cả 2\n",
        "    elif doc_id in ['23351887', '23353846', '23353860', '23355656', '23356998']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            \n",
        "\n",
        "            if ((\"...\" not in sent) and ('…' not in sent)):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif ((\"...\" in sent) and ('…' not in sent)):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ((\"...\" not in sent) and ('…' in sent)):\n",
        "                new_sents = sent.split(\"…\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '…'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "\n",
        "            elif ((\"...\" in sent) and ('…' in sent)):\n",
        "                new_sents = sent.replace('...', '…')\n",
        "                new_sents = new_sents.split('…')\n",
        "\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ######\n",
        "    # trong doc này việc chia câu bị lỗi, 2 câu bị gộp thành 1 câu\n",
        "    # những câu này thường có kí tự: .”\n",
        "    \n",
        "    elif doc_id in ['23351307', '23352704', '23353830', '23353878', \\\n",
        "                    '23356329', '23356902', '23356933', '23357491', '23357809']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('.”' in sent) and ('.”' != sent[-2:]):\n",
        "                new_sents = sent.split('.”')\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '.”'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ('.”' not in sent) or ('.”' == sent[-2:]):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "\n",
        "\n",
        "    elif doc_id == '23351946':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"du lịch.. (clip\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"du lịch.. (clip\" in sent):\n",
        "                tmppp = sent.find(\". (clip\")\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "        new_sentences_5 = []\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "\n",
        "            if (\"…\" not in sent):\n",
        "                new_sentences_5.append(sent)\n",
        "            \n",
        "            elif (\"…\" in sent):\n",
        "                new_sents = sent.split(\"…\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_5.append(str(new_sent.lstrip() + '…'))\n",
        "                    else:\n",
        "                        new_sentences_5.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences_5)\n",
        "        sentences = copy.deepcopy(new_sentences_5)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == '23354082':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            \n",
        "            if ((\"..\" not in sent) and ('…' not in sent)):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif ((\"..\" in sent) and ('…' not in sent)):\n",
        "                new_sents = sent.split(\"..\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '..'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ((\"..\" not in sent) and ('…' in sent)):\n",
        "                new_sents = sent.split(\"…\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '…'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "\n",
        "            elif ((\"..\" in sent) and ('…' in sent)):\n",
        "                assert False, str('my_sentence_tokenize. Problem in doc: ' + doc_id)\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id in ['23351985', '23352682', '23352822', '23354085', '23356858']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "\n",
        "            if doc_id not in ['23354085']:\n",
        "                assert (('...' not in sent) and (\"…\" not in sent)), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            if (\"..\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"..\" in sent):\n",
        "                new_sents = sent.split(\"..\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '..'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    # tach dau .\n",
        "    \n",
        "    if doc_id in ['23351436', '23351566', '23351960', '23352693', '23352701', '23353721', '23357095', '23357394', '23352663', '23356902']:\n",
        "        \n",
        "        # ith_sent là thứ tự các câu cần tách dấu . trong doc (thứ tự câu bắt đầu từ 0)\n",
        "        # ith_dot là số thứ tự của các dấu . trong câu mà tại các dấu . này ta sẽ tách câu thành các phần khác nhau\n",
        "        # ith_dot cần đếm thứ tự cẩn thận bằng tay (thứ tự dấu . bắt đầu từ 1)\n",
        "        # ith_dot là một list của list. list thứ n của ith_dot là danh sách vị trí những dấu chấm mà ta sẽ dùng để tách câu thứ n tương ứn\n",
        "        # trong ith_sent   <- cần lưu ý để đúng thứ tự, và ith_sent cần xếp theo thứ tự tăng dần\n",
        "        # cần làm vậy vì có thể 1 doc có nhiều cần cần tách, rồi trong các câu này lại có câu có nhiều dấu . cần tách\n",
        "\n",
        "        # ý tưởng: ta duyệt các câu trong doc, dựa vào ith_sent_lst để biết câu nào cần tách dấu .\n",
        "        # câu nào không cần tách thì ta thêm luôn vào danh sách các câu trong doc\n",
        "        # câu nào cần tách thì: ta dựa tiếp vào ith_dot_lst để biết ta sẽ tách tại những dấu . nào trong câu\n",
        "        # ví dụ câu cần tách tại 2 dấu .: thứ 2 và thứ 3 trong câu (-> từ 1 câu tách thành 3 câu)\n",
        "        # ta sẽ tìm vị trí index của các dấu . này trong câu cần tách\n",
        "        # rồi dựa vào index đó để cắt câu thành các phần cần chia\n",
        "\n",
        "        doc_nfix_lst = [{'doc_id': '23351436', 'ith_sent_lst': [0], 'ith_dot_lst': [[2]]},\n",
        "                        {'doc_id': '23351566', 'ith_sent_lst': [5], 'ith_dot_lst': [[4]]},\n",
        "                        {'doc_id': '23351960', 'ith_sent_lst': [3], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '23352693', 'ith_sent_lst': [10, 13, 17], 'ith_dot_lst': [[1], [1], [1]]},\n",
        "                        {'doc_id': '23352701', 'ith_sent_lst': [4], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '23353721', 'ith_sent_lst': [5, 8, 19], 'ith_dot_lst': [[1], [1], [1, 2]]},\n",
        "                        {'doc_id': '23357095', 'ith_sent_lst': [5], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '23357394', 'ith_sent_lst': [7], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '23352663', 'ith_sent_lst': [2], 'ith_dot_lst': [[1]]},   # contain small three dot\n",
        "                        {'doc_id': '23356902', 'ith_sent_lst': [36], 'ith_dot_lst': [[1]]}   # contain .”\n",
        "                        ]\n",
        "        \n",
        "        ##### to find pos of ith dot\n",
        "        def find_ith_dot_pos(haystack, needle, n):\n",
        "            start = haystack.find(needle)\n",
        "            while start >= 0 and n > 1:\n",
        "                start = haystack.find(needle, start+len(needle))\n",
        "                n -= 1\n",
        "            return start\n",
        "        #####\n",
        "\n",
        "\n",
        "        crr_doc_nfix = None\n",
        "        for doc_nfix in doc_nfix_lst:\n",
        "            if doc_nfix['doc_id'] == doc_id:\n",
        "                crr_doc_nfix = doc_nfix\n",
        "\n",
        "        new_sentences_6 = []\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if doc_id not in ['23353721', '23357394', '23352663']:\n",
        "                assert (('...' not in sent) and (\"…\" not in sent)), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            assert (len(crr_doc_nfix['ith_sent_lst']) == len(crr_doc_nfix['ith_dot_lst'])), \\\n",
        "            str('\\nLEN ith_sent_lst not equal to LEN ith_dot_lst. \\nDoc: ' + doc_id)\n",
        "\n",
        "            if isent not in crr_doc_nfix['ith_sent_lst']:\n",
        "                new_sentences_6.append(sent)\n",
        "            \n",
        "            else:\n",
        "                \n",
        "                assert ('...' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "                crr_ith_sent = crr_doc_nfix['ith_sent_lst'].index(isent)\n",
        "\n",
        "                ith_dot_pos_lst = [-1, (len(sent)-1)]\n",
        "\n",
        "                for ith_dot in crr_doc_nfix['ith_dot_lst'][crr_ith_sent]:\n",
        "                    dot_pos = find_ith_dot_pos(sent, '.', ith_dot)\n",
        "\n",
        "                    assert (dot_pos >= 0), str('\\nNot found ith dot. \\nDoc: ' + doc_id + '\\nSent: ' + sent)\n",
        "\n",
        "                    ith_dot_pos_lst.insert(-1, dot_pos)\n",
        "                \n",
        "                for iith in range(len(ith_dot_pos_lst) - 1):\n",
        "                    correct_sent = sent[(ith_dot_pos_lst[iith] + 1):(ith_dot_pos_lst[iith+1] + 1)]\n",
        "                    new_sentences_6.append(str(correct_sent).strip())\n",
        "\n",
        "\n",
        "        #print(new_sentences_6)\n",
        "        sentences = copy.deepcopy(new_sentences_6)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    if doc_id not in ['23351164', '23351436', '23351511', '23351515', '23351556', \\\n",
        "                      '23351579', '23351952', '23351959', '23351990', '23352665', \\\n",
        "                      '23352750', '23352753', '23352769', '23352896', '23353967', \\\n",
        "                      '23354055', '23354082', '23354320', '23354336', '23354879', \\\n",
        "                      '23355434', '23357233', '23357752']:\n",
        "        sentences = copy.deepcopy(split_by_colon_punc(sentences))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return sentences\n",
        "\n",
        "    ###### \n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_riTWvT_OHxy"
      },
      "source": [
        "def get_sentence_entities(docif, doc_entity_lst, sent, sspos, espos):\n",
        "    \n",
        "    sen_entity_lst = []\n",
        "\n",
        "    for i in range(len(doc_entity_lst)):   # each entity\n",
        "        first_token_eid = doc_entity_lst[i][0][0]\n",
        "        last_token_eid = doc_entity_lst[i][-1][0]\n",
        "            \n",
        "        # nếu điểm đầu của câu < điểm đầu của token đầu và điểm cuối của token cuối < điểm cuối của câu\n",
        "        # thì entity này là entity của câu này\n",
        "        if (sspos <= docif[\"pos\"][first_token_eid][0]) and (docif[\"pos\"][last_token_eid][1] <= espos):\n",
        "            sen_entity_lst.append(doc_entity_lst[i])\n",
        "\n",
        "        # điểm đầu của entity < điểm đầu của câu nhưng điểm cuối của entity lại lớn hơn điểm đầu của câu\n",
        "        # có lỗi: 1 entity nhưng thuộc 2 câu, tức là 1 phần của entity thuộc câu trước, phần còn lại lại thuộc câu đang xét.\n",
        "        # điều này có thể do chia câu bằng Underthesea có vấn đề hoặc dataset có vấn đề\n",
        "        elif (docif[\"pos\"][first_token_eid][0] < sspos) and (sspos < docif[\"pos\"][last_token_eid][1]):\n",
        "                \n",
        "        # trong dataset có lỗi này. \n",
        "        # tuy nhiên không có nhiều, nên để hiểu thêm về dataset, tôi chỉ sửa chính xác các lỗi này\n",
        "        # và đã sửa bên trên\n",
        "\n",
        "            assert False, str(\"\\n--- An entity belongs to two sentences instead of just one. (Error Code 1) \\nIn doc: \" + docif[\"doc_id\"] + \"\\nSentence: \" + repr(sent))\n",
        "                \n",
        "        # điểm đầu của entity < điểm cuối của câu nhưng điểm cuối của entity lại lớn hơn điểm cuối của câu\n",
        "        # có lỗi: 1 entity nhưng thuộc 2 câu, tức là 1 phần của entity thuộc câu đang xét, phần còn lại lại thuộc câu sau.\n",
        "        # điều này có thể do chia câu bằng Underthesea có vấn đề hoặc dataset có vấn đề\n",
        "        elif (docif[\"pos\"][first_token_eid][0] < espos) and (espos < docif[\"pos\"][last_token_eid][1]):\n",
        "                \n",
        "            assert False, str(\"\\nAn entity belongs to two sentences instead of just one. (Error Code 2) \\nIn doc: \" + docif[\"doc_id\"] + \"\\nSentence: \" + repr(sent))\n",
        "\n",
        "\n",
        "    # có thể xảy ra trường hợp chia câu bị lỗi, một câu to bị chia thành hai câu nhỏ\n",
        "    # mỗi câu nhỏ lại chứa các entity\n",
        "    # nhưng entity câu nhỏ này link tới câu nhỏ kia -> lỗi\n",
        "    # hoặc trong data có lỗi, entity câu này link tới câu khác.\n",
        "    # nên cần xem xem các relation trong câu có link tới các entity tìm thấy trong câu không\n",
        "        \n",
        "    # hay relation giữa 2 entity là đúng, nhưng stoken_id trong relation không trỏ vào token đầu tiên của entity id kia\n",
        "    # mà lại trỏ vào token giữa hoặc cuối entity kia (đã fix lỗi này bên trên)\n",
        "\n",
        "    # vì relation chỉ link tới token_ids của token đầu tiên trong entity khác\n",
        "    # nên ta sẽ thu thập danh sách token_ids của các token đầu tiên các entity trong câu\n",
        "    first_tkids_lst = []\n",
        "    for i in range(len(sen_entity_lst)):\n",
        "        first_tkids_lst.append(sen_entity_lst[i][0][1])\n",
        "\n",
        "\n",
        "    for i in range(len(sen_entity_lst)):\n",
        "        first_tkeid = sen_entity_lst[i][0][0]\n",
        "\n",
        "        if docif['relation'][first_tkeid] != None:   # từng relation trong câu\n",
        "            for j in range(len(docif['relation'][first_tkeid])):\n",
        "                if docif['relation'][first_tkeid][j][1] not in first_tkids_lst:\n",
        "                        \n",
        "                    '''\n",
        "                    print(str('\\nSentence tokenize has problem. \\nDoc: ' + docif['doc_id'] + '\\nRelation stoken ID: ' + str(docif['relation'][first_tkeid][j][1])  + ' \\nprvSent: ' + sentences[isent-1] + '\\nSent: ' + sent))\n",
        "\n",
        "                    print(docif['relation'][first_tkeid])\n",
        "                    '''\n",
        "\n",
        "                    assert False, \\\n",
        "                    str('\\nSentence tokenize has problem. \\nDoc: ' + docif['doc_id'] + '\\nRelation stoken ID: ' + str(docif['relation'][first_tkeid][j][1])  + ' \\nprvSent: ' + sentences[isent-1] + '\\nSent: ' + sent)\n",
        "\n",
        "\n",
        "\n",
        "    return sen_entity_lst\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZremDvk8tP7"
      },
      "source": [
        "def relation_name_to_sentence_label(relation_name, relation_entity):\n",
        "\n",
        "    sentence_label = None\n",
        "    # nếu entity chứa relation là entity 1 thì label ngược lại\n",
        "    if relation_entity == 1:\n",
        "        if relation_name == 'LOCATED':\n",
        "            sentence_label = 'IS_LOCATED'\n",
        "\n",
        "        elif relation_name == 'PART – WHOLE':\n",
        "            sentence_label = 'WHOLE_PART'\n",
        "\n",
        "        elif relation_name == 'PERSONAL - SOCIAL':\n",
        "            sentence_label = 'PERSONAL_SOCIAL'\n",
        "\n",
        "        elif relation_name == 'AFFILIATION':\n",
        "            sentence_label = 'AFFILIATION_TO'\n",
        "\n",
        "        else:\n",
        "            assert False, str('UNKNOW RELATION NAME: ' + relation_name)\n",
        "    \n",
        "    # nếu entity chứa relation là entity 2 thì label giữ nguyên\n",
        "    elif relation_entity == 2:\n",
        "        if relation_name == 'LOCATED':\n",
        "            sentence_label = 'LOCATED'\n",
        "\n",
        "        elif relation_name == 'PART – WHOLE':\n",
        "            sentence_label = 'PART_WHOLE'\n",
        "\n",
        "        elif relation_name == 'PERSONAL - SOCIAL':\n",
        "            sentence_label = 'PERSONAL_SOCIAL'\n",
        "\n",
        "        elif relation_name == 'AFFILIATION':\n",
        "            sentence_label = 'AFFILIATION'\n",
        "\n",
        "        else:\n",
        "            assert False, str('UNKNOW RELATION NAME: ' + relation_name)\n",
        "\n",
        "    else:\n",
        "        assert False, (\"Unexpect relation_entity. Expect 1 or 2 but got: \" + relation_entity + \".\")\n",
        "\n",
        "    \n",
        "    return sentence_label"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyIgy3tfLDTs"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKFuhu7TYFB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ccc99bb-ee5b-47d3-e6b7-d548a22f7ce3"
      },
      "source": [
        "# Tạo train data (câu chứa cặp entity và label)\n",
        "\n",
        "tdata = []\n",
        "\n",
        "raw_data = raw_tdata_new_v6\n",
        "\n",
        "sent_id = 0\n",
        "\n",
        "for docif in raw_data:\n",
        "\n",
        "    # find all entity in current doc    \n",
        "    doc_entity_lst = find_all_entity_in_doc(raw_data, docif['doc_id'])\n",
        "\n",
        "    # if whole doc has 0 or 1 entity -->  no relation in this doc --> skip\n",
        "    # there is many doc like this (like doc has only 3 columns,...)\n",
        "    if len(doc_entity_lst) <= 1:\n",
        "        continue\n",
        "\n",
        "\n",
        "    text = docif[\"text\"]\n",
        "    sentences = my_sentences_tokenize(docif['doc_id'], text)\n",
        "\n",
        "    \n",
        "    ######## extract training sentence\n",
        " \n",
        "    pre_espos = 0   # end of pre sentence\n",
        "\n",
        "    for isent, sent in enumerate(sentences):\n",
        "\n",
        "        sentif = {}\n",
        "        '''\n",
        "        sentif[\"doc_id\"] = docif[\"doc_id\"]\n",
        "        sentif[\"sentence\"] = sent\n",
        "        '''\n",
        "        ###### sentence position\n",
        "        # tìm vị trí của câu để dựa vào đó biết entity (các tokens) thuộc câu nào\n",
        "\n",
        "        # text may have two indentical sentences\n",
        "        # so we have to find start position of current sentence in the rest of the text that not contain previous sentences.\n",
        "\n",
        "        assert (text[pre_espos:].find(sent) >= 0), str(\"Position has problem. \\nDoc: \" + docif[\"doc_id\"] + \"\\nCurrent sentence: \" + sent)\n",
        "\n",
        "        sspos = text[pre_espos:].find(sent) + pre_espos\n",
        "        espos = sspos + len(sent)\n",
        "\n",
        "        # update pre_espos\n",
        "        pre_espos = espos\n",
        "        \n",
        "        assert (sent == text[sspos:espos]), str(\"Position founded is not matched in text. \\nDoc: \" + docif[\"doc_id\"] + \"\\nCurrent sentence: \" + sent)\n",
        "\n",
        "        '''\n",
        "        sentif[\"spos\"] = [sspos, espos]\n",
        "        '''\n",
        "\n",
        "        ###### get all entity in current sentence\n",
        "        \n",
        "        sen_entity_lst = get_sentence_entities(docif, doc_entity_lst, sent, sspos, espos)\n",
        "\n",
        "        # if current sentence has 0 or 1 entity -> no relation availabel to classify -> skip\n",
        "        if len(sen_entity_lst) <= 1: \n",
        "            continue\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "        ###### create n*(n-1)/2 sentence\n",
        "        \n",
        "        for ient, ent_1 in enumerate(sen_entity_lst):\n",
        "            for jent, ent_2 in enumerate(sen_entity_lst[(ient+1):]):\n",
        "                \n",
        "                first_tkeid_ent1 = ent_1[0][0]   # dòng chứa token đầu trong entity\n",
        "                first_tkeid_ent2 = ent_2[0][0]\n",
        "                \n",
        "                last_tkeid_ent1 = ent_1[-1][0]   # dòng chứa token cuối trong entity\n",
        "                last_tkeid_ent2 = ent_2[-1][0]\n",
        "\n",
        "                if (docif['entity'][first_tkeid_ent1][1] != \"MISCELLANEOUS\") and (docif['entity'][first_tkeid_ent2][1] != \"MISCELLANEOUS\"):\n",
        "                    \n",
        "                    # pos của entity trong doc: start của token đầu và end của token cuối trong entity\n",
        "                    ent1_pos_doc = [docif['pos'][first_tkeid_ent1][0], docif['pos'][last_tkeid_ent1][1]]\n",
        "                    ent2_pos_doc = [docif['pos'][first_tkeid_ent2][0], docif['pos'][last_tkeid_ent2][1]]\n",
        "\n",
        "                    # pos của entity trong câu chứa entity\n",
        "                    ent1_pos_sent = [(ent1_pos_doc[0] - sspos), (ent1_pos_doc[1] - sspos)]\n",
        "                    ent2_pos_sent = [(ent2_pos_doc[0] - sspos), (ent2_pos_doc[1] - sspos)]\n",
        "\n",
        "                    \n",
        "                    # kiểm tra xem pos trong doc và sent có khớp, trả về cùng entity không\n",
        "                    assert (sent[ent1_pos_sent[0]:ent1_pos_sent[1]] == text[ent1_pos_doc[0]:ent1_pos_doc[1]]), \\\n",
        "                    str('Entity 1: pos_doc and pos_sent not matched. \\nDoc: ' + docif['doc_id'] + '\\nSent: ' + sent)\n",
        "\n",
        "                    assert (sent[ent2_pos_sent[0]:ent2_pos_sent[1]] == text[ent2_pos_doc[0]:ent2_pos_doc[1]]), \\\n",
        "                    str('Entity 1: pos_doc and pos_sent not matched. \\nDoc: ' + docif['doc_id'] + '\\nSent: ' + sent)\n",
        "\n",
        "\n",
        "                    ent1_text = sent[ent1_pos_sent[0]:ent1_pos_sent[1]]\n",
        "                    ent2_text = sent[ent2_pos_sent[0]:ent2_pos_sent[1]]\n",
        "\n",
        "                    ###############\n",
        "                    # kiểm tra xem mọi token trong entity đã có mặt trong entity lấy từ pos hay chưa\n",
        "                    for itk, tk in enumerate(ent_1):\n",
        "                        eid_tk = tk[0]\n",
        "\n",
        "                        if itk < (len(ent_1) - 1):\n",
        "                            eid_n_tk = ent_1[itk+1][0]\n",
        "\n",
        "                            assert (docif['pos'][eid_tk][1] < docif['pos'][eid_n_tk][0]), \\\n",
        "                            str(\"Position not increase. Doc: \" + docif['doc_id'] + \"\\nSent: \" + sent + \"\\ncrr-pos: \" + str(docif['pos'][eid_tk][1]) + \"\\nnpos: \" + str(docif['pos'][eid_ntk][0]))\n",
        "\n",
        "\n",
        "                        assert (ent1_pos_doc[0] <= docif['pos'][eid_tk][0]) and (docif['pos'][eid_tk][1] <= ent1_pos_doc[1]), \\\n",
        "                        str('Entity\\'s token pos not inside entity pos')\n",
        "                    \n",
        "                    ###############\n",
        "\n",
        "                    ##########\n",
        "\n",
        "                    assert (ent_1[0][1] == docif['token_ids'][first_tkeid_ent1]), str('NOT MATCHED entity first token id')\n",
        "                    assert (ent_2[0][1] == docif['token_ids'][first_tkeid_ent2]), str('NOT MATCHED entity first token id')\n",
        "\n",
        "                    sentence_label = None\n",
        "\n",
        "                    # nếu cả 2 entity không có relation\n",
        "                    if (docif['relation'][first_tkeid_ent1] == None) and (docif['relation'][first_tkeid_ent2] == None):\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                    # nếu cả 2 entity có relation\n",
        "                    elif (docif['relation'][first_tkeid_ent1] != None) and (docif['relation'][first_tkeid_ent2] != None):\n",
        "                        \n",
        "                        # mặc định là OTHERS, nếu bên dưới tìm thấy relation link tới thì sẽ được thay đổi\n",
        "                        # còn nếu bên dưới tìm không thấy (tức là không có relation) thì sẽ không bị thay đổi và vẫn là OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                        for rel_1 in docif['relation'][first_tkeid_ent1]:\n",
        "                            if rel_1[1] == ent_2[0][1]:   # relation ở entity 1 link tới token đầu entity 2\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_1[0], 1)\n",
        "\n",
        "                                # kiem tra relation direction\n",
        "                                # do relation nam o ent_1 nen direction la: [ent_2, ent_1]\n",
        "                                if rel_1[3] != None:\n",
        "                                    assert (rel_1[3] == [ent_2[0][2], ent_1[0][2]]), str('CODE 1: Not match direction')\n",
        "                                \n",
        "\n",
        "                        for rel_2 in docif['relation'][first_tkeid_ent2]:\n",
        "                            if rel_2[1] == ent_1[0][1]:   # relation ở entity 2 link tới token đầu entity 1\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_2[0], 2)\n",
        "\n",
        "                                # do relation nam o ent_2 nen direction la: [ent_1, ent_2]\n",
        "                                if rel_2[3] != None:\n",
        "                                    assert (rel_2[3] == [ent_1[0][2], ent_2[0][2]]), str('CODE 2: Not match direction')\n",
        "        \n",
        "\n",
        "                    # nếu entity 1 có relation, entity 2 không có\n",
        "                    elif (docif['relation'][first_tkeid_ent1] != None) and (docif['relation'][first_tkeid_ent2] == None):\n",
        "                        \n",
        "                        # mặc định là OTHERS, nếu bên dưới tìm thấy relation link tới thì sẽ được thay đổi\n",
        "                        # còn nếu bên dưới tìm không thấy (tức là không có relation) thì sẽ không bị thay đổi và vẫn là OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                        for rel_1 in docif['relation'][first_tkeid_ent1]:\n",
        "                            if rel_1[1] == ent_2[0][1]:   # relation ở entity 1 link tới token đầu entity 2\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_1[0], 1)\n",
        "\n",
        "                                # do relation nam o ent_1 nen direction la: [ent_2, ent_1]\n",
        "                                if rel_1[3] != None:\n",
        "                                    assert (rel_1[3] == [ent_2[0][2], ent_1[0][2]]), str('CODE 3: Not match direction')\n",
        "\n",
        "                                \n",
        "\n",
        "                    # nếu entity 2 có relation, entity 1 không có\n",
        "                    elif (docif['relation'][first_tkeid_ent1] == None) and (docif['relation'][first_tkeid_ent2] != None):\n",
        "                        \n",
        "                        # mặc định là OTHERS, nếu bên dưới tìm thấy relation link tới thì sẽ được thay đổi\n",
        "                        # còn nếu bên dưới tìm không thấy (tức là không có relation) thì sẽ không bị thay đổi và vẫn là OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "                        \n",
        "                        for rel_2 in docif['relation'][first_tkeid_ent2]:\n",
        "                            if rel_2[1] == ent_1[0][1]:   # relation ở entity 2 link tới token đầu entity 1\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_2[0], 2)\n",
        "\n",
        "                                # do relation nam o ent_2 nen direction la: [ent_1, ent_2]\n",
        "                                if rel_2[3] != None:\n",
        "                                    assert (rel_2[3] == [ent_1[0][2], ent_2[0][2]]), str('CODE 4: Not match direction')\n",
        "\n",
        "                        \n",
        "                    ##########\n",
        "\n",
        "                    sentif[\"doc_id\"] = docif[\"doc_id\"]\n",
        "\n",
        "                    sent_id += 1\n",
        "                    sentif['sent_id'] = sent_id\n",
        "\n",
        "                    sentif[\"sentence\"] = sent\n",
        "                    sentif[\"spos\"] = [sspos, espos]\n",
        "\n",
        "                    entity_1 = {'text': copy.deepcopy(ent1_text), 'pos': copy.deepcopy(ent1_pos_sent)}\n",
        "                    entity_2 = {'text': copy.deepcopy(ent2_text), 'pos': copy.deepcopy(ent2_pos_sent)}\n",
        "\n",
        "                    sentif['entity_1'] = copy.deepcopy(entity_1)\n",
        "                    sentif['entity_2'] = copy.deepcopy(entity_2)\n",
        "\n",
        "                    sentif['label'] = copy.deepcopy(sentence_label)\n",
        "\n",
        "                    \n",
        "                    \n",
        "                    # may dong tren co the khong co copy.deepcopy nhung dong ben duoi khong co la bi loi\n",
        "                    tdata.append(copy.deepcopy(sentif))\n",
        "\n",
        "print(\"DONE\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AVi-oJSXQdY",
        "outputId": "5a0298d6-4844-41c7-f032-ed684cf664fa"
      },
      "source": [
        "count_label_others = 0\n",
        "\n",
        "for tdata_point in tdata:\n",
        "    if tdata_point['label'] == 'OTHERS':\n",
        "        count_label_others += 1\n",
        "\n",
        "print(count_label_others)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz_22I3CZvr9",
        "outputId": "9ac7bb8c-748b-475f-ef79-6f0bfe5a2391"
      },
      "source": [
        "print('Count of labels that is not OTHERS: ', (len(tdata) - count_label_others))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of labels that is not OTHERS:  2645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3O1aoGj796m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c127ca-8a4a-49be-9f05-ab745231c3d8"
      },
      "source": [
        "print(len(tdata))\n",
        "print(*tdata[0:15], sep='\\n')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15666\n",
            "{'doc_id': '23351113', 'sent_id': 1, 'sentence': 'Ảnh minh họa Thứ trưởng Bộ GD&ĐT Nguyễn Thị Nghĩa đã có ý kiến về vấn đề này.', 'spos': [311, 388], 'entity_1': {'text': 'Bộ GD&ĐT', 'pos': [24, 32]}, 'entity_2': {'text': 'Nguyễn Thị Nghĩa', 'pos': [33, 49]}, 'label': 'AFFILIATION_TO'}\n",
            "{'doc_id': '23351113', 'sent_id': 2, 'sentence': 'Ông Nguyễn Tùng Lâm Sắp tới, Bộ GD&ĐT sẽ tăng cường thanh, kiểm tra để có biện pháp chấn chỉnh việc thực hiện quy định Điều lệ của Ban đại diện cha mẹ học sinh.', 'spos': [894, 1054], 'entity_1': {'text': 'Nguyễn Tùng Lâm', 'pos': [4, 19]}, 'entity_2': {'text': 'Bộ GD&ĐT', 'pos': [29, 37]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 3, 'sentence': 'Cũng theo Thứ trưởng Nguyễn Thị Nghĩa , trước những biến tướng như hiện tại, Bộ GD&ĐT nghiên cứu có thể bỏ quy định này để tránh hiện tượng lách luật.', 'spos': [1220, 1370], 'entity_1': {'text': 'Nguyễn Thị Nghĩa', 'pos': [21, 37]}, 'entity_2': {'text': 'Bộ GD&ĐT', 'pos': [77, 85]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 4, 'sentence': 'Liên quan đến vấn đề lãnh đạo ngành Giáo dục trao đổi, thầy Nguyễn Văn Định – Hiệu trưởng Trường THPT Phú Điền , Đồng Tháp – chia sẻ:', 'spos': [1371, 1504], 'entity_1': {'text': 'Nguyễn Văn Định', 'pos': [60, 75]}, 'entity_2': {'text': 'Trường THPT Phú Điền', 'pos': [90, 110]}, 'label': 'AFFILIATION'}\n",
            "{'doc_id': '23351113', 'sent_id': 5, 'sentence': 'Liên quan đến vấn đề lãnh đạo ngành Giáo dục trao đổi, thầy Nguyễn Văn Định – Hiệu trưởng Trường THPT Phú Điền , Đồng Tháp – chia sẻ:', 'spos': [1371, 1504], 'entity_1': {'text': 'Nguyễn Văn Định', 'pos': [60, 75]}, 'entity_2': {'text': 'Đồng Tháp', 'pos': [113, 122]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 6, 'sentence': 'Liên quan đến vấn đề lãnh đạo ngành Giáo dục trao đổi, thầy Nguyễn Văn Định – Hiệu trưởng Trường THPT Phú Điền , Đồng Tháp – chia sẻ:', 'spos': [1371, 1504], 'entity_1': {'text': 'Trường THPT Phú Điền', 'pos': [90, 110]}, 'entity_2': {'text': 'Đồng Tháp', 'pos': [113, 122]}, 'label': 'LOCATED'}\n",
            "{'doc_id': '23351113', 'sent_id': 7, 'sentence': 'Nhận định ý tưởng của Bộ GD&ĐT bỏ quyền được thu tiền của Ban đại diện cha mẹ học sinh vào thời điểm hiện nay là khá phù hợp, thầy Nguyễn Văn Định đưa lý do:', 'spos': [1914, 2071], 'entity_1': {'text': 'Bộ GD&ĐT', 'pos': [22, 30]}, 'entity_2': {'text': 'Nguyễn Văn Định', 'pos': [131, 146]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 8, 'sentence': 'Cũng về câu chuyện lạm thu liên quan đến Ban đại diện cha mẹ học sinh, TS Nguyễn Tùng Lâm – Chủ tịch Hội Tâm lý giáo dục Hà Nội , nguyên Hiệu trưởng Trường THPT Đinh Tiên Hoàng (Hà Nội ) – cho rằng:', 'spos': [2627, 2825], 'entity_1': {'text': 'Nguyễn Tùng Lâm', 'pos': [74, 89]}, 'entity_2': {'text': 'Hội Tâm lý giáo dục Hà Nội', 'pos': [101, 127]}, 'label': 'AFFILIATION'}\n",
            "{'doc_id': '23351113', 'sent_id': 9, 'sentence': 'Cũng về câu chuyện lạm thu liên quan đến Ban đại diện cha mẹ học sinh, TS Nguyễn Tùng Lâm – Chủ tịch Hội Tâm lý giáo dục Hà Nội , nguyên Hiệu trưởng Trường THPT Đinh Tiên Hoàng (Hà Nội ) – cho rằng:', 'spos': [2627, 2825], 'entity_1': {'text': 'Nguyễn Tùng Lâm', 'pos': [74, 89]}, 'entity_2': {'text': 'Trường THPT Đinh Tiên Hoàng', 'pos': [149, 176]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 10, 'sentence': 'Cũng về câu chuyện lạm thu liên quan đến Ban đại diện cha mẹ học sinh, TS Nguyễn Tùng Lâm – Chủ tịch Hội Tâm lý giáo dục Hà Nội , nguyên Hiệu trưởng Trường THPT Đinh Tiên Hoàng (Hà Nội ) – cho rằng:', 'spos': [2627, 2825], 'entity_1': {'text': 'Nguyễn Tùng Lâm', 'pos': [74, 89]}, 'entity_2': {'text': '(Hà Nội', 'pos': [177, 184]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 11, 'sentence': 'Cũng về câu chuyện lạm thu liên quan đến Ban đại diện cha mẹ học sinh, TS Nguyễn Tùng Lâm – Chủ tịch Hội Tâm lý giáo dục Hà Nội , nguyên Hiệu trưởng Trường THPT Đinh Tiên Hoàng (Hà Nội ) – cho rằng:', 'spos': [2627, 2825], 'entity_1': {'text': 'Hội Tâm lý giáo dục Hà Nội', 'pos': [101, 127]}, 'entity_2': {'text': 'Trường THPT Đinh Tiên Hoàng', 'pos': [149, 176]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 12, 'sentence': 'Cũng về câu chuyện lạm thu liên quan đến Ban đại diện cha mẹ học sinh, TS Nguyễn Tùng Lâm – Chủ tịch Hội Tâm lý giáo dục Hà Nội , nguyên Hiệu trưởng Trường THPT Đinh Tiên Hoàng (Hà Nội ) – cho rằng:', 'spos': [2627, 2825], 'entity_1': {'text': 'Hội Tâm lý giáo dục Hà Nội', 'pos': [101, 127]}, 'entity_2': {'text': '(Hà Nội', 'pos': [177, 184]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 13, 'sentence': 'Cũng về câu chuyện lạm thu liên quan đến Ban đại diện cha mẹ học sinh, TS Nguyễn Tùng Lâm – Chủ tịch Hội Tâm lý giáo dục Hà Nội , nguyên Hiệu trưởng Trường THPT Đinh Tiên Hoàng (Hà Nội ) – cho rằng:', 'spos': [2627, 2825], 'entity_1': {'text': 'Trường THPT Đinh Tiên Hoàng', 'pos': [149, 176]}, 'entity_2': {'text': '(Hà Nội', 'pos': [177, 184]}, 'label': 'LOCATED'}\n",
            "{'doc_id': '23351113', 'sent_id': 14, 'sentence': 'Nhấn mạnh Bộ GD&ĐT đã đưa ra hướng dẫn những khoản nhà trường được và không được thu, TS Nguyễn Tùng Lâm cho rằng, hiện chúng ta đang lẫn lộn rất nhiều thứ và tất cả đều đổ lên Ban phụ huynh.', 'spos': [3981, 4172], 'entity_1': {'text': 'Bộ GD&ĐT', 'pos': [10, 18]}, 'entity_2': {'text': 'Nguyễn Tùng Lâm', 'pos': [89, 104]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351164', 'sent_id': 15, 'sentence': 'Quảng Bình : Cát tặc lộng hành, hiểm họa rình rập cầu Long Đại và dòng sông?', 'spos': [0, 76], 'entity_1': {'text': 'Quảng Bình', 'pos': [0, 10]}, 'entity_2': {'text': 'cầu Long Đại', 'pos': [50, 62]}, 'label': 'WHOLE_PART'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0mBtqdZthaa"
      },
      "source": [
        "# Write to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTM1DITfqJtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bd77e43-5938-4a17-a284-14f566c64f34"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xLBzf43i3NC"
      },
      "source": [
        "# write to file\n",
        "import codecs\n",
        "import json\n",
        "\n",
        "with codecs.open('train_data.json', 'w', encoding='utf-8') as fout:\n",
        "    json.dump(tdata, fout, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72boeFzXgq-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3864947-e9e1-4d97-8f5a-57b31cb4135f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B8JEv6kDzuV"
      },
      "source": [
        "!mkdir \"/gdrive/MyDrive/VLSP2020_RE\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ng7tJuLD4_V"
      },
      "source": [
        "!mkdir \"/gdrive/MyDrive/VLSP2020_RE/json_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU0dYmmshRFZ"
      },
      "source": [
        "!cp -i train_data.json \"/gdrive/MyDrive/VLSP2020_RE/json_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4ikf014Wj7R",
        "outputId": "3d85f5b0-087c-4169-814e-205aaa3f93f7"
      },
      "source": [
        "import filecmp\n",
        "filecmp.cmp('train_data.json', '/gdrive/MyDrive/VLSP2020_RE/json_data/train_data.json')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    }
  ]
}
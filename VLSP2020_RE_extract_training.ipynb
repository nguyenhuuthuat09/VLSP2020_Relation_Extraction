{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VLSP2020_RE_extract_training_V4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "H3ICier0JyzI",
        "PcS3CiYZxv8a",
        "jhOuiCNgaVqk",
        "chNxSZ0mx5wS",
        "-wzBGIIgKYPD",
        "EYMqFdP5eTPL",
        "RQKOe59wfEcS",
        "yNIBK7jecGMV",
        "kBr6Gx-Y2oj2",
        "oGR4QQfBauVE",
        "PQt6fgmG7foW",
        "8crlbk4MJTlx",
        "L0mBtqdZthaa"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQDbz_q5irs0"
      },
      "source": [
        "# Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxh7foXeJtrf"
      },
      "source": [
        "## Unrar dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etKZzzHSF6_i"
      },
      "source": [
        "Please upload VLSP2020_RE_training.rar to Colab then */content* folder then unrar it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXjtAzVJHNI_",
        "outputId": "108b2f62-ff97-40ce-df63-264f53757142"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6x3Nl_tfBf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "181068c2-84ff-4bf7-cfd6-96e1dac85ff5"
      },
      "source": [
        "!pip install unrar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unrar\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/0b/53130ccd483e3db8c8a460cb579bdb21b458d5494d67a261e1a5b273fbb9/unrar-0.4-py3-none-any.whl\n",
            "Installing collected packages: unrar\n",
            "Successfully installed unrar-0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnlpfnYxiSiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "569fd1ab-e486-4a9e-c8d0-770194ea4630"
      },
      "source": [
        "!unrar x VLSP2020_RE_training.rar"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from VLSP2020_RE_training.rar\n",
            "\n",
            "Creating    VLSP2020_RE_training                                      OK\n",
            "Creating    VLSP2020_RE_training/23351113.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351113.conll/CURATION_USER.tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351164.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351164.conll/CURATION_USER.tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351190.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351190.conll/CURATION_USER.tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351214.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351214.conll/CURATION_USER.tsv        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351225.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351225.conll/CURATION_USER.tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351260.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351260.conll/CURATION_USER.tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351307.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351307.conll/CURATION_USER.tsv        \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351316.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351316.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351318.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351318.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351385.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351385.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351391.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351391.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351392.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351392.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351393.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351393.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351394.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351394.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351416.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351416.conll/CURATION_USER.tsv        \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351422.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351422.conll/CURATION_USER.tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351424.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351424.conll/CURATION_USER.tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351425.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351425.conll/CURATION_USER.tsv        \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351426.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351426.conll/CURATION_USER.tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351427.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351427.conll/CURATION_USER.tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351430.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351430.conll/CURATION_USER.tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351431.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351431.conll/CURATION_USER.tsv        \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351432.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351432.conll/CURATION_USER.tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351433.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351433.conll/CURATION_USER.tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351434.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351434.conll/CURATION_USER.tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351435.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351435.conll/CURATION_USER.tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351436.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351436.conll/CURATION_USER.tsv        \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351437.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351437.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351438.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351438.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351440.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351440.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351460.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351460.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351489.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351489.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351493.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351493.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351494.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351494.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351510.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351510.conll/CURATION_USER.tsv        \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351511.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351511.conll/CURATION_USER.tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351514.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351514.conll/CURATION_USER.tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351515.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351515.conll/CURATION_USER.tsv        \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351516.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351516.conll/CURATION_USER.tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351518.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351518.conll/CURATION_USER.tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351519.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351519.conll/CURATION_USER.tsv        \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351521.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351521.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351522.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351522.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351524.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351524.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351542.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351542.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351543.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351543.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351549.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351549.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351554.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351554.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351555.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351555.conll/CURATION_USER.tsv        \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351556.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351556.conll/CURATION_USER.tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351561.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351561.conll/CURATION_USER.tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351562.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351562.conll/CURATION_USER.tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351563.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351563.conll/CURATION_USER.tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351564.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351564.conll/CURATION_USER.tsv        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351566.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351566.conll/CURATION_USER.tsv        \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351567.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351567.conll/CURATION_USER.tsv        \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351569.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351569.conll/CURATION_USER.tsv        \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351571.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351571.conll/CURATION_USER.tsv        \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351574.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351574.conll/CURATION_USER.tsv        \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351576.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351576.conll/CURATION_USER.tsv        \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351578.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351578.conll/CURATION_USER.tsv        \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351579.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351579.conll/CURATION_USER.tsv        \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351581.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351581.conll/CURATION_USER.tsv        \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351582.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351582.conll/CURATION_USER.tsv        \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351595.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351595.conll/CURATION_USER.tsv        \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351607.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351607.conll/CURATION_USER.tsv        \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351610.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351610.conll/CURATION_USER.tsv        \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351611.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351611.conll/CURATION_USER.tsv        \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351612.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351612.conll/CURATION_USER.tsv        \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351615.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351615.conll/CURATION_USER.tsv        \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351617.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351617.conll/CURATION_USER.tsv        \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351627.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351627.conll/CURATION_USER.tsv        \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351632.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351632.conll/CURATION_USER.tsv        \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351635.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351635.conll/CURATION_USER.tsv        \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351636.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351636.conll/CURATION_USER.tsv        \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351642.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351642.conll/CURATION_USER.tsv        \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351645.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351645.conll/CURATION_USER.tsv        \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351647.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351647.conll/CURATION_USER.tsv        \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351649.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351649.conll/CURATION_USER.tsv        \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351650.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351650.conll/CURATION_USER.tsv        \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351651.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351651.conll/CURATION_USER.tsv        \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351652.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351652.conll/CURATION_USER.tsv        \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351653.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351653.conll/CURATION_USER.tsv        \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351672.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351672.conll/CURATION_USER.tsv        \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351700.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351700.conll/CURATION_USER.tsv        \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351719.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351719.conll/CURATION_USER.tsv        \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351749.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351749.conll/CURATION_USER.tsv        \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351778.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351778.conll/CURATION_USER.tsv        \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351809.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351809.conll/CURATION_USER.tsv        \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351814.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351814.conll/CURATION_USER.tsv        \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351815.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351815.conll/CURATION_USER.tsv        \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351817.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351817.conll/CURATION_USER.tsv        \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351820.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351820.conll/CURATION_USER.tsv        \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351831.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351831.conll/CURATION_USER.tsv        \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351834.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351834.conll/CURATION_USER.tsv        \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351837.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351837.conll/CURATION_USER.tsv        \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351839.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351839.conll/CURATION_USER.tsv        \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351841.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351841.conll/CURATION_USER.tsv        \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351846.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351846.conll/CURATION_USER.tsv        \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351848.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351848.conll/CURATION_USER.tsv        \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351849.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351849.conll/CURATION_USER.tsv        \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351851.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351851.conll/CURATION_USER.tsv        \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351852.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351852.conll/CURATION_USER.tsv        \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351853.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351853.conll/CURATION_USER.tsv        \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351856.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351856.conll/CURATION_USER.tsv        \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351858.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351858.conll/CURATION_USER.tsv        \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351861.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351861.conll/CURATION_USER.tsv        \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351864.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351864.conll/CURATION_USER.tsv        \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351887.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351887.conll/CURATION_USER.tsv        \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351888.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351888.conll/CURATION_USER.tsv        \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351923.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351923.conll/CURATION_USER.tsv        \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351931.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv        \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351933.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351933.conll/CURATION_USER.tsv        \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351937.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351937.conll/CURATION_USER.tsv        \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351939.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351939.conll/CURATION_USER.tsv        \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351941.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351941.conll/CURATION_USER.tsv        \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351943.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351943.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351945.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351945.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351946.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351946.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351947.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351947.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351948.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351948.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351949.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351949.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351950.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351950.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351951.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351951.conll/CURATION_USER.tsv        \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351952.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351952.conll/CURATION_USER.tsv        \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351956.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351956.conll/CURATION_USER.tsv        \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351959.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351959.conll/CURATION_USER.tsv        \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351960.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351960.conll/CURATION_USER.tsv        \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351961.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351961.conll/CURATION_USER.tsv        \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351963.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351963.conll/CURATION_USER.tsv        \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351965.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351965.conll/CURATION_USER.tsv        \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351967.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351967.conll/CURATION_USER.tsv        \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351969.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351969.conll/CURATION_USER.tsv        \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351970.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351970.conll/CURATION_USER.tsv        \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351971.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351971.conll/CURATION_USER.tsv        \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351974.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351974.conll/CURATION_USER.tsv        \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351976.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351976.conll/CURATION_USER.tsv        \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351978.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351978.conll/CURATION_USER.tsv        \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351979.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351979.conll/CURATION_USER.tsv        \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351981.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351981.conll/CURATION_USER.tsv        \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351982.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351982.conll/CURATION_USER.tsv        \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351983.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351983.conll/CURATION_USER.tsv        \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351984.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351984.conll/CURATION_USER.tsv        \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351985.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351985.conll/CURATION_USER.tsv        \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351987.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351987.conll/CURATION_USER.tsv        \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351988.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351988.conll/CURATION_USER.tsv        \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351990.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351990.conll/CURATION_USER.tsv        \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351992.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351992.conll/CURATION_USER.tsv        \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351994.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351994.conll/CURATION_USER.tsv        \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23351995.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23351995.conll/CURATION_USER.tsv        \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352656.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352656.conll/CURATION_USER.tsv        \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352659.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352659.conll/CURATION_USER.tsv        \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352662.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352662.conll/CURATION_USER.tsv        \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352663.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352663.conll/CURATION_USER.tsv        \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352665.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352665.conll/CURATION_USER.tsv        \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352671.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352671.conll/CURATION_USER.tsv        \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352674.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352674.conll/CURATION_USER.tsv        \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352675.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352675.conll/CURATION_USER.tsv        \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352676.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352676.conll/CURATION_USER.tsv        \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352677.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352677.conll/CURATION_USER.tsv        \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352681.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352681.conll/CURATION_USER.tsv        \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352682.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352682.conll/CURATION_USER.tsv        \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352683.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352683.conll/CURATION_USER.tsv        \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352684.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352684.conll/CURATION_USER.tsv        \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352686.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352686.conll/CURATION_USER.tsv        \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352687.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352687.conll/CURATION_USER.tsv        \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352690.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352690.conll/CURATION_USER.tsv        \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352693.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352693.conll/CURATION_USER.tsv        \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352695.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352695.conll/CURATION_USER.tsv        \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352696.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352696.conll/CURATION_USER.tsv        \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352701.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352701.conll/CURATION_USER.tsv        \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352702.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352702.conll/CURATION_USER.tsv        \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352704.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352704.conll/CURATION_USER.tsv        \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352706.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352706.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352707.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352707.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352708.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352708.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352710.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352710.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352713.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352713.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352715.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352715.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352717.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352717.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352718.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352718.conll/CURATION_USER.tsv        \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352719.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352719.conll/CURATION_USER.tsv        \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352720.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352720.conll/CURATION_USER.tsv        \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352725.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352725.conll/CURATION_USER.tsv        \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352730.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352730.conll/CURATION_USER.tsv        \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352731.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352731.conll/CURATION_USER.tsv        \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352738.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352738.conll/CURATION_USER (1).tsv     \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352739.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352739.conll/CURATION_USER (1).tsv     \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352743.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352743.conll/CURATION_USER (1).tsv     \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352746.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352746.conll/CURATION_USER (1).tsv     \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352747.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352747.conll/CURATION_USER (1).tsv     \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352748.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv     \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352750.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352750.conll/CURATION_USER (1).tsv     \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352751.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352751.conll/CURATION_USER (1).tsv     \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352752.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352752.conll/CURATION_USER (1).tsv     \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352753.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352753.conll/CURATION_USER (1).tsv     \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352754.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352754.conll/CURATION_USER (1).tsv     \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352755.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352755.conll/CURATION_USER (1).tsv     \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352757.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352757.conll/CURATION_USER (1).tsv     \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352761.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352761.conll/CURATION_USER (1).tsv     \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352765.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352765.conll/CURATION_USER (1).tsv     \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352769.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352769.conll/CURATION_USER (1).tsv     \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352774.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352774.conll/CURATION_USER (1).tsv     \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352777.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352777.conll/CURATION_USER (1).tsv     \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352778.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352778.conll/CURATION_USER (1).tsv     \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352781.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352781.conll/CURATION_USER (1).tsv     \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352785.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352785.conll/CURATION_USER (1).tsv     \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352787.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352787.conll/CURATION_USER (1).tsv     \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352792.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352792.conll/CURATION_USER (1).tsv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352795.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352795.conll/CURATION_USER (1).tsv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352800.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352800.conll/CURATION_USER (1).tsv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352802.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352802.conll/CURATION_USER (1).tsv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352804.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352804.conll/CURATION_USER (1).tsv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352806.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352806.conll/CURATION_USER (1).tsv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352807.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352807.conll/CURATION_USER (1).tsv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352814.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352814.conll/CURATION_USER (1).tsv     \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352816.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv        \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352820.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352820.conll/CURATION_USER (1).tsv     \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352821.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352821.conll/CURATION_USER (1).tsv     \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352822.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352822.conll/CURATION_USER (1).tsv     \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352824.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352824.conll/CURATION_USER (1).tsv     \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352825.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352825.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352829.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352829.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352830.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352830.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352831.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352831.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352844.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352844.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352845.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352845.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352849.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352849.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352853.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352853.conll/CURATION_USER (1).tsv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352856.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352856.conll/CURATION_USER (1).tsv     \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352857.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352857.conll/CURATION_USER (1).tsv     \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352870.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352870.conll/CURATION_USER (1).tsv     \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352871.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352871.conll/CURATION_USER (1).tsv     \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352872.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352872.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352874.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352874.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352876.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352876.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352878.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352878.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352880.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352880.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352883.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352883.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352886.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352886.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352887.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352887.conll/CURATION_USER (1).tsv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352892.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352892.conll/CURATION_USER (1).tsv     \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352894.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352894.conll/CURATION_USER (1).tsv     \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352896.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352896.conll/CURATION_USER (1).tsv     \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352899.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352899.conll/CURATION_USER (1).tsv     \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23352900.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23352900.conll/CURATION_USER (1).tsv     \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353721.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353721.conll/CURATION_USER.tsv        \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353723.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353723.conll/CURATION_USER.tsv        \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353727.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353727.conll/CURATION_USER.tsv        \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353732.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353732.conll/CURATION_USER.tsv        \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353739.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353739.conll/CURATION_USER.tsv        \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353755.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353755.conll/CURATION_USER.tsv        \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353757.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353757.conll/CURATION_USER.tsv        \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353760.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353760.conll/CURATION_USER.tsv        \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353763.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353763.conll/CURATION_USER.tsv        \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353773.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353773.conll/CURATION_USER.tsv        \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353779.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353779.conll/CURATION_USER.tsv        \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353780.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353780.conll/CURATION_USER.tsv        \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353785.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353785.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353786.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353786.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353787.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353787.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353791.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353791.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353794.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353794.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353799.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353799.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353801.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353801.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353824.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353824.conll/CURATION_USER.tsv        \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353825.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353825.conll/CURATION_USER.tsv        \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353830.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353830.conll/CURATION_USER.tsv        \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353834.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353834.conll/CURATION_USER.tsv        \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353838.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353838.conll/CURATION_USER.tsv        \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353840.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353840.conll/CURATION_USER.tsv        \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353841.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353841.conll/CURATION_USER.tsv        \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353842.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353842.conll/CURATION_USER.tsv        \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353846.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353846.conll/CURATION_USER.tsv        \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353849.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353849.conll/CURATION_USER.tsv        \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353857.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353857.conll/CURATION_USER.tsv        \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353860.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353860.conll/CURATION_USER.tsv        \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353861.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353861.conll/CURATION_USER.tsv        \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353863.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353863.conll/CURATION_USER.tsv        \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353864.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353864.conll/CURATION_USER.tsv        \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353867.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353867.conll/CURATION_USER.tsv        \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353872.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353872.conll/CURATION_USER.tsv        \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353874.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353874.conll/CURATION_USER.tsv        \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353878.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353878.conll/CURATION_USER.tsv        \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353891.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353891.conll/CURATION_USER.tsv        \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353901.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353901.conll/CURATION_USER.tsv        \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353904.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353904.conll/CURATION_USER.tsv        \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353913.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv        \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353916.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353916.conll/CURATION_USER.tsv        \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353931.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353931.conll/CURATION_USER.tsv        \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353944.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353944.conll/CURATION_USER.tsv        \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353945.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353945.conll/CURATION_USER.tsv        \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353950.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353950.conll/CURATION_USER.tsv        \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353954.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353954.conll/CURATION_USER.tsv        \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353967.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353967.conll/CURATION_USER.tsv        \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353973.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353973.conll/CURATION_USER.tsv        \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353975.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353975.conll/CURATION_USER.tsv        \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353976.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353976.conll/CURATION_USER.tsv        \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353995.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353995.conll/CURATION_USER.tsv        \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23353996.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23353996.conll/CURATION_USER.tsv        \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354010.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354010.conll/CURATION_USER.tsv        \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354027.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354027.conll/CURATION_USER.tsv        \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354028.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354028.conll/CURATION_USER.tsv        \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354030.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354030.conll/CURATION_USER.tsv        \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354032.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354032.conll/CURATION_USER.tsv        \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354034.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354034.conll/CURATION_USER.tsv        \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354042.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354042.conll/CURATION_USER.tsv        \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354045.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354045.conll/CURATION_USER.tsv        \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354055.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354055.conll/CURATION_USER.tsv        \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354065.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354065.conll/CURATION_USER.tsv        \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354082.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354082.conll/CURATION_USER.tsv        \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354085.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354085.conll/CURATION_USER.tsv        \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354088.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354088.conll/CURATION_USER.tsv        \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354089.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354089.conll/CURATION_USER.tsv        \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354091.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354091.conll/CURATION_USER.tsv        \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354092.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354092.conll/CURATION_USER.tsv        \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354093.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354093.conll/CURATION_USER.tsv        \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354098.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354098.conll/CURATION_USER.tsv        \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354103.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354103.conll/CURATION_USER.tsv        \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354126.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354126.conll/CURATION_USER.tsv        \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354130.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354130.conll/CURATION_USER.tsv        \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354166.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354166.conll/CURATION_USER.tsv        \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354202.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354202.conll/CURATION_USER.tsv        \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354219.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354219.conll/CURATION_USER.tsv        \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354244.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354244.conll/CURATION_USER.tsv        \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354253.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354253.conll/CURATION_USER.tsv        \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354265.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354265.conll/CURATION_USER.tsv        \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354285.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354285.conll/CURATION_USER.tsv        \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354288.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354288.conll/CURATION_USER.tsv        \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354310.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354310.conll/CURATION_USER.tsv        \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354318.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354318.conll/CURATION_USER.tsv        \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354320.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354320.conll/CURATION_USER.tsv        \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354336.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354336.conll/CURATION_USER.tsv        \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354396.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354396.conll/CURATION_USER.tsv        \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354400.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354400.conll/CURATION_USER.tsv        \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354442.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354442.conll/CURATION_USER.tsv        \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354450.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354450.conll/CURATION_USER.tsv        \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354460.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354460.conll/CURATION_USER.tsv        \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354474.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354474.conll/CURATION_USER.tsv        \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354516.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354516.conll/CURATION_USER.tsv        \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354536.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354536.conll/CURATION_USER.tsv        \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354538.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354538.conll/CURATION_USER.tsv        \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354544.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354544.conll/CURATION_USER.tsv        \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354545.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354545.conll/CURATION_USER.tsv        \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354575.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354575.conll/CURATION_USER.tsv        \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354619.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354619.conll/CURATION_USER.tsv        \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354627.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354627.conll/CURATION_USER.tsv        \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354648.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354648.conll/CURATION_USER.tsv        \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354656.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354656.conll/CURATION_USER.tsv        \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354695.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354695.conll/CURATION_USER.tsv        \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354697.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354697.conll/CURATION_USER.tsv        \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354698.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354698.conll/CURATION_USER.tsv        \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354699.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354699.conll/CURATION_USER.tsv        \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354717.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354717.conll/CURATION_USER.tsv        \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354718.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354718.conll/CURATION_USER.tsv        \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354719.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354719.conll/CURATION_USER.tsv        \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354738.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354738.conll/CURATION_USER.tsv        \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354739.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354739.conll/CURATION_USER.tsv        \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354751.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354751.conll/CURATION_USER.tsv        \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354780.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354780.conll/CURATION_USER.tsv        \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354793.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354793.conll/CURATION_USER.tsv        \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354796.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354796.conll/CURATION_USER.tsv        \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354803.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354803.conll/CURATION_USER.tsv        \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354816.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354816.conll/CURATION_USER.tsv        \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354831.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354831.conll/CURATION_USER.tsv        \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354879.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354879.conll/CURATION_USER.tsv        \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354880.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354880.conll/CURATION_USER.tsv        \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354881.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354881.conll/CURATION_USER.tsv        \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354910.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354910.conll/CURATION_USER.tsv        \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354912.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354912.conll/CURATION_USER.tsv        \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354916.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354916.conll/CURATION_USER.tsv        \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354920.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354920.conll/CURATION_USER.tsv        \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354935.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354935.conll/CURATION_USER.tsv        \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354944.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354944.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354946.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354946.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354953.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354953.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354956.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354956.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354977.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354977.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23354982.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23354982.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355001.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355001.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355040.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355040.conll/CURATION_USER.tsv        \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355061.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355061.conll/CURATION_USER.tsv        \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355064.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355064.conll/CURATION_USER.tsv        \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355095.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355095.conll/CURATION_USER.tsv        \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355132.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355132.conll/CURATION_USER.tsv        \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355228.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355228.conll/CURATION_USER.tsv        \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355250.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355250.conll/CURATION_USER.tsv        \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355254.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355254.conll/CURATION_USER.tsv        \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355290.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv        \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355416.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355416.conll/CURATION_USER.tsv        \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355434.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355434.conll/CURATION_USER.tsv        \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355470.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355470.conll/CURATION_USER.tsv        \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355557.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355557.conll/CURATION_USER.tsv        \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355571.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355571.conll/CURATION_USER.tsv        \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355626.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355626.conll/CURATION_USER.tsv        \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355656.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355656.conll/CURATION_USER.tsv        \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355773.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355773.conll/CURATION_USER.tsv        \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355817.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355817.conll/CURATION_USER.tsv        \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355858.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355858.conll/CURATION_USER.tsv        \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355917.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355917.conll/CURATION_USER.tsv        \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355935.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355935.conll/CURATION_USER.tsv        \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23355988.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23355988.conll/CURATION_USER.tsv        \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356093.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356093.conll/CURATION_USER.tsv        \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356193.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356193.conll/CURATION_USER.tsv        \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356205.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356205.conll/CURATION_USER.tsv        \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356221.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356221.conll/CURATION_USER.tsv        \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356245.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv        \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356247.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356247.conll/CURATION_USER.tsv        \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356295.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356295.conll/CURATION_USER.tsv        \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356299.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356299.conll/CURATION_USER.tsv        \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356314.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356314.conll/CURATION_USER.tsv        \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356315.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356315.conll/CURATION_USER.tsv        \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356329.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356329.conll/CURATION_USER.tsv        \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356339.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356339.conll/CURATION_USER.tsv        \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356494.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356494.conll/CURATION_USER.tsv        \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356505.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356505.conll/CURATION_USER.tsv        \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356511.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356511.conll/CURATION_USER.tsv        \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356574.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356574.conll/CURATION_USER.tsv        \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356604.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356604.conll/CURATION_USER.tsv        \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356622.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356622.conll/CURATION_USER.tsv        \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356624.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356624.conll/CURATION_USER.tsv        \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356638.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356638.conll/CURATION_USER.tsv        \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356715.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356715.conll/CURATION_USER.tsv        \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356716.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356716.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356724.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356724.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356731.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356731.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356745.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356745.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356765.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356765.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356767.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356767.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356771.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356771.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356782.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356782.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356793.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356793.conll/CURATION_USER.tsv        \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356798.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356798.conll/CURATION_USER.tsv        \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356858.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356858.conll/CURATION_USER.tsv        \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356874.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356874.conll/CURATION_USER.tsv        \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356885.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356885.conll/CURATION_USER.tsv        \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356887.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356887.conll/CURATION_USER.tsv        \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356902.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356902.conll/CURATION_USER.tsv        \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356906.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356906.conll/CURATION_USER.tsv        \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356907.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356907.conll/CURATION_USER.tsv        \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356915.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356915.conll/CURATION_USER.tsv        \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356918.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356918.conll/CURATION_USER.tsv        \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356933.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356933.conll/CURATION_USER.tsv        \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356960.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356960.conll/CURATION_USER.tsv        \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356961.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356961.conll/CURATION_USER.tsv        \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356992.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356992.conll/CURATION_USER.tsv        \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356993.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356993.conll/CURATION_USER.tsv        \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23356998.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23356998.conll/CURATION_USER.tsv        \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357000.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357000.conll/CURATION_USER.tsv        \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357028.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357028.conll/CURATION_USER.tsv        \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357037.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357037.conll/CURATION_USER.tsv        \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357062.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357062.conll/CURATION_USER.tsv        \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357063.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357063.conll/CURATION_USER.tsv        \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357081.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357081.conll/CURATION_USER.tsv        \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357094.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357094.conll/CURATION_USER.tsv        \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357095.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357095.conll/CURATION_USER.tsv        \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357097.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357097.conll/CURATION_USER.tsv        \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357120.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357120.conll/CURATION_USER.tsv        \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357135.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357135.conll/CURATION_USER.tsv        \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357151.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357151.conll/CURATION_USER.tsv        \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357167.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357167.conll/CURATION_USER.tsv        \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357190.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357190.conll/CURATION_USER.tsv        \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357233.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357233.conll/CURATION_USER.tsv        \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357240.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357240.conll/CURATION_USER.tsv        \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357258.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357258.conll/CURATION_USER.tsv        \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357263.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357263.conll/CURATION_USER.tsv        \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357266.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357266.conll/CURATION_USER.tsv        \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357288.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357288.conll/CURATION_USER.tsv        \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357308.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357308.conll/CURATION_USER.tsv        \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357309.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357309.conll/CURATION_USER.tsv        \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357329.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357329.conll/CURATION_USER.tsv        \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357336.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357336.conll/CURATION_USER.tsv        \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357341.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357341.conll/CURATION_USER.tsv        \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357344.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357344.conll/CURATION_USER.tsv        \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357389.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357389.conll/CURATION_USER.tsv        \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357394.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357394.conll/CURATION_USER.tsv        \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357396.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357396.conll/CURATION_USER.tsv        \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357443.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357443.conll/CURATION_USER.tsv        \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357457.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357457.conll/CURATION_USER.tsv        \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357471.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357471.conll/CURATION_USER.tsv        \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357489.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357489.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357491.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357491.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357534.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357534.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357544.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357544.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357550.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357550.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357652.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357652.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357711.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357711.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357741.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357741.conll/CURATION_USER.tsv        \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357752.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357752.conll/CURATION_USER.tsv        \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357765.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357765.conll/CURATION_USER.tsv        \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357779.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357779.conll/CURATION_USER.tsv        \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357809.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357809.conll/CURATION_USER.tsv        \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357851.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357851.conll/CURATION_USER.tsv        \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357897.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357897.conll/CURATION_USER.tsv        \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357937.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357937.conll/CURATION_USER.tsv        \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23357994.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23357994.conll/CURATION_USER.tsv        \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23358011.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23358011.conll/CURATION_USER.tsv        \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23358086.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23358086.conll/CURATION_USER.tsv        \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23358097.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23358097.conll/CURATION_USER.tsv        \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23358104.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23358104.conll/CURATION_USER.tsv        \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23358261.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23358261.conll/CURATION_USER.tsv        \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23366716.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23366716.conll/CURATION_USER.tsv        \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23366722.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23366722.conll/CURATION_USER.tsv        \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23366740.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23366740.conll/CURATION_USER.tsv        \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23366751.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23366751.conll/CURATION_USER.tsv        \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Creating    VLSP2020_RE_training/23366765.conll                       OK\n",
            "Extracting  VLSP2020_RE_training/23366765.conll/CURATION_USER.tsv        \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3ICier0JyzI"
      },
      "source": [
        "## Install Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcS3CiYZxv8a"
      },
      "source": [
        "### Install VNCoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzsgsAb4uET6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26b6824-9310-448e-c064-870c27997d9f"
      },
      "source": [
        "# Install the vncorenlp python wrapper\n",
        "!pip install vncorenlp==1.0.3"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vncorenlp==1.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n",
            "\r\u001b[K     |                               | 10kB 16.4MB/s eta 0:00:01\r\u001b[K     |                               | 20kB 21.6MB/s eta 0:00:01\r\u001b[K     |                               | 30kB 25.5MB/s eta 0:00:01\r\u001b[K     |                               | 40kB 23.4MB/s eta 0:00:01\r\u001b[K     |                               | 51kB 14.2MB/s eta 0:00:01\r\u001b[K     |                               | 61kB 16.1MB/s eta 0:00:01\r\u001b[K     |                               | 71kB 13.4MB/s eta 0:00:01\r\u001b[K     |                               | 81kB 14.2MB/s eta 0:00:01\r\u001b[K     |                              | 92kB 12.3MB/s eta 0:00:01\r\u001b[K     |                              | 102kB 11.8MB/s eta 0:00:01\r\u001b[K     |                              | 112kB 11.8MB/s eta 0:00:01\r\u001b[K     |                              | 122kB 11.8MB/s eta 0:00:01\r\u001b[K     |                              | 133kB 11.8MB/s eta 0:00:01\r\u001b[K     |                              | 143kB 11.8MB/s eta 0:00:01\r\u001b[K     |                              | 153kB 11.8MB/s eta 0:00:01\r\u001b[K     |                              | 163kB 11.8MB/s eta 0:00:01\r\u001b[K     |                              | 174kB 11.8MB/s eta 0:00:01\r\u001b[K     |                             | 184kB 11.8MB/s eta 0:00:01\r\u001b[K     |                             | 194kB 11.8MB/s eta 0:00:01\r\u001b[K     |                             | 204kB 11.8MB/s eta 0:00:01\r\u001b[K     |                             | 215kB 11.8MB/s eta 0:00:01\r\u001b[K     |                             | 225kB 11.8MB/s eta 0:00:01\r\u001b[K     |                             | 235kB 11.8MB/s eta 0:00:01\r\u001b[K     |                             | 245kB 11.8MB/s eta 0:00:01\r\u001b[K     |                             | 256kB 11.8MB/s eta 0:00:01\r\u001b[K     |                            | 266kB 11.8MB/s eta 0:00:01\r\u001b[K     |                            | 276kB 11.8MB/s eta 0:00:01\r\u001b[K     |                            | 286kB 11.8MB/s eta 0:00:01\r\u001b[K     |                            | 296kB 11.8MB/s eta 0:00:01\r\u001b[K     |                            | 307kB 11.8MB/s eta 0:00:01\r\u001b[K     |                            | 317kB 11.8MB/s eta 0:00:01\r\u001b[K     |                            | 327kB 11.8MB/s eta 0:00:01\r\u001b[K     |                            | 337kB 11.8MB/s eta 0:00:01\r\u001b[K     |                           | 348kB 11.8MB/s eta 0:00:01\r\u001b[K     |                           | 358kB 11.8MB/s eta 0:00:01\r\u001b[K     |                           | 368kB 11.8MB/s eta 0:00:01\r\u001b[K     |                           | 378kB 11.8MB/s eta 0:00:01\r\u001b[K     |                           | 389kB 11.8MB/s eta 0:00:01\r\u001b[K     |                           | 399kB 11.8MB/s eta 0:00:01\r\u001b[K     |                           | 409kB 11.8MB/s eta 0:00:01\r\u001b[K     |                           | 419kB 11.8MB/s eta 0:00:01\r\u001b[K     |                          | 430kB 11.8MB/s eta 0:00:01\r\u001b[K     |                          | 440kB 11.8MB/s eta 0:00:01\r\u001b[K     |                          | 450kB 11.8MB/s eta 0:00:01\r\u001b[K     |                          | 460kB 11.8MB/s eta 0:00:01\r\u001b[K     |                          | 471kB 11.8MB/s eta 0:00:01\r\u001b[K     |                          | 481kB 11.8MB/s eta 0:00:01\r\u001b[K     |                          | 491kB 11.8MB/s eta 0:00:01\r\u001b[K     |                          | 501kB 11.8MB/s eta 0:00:01\r\u001b[K     |                         | 512kB 11.8MB/s eta 0:00:01\r\u001b[K     |                         | 522kB 11.8MB/s eta 0:00:01\r\u001b[K     |                         | 532kB 11.8MB/s eta 0:00:01\r\u001b[K     |                         | 542kB 11.8MB/s eta 0:00:01\r\u001b[K     |                         | 552kB 11.8MB/s eta 0:00:01\r\u001b[K     |                         | 563kB 11.8MB/s eta 0:00:01\r\u001b[K     |                         | 573kB 11.8MB/s eta 0:00:01\r\u001b[K     |                         | 583kB 11.8MB/s eta 0:00:01\r\u001b[K     |                        | 593kB 11.8MB/s eta 0:00:01\r\u001b[K     |                        | 604kB 11.8MB/s eta 0:00:01\r\u001b[K     |                        | 614kB 11.8MB/s eta 0:00:01\r\u001b[K     |                        | 624kB 11.8MB/s eta 0:00:01\r\u001b[K     |                        | 634kB 11.8MB/s eta 0:00:01\r\u001b[K     |                        | 645kB 11.8MB/s eta 0:00:01\r\u001b[K     |                        | 655kB 11.8MB/s eta 0:00:01\r\u001b[K     |                        | 665kB 11.8MB/s eta 0:00:01\r\u001b[K     |                       | 675kB 11.8MB/s eta 0:00:01\r\u001b[K     |                       | 686kB 11.8MB/s eta 0:00:01\r\u001b[K     |                       | 696kB 11.8MB/s eta 0:00:01\r\u001b[K     |                       | 706kB 11.8MB/s eta 0:00:01\r\u001b[K     |                       | 716kB 11.8MB/s eta 0:00:01\r\u001b[K     |                       | 727kB 11.8MB/s eta 0:00:01\r\u001b[K     |                       | 737kB 11.8MB/s eta 0:00:01\r\u001b[K     |                       | 747kB 11.8MB/s eta 0:00:01\r\u001b[K     |                      | 757kB 11.8MB/s eta 0:00:01\r\u001b[K     |                      | 768kB 11.8MB/s eta 0:00:01\r\u001b[K     |                      | 778kB 11.8MB/s eta 0:00:01\r\u001b[K     |                      | 788kB 11.8MB/s eta 0:00:01\r\u001b[K     |                      | 798kB 11.8MB/s eta 0:00:01\r\u001b[K     |                      | 808kB 11.8MB/s eta 0:00:01\r\u001b[K     |                      | 819kB 11.8MB/s eta 0:00:01\r\u001b[K     |                      | 829kB 11.8MB/s eta 0:00:01\r\u001b[K     |                     | 839kB 11.8MB/s eta 0:00:01\r\u001b[K     |                     | 849kB 11.8MB/s eta 0:00:01\r\u001b[K     |                     | 860kB 11.8MB/s eta 0:00:01\r\u001b[K     |                     | 870kB 11.8MB/s eta 0:00:01\r\u001b[K     |                     | 880kB 11.8MB/s eta 0:00:01\r\u001b[K     |                     | 890kB 11.8MB/s eta 0:00:01\r\u001b[K     |                     | 901kB 11.8MB/s eta 0:00:01\r\u001b[K     |                     | 911kB 11.8MB/s eta 0:00:01\r\u001b[K     |                    | 921kB 11.8MB/s eta 0:00:01\r\u001b[K     |                    | 931kB 11.8MB/s eta 0:00:01\r\u001b[K     |                    | 942kB 11.8MB/s eta 0:00:01\r\u001b[K     |                    | 952kB 11.8MB/s eta 0:00:01\r\u001b[K     |                    | 962kB 11.8MB/s eta 0:00:01\r\u001b[K     |                    | 972kB 11.8MB/s eta 0:00:01\r\u001b[K     |                    | 983kB 11.8MB/s eta 0:00:01\r\u001b[K     |                    | 993kB 11.8MB/s eta 0:00:01\r\u001b[K     |                   | 1.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |                   | 1.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |                   | 1.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |                   | 1.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |                   | 1.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |                   | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |                   | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |                   | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |                  | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |                  | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |                | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |               | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |               | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |               | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |               | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |               | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |               | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |               | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |               | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |              | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |              | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |              | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |              | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |              | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |              | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |              | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |              | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |             | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |             | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |             | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |             | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |             | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |             | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |             | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |             | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |            | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |           | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |          | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |         | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |         | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |         | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |         | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |         | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |         | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |         | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |         | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |        | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |        | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |        | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |        | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |        | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |        | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |        | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |        | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |       | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |       | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |       | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |       | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |       | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |       | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |       | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |       | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |      | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |      | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |      | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |      | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |      | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |      | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |      | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |      | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |      | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |     | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |    | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |   | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |   | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |   | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |   | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |   | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |   | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |   | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |   | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |  | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |  | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |  | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |  | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |  | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |  | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |  | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |  | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     | | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     | | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     | | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     | | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     | | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     | | 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     | | 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     | | 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     || 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     || 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     || 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     || 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     || 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     || 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     || 2.6MB 11.8MB/s eta 0:00:01\r\u001b[K     || 2.7MB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp==1.0.3) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp==1.0.3) (2.10)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp37-none-any.whl size=2645936 sha256=ecd7fe32877ad0d0958286383980044d627959154b0ba1c030c8becba66c6fbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxVE9cR6yZ3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e20bd25-ff1d-45ce-9fcb-88e4d32ceb3d"
      },
      "source": [
        "# Download VnCoreNLP-1.1.1.jar & all of its  component (i.e. RDRSegmenter, pos, ner, deprel) \n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!mkdir -p vncorenlp/models/dep\n",
        "!mkdir -p vncorenlp/models/ner\n",
        "!mkdir -p vncorenlp/models/postagger\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/dep/vi-dep.xz\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger\n",
        "\n",
        "\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "\n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/\n",
        "\n",
        "!mv vi-dep.xz vncorenlp/models/dep/\n",
        "\n",
        "!mv vi-500brownclusters.xz vncorenlp/models/ner/\n",
        "!mv vi-ner.xz vncorenlp/models/ner/\n",
        "!mv vi-pretrainedembeddings.xz vncorenlp/models/ner/\n",
        "\n",
        "!mv vi-tagger vncorenlp/models/postagger/\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-09 12:15:00--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: VnCoreNLP-1.1.1.jar\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-09 12:15:01 (260 MB/s) - VnCoreNLP-1.1.1.jar saved [27412575/27412575]\n",
            "\n",
            "--2021-05-09 12:15:01--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: vi-vocab\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-05-09 12:15:01 (28.5 MB/s) - vi-vocab saved [526544/526544]\n",
            "\n",
            "--2021-05-09 12:15:01--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: wordsegmenter.rdr\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2021-05-09 12:15:01 (38.1 MB/s) - wordsegmenter.rdr saved [128508/128508]\n",
            "\n",
            "--2021-05-09 12:15:01--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/dep/vi-dep.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16048864 (15M) [application/octet-stream]\n",
            "Saving to: vi-dep.xz\n",
            "\n",
            "vi-dep.xz           100%[===================>]  15.30M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-09 12:15:02 (123 MB/s) - vi-dep.xz saved [16048864/16048864]\n",
            "\n",
            "--2021-05-09 12:15:02--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5599844 (5.3M) [application/octet-stream]\n",
            "Saving to: vi-500brownclusters.xz\n",
            "\n",
            "vi-500brownclusters 100%[===================>]   5.34M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-05-09 12:15:02 (78.0 MB/s) - vi-500brownclusters.xz saved [5599844/5599844]\n",
            "\n",
            "--2021-05-09 12:15:02--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9956876 (9.5M) [application/octet-stream]\n",
            "Saving to: vi-ner.xz\n",
            "\n",
            "vi-ner.xz           100%[===================>]   9.50M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-05-09 12:15:02 (121 MB/s) - vi-ner.xz saved [9956876/9956876]\n",
            "\n",
            "--2021-05-09 12:15:02--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 57313672 (55M) [application/octet-stream]\n",
            "Saving to: vi-pretrainedembeddings.xz\n",
            "\n",
            "vi-pretrainedembedd 100%[===================>]  54.66M   231MB/s    in 0.2s    \n",
            "\n",
            "2021-05-09 12:15:03 (231 MB/s) - vi-pretrainedembeddings.xz saved [57313672/57313672]\n",
            "\n",
            "--2021-05-09 12:15:03--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29709468 (28M) [application/octet-stream]\n",
            "Saving to: vi-tagger\n",
            "\n",
            "vi-tagger           100%[===================>]  28.33M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-09 12:15:03 (192 MB/s) - vi-tagger saved [29709468/29709468]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXfVgT46BB-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e57f7d40-232b-4bef-a935-4e9b153420d7"
      },
      "source": [
        "import unicodedata\n",
        "from vncorenlp import VnCoreNLP\n",
        "\n",
        "# To perform word segmentation, POS tagging, NER and then dependency parsing\n",
        "annotator1 = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx2g') \n",
        "\n",
        "# To perform word segmentation, POS tagging and then NER\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg,pos,ner\", max_heap_size='-Xmx2g') \n",
        "# To perform word segmentation and then POS tagging\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg,pos\", max_heap_size='-Xmx2g') \n",
        "# To perform word segmentation only\n",
        "# annotator = VnCoreNLP(\"<FULL-PATH-to-VnCoreNLP-jar-file>\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
        "# Input \n",
        "text = unicodedata.normalize(\"NFD\", \"Thanh Thy\")\n",
        "\n",
        "\n",
        "# To perform word segmentation only\n",
        "word_segmented_text = annotator1.tokenize(text) \n",
        "\n",
        "print(*word_segmented_text, sep=\"\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Thanh', 'Thuy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhOuiCNgaVqk"
      },
      "source": [
        "### Install Underthesea"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IujpzlPaKfo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b34da190-e565-491c-b165-3819972bcfe0"
      },
      "source": [
        "!pip install underthesea==1.2.3"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting underthesea==1.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/46/1acb7e83092bbcbc9082afe3901ec51e98a303a19c8152655c43bd51583f/underthesea-1.2.3-py3-none-any.whl (7.5MB)\n",
            "\u001b[K     || 7.5MB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (3.2.5)\n",
            "Collecting scikit-learn<0.22,>=0.20\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/c5/e5267eb84994e9a92a2c6a6ee768514f255d036f3c8378acfa694e9f2c99/scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     || 6.7MB 46.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (3.13)\n",
            "Collecting python-crfsuite>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     || 747kB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (7.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (0.8.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea==1.2.3) (4.41.1)\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     || 51kB 6.5MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     || 245kB 54.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea==1.2.3) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.22,>=0.20->underthesea==1.2.3) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.22,>=0.20->underthesea==1.2.3) (1.19.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea==1.2.3) (3.0.4)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=1e38e7c80ddbbf8c109a484b0986a592fa91290fd9036d45f5e0e2df28213688\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: scikit-learn, python-crfsuite, seqeval, unidecode, underthesea\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed python-crfsuite-0.9.7 scikit-learn-0.21.3 seqeval-1.2.2 underthesea-1.2.3 unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjsyeDFRKpfF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d50b9f-89f2-4d3c-82be-e67d853fa017"
      },
      "source": [
        "from underthesea import sent_tokenize\n",
        "text = 'Qung Bnh : Ct tc lng hnh, him ha rnh rp cu Long i v dng sng? Hin tng khai thc ct lu trn sng cch cu Long i vi trm mt v pha h lu, khin cu v sng Long i ang ng trc him ha kh lng? Va qua Php lut Plus nhn phn nh ca nhng ngi dn sng  x Xun Ninh , huyn Qung Ninh , tnh Qung Bnh v vic hin nay  x ny, c th l ti thn Xun Dc 1 khu vc ven sng Long i lu ny xut hin nhng bi tp kt ct tri php v hin tng khai thc ct lu trn sng c ngy ln m gy nh hng n n cuc sng thng nht ca ngi dn ni y. T nhng ngun tin nu trn sng ngy 21/9, PV  tip cn hin trng on sng Long i thuc thn Xun D 1 , x Xun Ninh , huyn Qung Ninh , tnh Qung Bnh ni ngi dn phn nh  cng c thng tin. Ti y, PV nhn thy nhiu bi tp kt ct gn khu vc dn c sinh sng, hng ngy nhiu tu ch ct vo y  tp kt ct, gy ra ting n kh chu nh hng khng nh n sinh hot ca ngi dn. Nhng bi tp kt ct tri php. Hn th na theo tm hiu ca PV c bit, nhng v tr c bi tp kt ct k trn khng  tiu chun  tu cp bn tp kt?. Tra cng ngy PV  i theo hng thng ngun sng Long i m theo phn nh l xy ra tnh trng khai thc ct tri php thng xuyn din ra. PV nhn thy mt chic thuyn ang neo u cch b chng vi chc mt v cch mng cu Long i chng vi trm mt theo hng h ngun ang ht ct ln thuyn. Chic thuyn ( ) ang khai thc ct tri php cch cu Long i khng xa. Tip tc ghi nhn v theo di v vic khong chng hn 30 pht, chic thuyn  y ct  c i chuyn i tp kt. Chic thuyn sau khi ht ct tri php di chuyn v bi tp kt. Qua tm hiu ca PV c bit, ct  khu vc gn cu Long i l ct nhim mn nu dng vo vic thi cng cng trnh s nh hng n cht lng ca cng trnh  V ct ny c ch thuyn bn li cho ngi s dng vi gi r hn so vi ct c khai thc  m c cp php gy nn s cnh tranh khng lnh mnh v gi ct. Tuy nhin nhiu ngi dn cha nhn thy n vic cht lng ca cng trnh sau ny khi s dng ct nhim mn ny. iu ng ni l vic khai thc ct tri php li din ra khu vc gn mng cu Long i (c ng st ln ng b) nguy c sc l t khu vc mng cu, khin cu Long i ng trc him ha kh lng?. Lin quan n vn  ny, trao i vi PV ng Nguyn Trng Tin  Ch tch x Xun Ninh cho bit v pha x cng  nhiu ln x l nhc nh ngi dn trong vn  tp kt ct ng ni quy nh. Ngoi ra, x ang hng dn v hon thnh cc th tc nhm a cc im tp kt tri php ny ng vo ni quy nh trong thi gian sm nht, ng Tin cho bit thm. ng Nguyn Trng Tin  Ch tch x Xun Ninh (bn phi) ti bui lm vic vi PV. Khi c PV cng cp bng chng v vic thuyn khai thc ct tri php ngay gia ban ngy gn khu vc mng cu Long i , ng Tin  ht sc bt ng ni Nh vy l khng c ri, khng c ri s cho x l ngay Tip  PV lin lc qua in thoi vi ng Phm Trung ng  Ch tch UBND huyn Qung Ninh  phn nh s vic th ng ng cho bit, ang bn v hng dn PV lin h vi ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh  lm vic. Ti bui l vic vi ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh PV  cung cp clip v vic nn khai thc tri php din ra ngay trn sng Long i on gn mng cu ng Giai cng  kin quyt v ha s u tranh x l, ng thi phi hp vi cc c quan chc nng khc thng xuyn kim tra  chm dt tnh trng ny. ng Nguyn Vit Giai cho bit s u tranh x l Cn v vic cc bi tp kt tri php, ng Giai cho bit s x l dt im trong thi gian sm nht  khng nh hng ti cuc sng ngi dn xung quanh. Khi c PV hi thi gian sm nht l bao lu ng Giai cho bit:  y ang cn vng mt khu th tc. thi gian gii quyt sm nht cng phi mt chng 7 n 10 ngy. Vic khai thc ct tri php gn cu Long i (c ng st ln ng b) nguy c st l t khu vc mng cu, khin cu Long i ng trc him ha kh lng? Tuy l vy nhng trong sng 22/9, PV mt ln na n ti hin trng chic thuyn khai thc tri php th nhn thy tnh hnh khai thc ct tri php vn khng h thay i. Mt ln na PV  gi in thoi cho ng Nguyn Trng Tin  Ch tch x Xun Ninh v ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh  phn nh th li c 2 v ha s x l. Trong sng 22/9 vic khai thc ct tri php vn din ra m khng c s can thip ca c quyan chc nng? T nhng vic nu trn, d lun khng th khng t ra cu hi liu nhng vic xy ra  y c phi l c s bo k hoc c s tip tay ca lc lng chc nng c thm thm quyn hay khng? Php lut Plus s tip tc thng tin v vic n bn c.'\n",
        "sent_tokenize(text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Qung Bnh : Ct tc lng hnh, him ha rnh rp cu Long i v dng sng?',\n",
              " 'Hin tng khai thc ct lu trn sng cch cu Long i vi trm mt v pha h lu, khin cu v sng Long i ang ng trc him ha kh lng?',\n",
              " 'Va qua Php lut Plus nhn phn nh ca nhng ngi dn sng  x Xun Ninh , huyn Qung Ninh , tnh Qung Bnh v vic hin nay  x ny, c th l ti thn Xun Dc 1 khu vc ven sng Long i lu ny xut hin nhng bi tp kt ct tri php v hin tng khai thc ct lu trn sng c ngy ln m gy nh hng n n cuc sng thng nht ca ngi dn ni y.',\n",
              " 'T nhng ngun tin nu trn sng ngy 21/9, PV  tip cn hin trng on sng Long i thuc thn Xun D 1 , x Xun Ninh , huyn Qung Ninh , tnh Qung Bnh ni ngi dn phn nh  cng c thng tin.',\n",
              " 'Ti y, PV nhn thy nhiu bi tp kt ct gn khu vc dn c sinh sng, hng ngy nhiu tu ch ct vo y  tp kt ct, gy ra ting n kh chu nh hng khng nh n sinh hot ca ngi dn.',\n",
              " 'Nhng bi tp kt ct tri php.',\n",
              " 'Hn th na theo tm hiu ca PV c bit, nhng v tr c bi tp kt ct k trn khng  tiu chun  tu cp bn tp kt?.',\n",
              " 'Tra cng ngy PV  i theo hng thng ngun sng Long i m theo phn nh l xy ra tnh trng khai thc ct tri php thng xuyn din ra.',\n",
              " 'PV nhn thy mt chic thuyn ang neo u cch b chng vi chc mt v cch mng cu Long i chng vi trm mt theo hng h ngun ang ht ct ln thuyn.',\n",
              " 'Chic thuyn ( ) ang khai thc ct tri php cch cu Long i khng xa.',\n",
              " 'Tip tc ghi nhn v theo di v vic khong chng hn 30 pht, chic thuyn  y ct  c i chuyn i tp kt.',\n",
              " 'Chic thuyn sau khi ht ct tri php di chuyn v bi tp kt.',\n",
              " 'Qua tm hiu ca PV c bit, ct  khu vc gn cu Long i l ct nhim mn nu dng vo vic thi cng cng trnh s nh hng n cht lng ca cng trnh  V ct ny c ch thuyn bn li cho ngi s dng vi gi r hn so vi ct c khai thc  m c cp php gy nn s cnh tranh khng lnh mnh v gi ct.',\n",
              " 'Tuy nhin nhiu ngi dn cha nhn thy n vic cht lng ca cng trnh sau ny khi s dng ct nhim mn ny.',\n",
              " 'iu ng ni l vic khai thc ct tri php li din ra khu vc gn mng cu Long i (c ng st ln ng b) nguy c sc l t khu vc mng cu, khin cu Long i ng trc him ha kh lng?.',\n",
              " 'Lin quan n vn  ny, trao i vi PV ng Nguyn Trng Tin  Ch tch x Xun Ninh cho bit v pha x cng  nhiu ln x l nhc nh ngi dn trong vn  tp kt ct ng ni quy nh.',\n",
              " 'Ngoi ra, x ang hng dn v hon thnh cc th tc nhm a cc im tp kt tri php ny ng vo ni quy nh trong thi gian sm nht, ng Tin cho bit thm.',\n",
              " 'ng Nguyn Trng Tin  Ch tch x Xun Ninh (bn phi) ti bui lm vic vi PV.',\n",
              " 'Khi c PV cng cp bng chng v vic thuyn khai thc ct tri php ngay gia ban ngy gn khu vc mng cu Long i , ng Tin  ht sc bt ng ni Nh vy l khng c ri, khng c ri s cho x l ngay Tip  PV lin lc qua in thoi vi ng Phm Trung ng  Ch tch UBND huyn Qung Ninh  phn nh s vic th ng ng cho bit, ang bn v hng dn PV lin h vi ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh  lm vic.',\n",
              " 'Ti bui l vic vi ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh PV  cung cp clip v vic nn khai thc tri php din ra ngay trn sng Long i on gn mng cu ng Giai cng  kin quyt v ha s u tranh x l, ng thi phi hp vi cc c quan chc nng khc thng xuyn kim tra  chm dt tnh trng ny.',\n",
              " 'ng Nguyn Vit Giai cho bit s u tranh x l Cn v vic cc bi tp kt tri php, ng Giai cho bit s x l dt im trong thi gian sm nht  khng nh hng ti cuc sng ngi dn xung quanh.',\n",
              " 'Khi c PV hi thi gian sm nht l bao lu ng Giai cho bit:  y ang cn vng mt khu th tc.',\n",
              " 'thi gian gii quyt sm nht cng phi mt chng 7 n 10 ngy. Vic khai thc ct tri php gn cu Long i (c ng st ln ng b) nguy c st l t khu vc mng cu, khin cu Long i ng trc him ha kh lng?',\n",
              " 'Tuy l vy nhng trong sng 22/9, PV mt ln na n ti hin trng chic thuyn khai thc tri php th nhn thy tnh hnh khai thc ct tri php vn khng h thay i.',\n",
              " 'Mt ln na PV  gi in thoi cho ng Nguyn Trng Tin  Ch tch x Xun Ninh v ng Nguyn Vit Giai - Trng Phng Ti nguyn mi trng huyn Qung Ninh  phn nh th li c 2 v ha s x l.',\n",
              " 'Trong sng 22/9 vic khai thc ct tri php vn din ra m khng c s can thip ca c quyan chc nng?',\n",
              " 'T nhng vic nu trn, d lun khng th khng t ra cu hi liu nhng vic xy ra  y c phi l c s bo k hoc c s tip tay ca lc lng chc nng c thm thm quyn hay khng?',\n",
              " 'Php lut Plus s tip tc thng tin v vic n bn c.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chNxSZ0mx5wS"
      },
      "source": [
        "# Extract raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phjF0edvNGpc"
      },
      "source": [
        "import os\n",
        "import re"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pTphYOiMYho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e786bb7-f88c-43ad-f81f-32e3bfae7f0a"
      },
      "source": [
        "# get all subfolers and files in subfolers\n",
        "# [(\"top_subfolders\", [subfolders_in_top_subfolders], [files_in_top_subfolders])]\n",
        "sub_folders = [f for f in os.walk(\"VLSP2020_RE_training\")][1:]\n",
        "sub_folders = sorted(sub_folders, key=lambda x: x[0])   # sort by top_subfolder name\n",
        "\n",
        "## top subfolder contain only 1 single file.\n",
        "check = False\n",
        "for i in sub_folders:\n",
        "    if i[1] or len(i[-1])!= 1:\n",
        "        print(\"ALERT!!!\")\n",
        "        check = True\n",
        "\n",
        "if not check:\n",
        "    print(\"There is \", len(sub_folders), \" subfolders. All subfolders contain only 1 file.\",\n",
        "          \" So that we have \", len(sub_folders), \" files.\")\n",
        "\n",
        "# generate data files name\n",
        "files_path = [os.path.join(i[0], i[-1][0]) for i in sub_folders]\n",
        "\n",
        "# print(*files_path, sep=\"\\n\")\n",
        "\n",
        "# print(files_path)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is  506  subfolders. All subfolders contain only 1 file.  So that we have  506  files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LJk2k0vD0dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37164169-9fca-4519-8202-c15cd5298267"
      },
      "source": [
        "# Xem trong b d liu c nhng character g\n",
        "\n",
        "character_lst = []\n",
        "for file in files_path:\n",
        "    with open(file, mode='r') as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "        # find line start with \"#Text=\"\n",
        "        textline_id = []\n",
        "        for i, text in enumerate(lines):\n",
        "            if (\"#Text=\" == text[0:6]):\n",
        "                textline_id.append(i)\n",
        "\n",
        "        # every data file has only one line that start with \"#Text=\"\"\n",
        "        assert (len(textline_id) == 1), str(\"1 is not number of line start with #Text=. \\nDoc: \" + file)\n",
        "\n",
        "        for c in lines[textline_id[0]][6:]:\n",
        "            if c not in character_lst:\n",
        "                character_lst.append(c)\n",
        "\n",
        "\n",
        "# Print all of the single characters, 30 per row.\n",
        "# For every batch of 30 tokens...\n",
        "for i in range(0, len(character_lst), 30):\n",
        "    \n",
        "    # Limit the end index so we don't go past the end of the list.\n",
        "    end = min(i + 30, len(character_lst) + 1)\n",
        "    \n",
        "    # Print out the tokens, separated by a space.\n",
        "    print(repr(' '.join(character_lst[i:end])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'C  u   c h y  n l  m t p  i    g q   3  a X  B d'\n",
            "'  s     ,    o r    v   k      .  T  '\n",
            "' G D &  N          x e  b       H   L S'\n",
            "'     V  P  :   2 ;   ( )  Q ? 1 / 9  0  K U -'\n",
            "'7 M  R W w  O I 5 % 6 4 !  8 A f  \" F  E z     J j'\n",
            "\"' \\ufeff Z  \\xa0 _ Y           +         |   \"\n",
            "' >    @     $ *  < = #       [ ]      '\n",
            "'          '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugs-iTcldECs"
      },
      "source": [
        "constant = {\"entity_name\": [\"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"MISCELLANEOUS\"],\n",
        "          \"relation_name\": [\"LOCATED\", \"PART  WHOLE\", \"AFFILIATION\", \"PERSONAL - SOCIAL\"]\n",
        "           }"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rqndrEtimOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b4786bd-5bbb-460e-dc59-437295ccabd8"
      },
      "source": [
        "\"\"\"\n",
        "            dict = {\"doc_id\": id of folder contain doc, \n",
        "                      \"text\": doc, \n",
        "                 \"token_ids\": [                                        tokens_id, ...], \n",
        "              \"subtoken_ids\": [                                   None or sub-id, ...],\n",
        "                       \"pos\": [                             [start pos, end pos], ...],\n",
        "                    \"tokens\": [                                      tokens_text, ...],\n",
        "                    \"entity\": [                        [entity_ids, entity_name], ...],\n",
        "                  \"relation\": [    [relation, stoken_id, ssubtoken_id direction], ...]\n",
        "                   }\n",
        "\n",
        "\n",
        "                      doc_id: id of folder contain doc                                            (str)\n",
        "                         doc: doc. line start with \"#Text=\"                                       (str)\n",
        "                   token_ids: ids of tokens.                                    first column       (list int)\n",
        "                subtoken_ids: int if crr token is a subtoken, otherwise None    first column      (list int, None)\n",
        "                         pos: posittion of tokens.                              second column     (list list int) \n",
        "                              [\n",
        "                                  [start pos, end pos],\n",
        "                                  ...\n",
        "                              ]\n",
        "                       token: tokens.                                           third column      (list str)\n",
        "                      entity: entity infor if token is entity, else None.       4th, 5th column   (list list, None)\n",
        "                              [\n",
        "                                  [entity_id, entity_name],\n",
        "                                  ...\n",
        "                              ]\n",
        "                    relation: relation if token is in a relation, else None.    other column      (list list, None)\n",
        "                              [\n",
        "                                  [[relation1_name, relation1_start_tokenID, relation1_start_subtokenID, [start_entity_id, end_entity_id]], ...],  \n",
        "                                                                         --> relation1_start_subtokenID may be None if start token is not a sub token\n",
        "                                  [[relation1_name, relation1_start_tokenID, relation1_start_subtokenID,                             None], ...],  <-- dataset has mistake. Don't have direction.\n",
        "                                                                                 \n",
        "                                  \n",
        "                                  ....\n",
        "                              ]\n",
        "                    \n",
        "\"\"\"\n",
        "\n",
        "raw_tdata = []\n",
        "\n",
        "for file in files_path:\n",
        "    docif = {}\n",
        "    with open(file, mode='r') as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "        # example: VLSP2020_RE_training/23351113.conll/CURATION_USER.tsv -> 23351113\n",
        "        docif[\"doc_id\"] = file[(file.find(\"/\") + 1): file.find(\".\")]\n",
        "\n",
        "        # find line start with \"#Text=\"\n",
        "        textline_id = []\n",
        "        for i, text in enumerate(lines):\n",
        "            if (\"#Text=\" == text[0:6]):\n",
        "                textline_id.append(i)\n",
        "\n",
        "        # every data file has only one line that start with \"#Text=\"\"\n",
        "        assert (len(textline_id) == 1), str(\"1 is not number of line start with #Text=. \\nDoc: \" + file)\n",
        "\n",
        "        docif[\"text\"] = lines[textline_id[0]][6:]\n",
        "\n",
        "        first_cline = lines[(textline_id[0] + 1)].rstrip(\"\\t\").split(\"\\t\")   # first column_line\n",
        "        assert (len(first_cline) in [3, 5, 7, 8]), str(\"Doc has problem. doc: \" + file)\n",
        "\n",
        "\n",
        "        token_ids, subtoken_ids, pos, tokens = [], [], [], []\n",
        "        entity = []\n",
        "        relation = []\n",
        "\n",
        "        pretk_id = 0\n",
        "\n",
        "        for tk_id, line in enumerate(lines[(textline_id[0] + 1):]):\n",
        "            lineif = line.rstrip(\"\\t\").split(\"\\t\")   # seperate by one \\t between columns: [abc\\txyz\\t]\n",
        "\n",
        "            # check if columns is seperated by only one single Tab character '\\t'\n",
        "            lineif1 = re.split(r'\\t+', line.rstrip('\\t'))   # seperate by all \\t between column: [abc\\t\\t\\txyz\\t]\n",
        "            assert (lineif == lineif1), str(\"Columns is not seperated by only one single TAB '\\\\t'. doc: \" + file + \" line: \" + line)\n",
        "\n",
        "            # check if inside a doc, only exist one number of (no) columns\n",
        "            # above we check if len(lineif) in [3, 5, 7, 8], too. so we can make sure that\n",
        "            # in a doc, number of columns only in [3, 5, 7, 8]\n",
        "            # and all line in a doc has same no columns\n",
        "            assert len(lineif) == len(first_cline), str(\"Number of columns in doc is not consistent. \\nDoc: \" + file + \" line: \" + line)\n",
        "\n",
        "\n",
        "            # remove all \"_\" in lineif because we don't need it\n",
        "            # [3, 5, 7, 8] -> [3, 4, 5, 7]\n",
        "            # and all data has first three column. (4th and 5th) is a pair, (6th and 7th) is a pair\n",
        "            # after removing all \"_\", if:\n",
        "            # len(lineif) = 3 -> token_ids, pos, no entity, no relation\n",
        "            # len(lineif) = 5 -> token_ids, pos, entity, no relation\n",
        "            # len(lineif) = 7 -> token_ids, pos, entity, relation\n",
        "\n",
        "            # len(lineif) = 4 --> token_ids, pos, no entity, no relation (this is a mistake in dataset, in data file has 8 columns)\n",
        "\n",
        "            lineif = [col for col in lineif if col != \"_\"]\n",
        "\n",
        "            assert (len(lineif) in [3, 4, 5, 7]), str(\"Problem with number of columns after remove \\'_\\'.\\nIn doc: \" + file + \" line \" + line)\n",
        "\n",
        "            # match first column format\n",
        "            # startwith (\"1-\") then (number) end:   1-id\n",
        "            pattern_token_ids = re.compile(\"^(1-)([\\d]+)$\")\n",
        "\n",
        "            # a token may has many subtokens\n",
        "            # startwith (\"1-\") then (number) then (. char) then (number) end:   1-id.subid \n",
        "            # pattern_subtoken_ids = re.compile(\"^(1-)([\\d]+)(\\.)([\\d]+)$\")\n",
        "\n",
        "            # Currently in train dataset, number of subtoken of a token is 0 or 1\n",
        "            # startwith (\"1-\") then (number) then (.1) end:   1-id.1 \n",
        "            pattern_subtoken_ids = re.compile(\"^(1-)([\\d]+)(\\.1)$\")\n",
        "\n",
        "            assert (pattern_token_ids.match(lineif[0]) or pattern_subtoken_ids.match(lineif[0])), \\\n",
        "            str(\"Unexpected first column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "            if pattern_token_ids.match(lineif[0]):\n",
        "                # 1-id\n",
        "                # Check if token id is increased by one in each line or not.\n",
        "                assert (int(lineif[0][2:]) == (pretk_id + 1)), str(\"First column, Token_ID is not increased by one in each line. \\nIn doc: \" + file + \" \\nline: \" + line)\n",
        "\n",
        "                token_ids.append(int(lineif[0][2:]))\n",
        "                subtoken_ids.append(None)   # Not a subtoken\n",
        "\n",
        "                pretk_id += 1\n",
        "            \n",
        "            else:\n",
        "                # 1-id.1\n",
        "                # 1-id.subid\n",
        "                tmp = lineif[0].find(\".\")\n",
        "                tokenID = int(lineif[0][2:tmp])\n",
        "                subtokenID = int(lineif[0][(tmp+1):])\n",
        "                \n",
        "                assert (tokenID == token_ids[-1]), str(\"Exist subtoken without a token before it. \\nIn doc: \" + file + \" \\nline\" + line)\n",
        "\n",
        "                token_ids.append(tokenID)\n",
        "                subtoken_ids.append(subtokenID)\n",
        "\n",
        "                print(\"\\nTHERE IS A SUBTOKEN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "\n",
        "            # match second column format\n",
        "            # startwith (number) then (\"-\" char) then (number) end\n",
        "            pattern_pos = re.compile(\"^([\\d]+)(\\-)([\\d]+)$\")\n",
        "            assert (pattern_pos.match(lineif[1])), str(\"Unexpected second column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "            pos.append([int(ele) for ele in lineif[1].split(\"-\")])    # example: \"3-6\" -> [3, 6]\n",
        "            \n",
        "            # if current token is a subtoken, check if pos subtoken is inside father token or not.\n",
        "            if pattern_subtoken_ids.match(lineif[0]):\n",
        "                father_token = token_ids.index(token_ids[-1])\n",
        "\n",
        "                assert (pos[father_token][0] <= pos[-1][0]) and (pos[-1][1] <= pos[father_token][1]), \\\n",
        "                str(\"Subtoken\\'s position is not inside father token\\'s position. \\nIndoc: \" + file + \"\\Line: \" + line)\n",
        "\n",
        "\n",
        "            # third column\n",
        "            #check if token is matched with pos (second column) or not\n",
        "            crr_token_pos = [int(ele) for ele in lineif[1].split(\"-\")]\n",
        "            if lineif[2] == lines[textline_id[0]][6:][crr_token_pos[0]:crr_token_pos[1]]:\n",
        "                tokens.append(lineif[2])\n",
        "            else:\n",
        "                assert False, str(\"Token in 3th column not match with position at 2th column. \\nIn doc: \" + file + \" \\nline: \" + line)\n",
        "            \n",
        "\n",
        "            if (len(lineif) == 3) or (len(lineif) == 4):\n",
        "                entity.append(None)\n",
        "                relation.append(None)\n",
        "\n",
        "                if len(lineif) == 4:\n",
        "                    print(\"\\n4 COLUMNS.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "\n",
        "            # because we removed all \"_\", \n",
        "            # so when len(lineif) = 5 or len(lineif) = 7, this line must has: token_ids, pos, tokens and entity.\n",
        "            # (we don't have to check if 4th, 5th column is \"_\" anymore, since we removed all \"_\")\n",
        "            if (len(lineif) == 5) or (len(lineif) == 7):\n",
        "                # 4th column now only have two posibilities: \"*\" or \"*[number]\"\n",
        "                pattern_entity_id = re.compile(\"^(\\*)(\\[)([\\d]+)(\\])$\")\n",
        "                assert ((lineif[3] == \"*\") or pattern_entity_id.match(lineif[3])), str(\"Unexpected fourth column's format. In doc: \" + file + \" line \" + line)\n",
        "\n",
        "                # in doc: 23352816\n",
        "                # line: 1-23\t126-136\t</ENAMEX>)\t*\t*\t_\t_\t_\t\n",
        "                # there is a mistake in 5th column. Unknow enity name\n",
        "                # I will let this token entity is None.\n",
        "\n",
        "                # We can just let all token entity is None\n",
        "                # if 5th column is not in constant[\"entity_name\"]\n",
        "                # but below, I just code for this specific case\n",
        "                # because I want to know more about dataset\n",
        "\n",
        "                if (lineif[3] == \"*\"):\n",
        "\n",
        "                    if (lineif[4] == \"*\"):   # specific mistake case\n",
        "                        entity.append(None)\n",
        "                        print(\"\\nENTITY NAME MISTAKE IN 5TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                    else:\n",
        "                        assert (lineif[4] in constant[\"entity_name\"]), str(\"Unknown entity name. \\nDoc: \" + file + \" \\nline \" + line)\n",
        "\n",
        "                        entity_id = 0\n",
        "                        entity_n = lineif[4]\n",
        "\n",
        "                        entity.append([entity_id, entity_n])\n",
        "                \n",
        "                elif pattern_entity_id.match(lineif[3]):\n",
        "                    # *[number]: *[26] -> 26\n",
        "                    entity_id = int(lineif[3][2:-1])\n",
        "                    \n",
        "                    # PERSON[26]\n",
        "                    tmp = lineif[4].find(\"[\")\n",
        "                    \n",
        "                    assert (entity_id == int(lineif[4][(tmp+1):-1])), str(\"Entity ID in 4th and 5th column are not the same. In doc: \" + file + \" line \" + line)\n",
        "                    \n",
        "                    assert (lineif[4][:tmp] in constant[\"entity_name\"]), str(\"Unknown entity name in doc: \" + file + \" line \" + line)\n",
        "                    \n",
        "                    entity_n = lineif[4][:tmp]\n",
        "\n",
        "                    entity.append([entity_id, entity_n])\n",
        "\n",
        "                # may be we dont need this last else because we use regex above\n",
        "                else:\n",
        "                    assert False, str(\"4th, 5th column has UNKNOWN MISTAKE. In Doc: \" + file + \"\\nline: \" + line)\n",
        "\n",
        "\n",
        "\n",
        "            if len(lineif) == 5:\n",
        "                relation.append(None)\n",
        "            \n",
        "\n",
        "            if len(lineif) == 7:\n",
        "                # example:\n",
        "                # AFFILIATION\t1-593[13_14]\n",
        "                # PART  WHOLE\t1-42[1_2]\n",
        "                # PERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-80[3_8]|1-105[7_8]\n",
        "\n",
        "                # PART  WHOLE\t1-42    (an error in dataset that need to be handled)\n",
        "\n",
        "                rel_names = lineif[5].split(\"|\")    # PERSONAL - SOCIAL|PERSONAL - SOCIAL --> [\"PERSONAL - SOCIAL\", \"PERSONAL - SOCIAL\"]\n",
        "                rel_oifs = lineif[6].split(\"|\")     # 1-80[3_8]|1-105[7_8] --> [\"1-80[3_8]\", \"1-105[7_8]\"]\n",
        "\n",
        "                # in doc: 23351515\n",
        "                # line: 1-318\n",
        "                # 6th column: PART  WHOLE|LOCATED|PART  WHOLE|*\n",
        "                # last relation name is: *  -> mistake\n",
        "                # We can read data and change it to right one \n",
        "                # but I will remove this \"*\" relation in 6th and 7th column\n",
        "\n",
        "                if '*' in rel_names:\n",
        "                    tmp = rel_names.index('*')\n",
        "\n",
        "                    del rel_names[tmp]\n",
        "                    del rel_oifs[tmp]\n",
        "\n",
        "                    print(\"\\nRELATION MISTAKE IN 6TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                # MISTAKE In doc: 23351856\n",
        "                # line: 1-185\t807-812\tTriu\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
        "                # assert ((len(rel_names) == len(rel_oifs)) and (len(rel_names) >= 1)), str(\"Number of relations in 6th and 7th columns is different to each other. In doc: \" + file + \" line \" + line )\n",
        "                # handle later\n",
        "\n",
        "                assert (len(rel_names) == len(rel_oifs)), str(\"Number of relations in 6th and 7th columns is different to each other. \\nIn doc: \" + file + \" \\nline \" + line )\n",
        "\n",
        "\n",
        "                rels = []\n",
        "                for i in range(len(rel_names)):\n",
        "                    assert (rel_names[i] in constant[\"relation_name\"]), \\\n",
        "                    str(\"Unknown relation_name in doc: \" + file + \" \\nline \" + line)\n",
        "                    \n",
        "                    relation_n = rel_names[i]\n",
        "\n",
        "                    # (startwith \"1-\") then (number) then ([ char) then (number) then (_ char) then (number) then (] char) end\n",
        "                    #             1-         26            [             3             _             0             ]   \n",
        "                    pattern_relation_oif = re.compile(\"^(1-)([\\d]+)(\\[)([\\d]+)(\\_)([\\d]+)(\\])$\")\n",
        "\n",
        "                    # (startwith \"1-\") then (number) then (.1) then ([ char) then (number) then (_ char) then (number) then (] char) end\n",
        "                    #             1-         26            .1        [             3             _             0             ]   \n",
        "                    pattern_relation_oif_1 = re.compile(\"^(1-)([\\d]+)(\\.1)(\\[)([\\d]+)(\\_)([\\d]+)(\\])$\")\n",
        "                    \n",
        "                    # below is a mistake in dataset\n",
        "                    # but currently, this type mistake has only below form (only has token id).\n",
        "                    # (don't have subtoken id mistake type, yet)\n",
        "                    # (startwith \"1-\") then (number)  end\n",
        "                    #             1-         26          \n",
        "                    pattern_relation_oif_mistake_1 = re.compile(\"^(1-)([\\d]+)$\")\n",
        "\n",
        "\n",
        "\n",
        "                    assert (pattern_relation_oif.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_1.match(rel_oifs[i]) \\\n",
        "                            or pattern_relation_oif_mistake_1.match(rel_oifs[i])), \\\n",
        "                            str(\"Unexpected seventh column's format. \\nIn doc: \" + file + \" \\nline \" + line)\n",
        "                    \n",
        "\n",
        "                    # NOTICE:\n",
        "                    # IN BELOW CODE, I DONT CHECK IF ONE OF TWO ENTITIES OF A RELATION\n",
        "                    # IS \"MISCELLANEOUS\" OR NOT. \n",
        "                    # MISCELLANEOUS IS A LEGIT ENTITY NAME, BUT IT IS NOT USED IN ANY RELATION TYPE.\n",
        "                    # I WONDER IF DATASET HAS THIS MISTAKE OR NOT.\n",
        "                    # I WILL CHECK IT WHEN I CREATE SENTENCES AS INPUT OF BERT.\n",
        "\n",
        "\n",
        "                    if pattern_relation_oif.match(rel_oifs[i]):\n",
        "                        # 1-id[id_id]\n",
        "\n",
        "                        tmp_stkid = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_etkid = rel_oifs[i].find(\"[\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp_stkid:tmp_etkid]\n",
        "\n",
        "                        # start subtoken id\n",
        "                        sstoken_id = None\n",
        "\n",
        "                        direction = rel_oifs[i][(tmp_etkid+1):-1]\n",
        "\n",
        "                        direction = direction.split(\"_\")   # [sentity_id, eentity_id]\n",
        "\n",
        "                        rels.append([relation_n, int(stoken_id), sstoken_id, [int(direction[0]), int(direction[1])]])\n",
        "\n",
        "                    elif pattern_relation_oif_1.match(rel_oifs[i]):\n",
        "                        # 1-id.subid[id_id]\n",
        "\n",
        "                        tmp_sid = rel_oifs[i].find(\"-\") + 1\n",
        "                        tmp_eid = rel_oifs[i].find(\".\")\n",
        "\n",
        "                        tmp_ssid = rel_oifs[i].find(\".\") + 1\n",
        "                        tmp_esid = rel_oifs[i].find(\"[\")\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp_sid:tmp_eid]\n",
        "\n",
        "                        sstoken_id = rel_oifs[i][tmp_ssid:tmp_esid]\n",
        "\n",
        "\n",
        "                        direction = rel_oifs[i][(tmp_esid+1):-1]\n",
        "                        direction = direction.split(\"_\")\n",
        "\n",
        "                        rels.append([relation_n, int(stoken_id), int(sstoken_id), [int(direction[0]), int(direction[1])]])\n",
        "\n",
        "                        print(\"\\nSPECIAL SUBTOKEN IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "                    \n",
        "                    else:\n",
        "                        # 1-id\n",
        "                        tmp = rel_oifs[i].find(\"-\") + 1\n",
        "\n",
        "                        stoken_id = rel_oifs[i][tmp:]\n",
        "                        \n",
        "                        sstoken_id = None\n",
        "                        direction = None\n",
        "\n",
        "                        # rels.append([relation_n, stoken_id, sstoken_id, direction])\n",
        "                        rels.append([relation_n, int(stoken_id), None, None])\n",
        "\n",
        "                        print(\"\\nMISTAKE IN 7TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                \n",
        "                if len(rels) == 0:\n",
        "                    # MISTAKE In doc: 23351856\n",
        "                    # line: 1-185\t807-812\tTriu\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
        "                    relation.append(None)\n",
        "                    print(\"\\nREALTION NAME MISTAKE IN 6TH COLUMN.\\nDOC: \", file, \"\\nLine: \", line)\n",
        "\n",
        "                else:\n",
        "                    relation.append(rels)\n",
        "\n",
        "\n",
        "        docif[\"token_ids\"] = token_ids\n",
        "        docif[\"subtoken_ids\"] = subtoken_ids\n",
        "        docif[\"pos\"] = pos\n",
        "        docif[\"tokens\"] = tokens\n",
        "        docif[\"entity\"] = entity\n",
        "        docif[\"relation\"] = relation\n",
        "\n",
        "    raw_tdata.append(docif)\n",
        "\n",
        "\n",
        "print(len(raw_tdata))           \n",
        "                \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351316.conll/CURATION_USER.tsv \n",
            "Line:  1-651\t2931-2939\tAlphabet\t*\tORGANIZATION\tPART  WHOLE\t1-649\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351316.conll/CURATION_USER.tsv \n",
            "Line:  1-687\t3105-3109\tLyft\t*\tORGANIZATION\tAFFILIATION\t1-683\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351316.conll/CURATION_USER.tsv \n",
            "Line:  1-726\t3275-3276\t\t*\tLOCATION\tLOCATED\t1-728\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351394.conll/CURATION_USER.tsv \n",
            "Line:  1-103\t459-464\tValve\t*\tORGANIZATION\tAFFILIATION\t1-106\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351426.conll/CURATION_USER.tsv \n",
            "Line:  1-299\t1355-1361\tH.T.H.\t*\tPERSON\tPERSONAL - SOCIAL\t1-291\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351426.conll/CURATION_USER.tsv \n",
            "Line:  1-318\t1438-1444\tNguyn\t*\tPERSON\tPERSONAL - SOCIAL\t1-313\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351430.conll/CURATION_USER.tsv \n",
            "Line:  1-762\t3550-3555\tTPHCM\t*\tLOCATION\tLOCATED\t1-766\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23351433.conll/CURATION_USER.tsv \n",
            "Line:  1-583.1\t2570-2573\tTp\t*[19]\tORGANIZATION[19]\t_\t_\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351515.conll/CURATION_USER.tsv \n",
            "Line:  1-318\t1372-1376\ttnh\t*[19]\tLOCATION[19]\tPART  WHOLE|LOCATED|PART  WHOLE|*\t1-315[18_19]|1-301[15_19]|1-307[16_19]|1-310[17_19]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351516.conll/CURATION_USER.tsv \n",
            "Line:  1-423\t1851-1853\tM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-421|1-418[17_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351519.conll/CURATION_USER.tsv \n",
            "Line:  1-221\t972-979\t(TP.HCM\t*\tLOCATION\tLOCATED|LOCATED\t1-212[12_0]|1-189\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351519.conll/CURATION_USER.tsv \n",
            "Line:  1-231\t1015-1019\tHng\t*\tPERSON\tPERSONAL - SOCIAL\t1-229\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351519.conll/CURATION_USER.tsv \n",
            "Line:  1-349\t1543-1546\tHi\t*\tPERSON\tPERSONAL - SOCIAL\t1-347\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351519.conll/CURATION_USER.tsv \n",
            "Line:  1-485\t2141-2144\tHi\t*\tPERSON\tPERSONAL - SOCIAL\t1-476\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351519.conll/CURATION_USER.tsv \n",
            "Line:  1-662\t2946-2949\tHi\t*\tPERSON\tPERSONAL - SOCIAL\t1-659\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351522.conll/CURATION_USER.tsv \n",
            "Line:  1-40\t186-190\tReal\t*\tORGANIZATION\tAFFILIATION\t1-33\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351556.conll/CURATION_USER.tsv \n",
            "Line:  1-119\t534-540\tLondon\t*\tLOCATION\tLOCATED|LOCATED\t1-121|1-123[8_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351556.conll/CURATION_USER.tsv \n",
            "Line:  1-495\t2256-2263\tWembley\t*\tLOCATION\tLOCATED\t1-488\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351564.conll/CURATION_USER.tsv \n",
            "Line:  1-567\t2526-2528\tH.\t*\tPERSON\tPERSONAL - SOCIAL\t1-579\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351566.conll/CURATION_USER.tsv \n",
            "Line:  1-461\t2163-2174\tTechcombank\t*\tORGANIZATION\tAFFILIATION\t1-455\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351566.conll/CURATION_USER.tsv \n",
            "Line:  1-488\t2295-2306\tTechcombank\t*\tORGANIZATION\tAFFILIATION\t1-478\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351566.conll/CURATION_USER.tsv \n",
            "Line:  1-583\t2734-2740\tTP.HCM\t*\tLOCATION\tLOCATED\t1-579\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23351610.conll/CURATION_USER.tsv \n",
            "Line:  1-201.1\t906-912\ttrng\t*[10]\tORGANIZATION[10]\t_\t_\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351612.conll/CURATION_USER.tsv \n",
            "Line:  1-39\t197-202\t(Php\t*\tLOCATION\tLOCATED\t1-38\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351627.conll/CURATION_USER.tsv \n",
            "Line:  1-88\t409-413\tMack\t*\tPERSON\tPERSONAL - SOCIAL\t1-79\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351651.conll/CURATION_USER.tsv \n",
            "Line:  1-214\t984-986\tM\t*\tLOCATION\tAFFILIATION\t1-215\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351672.conll/CURATION_USER.tsv \n",
            "Line:  1-465\t2123-2130\tChelsea\t*\tORGANIZATION\tAFFILIATION|AFFILIATION|AFFILIATION\t1-462|1-460|1-457[13_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351672.conll/CURATION_USER.tsv \n",
            "Line:  1-465\t2123-2130\tChelsea\t*\tORGANIZATION\tAFFILIATION|AFFILIATION|AFFILIATION\t1-462|1-460|1-457[13_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351841.conll/CURATION_USER.tsv \n",
            "Line:  1-108\t473-479\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED\t1-106|1-101[2_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351856.conll/CURATION_USER.tsv \n",
            "Line:  1-142\t611-616\tSeoul\t*\tLOCATION\tAFFILIATION\t1-121\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351856.conll/CURATION_USER.tsv \n",
            "Line:  1-185\t807-812\tTriu\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351856.conll/CURATION_USER.tsv \n",
            "Line:  1-185\t807-812\tTriu\t*[13]\tLOCATION[13]\t*\t1-198[0_13]\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351856.conll/CURATION_USER.tsv \n",
            "Line:  1-201\t883-887\tBnh\t*[14]\tLOCATION[14]\t*\t1-198[0_14]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351856.conll/CURATION_USER.tsv \n",
            "Line:  1-201\t883-887\tBnh\t*[14]\tLOCATION[14]\t*\t1-198[0_14]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351887.conll/CURATION_USER.tsv \n",
            "Line:  1-514\t2332-2337\tASEAN\t*\tORGANIZATION\tAFFILIATION\t1-510\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv \n",
            "Line:  1-624\t2833-2842\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-632\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv \n",
            "Line:  1-648\t2941-2950\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-646\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv \n",
            "Line:  1-669\t3040-3043\tPVN\t*\tORGANIZATION\tAFFILIATION\t1-671\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv \n",
            "Line:  1-698\t3170-3179\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-692\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv \n",
            "Line:  1-739\t3347-3350\tPVN\t*\tORGANIZATION\tAFFILIATION\t1-734\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv \n",
            "Line:  1-836\t3779-3788\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-830\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351931.conll/CURATION_USER.tsv \n",
            "Line:  1-844\t3817-3820\tPVN\t*\tORGANIZATION\tAFFILIATION\t1-830\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351945.conll/CURATION_USER.tsv \n",
            "Line:  1-67\t310-316\tTP.HCM\t*\tLOCATION\tAFFILIATION\t1-58\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351945.conll/CURATION_USER.tsv \n",
            "Line:  1-101\t465-471\tPrague\t*\tLOCATION\tLOCATED\t1-91\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23351945.conll/CURATION_USER.tsv \n",
            "Line:  1-466.1\t2183-2188\tChris\t*\tPERSON\t_\t_\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351951.conll/CURATION_USER.tsv \n",
            "Line:  1-49\t263-268\tGabon\t*\tLOCATION\tAFFILIATION\t1-55\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351965.conll/CURATION_USER.tsv \n",
            "Line:  1-320\t1419-1427\tBrussels\t*\tLOCATION\tPART  WHOLE\t1-321\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351970.conll/CURATION_USER.tsv \n",
            "Line:  1-118\t537-542\tASEAN\t*\tORGANIZATION\tAFFILIATION|PART  WHOLE\t1-102|1-99[8_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351978.conll/CURATION_USER.tsv \n",
            "Line:  1-406\t1763-1767\tTho\t*\tPERSON\tPERSONAL - SOCIAL\t1-404\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351981.conll/CURATION_USER.tsv \n",
            "Line:  1-168\t782-784\tM\t*\tLOCATION\tAFFILIATION\t1-145\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23351984.conll/CURATION_USER.tsv \n",
            "Line:  1-610.1\t2837-2839\tc\t*\tLOCATION\tLOCATED\t1-599\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351984.conll/CURATION_USER.tsv \n",
            "Line:  1-610.1\t2837-2839\tc\t*\tLOCATION\tLOCATED\t1-599\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351985.conll/CURATION_USER.tsv \n",
            "Line:  1-331\t1448-1451\tMai\t*\tPERSON\tPERSONAL - SOCIAL\t1-323\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351988.conll/CURATION_USER.tsv \n",
            "Line:  1-309\t1340-1344\tTho\t*\tPERSON\tPERSONAL - SOCIAL\t1-297\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351992.conll/CURATION_USER.tsv \n",
            "Line:  1-296\t1313-1317\tNHNN\t*\tORGANIZATION\tPART  WHOLE\t1-288\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351992.conll/CURATION_USER.tsv \n",
            "Line:  1-471\t2111-2120\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-479\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351992.conll/CURATION_USER.tsv \n",
            "Line:  1-542\t2427-2436\tOceanBank\t*\tORGANIZATION\tAFFILIATION\t1-538\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351992.conll/CURATION_USER.tsv \n",
            "Line:  1-548\t2455-2458\tPVN\t*\tORGANIZATION\tAFFILIATION\t1-538\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23351994.conll/CURATION_USER.tsv \n",
            "Line:  1-393\t1708-1713\tTISCO\t*\tORGANIZATION\tAFFILIATION\t1-391\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352662.conll/CURATION_USER.tsv \n",
            "Line:  1-432\t1954-1959\tBarca\t*\tORGANIZATION\tAFFILIATION\t1-430\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352671.conll/CURATION_USER.tsv \n",
            "Line:  1-384\t1818-1821\tUAE\t*\tLOCATION\tPART  WHOLE\t1-374\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352690.conll/CURATION_USER.tsv \n",
            "Line:  1-112\t515-520\tSyria\t*\tLOCATION\tLOCATED\t1-111\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352690.conll/CURATION_USER.tsv \n",
            "Line:  1-807\t3604-3609\tSyria\t*\tLOCATION\tLOCATED|LOCATED\t1-805|1-803\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352690.conll/CURATION_USER.tsv \n",
            "Line:  1-807\t3604-3609\tSyria\t*\tLOCATION\tLOCATED|LOCATED\t1-805|1-803\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352696.conll/CURATION_USER.tsv \n",
            "Line:  1-129\t592-600\tAtletico\t*\tORGANIZATION\tAFFILIATION\t1-137\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352696.conll/CURATION_USER.tsv \n",
            "Line:  1-480\t2138-2146\tValencia\t*\tORGANIZATION\tAFFILIATION\t1-495\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352696.conll/CURATION_USER.tsv \n",
            "Line:  1-541\t2424-2429\tBarca\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-535|1-533\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352696.conll/CURATION_USER.tsv \n",
            "Line:  1-541\t2424-2429\tBarca\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-535|1-533\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352696.conll/CURATION_USER.tsv \n",
            "Line:  1-547\t2449-2456\tSevilla\t*\tORGANIZATION\tAFFILIATION\t1-543\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352696.conll/CURATION_USER.tsv \n",
            "Line:  1-549\t2459-2467\tValencia\t*\tORGANIZATION\tAFFILIATION\t1-552\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352701.conll/CURATION_USER.tsv \n",
            "Line:  1-162\t744-754\t(Indonesia\t*\tLOCATION\tPART  WHOLE\t1-161\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352701.conll/CURATION_USER.tsv \n",
            "Line:  1-329\t1519-1523\tHAGL\t*\tORGANIZATION\tAFFILIATION\t1-324\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352702.conll/CURATION_USER.tsv \n",
            "Line:  1-197\t928-931\tVn\t*\tPERSON\tPERSONAL - SOCIAL\t1-193\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352702.conll/CURATION_USER.tsv \n",
            "Line:  1-252\t1171-1174\tLm\t*\tPERSON\tPERSONAL - SOCIAL\t1-250\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352719.conll/CURATION_USER.tsv \n",
            "Line:  1-585\t2580-2586\tDalton\t*\tPERSON\tPERSONAL - SOCIAL\t1-582\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352730.conll/CURATION_USER.tsv \n",
            "Line:  1-129\t563-565\to\t*\tLOCATION\tPART  WHOLE\t1-127\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352730.conll/CURATION_USER.tsv \n",
            "Line:  1-164\t715-722\tNamibia\t*\tLOCATION\tPART  WHOLE\t1-162\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352730.conll/CURATION_USER.tsv \n",
            "Line:  1-333\t1491-1503\tPennsylvania\t*\tLOCATION\tPART  WHOLE\t1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352730.conll/CURATION_USER.tsv \n",
            "Line:  1-335\t1506-1508\tM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-333|1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352730.conll/CURATION_USER.tsv \n",
            "Line:  1-335\t1506-1508\tM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-333|1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352730.conll/CURATION_USER.tsv \n",
            "Line:  1-509\t2334-2338\tPhp\t*\tLOCATION\tPART  WHOLE\t1-507\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352738.conll/CURATION_USER (1).tsv \n",
            "Line:  1-222\t1062-1070\tDamascus\t*\tLOCATION\tLOCATED|LOCATED\t1-238|1-236[3_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352738.conll/CURATION_USER (1).tsv \n",
            "Line:  1-547\t2598-2606\tDamascus\t*\tLOCATION\tLOCATED|LOCATED\t1-561[7_0]|1-563\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-545\t2518-2521\tAnh\t*\tLOCATION\tLOCATED\t1-547\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-680\t3150-3159\tBarcelona\t*\tORGANIZATION\tAFFILIATION\t1-673\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-686\t3181-3190\tLiverpool\t*\tORGANIZATION\tAFFILIATION\t1-673\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-688\t3193-3201\tValencia\t*\tORGANIZATION\tAFFILIATION\t1-673\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-772\t3607-3622\tgmEstudiantes\t*\tORGANIZATION\tAFFILIATION\t1-756\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-774\t3625-3638\tIndependiente\t*\tORGANIZATION\tAFFILIATION\t1-756\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-775\t3639-3648\tvAlavs\t*\tORGANIZATION\tAFFILIATION\t1-756\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352748.conll/CURATION_USER (1).tsv \n",
            "Line:  1-838\t3945-3948\tAnh\t*\tLOCATION\tLOCATED\t1-822\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352751.conll/CURATION_USER (1).tsv \n",
            "Line:  1-173\t778-787\tCampuchia\t*\tLOCATION\tPART  WHOLE\t1-181\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352753.conll/CURATION_USER (1).tsv \n",
            "Line:  1-815\t3578-3581\tPVN\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-822|1-824\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352753.conll/CURATION_USER (1).tsv \n",
            "Line:  1-815\t3578-3581\tPVN\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-822|1-824\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352753.conll/CURATION_USER (1).tsv \n",
            "Line:  1-884\t3867-3870\tPVN\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE\t1-885|1-887|1-889\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352753.conll/CURATION_USER (1).tsv \n",
            "Line:  1-884\t3867-3870\tPVN\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE\t1-885|1-887|1-889\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352753.conll/CURATION_USER (1).tsv \n",
            "Line:  1-884\t3867-3870\tPVN\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE\t1-885|1-887|1-889\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352753.conll/CURATION_USER (1).tsv \n",
            "Line:  1-1500\t6612-6617\tTISCO\t*\tORGANIZATION\tAFFILIATION\t1-1489\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352769.conll/CURATION_USER (1).tsv \n",
            "Line:  1-420\t1842-1846\t(TTI\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-385[23_0]|1-393\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352800.conll/CURATION_USER (1).tsv \n",
            "Line:  1-113\t527-529\tM\t*\tLOCATION\tPART  WHOLE\t1-94\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352802.conll/CURATION_USER (1).tsv \n",
            "Line:  1-261\t1128-1131\tAnh\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-271|1-276\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352802.conll/CURATION_USER (1).tsv \n",
            "Line:  1-261\t1128-1131\tAnh\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-271|1-276\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352804.conll/CURATION_USER (1).tsv \n",
            "Line:  1-116\t538-545\tKashmir\t*\tLOCATION\tPART  WHOLE\t1-113\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352804.conll/CURATION_USER (1).tsv \n",
            "Line:  1-240\t1104-1112\tPakistan\t*\tLOCATION\tPART  WHOLE\t1-230\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352814.conll/CURATION_USER (1).tsv \n",
            "Line:  1-603\t2705-2707\tM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-623|1-625\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352814.conll/CURATION_USER (1).tsv \n",
            "Line:  1-603\t2705-2707\tM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-623|1-625\t\n",
            "\n",
            "4 COLUMNS.\n",
            "DOC:  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv \n",
            "Line:  1-12\t53-61\t(<ENAMEX\t_\t_\t*\t_\t_\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv \n",
            "Line:  1-13.1\t82-85\tHi\t*[2]\tORGANIZATION[2]\t_\tAFFILIATION\t1-11[0_2]\t\n",
            "\n",
            "ENTITY NAME MISTAKE IN 5TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv \n",
            "Line:  1-23\t126-136\t</ENAMEX>)\t*\t*\t_\t_\t_\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv \n",
            "Line:  1-23.1\t126-135\t</ENAMEX>\t*\t*\t_\t_\t_\t\n",
            "\n",
            "ENTITY NAME MISTAKE IN 5TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv \n",
            "Line:  1-23.1\t126-135\t</ENAMEX>\t*\t*\t_\t_\t_\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv \n",
            "Line:  1-71.1\t377-380\tHi\t*[7]\tORGANIZATION[7]\t_\t_\t_\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352816.conll/CURATION_USER.tsv \n",
            "Line:  1-77\t404-407\ti\t*[8]\tORGANIZATION[8]\t_\tPART  WHOLE\t1-71.1[7_8]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352825.conll/CURATION_USER (1).tsv \n",
            "Line:  1-7\t25-31\tLondon\t*\tLOCATION\tLOCATED\t1-1\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352825.conll/CURATION_USER (1).tsv \n",
            "Line:  1-138\t608-614\tLondon\t*\tLOCATION\tLOCATED\t1-136\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352825.conll/CURATION_USER (1).tsv \n",
            "Line:  1-238\t1041-1043\tM\t*\tLOCATION\tLOCATED\t1-204\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352849.conll/CURATION_USER (1).tsv \n",
            "Line:  1-87\t403-414\tal-Bavitieh\t*\tLOCATION\tAFFILIATION\t1-79\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352849.conll/CURATION_USER (1).tsv \n",
            "Line:  1-181\t845-852\tAl-Hawi\t*\tLOCATION\tAFFILIATION\t1-168\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352849.conll/CURATION_USER (1).tsv \n",
            "Line:  1-183\t856-863\tAl-Hamd\t*\tLOCATION\tAFFILIATION\t1-168\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352857.conll/CURATION_USER (1).tsv \n",
            "Line:  1-77\t353-360\tL'Oral\t*\tORGANIZATION\tAFFILIATION|AFFILIATION\t1-68|1-83[2_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352857.conll/CURATION_USER (1).tsv \n",
            "Line:  1-435\t2054-2061\tL'Oral\t*\tORGANIZATION\tAFFILIATION\t1-426\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352857.conll/CURATION_USER (1).tsv \n",
            "Line:  1-449\t2123-2129\tNestl\t*\tORGANIZATION\tAFFILIATION\t1-445\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352857.conll/CURATION_USER (1).tsv \n",
            "Line:  1-469\t2210-2216\tNestl\t*\tORGANIZATION\tAFFILIATION\t1-445\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352857.conll/CURATION_USER (1).tsv \n",
            "Line:  1-498\t2338-2345\tL'Oral\t*\tORGANIZATION\tAFFILIATION\t1-490\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352892.conll/CURATION_USER (1).tsv \n",
            "Line:  1-51\t240-243\tNga\t*\tLOCATION\tPART  WHOLE\t1-45\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352899.conll/CURATION_USER (1).tsv \n",
            "Line:  1-440\t1993-2000\tFrancia\t*\tPERSON\tPERSONAL - SOCIAL\t1-428\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23352899.conll/CURATION_USER (1).tsv \n",
            "Line:  1-508\t2311-2317\tSelena\t*\tPERSON\tPERSONAL - SOCIAL\t1-498\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353785.conll/CURATION_USER.tsv \n",
            "Line:  1-191\t859-864\tLibya\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-187[5_0]|1-183\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353786.conll/CURATION_USER.tsv \n",
            "Line:  1-42\t201-206\t(Php\t*\tLOCATION\tPART  WHOLE\t1-41\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353786.conll/CURATION_USER.tsv \n",
            "Line:  1-71\t344-348\tPhp\t*\tLOCATION\tLOCATED\t1-72\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353864.conll/CURATION_USER.tsv \n",
            "Line:  1-285\t1257-1264\tTimothy\t*\tPERSON\tPERSONAL - SOCIAL\t1-278\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353864.conll/CURATION_USER.tsv \n",
            "Line:  1-287\t1268-1275\tSolomon\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-285|1-278\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353864.conll/CURATION_USER.tsv \n",
            "Line:  1-287\t1268-1275\tSolomon\t*\tPERSON\tPERSONAL - SOCIAL|PERSONAL - SOCIAL\t1-285|1-278\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353864.conll/CURATION_USER.tsv \n",
            "Line:  1-317\t1412-1419\tTimothy\t*\tPERSON\tPERSONAL - SOCIAL\t1-306\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353867.conll/CURATION_USER.tsv \n",
            "Line:  1-592\t2632-2635\tMai\t*\tPERSON\tPERSONAL - SOCIAL\t1-582\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353867.conll/CURATION_USER.tsv \n",
            "Line:  1-601\t2667-2670\tSn\t*\tPERSON\tPERSONAL - SOCIAL\t1-582\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353874.conll/CURATION_USER.tsv \n",
            "Line:  1-29\t132-138\tTP.HCM\t*\tLOCATION\tLOCATED|LOCATED\t1-32|1-24[1_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353874.conll/CURATION_USER.tsv \n",
            "Line:  1-41\t183-192\tSingapore\t*\tLOCATION\tLOCATED\t1-32\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353874.conll/CURATION_USER.tsv \n",
            "Line:  1-142\t640-649\tSingapore\t*\tLOCATION\tLOCATED\t1-128\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353891.conll/CURATION_USER.tsv \n",
            "Line:  1-72\t319-323\t(Anh\t*\tLOCATION\tLOCATED\t1-71\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353891.conll/CURATION_USER.tsv \n",
            "Line:  1-391\t1786-1793\t(Italia\t*\tLOCATION\tPART  WHOLE\t1-390\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-213\t969-972\tAnh\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-215\t975-981\tAi-len\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-220\t995-1001\tCanada\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-225\t1014-1016\tB\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-230\t1036-1046\t(Mauritius\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-232\t1050-1052\tc\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-234\t1055-1057\to\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-239\t1069-1077\tM-hi-c\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353913.conll/CURATION_USER.tsv \n",
            "Line:  1-241\t1080-1086\tMa-rc\t*\tLOCATION\tLOCATED\t1-208\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353931.conll/CURATION_USER.tsv \n",
            "Line:  1-200\t880-884\tVn\t*[15]\tLOCATION[15]\t*\t1-190[14_15]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353931.conll/CURATION_USER.tsv \n",
            "Line:  1-200\t880-884\tVn\t*[15]\tLOCATION[15]\t*\t1-190[14_15]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353954.conll/CURATION_USER.tsv \n",
            "Line:  1-617\t2791-2796\tAnbar\t*\tLOCATION\tPART  WHOLE|LOCATED|LOCATED|LOCATED\t1-615[28_0]|1-631[29_0]|1-635|1-635\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353954.conll/CURATION_USER.tsv \n",
            "Line:  1-617\t2791-2796\tAnbar\t*\tLOCATION\tPART  WHOLE|LOCATED|LOCATED|LOCATED\t1-615[28_0]|1-631[29_0]|1-635|1-635\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353973.conll/CURATION_USER.tsv \n",
            "Line:  1-192\t882-884\tM\t*\tLOCATION\tAFFILIATION\t1-193\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353976.conll/CURATION_USER.tsv \n",
            "Line:  1-93\t389-392\t(M\t*\tLOCATION\tLOCATED\t1-92\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23353976.conll/CURATION_USER.tsv \n",
            "Line:  1-1012\t4532-4540\t(Lebanon\t*\tLOCATION\tLOCATED\t1-1011\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354055.conll/CURATION_USER.tsv \n",
            "Line:  1-57\t238-242\tHng\t*\tPERSON\tPERSONAL - SOCIAL\t1-33\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354055.conll/CURATION_USER.tsv \n",
            "Line:  1-151\t640-644\tNgc\t*\tPERSON\tPERSONAL - SOCIAL\t1-156\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354285.conll/CURATION_USER.tsv \n",
            "Line:  1-1263\t5606-5609\tAnh\t*\tLOCATION\tLOCATED|PART  WHOLE\t1-1256[13_0]|1-1261\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354285.conll/CURATION_USER.tsv \n",
            "Line:  1-1598\t7094-7096\tc\t*\tLOCATION\tLOCATED|PART  WHOLE\t1-1594[16_0]|1-1596\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354285.conll/CURATION_USER.tsv \n",
            "Line:  1-1833\t8159-8169\t(Cambridge\t*\tLOCATION\tLOCATED\t1-1832\t\n",
            "\n",
            "ENTITY NAME MISTAKE IN 5TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354320.conll/CURATION_USER.tsv \n",
            "Line:  1-547\t2478-2482\tMng\t*\t*\t_\t_\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354336.conll/CURATION_USER.tsv \n",
            "Line:  1-305\t1317-1323\tMexico\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-299[16_0]|1-303\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354336.conll/CURATION_USER.tsv \n",
            "Line:  1-333\t1441-1448\tIceland\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-327[17_0]|1-331\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354400.conll/CURATION_USER.tsv \n",
            "Line:  1-30\t145-154\tSingapore\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-27[3_0]|1-17|1-19[1_0]|1-23[2_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354400.conll/CURATION_USER.tsv \n",
            "Line:  1-194\t934-943\tSingapore\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-181|1-183[7_0]|1-187[8_0]|1-191[9_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354619.conll/CURATION_USER.tsv \n",
            "Line:  1-351\t1663-1668\t(Php\t*\tLOCATION\tPART  WHOLE\t1-350\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354619.conll/CURATION_USER.tsv \n",
            "Line:  1-424\t2018-2023\t(Php\t*\tLOCATION\tPART  WHOLE\t1-423\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354619.conll/CURATION_USER.tsv \n",
            "Line:  1-1147\t5372-5375\tc\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-1138|1-1136[13_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354619.conll/CURATION_USER.tsv \n",
            "Line:  1-1243\t5820-5823\tc\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-1234|1-1232[14_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354880.conll/CURATION_USER.tsv \n",
            "Line:  1-1376\t6085-6088\t(B\t*\tLOCATION\tPART  WHOLE\t1-1375\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354880.conll/CURATION_USER.tsv \n",
            "Line:  1-1472\t6506-6510\tVit\t*[118]\tLOCATION[118]\tPART  WHOLE|*\t1-1479[119_118]|1-1482[120_118]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354982.conll/CURATION_USER.tsv \n",
            "Line:  1-113\t513-519\tOregon\t*\tLOCATION\tPART  WHOLE|LOCATED\t1-111|1-106[5_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354982.conll/CURATION_USER.tsv \n",
            "Line:  1-115\t522-524\tM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|LOCATED\t1-113|1-111|1-106[5_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23354982.conll/CURATION_USER.tsv \n",
            "Line:  1-115\t522-524\tM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|LOCATED\t1-113|1-111|1-106[5_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-78\t366-372\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED\t1-76|1-69[7_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-101\t477-480\tQ.7\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE\t1-98[11_0]|1-92[10_0]|1-96\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-103\t483-489\tTP.HCM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE\t1-101|1-92[10_0]|1-96\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-103\t483-489\tTP.HCM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE\t1-101|1-92[10_0]|1-96\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-111\t522-525\tQ.3\t*\tLOCATION\tPART  WHOLE\t1-109\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-113\t528-534\tTP.HCM\t*\tLOCATION\t*|PART  WHOLE\t1-111|1-109\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-113\t528-534\tTP.HCM\t*\tLOCATION\t*|PART  WHOLE\t1-111|1-109\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-121\t567-573\tTP.HCM\t*\tLOCATION\tPART  WHOLE\t1-119\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-136\t633-639\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED\t1-134|1-126[12_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-150\t703-706\tQ.3\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE\t1-148|1-140[14_0]|1-143[15_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-152\t709-715\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE|PART  WHOLE\t1-150|1-140[14_0]|1-143[15_0]|1-148\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-152\t709-715\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE|PART  WHOLE\t1-150|1-140[14_0]|1-143[15_0]|1-148\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-166\t780-786\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE\t1-164|1-156[16_0]|1-159[17_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-177\t831-838\t(TP.HCM\t*\tLOCATION\tLOCATED|LOCATED|LOCATED\t1-176|1-172|1-174\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-177\t831-838\t(TP.HCM\t*\tLOCATION\tLOCATED|LOCATED|LOCATED\t1-176|1-172|1-174\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-177\t831-838\t(TP.HCM\t*\tLOCATION\tLOCATED|LOCATED|LOCATED\t1-176|1-172|1-174\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-186\t879-885\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED\t1-184|1-181[18_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-196\t928-934\tTP.HCM\t*\tLOCATION\tLOCATED|PART  WHOLE\t1-191[19_0]|1-194\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-206\t981-987\tTP.HCM\t*\tLOCATION\tLOCATED|PART  WHOLE\t1-201[20_0]|1-204\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-218\t1037-1043\tTP.HCM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|LOCATED\t1-216|1-213[22_0]|1-210[21_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-226\t1077-1083\tTP.HCM\t*\tLOCATION\tPART  WHOLE\t1-224\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-242\t1159-1165\tTP.HCM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE\t1-239[24_0]|1-232[23_0]|1-237\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-255\t1220-1223\tQ.3\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-253|1-248[25_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-257\t1226-1232\tTP.HCM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE\t1-255|1-248[25_0]|1-253\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-257\t1226-1232\tTP.HCM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE\t1-255|1-248[25_0]|1-253\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-273\t1302-1308\tTP.HCM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE\t1-270[27_0]|1-263[26_0]|1-268\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-288\t1377-1383\tTP.HCM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|LOCATED\t1-286|1-281[29_0]|1-277[28_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-302\t1442-1445\tQ.3\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE\t1-300|1-292[30_0]|1-295[31_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-304\t1448-1454\tTP.HCM\t*\tLOCATION\tLOCATED|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-292[30_0]|1-295[31_0]|1-300|1-302\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-304\t1448-1454\tTP.HCM\t*\tLOCATION\tLOCATED|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-292[30_0]|1-295[31_0]|1-300|1-302\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-317\t1509-1512\tQ.6\t*\tLOCATION\tLOCATED|PART  WHOLE|PART  WHOLE\t1-308[32_0]|1-311[33_0]|1-315\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-319\t1515-1521\tTP.HCM\t*\tLOCATION\tLOCATED|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-308[32_0]|1-311[33_0]|1-315|1-317\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-319\t1515-1521\tTP.HCM\t*\tLOCATION\tLOCATED|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-308[32_0]|1-311[33_0]|1-315|1-317\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-356\t1701-1707\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED\t1-354|1-350[40_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-364\t1740-1746\tTP.HCM\t*\tLOCATION\tPART  WHOLE\t1-362\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-380\t1820-1826\tTP.HCM\t*\tLOCATION\tLOCATED|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-368[41_0]|1-377[43_0]|1-371[42_0]|1-375\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-398\t1913-1919\tTP.HCM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE|LOCATED\t1-395[46_0]|1-388[45_0]|1-393|1-384[44_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-409\t1969-1972\tQ.1\t*\tLOCATION\tPART  WHOLE|LOCATED\t1-404[47_0]|1-403\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-411\t1975-1981\tTP.HCM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|LOCATED\t1-409|1-404[47_0]|1-403\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-411\t1975-1981\tTP.HCM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|LOCATED\t1-409|1-404[47_0]|1-403\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-424\t2041-2047\tTP.HCM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-422|1-417[48_0]\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-457\t2210-2214\ttnh\t*[54]\tLOCATION[54]\tLOCATED|*|PART  WHOLE|PART  WHOLE\t1-446[51_54]|1-454[53_54]|1-449[52_54]|1-449[52_54]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-473\t2288-2291\tQ.3\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|LOCATED\t1-471|1-466[56_0]|1-463[55_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-475\t2294-2300\tTP.HCM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE|LOCATED\t1-473|1-466[56_0]|1-471|1-463[55_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-475\t2294-2300\tTP.HCM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE|LOCATED\t1-473|1-466[56_0]|1-471|1-463[55_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-483\t2337-2340\tQ.3\t*\tLOCATION\tLOCATED|PART  WHOLE\t1-479[57_0]|1-481\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-485\t2343-2349\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE\t1-483|1-479[57_0]|1-481\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-485\t2343-2349\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE\t1-483|1-479[57_0]|1-481\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-499\t2409-2415\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE\t1-497|1-489[58_0]|1-493[59_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-525\t2534-2540\tTP.HCM\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-523|1-518[62_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-531\t2568-2572\t(P.1\t*\tLOCATION\tLOCATED\t1-530\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-533\t2575-2578\tQ.3\t*\tLOCATION\tLOCATED|PART  WHOLE\t1-530|1-531\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-533\t2575-2578\tQ.3\t*\tLOCATION\tLOCATED|PART  WHOLE\t1-530|1-531\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-535\t2581-2587\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE\t1-533|1-530|1-531\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-535\t2581-2587\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE\t1-533|1-530|1-531\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-535\t2581-2587\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE\t1-533|1-530|1-531\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-552\t2661-2667\tTP.HCM\t*\tLOCATION\tLOCATED|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-539[63_0]|1-542[64_0]|1-547|1-549[65_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-564\t2722-2728\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE\t1-562|1-556[66_0]|1-559[67_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-570\t2755-2759\t(Q.3\t*\tLOCATION\tLOCATED\t1-569\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-572\t2762-2768\tTP.HCM\t*\tLOCATION\tPART  WHOLE|*\t1-570|1-569\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-572\t2762-2768\tTP.HCM\t*\tLOCATION\tPART  WHOLE|*\t1-570|1-569\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-604\t2909-2915\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED\t1-602|1-599[73_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-613\t2955-2958\tQ.3\t*\tLOCATION\tPART  WHOLE|LOCATED\t1-611|1-608[74_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-615\t2961-2967\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE\t1-613|1-608[74_0]|1-611\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-615\t2961-2967\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE\t1-613|1-608[74_0]|1-611\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355290.conll/CURATION_USER.tsv \n",
            "Line:  1-954\t4754-4760\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED|PART  WHOLE\t1-952|1-944[122_0]|1-948[123_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355434.conll/CURATION_USER.tsv \n",
            "Line:  1-370\t1724-1728\tPC67\t*\tORGANIZATION\tAFFILIATION\t1-360\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355817.conll/CURATION_USER.tsv \n",
            "Line:  1-92\t445-451\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED\t1-90|1-85[4_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23355817.conll/CURATION_USER.tsv \n",
            "Line:  1-339\t1564-1570\tTP.HCM\t*\tLOCATION\tPART  WHOLE|LOCATED\t1-337|1-331[7_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-482\t2151-2156\tASEAN\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-486|1-488|1-490|1-492|1-494|1-496[13_0]|1-499\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-482\t2151-2156\tASEAN\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-486|1-488|1-490|1-492|1-494|1-496[13_0]|1-499\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-482\t2151-2156\tASEAN\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-486|1-488|1-490|1-492|1-494|1-496[13_0]|1-499\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-482\t2151-2156\tASEAN\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-486|1-488|1-490|1-492|1-494|1-496[13_0]|1-499\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-482\t2151-2156\tASEAN\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-486|1-488|1-490|1-492|1-494|1-496[13_0]|1-499\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-482\t2151-2156\tASEAN\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE|PART  WHOLE\t1-486|1-488|1-490|1-492|1-494|1-496[13_0]|1-499\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-869\t3908-3913\tASEAN\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE\t1-871|1-873|1-875[28_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356245.conll/CURATION_USER.tsv \n",
            "Line:  1-869\t3908-3913\tASEAN\t*\tORGANIZATION\tPART  WHOLE|PART  WHOLE|PART  WHOLE\t1-871|1-873|1-875[28_0]\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356511.conll/CURATION_USER.tsv \n",
            "Line:  1-22\t92-95\t(B\t*[3]\tORGANIZATION[3]\tPART  WHOLE|*\t1-17[2_3]|1-10[1_3]\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23356574.conll/CURATION_USER.tsv \n",
            "Line:  1-743.1\t3347-3353\tnghip\t*[18]\tORGANIZATION[18]\tPART  WHOLE\t1-733[17_18]\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356574.conll/CURATION_USER.tsv \n",
            "Line:  1-744\t3355-3357\tS\t*[19]\tORGANIZATION[19]\tAFFILIATION|PART  WHOLE\t1-733[17_19]|1-743.1[18_19]\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356771.conll/CURATION_USER.tsv \n",
            "Line:  1-513\t2282-2286\tHip\t*[24]\tORGANIZATION[24]\t*\t1-506[23_24]\t\n",
            "\n",
            "REALTION NAME MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356771.conll/CURATION_USER.tsv \n",
            "Line:  1-513\t2282-2286\tHip\t*[24]\tORGANIZATION[24]\t*\t1-506[23_24]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356858.conll/CURATION_USER.tsv \n",
            "Line:  1-267\t1169-1171\tH\t*\tPERSON\tPERSONAL - SOCIAL\t1-270\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356858.conll/CURATION_USER.tsv \n",
            "Line:  1-456\t2054-2057\tLn\t*\tPERSON\tPERSONAL - SOCIAL\t1-454\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356858.conll/CURATION_USER.tsv \n",
            "Line:  1-516\t2308-2312\tLnh\t*\tPERSON\tPERSONAL - SOCIAL\t1-483\t\n",
            "\n",
            "RELATION MISTAKE IN 6TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23356874.conll/CURATION_USER.tsv \n",
            "Line:  1-59\t281-288\t(Trng\t*[6]\tORGANIZATION[6]\tAFFILIATION|*|AFFILIATION|AFFILIATION|AFFILIATION\t1-56[5_6]|1-40[1_6]|1-44[2_6]|1-48[3_6]|1-52[4_6]\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23357000.conll/CURATION_USER.tsv \n",
            "Line:  1-177.1\t827-832\t1+1>2\t*[8]\tORGANIZATION[8]\tAFFILIATION\t1-166[7_8]\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23357000.conll/CURATION_USER.tsv \n",
            "Line:  1-263.1\t1223-1228\t1+1>2\t*[12]\tORGANIZATION[12]\tAFFILIATION\t1-252[11_12]\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23357063.conll/CURATION_USER.tsv \n",
            "Line:  1-101.1\t456-460\t4006\t*[1]\tORGANIZATION[1]\t_\t_\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357063.conll/CURATION_USER.tsv \n",
            "Line:  1-102\t462-465\tHi\t*[2]\tORGANIZATION[2]\tPART  WHOLE\t1-101.1[1_2]\t\n",
            "\n",
            "SPECIAL SUBTOKEN IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357063.conll/CURATION_USER.tsv \n",
            "Line:  1-106\t476-480\tVng\t*[3]\tLOCATION[3]\tPART  WHOLE|PART  WHOLE\t1-102[2_3]|1-101.1[1_3]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357095.conll/CURATION_USER.tsv \n",
            "Line:  1-112\t536-546\tCalifornia\t*\tLOCATION\tLOCATED|PART  WHOLE\t1-103|1-109[3_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357095.conll/CURATION_USER.tsv \n",
            "Line:  1-271\t1275-1283\tTanzania\t*\tLOCATION\tLOCATED\t1-251\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357240.conll/CURATION_USER.tsv \n",
            "Line:  1-166\t745-750\tApple\t*\tORGANIZATION\tPART  WHOLE\t1-195\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357329.conll/CURATION_USER.tsv \n",
            "Line:  1-27\t122-131\tIndonesia\t*\tLOCATION\tPART  WHOLE|PART  WHOLE\t1-25|1-21[1_0]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357329.conll/CURATION_USER.tsv \n",
            "Line:  1-297\t1370-1380\t(Australia\t*\tLOCATION\tLOCATED\t1-296\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357752.conll/CURATION_USER.tsv \n",
            "Line:  1-458\t2025-2031\tTP.HCM\t*\tLOCATION\tLOCATED\t1-456\t\n",
            "\n",
            "THERE IS A SUBTOKEN.\n",
            "DOC:  VLSP2020_RE_training/23357779.conll/CURATION_USER.tsv \n",
            "Line:  1-27.1\t123-125\tH\t*[2]\tLOCATION[2]\tLOCATED\t1-18[1_2]\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357809.conll/CURATION_USER.tsv \n",
            "Line:  1-603\t2716-2722\tOregon\t*\tLOCATION\tPART  WHOLE\t1-601\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23357809.conll/CURATION_USER.tsv \n",
            "Line:  1-744\t3358-3365\tHarvard\t*\tORGANIZATION\tAFFILIATION\t1-742\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23358104.conll/CURATION_USER.tsv \n",
            "Line:  1-15\t69-72\tMai\t*\tPERSON\tPERSONAL - SOCIAL\t1-13\t\n",
            "\n",
            "MISTAKE IN 7TH COLUMN.\n",
            "DOC:  VLSP2020_RE_training/23366740.conll/CURATION_USER.tsv \n",
            "Line:  1-503\t2299-2307\tLabrador\t*\tLOCATION\tLOCATED\t1-489\t\n",
            "506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wzBGIIgKYPD"
      },
      "source": [
        "# Fix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruagt4UGbqL5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "41372e03-1537-4741-dcc9-59d1645d094d"
      },
      "source": [
        "\"\"\"\n",
        "raw_data: - is a list (len = 506)\n",
        "          - each row is a dict {}, information from a single doc (506 doc)\n",
        "            - doc_id: number id in name of folder that contain doc\n",
        "            - text: text in line start with \"#Text=\"\n",
        "            - token_ids: list of int. (1st column)\n",
        "            - subtoken_ids: list. (1st column)\n",
        "                            an element can be None if token is not a subtoken: 1-id -> 1.26,\n",
        "                                           or int (subtoken_id) if token is a subtoken: 1-id.subid -> 1.26.1.\n",
        "                                           currently in train data, only exist subtoken id 1.\n",
        "                            (in extract raw data code, my code can get any subid, not just specify subid = 1.\n",
        "                             but i check if in data has other subid, it will return error -> to know more about data)\n",
        "            - pos: list of child list. (2st column)\n",
        "                   each child list has two int elements.\n",
        "                   [start_position, end_position]\n",
        "            - tokens: list of strings. (3th column)\n",
        "            - entity: list.\n",
        "                      an element is: None if crr token is not entity\n",
        "                                     a list with: 2 element if crr token is an entity.\n",
        "                                                  [entity_id, entity_name]\n",
        "                                                  entity_id: int, from 4th column\n",
        "                                                  entity_name: string, from 5th column\n",
        "            - relation: list\n",
        "                        an element is: None if there is no relation in 6ht, 7th column.\n",
        "                                       a list of child list. number of child list is number of relation in 6th, 7th column.\n",
        "                                                 each child list has: 4 elemnt\n",
        "                                                 [relation_name, stoken_id, sstoken_id, direction[sentity_id, eentity_id]]\n",
        "                                                 relation_name: string, from 6h column\n",
        "                                                 stoken_id: int, tokenid from 7th column\n",
        "                                                 sstoken_id: from 7th column\n",
        "                                                             None, if entity_1 is a token\n",
        "                                                             else: int, subid if entity_1 is subtoken\n",
        "                                                 direction: from 7th column\n",
        "                                                            None, if there is a mistake in dataset\n",
        "                                                            else: [entity_1_id, entity_2_id]\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nraw_data: - is a list (len = 506)\\n          - each row is a dict {}, information from a single doc (506 doc)\\n            - doc_id: number id in name of folder that contain doc\\n            - text: text in line start with \"#Text=\"\\n            - token_ids: list of int. (1st column)\\n            - subtoken_ids: list. (1st column)\\n                            an element can be None if token is not a subtoken: 1-id -> 1.26,\\n                                           or int (subtoken_id) if token is a subtoken: 1-id.subid -> 1.26.1.\\n                                           currently in train data, only exist subtoken id 1.\\n                            (in extract raw data code, my code can get any subid, not just specify subid = 1.\\n                             but i check if in data has other subid, it will return error -> to know more about data)\\n            - pos: list of child list. (2st column)\\n                   each child list has two int elements.\\n                   [start_position, end_position]\\n            - tokens: list of strings. (3th column)\\n            - entity: list.\\n                      an element is: None if crr token is not entity\\n                                     a list with: 2 element if crr token is an entity.\\n                                                  [entity_id, entity_name]\\n                                                  entity_id: int, from 4th column\\n                                                  entity_name: string, from 5th column\\n            - relation: list\\n                        an element is: None if there is no relation in 6ht, 7th column.\\n                                       a list of child list. number of child list is number of relation in 6th, 7th column.\\n                                                 each child list has: 4 elemnt\\n                                                 [relation_name, stoken_id, sstoken_id, direction[sentity_id, eentity_id]]\\n                                                 relation_name: string, from 6h column\\n                                                 stoken_id: int, tokenid from 7th column\\n                                                 sstoken_id: from 7th column\\n                                                             None, if entity_1 is a token\\n                                                             else: int, subid if entity_1 is subtoken\\n                                                 direction: from 7th column\\n                                                            None, if there is a mistake in dataset\\n                                                            else: [entity_1_id, entity_2_id]\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsVmEapDfdYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67e07049-347c-451b-f1d3-1de00ea6a6c9"
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "a = \"mnh,\\xa0\"\n",
        "b = unicodedata.normalize('NFKD',a)\n",
        "\n",
        "print(repr(a))\n",
        "print(repr(b))\n",
        "\n",
        "space_count = 0\n",
        "for s in a[::-1]:\n",
        "    \n",
        "    if s == ' ':\n",
        "        space_count += 1\n",
        "    if s != ' ':\n",
        "        break\n",
        "                \n",
        "a = a.rstrip()\n",
        "\n",
        "\n",
        "print(space_count)\n",
        "print(repr(a))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'mnh,\\xa0'\n",
            "'minh, '\n",
            "0\n",
            "'mnh,'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYMqFdP5eTPL"
      },
      "source": [
        "## Fix1: Subtoken to token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdwVXKOHs1Ql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d44e2dc5-28ee-4388-8b8b-2df52ce6cd4c"
      },
      "source": [
        "# Trong data c subtoken. Mc d hin ti trong train data 1 token nu c subtoken th ch c 1 subtoken \n",
        "# v subtoken ny thng l entity nn mi cn tch ra. ngoi ra subtoken thng nm  cui hoc u token nn ta c d dng tch ra c.\n",
        "# nn  gi ng ngha tt nht cho cu th c th chia token thnh 2 token: subtoken v phn cn li t token gc\n",
        "\n",
        "# Tuy nhin, nu trong tng lai, vi b dev v test,\n",
        "# - nu c nhiu hn 1 subtoken v cc subtoken k b overlap th cng chia nh trn\n",
        "# - cn nu xy ra hin tng overlap (subtoken b  ln nhau) gia cc subtoken\n",
        "# th lc ny ta khng th tch token ra c na\n",
        "# lc ny ta s chn cch x l s l khng tch token gc ra na. chn thm cc subtoken vo cu v coi chng nh 1 token bnh thng\n",
        "\n",
        "\n",
        "#  y do c 1 subtoken nn chn cch tch token gc ra cho n gin v gi c ng ngha tt nht.\n",
        "# on code bn di ch cho trng hp 1 subtoken\n",
        "# trng hp nhiu subtoken nhng khng overlap th  tng cng tng t, \n",
        "# nhng cn ch  gom cc subtoken ca 1 token li v chn th t x l\n",
        "# v ch  ti c du bng hay khng khi so snh pos\n",
        "\n",
        "import copy\n",
        "\n",
        "raw_tdata_new = copy.deepcopy(raw_tdata)\n",
        "\n",
        "for docif in raw_tdata_new:\n",
        "    for i in range(len(docif['subtoken_ids'])):\n",
        "        \n",
        "        if docif['subtoken_ids'][i] != None:\n",
        "            \n",
        "            print('\\n\\n-----Doc: ', docif['doc_id'])\n",
        "            print('Before: ')\n",
        "            for key in docif:\n",
        "                if key not in ['doc_id', 'text']:\n",
        "                    print(docif[key][i-1], end='\\t')\n",
        "            print('', end='\\n')\n",
        "            for key in docif:\n",
        "                if key not in ['doc_id', 'text']:\n",
        "                    print(docif[key][i], end='\\t')\n",
        "\n",
        "\n",
        "\n",
        "            #print(docif['token_ids'][i-1], '\\t', docif['pos'][i-1][0], '-', docif['pos'][i-1][1], '\\t', docif['tokens'][i-1])\n",
        "            #print(docif['token_ids'][i] , '.', docif['subtoken_ids'], '\\t', docif['pos'][i][0], '-', docif['pos'][i][1], '\\t', docif['tokens'][i])\n",
        "\n",
        "            # subtoken l mt on u ca token\n",
        "            # 1-200\t    902-905\tmi\t        *[10]\tORGANIZATION[10]\t_\t_\t\n",
        "            # 1-201\t    906-913\ttrng,\t     _     _\t                _\t_\t        <--- i-1\n",
        "            # 1-201.1\t906-912\ttrng  \t*[10]\tORGANIZATION[10]\t_\t_           <--- i\n",
        "\n",
        "            if (docif['pos'][i][0] == docif['pos'][i-1][0]) and (docif['pos'][i][1] < docif['pos'][i-1][1]):\n",
        "                \n",
        "                # i ch 2 dng, dng subtoken ln trn, dng token gc xung di\n",
        "                # i ang l dng cha subtoken s thnh dng cha token\n",
        "                # i-1 ang l dng cha token s thnh dng cha subtoken\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        tmp = copy.deepcopy(docif[key][i])\n",
        "                        docif[key][i] = copy.deepcopy(docif[key][i-1]) \n",
        "                        docif[key][i-1] = copy.deepcopy(tmp)\n",
        "\n",
        "                assert (docif['subtoken_ids'][i-1] != None), str(\"Swap failed.\")\n",
        "\n",
        "                ## xa b subtoken ids\n",
        "                docif['subtoken_ids'][i-1] = None\n",
        "\n",
        "                ## thay i tokens, b phn subtoken  tch ra\n",
        "                docif['tokens'][i] = docif['tokens'][i].replace(docif['tokens'][i-1], '')\n",
        "\n",
        "                # tuy nhin, c trng hp sau khi b phn subtoken i b tha du cch, nn ta cn x l\n",
        "                # do subtoken nm  u token, nn ta ch m du cch  bn tri phn cn li thi\n",
        "                space_count = 0\n",
        "                for s in docif['tokens'][i]:\n",
        "                    if s in [' ', '\\xa0']:\n",
        "                        space_count += 1\n",
        "                    else:\n",
        "                        break\n",
        "                \n",
        "\n",
        "                docif['tokens'][i] = docif['tokens'][i].lstrip()\n",
        "\n",
        "\n",
        "                ## thay i pos\n",
        "                docif['pos'][i] = [(docif['pos'][i-1][1] + space_count), docif['pos'][i][1]]\n",
        "                \n",
        "                \n",
        "\n",
        "                assert ((docif['text'][docif['pos'][i][0]:docif['pos'][i][1]]) == docif['tokens'][i]), \\\n",
        "                str('\\nWrong postions \\npos' + str(docif['pos'][i][0]) + '-' + str(docif['pos'][i][1]) + '  token: ' + str(docif['tokens'][i]))\n",
        "\n",
        "                # thay i token_ids ca ton b phn di, nu c subid trong relation  u th thay i, thay i stoken_id trong relation\n",
        "                # trng hp ny stoken_id s khng i nn khng cn thay i\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if j >= i:\n",
        "                        docif['token_ids'][j] += 1\n",
        "\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][1] == docif['token_ids'][i-1]:   # tm xem c relation no tr ti subtoken trc kia khng\n",
        "                                docif['relation'][j][k][2] = None   # nu c th thay bng None\n",
        "\n",
        "                            # do  trn, ton b token_ids pha sau (>= i) s b thay i (cng thm 1)\n",
        "                            # nn nhng relation c stoken_id nm  phn pha sau ny cng s cn thay i theo (cng thm 1)\n",
        "                            elif (docif['relation'][j][k][1] > docif['token_ids'][i-1]):\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1\n",
        "\n",
        "\n",
        "                '''\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][2] != None:\n",
        "                                assert False, str('Failed to replace subid in relation')\n",
        "                '''\n",
        "                            \n",
        "\n",
        "                print('\\n\\nAfter:')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i-1], end='\\t')\n",
        "                print('', end='\\n')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i], end='\\t')\n",
        "\n",
        "\n",
        "            # subtoken l mt on cui ca token\n",
        "            # 1-583\t    2567-2573\tm-Tp\t _\t    _\t                _\t_\t<--- i-1\n",
        "            # 1-583.1\t2570-2573\tTp\t    *[19]\tORGANIZATION[19]\t_\t_\t<--- i\n",
        "\n",
        "            elif (docif['pos'][i][1] == docif['pos'][i-1][1]) and (docif['pos'][i][0] > docif['pos'][i-1][0]):\n",
        "                ## thay i subid\n",
        "                docif['subtoken_ids'][i] = None\n",
        "\n",
        "                ## thay i tokens\n",
        "                docif['tokens'][i-1] = docif['tokens'][i-1].replace(docif['tokens'][i], '')\n",
        "\n",
        "                print(repr(docif['tokens'][i-1]))\n",
        "                # tuy nhin, c trng hp sau khi b phn subtoken i b tha du cch, nn ta cn x l\n",
        "                # do subtoken nm  cui token, nn ta ch m du cch  bn phi phn cn li thi\n",
        "                space_count = 0\n",
        "                for s in docif['tokens'][i-1][::-1]:\n",
        "                    if s in [' ', '\\xa0']:\n",
        "                        space_count += 1\n",
        "                    else:\n",
        "                        break\n",
        "                \n",
        "                \n",
        "                docif['tokens'][i-1] = docif['tokens'][i-1].rstrip()\n",
        "\n",
        "\n",
        "                ## thay i pos\n",
        "                docif['pos'][i-1] = [docif['pos'][i-1][0], (docif['pos'][i][0] - space_count)]\n",
        "\n",
        "                \n",
        "\n",
        "                assert ((docif['text'][docif['pos'][i-1][0]:docif['pos'][i-1][1]]) == docif['tokens'][i-1]), \\\n",
        "                str('\\nWrong postions \\npos ' + str(docif['pos'][i-1][0]) + '-' + str(docif['pos'][i-1][1]) + '  token: ' + str(docif['tokens'][i-1]))\n",
        "\n",
        "                # thay i token_ids ca ton b phn di, nu c subid trong relation  u th thay i, thay i stoken_id trong relation\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if j >= i:\n",
        "                        docif['token_ids'][j] += 1\n",
        "\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][1] == docif['token_ids'][i-1]:   # tm xem c relation no tr ti subtoken trc kia khng\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1   # do token_id thay i nn relation c stoken_id ny cng phi thay i \n",
        "                                docif['relation'][j][k][2] = None   # nu c th thay bng None\n",
        "\n",
        "                            # do  trn, ton b token_ids pha sau (>= i) s b thay i (cng thm 1)\n",
        "                            # nn nhng relation c stoken_id nm  phn pha sau ny cng s cn thay i theo (cng thm 1)\n",
        "                            elif (docif['relation'][j][k][1] > docif['token_ids'][i-1]):\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1\n",
        "\n",
        "                '''\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            if docif['relation'][j][k][2] != None:\n",
        "                                assert False, str('Failed to replace subid in relation')\n",
        "                            \n",
        "                            stoken_eid = docif['token_ids'].index(docif['relation'][j][k][1])\n",
        "                            if docif['entity'][stoken_eid] == None:\n",
        "                                print(docif['doc_id'])\n",
        "                                print(docif['relation'][j][k][1])\n",
        "                                print(stoken_eid)\n",
        "                                print(docif['token_ids'][stoken_eid])\n",
        "                                assert False, str('Failed to replace stoken_id in relation')\n",
        "                        \n",
        "                '''\n",
        "                        \n",
        "\n",
        "\n",
        "\n",
        "                print('\\n\\nAfter:')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i-1], end='\\t')\n",
        "                print('', end='\\n')\n",
        "                for key in docif:\n",
        "                    if key not in ['doc_id', 'text']:\n",
        "                        print(docif[key][i], end='\\t')\n",
        "\n",
        "            else:\n",
        "                assert False, str(\"\\nExist subtoken in middle of token.\\nDoc: \" + docif['doc_id'] + \"\\ntoken_id\" + str(docif['token_ids'][i]))\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "\n",
        "\n",
        "# kim tra xem code trn c li g khng\n",
        "print('\\n\\nCHECKING')\n",
        "for idoc, docif in enumerate(raw_tdata_new):\n",
        "\n",
        "    relation_lst = []\n",
        "    for i in range(len(raw_tdata[idoc]['relation'])):\n",
        "        if raw_tdata[idoc]['relation'][i] != None:\n",
        "            relation_lst.append(raw_tdata[idoc]['relation'][i])\n",
        "\n",
        "    \n",
        "    relation_ith = 0\n",
        "    for i in range(len(docif['token_ids'])):\n",
        "        \n",
        "        # nu code chy ng th s khng cn subid\n",
        "        if (docif['subtoken_ids'][i] != None):\n",
        "            assert False, str('ERROR CODE 1')\n",
        "\n",
        "        if docif['relation'][i] != None:\n",
        "            for j in range(len(docif['relation'][i])):\n",
        "                # nu code chy ng th s khng cn subid\n",
        "                if docif['relation'][i][j][2] != None:\n",
        "                    assert False, str('ERROR CODE 2')\n",
        "\n",
        "                # so snh xem thay i stoken_id c ng khng\n",
        "                # string ca token v pos ca token s khng i so vi raw_tdata\n",
        "                stoken_new = docif['relation'][i][j][1]\n",
        "                stoken_new_ele_id = docif['token_ids'].index(stoken_new)\n",
        "                \n",
        "                stoken = relation_lst[relation_ith][j][1]\n",
        "                if relation_lst[relation_ith][j][2] == 1:\n",
        "                    stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken) + 1\n",
        "                else:\n",
        "                    stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken)\n",
        "\n",
        "                if docif['pos'][stoken_new_ele_id] != raw_tdata[idoc]['pos'][stoken_ele_id]:\n",
        "                    '''\n",
        "                    print('\\n-----Doc: ', docif['doc_id'])\n",
        "                    print(relation_ith)\n",
        "                    print(docif['relation'][i])\n",
        "                    print(relation_lst[relation_ith])\n",
        "                    print(docif['pos'][stoken_new_ele_id])\n",
        "                    print(raw_tdata[idoc]['pos'][stoken_ele_id])\n",
        "                    '''\n",
        "                    assert False, str('ERROR CODE 3')\n",
        "                \n",
        "            relation_ith += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('DONE. EVERYTHINGS SEEM TO BE CORRECTED :D')\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "   "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "-----Doc:  23351433\n",
            "Before: \n",
            "583\tNone\t[2567, 2573]\tm-Tp\tNone\tNone\t\n",
            "583\t1\t[2570, 2573]\tTp\t[19, 'ORGANIZATION']\tNone\t'm-'\n",
            "\n",
            "\n",
            "After:\n",
            "583\tNone\t[2567, 2570]\tm-\tNone\tNone\t\n",
            "584\tNone\t[2570, 2573]\tTp\t[19, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "-----Doc:  23351610\n",
            "Before: \n",
            "201\tNone\t[906, 913]\ttrng,\tNone\tNone\t\n",
            "201\t1\t[906, 912]\ttrng\t[10, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "After:\n",
            "201\tNone\t[906, 912]\ttrng\t[10, 'ORGANIZATION']\tNone\t\n",
            "202\tNone\t[912, 913]\t,\tNone\tNone\t\n",
            "\n",
            "-----Doc:  23351945\n",
            "Before: \n",
            "466\tNone\t[2176, 2188]\tmnh,Chris\tNone\tNone\t\n",
            "466\t1\t[2183, 2188]\tChris\t[0, 'PERSON']\tNone\t'mnh,\\xa0'\n",
            "\n",
            "\n",
            "After:\n",
            "466\tNone\t[2176, 2182]\tmnh,\tNone\tNone\t\n",
            "467\tNone\t[2183, 2188]\tChris\t[0, 'PERSON']\tNone\t\n",
            "\n",
            "-----Doc:  23351984\n",
            "Before: \n",
            "610\tNone\t[2832, 2839]\tnhtc\tNone\tNone\t\n",
            "610\t1\t[2837, 2839]\tc\t[0, 'LOCATION']\t[['LOCATED', 599, None, None]]\t'nht\\xa0'\n",
            "\n",
            "\n",
            "After:\n",
            "610\tNone\t[2832, 2836]\tnht\tNone\tNone\t\n",
            "611\tNone\t[2837, 2839]\tc\t[0, 'LOCATION']\t[['LOCATED', 599, None, None]]\t\n",
            "\n",
            "-----Doc:  23352816\n",
            "Before: \n",
            "13\tNone\t[62, 85]\tTYPE=\"ORGANIZATION\">Hi\tNone\tNone\t\n",
            "13\t1\t[82, 85]\tHi\t[2, 'ORGANIZATION']\t[['AFFILIATION', 11, None, [0, 2]]]\t'TYPE=\"ORGANIZATION\">'\n",
            "\n",
            "\n",
            "After:\n",
            "13\tNone\t[62, 82]\tTYPE=\"ORGANIZATION\">\tNone\tNone\t\n",
            "14\tNone\t[82, 85]\tHi\t[2, 'ORGANIZATION']\t[['AFFILIATION', 11, None, [0, 2]]]\t\n",
            "\n",
            "-----Doc:  23352816\n",
            "Before: \n",
            "24\tNone\t[126, 136]\t</ENAMEX>)\tNone\tNone\t\n",
            "24\t1\t[126, 135]\t</ENAMEX>\tNone\tNone\t\n",
            "\n",
            "After:\n",
            "24\tNone\t[126, 135]\t</ENAMEX>\tNone\tNone\t\n",
            "25\tNone\t[135, 136]\t)\tNone\tNone\t\n",
            "\n",
            "-----Doc:  23352816\n",
            "Before: \n",
            "73\tNone\t[357, 380]\tTYPE=\"ORGANIZATION\">Hi\tNone\tNone\t\n",
            "73\t1\t[377, 380]\tHi\t[7, 'ORGANIZATION']\tNone\t'TYPE=\"ORGANIZATION\">'\n",
            "\n",
            "\n",
            "After:\n",
            "73\tNone\t[357, 377]\tTYPE=\"ORGANIZATION\">\tNone\tNone\t\n",
            "74\tNone\t[377, 380]\tHi\t[7, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "-----Doc:  23356574\n",
            "Before: \n",
            "743\tNone\t[3347, 3354]\tnghip,\tNone\tNone\t\n",
            "743\t1\t[3347, 3353]\tnghip\t[18, 'ORGANIZATION']\t[['PART  WHOLE', 733, None, [17, 18]]]\t\n",
            "\n",
            "After:\n",
            "743\tNone\t[3347, 3353]\tnghip\t[18, 'ORGANIZATION']\t[['PART  WHOLE', 733, None, [17, 18]]]\t\n",
            "744\tNone\t[3353, 3354]\t,\tNone\tNone\t\n",
            "\n",
            "-----Doc:  23357000\n",
            "Before: \n",
            "177\tNone\t[827, 833]\t1+1>2;\tNone\tNone\t\n",
            "177\t1\t[827, 832]\t1+1>2\t[8, 'ORGANIZATION']\t[['AFFILIATION', 166, None, [7, 8]]]\t\n",
            "\n",
            "After:\n",
            "177\tNone\t[827, 832]\t1+1>2\t[8, 'ORGANIZATION']\t[['AFFILIATION', 166, None, [7, 8]]]\t\n",
            "178\tNone\t[832, 833]\t;\tNone\tNone\t\n",
            "\n",
            "-----Doc:  23357000\n",
            "Before: \n",
            "264\tNone\t[1223, 1229]\t1+1>2;\tNone\tNone\t\n",
            "264\t1\t[1223, 1228]\t1+1>2\t[12, 'ORGANIZATION']\t[['AFFILIATION', 253, None, [11, 12]]]\t\n",
            "\n",
            "After:\n",
            "264\tNone\t[1223, 1228]\t1+1>2\t[12, 'ORGANIZATION']\t[['AFFILIATION', 253, None, [11, 12]]]\t\n",
            "265\tNone\t[1228, 1229]\t;\tNone\tNone\t\n",
            "\n",
            "-----Doc:  23357063\n",
            "Before: \n",
            "101\tNone\t[456, 461]\t4006,\tNone\tNone\t\n",
            "101\t1\t[456, 460]\t4006\t[1, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "After:\n",
            "101\tNone\t[456, 460]\t4006\t[1, 'ORGANIZATION']\tNone\t\n",
            "102\tNone\t[460, 461]\t,\tNone\tNone\t\n",
            "\n",
            "-----Doc:  23357779\n",
            "Before: \n",
            "27\tNone\t[122, 125]\t(H\tNone\tNone\t\n",
            "27\t1\t[123, 125]\tH\t[2, 'LOCATION']\t[['LOCATED', 18, None, [1, 2]]]\t'('\n",
            "\n",
            "\n",
            "After:\n",
            "27\tNone\t[122, 123]\t(\tNone\tNone\t\n",
            "28\tNone\t[123, 125]\tH\t[2, 'LOCATION']\t[['LOCATED', 18, None, [1, 2]]]\t\n",
            "\n",
            "CHECKING\n",
            "DONE. EVERYTHINGS SEEM TO BE CORRECTED :D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQKOe59wfEcS"
      },
      "source": [
        "## Fix2: Others fix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpazvkcLtgm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c7b153-8820-4131-b1ef-defdbfbf4b2b"
      },
      "source": [
        "# doc: 23352816\n",
        "# hai token 152, 153 mc d cng c entity_id = 0 v ng cnh nhau\n",
        "# nhng hai token ny khng phi l 1 entity m l 2 entity v chng c entity_name khc nhau v chng thuc 2 cu khc nhau\n",
        "# ngoi ra token 153: \". VMISS\" dnh du chm  cu trc, v y cng l 1 entity\n",
        "# ta cn sa li trong doc ny: tch \". VMISS\" thnh 2 token \".\" v \"VMISS\"\n",
        "# c ngha l ta s thm 1 dng na cho \".\" v sa li \". VMISS\" thnh \"VMISS\"\n",
        "\n",
        "raw_tdata_new_v2 = copy.deepcopy(raw_tdata_new)\n",
        "\n",
        "for docif in raw_tdata_new_v2:\n",
        "    if docif['doc_id'] == '23352816':\n",
        "\n",
        "        for i in range(len(docif['token_ids'])):\n",
        "\n",
        "            if (docif['tokens'][i] == \".VMISS\") and (docif['tokens'][i-1] == \"c\"):\n",
        "                \n",
        "                # them 1 dong cho dau '.'\n",
        "                docif['token_ids'].insert(i, docif['token_ids'][i])\n",
        "                docif['pos'].insert(i, [docif['pos'][i][0], (docif['pos'][i][0]+1)])\n",
        "                docif['tokens'].insert(i, '.')   # copy k t '.' trong doc ri paste vo  trnh li unicode\n",
        "                docif['subtoken_ids'].insert(i, None)\n",
        "                docif['entity'].insert(i, None)\n",
        "                docif['relation'].insert(i, None)\n",
        "\n",
        "                # sua lai dong '. VMISS' (do thm '.' vo trc nn dng '. VMISS' thnh i+1)\n",
        "                docif['tokens'][i+1] = 'VMISS'\n",
        "                docif['pos'][i+1] = [docif['pos'][i+1][0] + 2, docif['pos'][i+1][1]]\n",
        "\n",
        "                # do them 1 dong nen phai sua lai token_ids phia sau va relation link toi token_ids phia sau\n",
        "                # thay i token_ids ca ton b phn di, nu c subid trong relation  u th thay i, thay i stoken_id trong relation\n",
        "                for j in range(len(docif['token_ids'])):\n",
        "                    if j > i:\n",
        "                        docif['token_ids'][j] += 1\n",
        "\n",
        "                    if docif['relation'][j] != None:\n",
        "                        for k in range(len(docif['relation'][j])):\n",
        "                            # do  trn, ton b token_ids pha sau (> i) s b thay i (cng thm 1)\n",
        "                            # nn nhng relation c stoken_id nm  phn pha sau ny cng s cn thay i theo (cng thm 1)\n",
        "                            if (docif['relation'][j][k][1] > docif['token_ids'][i]):\n",
        "                                docif['relation'][j][k][1] = docif['relation'][j][k][1] + 1\n",
        "\n",
        "\n",
        "\n",
        "print('\\n\\nCHECKING')\n",
        "for idoc, docif in enumerate(raw_tdata_new_v2):\n",
        "\n",
        "    if docif['doc_id'] == '23352816':\n",
        "\n",
        "        relation_lst = []\n",
        "        for i in range(len(raw_tdata[idoc]['relation'])):\n",
        "            if raw_tdata[idoc]['relation'][i] != None:\n",
        "                relation_lst.append(raw_tdata[idoc]['relation'][i])\n",
        "\n",
        "\n",
        "        relation_ith = 0\n",
        "        for i in range(len(docif['token_ids'])):\n",
        "\n",
        "            assert (docif['tokens'][i] == docif['text'][docif['pos'][i][0]:docif['pos'][i][1]]), \\\n",
        "            str('Wrong position')\n",
        "\n",
        "            if i < (len(docif['token_ids']) - 1):\n",
        "                if (docif['token_ids'][i] + 1) != docif['token_ids'][i+1]:\n",
        "                    assert False, str('Tokens_ids has problem')\n",
        "            \n",
        "            if docif['relation'][i] != None:\n",
        "                for j in range(len(docif['relation'][i])):\n",
        "   \n",
        "                    # so snh xem thay i stoken_id c ng khng\n",
        "                    # string ca token v pos ca token s khng i so vi raw_tdata\n",
        "                    stoken_new = docif['relation'][i][j][1]\n",
        "                    stoken_new_ele_id = docif['token_ids'].index(stoken_new)\n",
        "                    \n",
        "                    stoken = relation_lst[relation_ith][j][1]\n",
        "                    if relation_lst[relation_ith][j][2] == 1:\n",
        "                        stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken) + 1\n",
        "                    else:\n",
        "                        stoken_ele_id = raw_tdata[idoc]['token_ids'].index(stoken)\n",
        "\n",
        "                    if docif['pos'][stoken_new_ele_id] != raw_tdata[idoc]['pos'][stoken_ele_id]:\n",
        "                        '''\n",
        "                        print('\\n-----Doc: ', docif['doc_id'])\n",
        "                        print(relation_ith)\n",
        "                        print(docif['relation'][i])\n",
        "                        print(relation_lst[relation_ith])\n",
        "                        print(docif['pos'][stoken_new_ele_id])\n",
        "                        print(raw_tdata[idoc]['pos'][stoken_ele_id])\n",
        "                        '''\n",
        "                        assert False, str('ERROR CODE 3')\n",
        "                    \n",
        "                relation_ith += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('DONE. EVERYTHINGS SEEM TO BE CORRECTED :D')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "CHECKING\n",
            "DONE. EVERYTHINGS SEEM TO BE CORRECTED :D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QeZVNtl9rxT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3eef630d-0ee9-4b82-82a1-8dc5e0d39645"
      },
      "source": [
        "'''\n",
        "for idoc, docif in enumerate(raw_tdata_new_v2):\n",
        "\n",
        "    if docif['doc_id'] == '23352816':\n",
        "        for i in range(len(docif['token_ids'])):\n",
        "            for key in docif:\n",
        "                if key not in ['doc_id', 'text']:\n",
        "                    print(docif[key][i], end='\\t')\n",
        "            print('\\n')\n",
        "'''"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfor idoc, docif in enumerate(raw_tdata_new_v2):\\n\\n    if docif['doc_id'] == '23352816':\\n        for i in range(len(docif['token_ids'])):\\n            for key in docif:\\n                if key not in ['doc_id', 'text']:\\n                    print(docif[key][i], end='\\t')\\n            print('\\n')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0VsUK91_VNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f0fdd6-711d-49fe-bc77-89c8dbc2f726"
      },
      "source": [
        "### doc: 23352687\n",
        "# sent: Vn Mnh hng thng tt T37  thnh cng  ni dung chy 200m.\n",
        "# trong doc ny, trc cu trn c 2 du cch, khi dng Underthesea th Underthesea tch cu chnh xc l b 2 du cch i.\n",
        "# nhng trong doc th tokenize b li, t \"Vn\" trong doc l \" Vn\" tc l dnh du 1 cch\n",
        "# dn ti start pos ca entity khc vi start pos ca cu (chnh nhau 1 n v cho chnh nhau 1 du cch)\n",
        "# phng n: sa li pos v token ca entity trong trng hp ny, b i du cch tha. \n",
        "\n",
        "raw_tdata_new_v3 = copy.deepcopy(raw_tdata_new_v2)\n",
        "\n",
        "for docif in raw_tdata_new_v3:\n",
        "    \n",
        "    if docif['doc_id'] == '23352687':\n",
        "        for i in range(len(docif['tokens'])):\n",
        "            if docif['tokens'][i] == \"Vn\":\n",
        "                print(docif[\"pos\"][i][0])\n",
        "                print(repr(docif[\"tokens\"][i]))\n",
        "\n",
        "                docif[\"pos\"][i][0] = docif[\"pos\"][i][0] + 1   # b 1 du cch  u th pos s dch ln 1 n v\n",
        "                docif[\"tokens\"][i] = docif[\"tokens\"][i].lstrip()   # b du cch i\n",
        "\n",
        "                assert (docif['text'][docif[\"pos\"][i][0]:docif[\"pos\"][i][1]] == docif[\"tokens\"][i]), \\\n",
        "                str(\"Change Failed!\")\n",
        "\n",
        "                print(docif[\"pos\"][i][0])\n",
        "                print(repr(docif[\"tokens\"][i]))\n",
        "\n",
        "                break\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "768\n",
            "'\\xa0Vn'\n",
            "769\n",
            "'Vn'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNIBK7jecGMV"
      },
      "source": [
        "## Fix 3: remove relation between entities belong to different sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT88mgCOg4zS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c18eae-2394-4973-ef89-cf82574ea4fa"
      },
      "source": [
        "a = [[1,2,3], [4,5,6], [8,9]]\n",
        "\n",
        "print(a)\n",
        "\n",
        "del a[1][0]\n",
        "del a[1][0]\n",
        "del a[1][0]\n",
        "\n",
        "print(a)\n",
        "\n",
        "if len(a[1]) == 0:\n",
        "    print('ok')\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 3], [4, 5, 6], [8, 9]]\n",
            "[[1, 2, 3], [], [8, 9]]\n",
            "ok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkQg8RftcfoC"
      },
      "source": [
        "# trong mt s doc c trng hp entity thuc mt cu c link relation ti entity thuc mt cu khc\n",
        "#  sa li ny, ta s b i relation li gia hai entity ny\n",
        "\n",
        "def remove_error_relation(raw_data, doc_id, relation_name, relation_direction, token, id_entity, n_entity):\n",
        "\n",
        "    for docif in raw_data:\n",
        "        if docif['doc_id'] == doc_id:\n",
        "            for i in range(len(docif['relation'])):\n",
        "                if docif['relation'][i] != None:\n",
        "                    for j in range(len(docif['relation'][i])):\n",
        "\n",
        "                        # relation_name, relation_direction ca relation cn xa\n",
        "                        # token, entity_id, entity_name cng dng vi relation cn xa\n",
        "                        \n",
        "                        if (docif['relation'][i][j][0] == relation_name) and (docif['relation'][i][j][3] == relation_direction) \\\n",
        "                        and (docif['tokens'][i] == token) and (docif['entity'][i][0] == id_entity) and ((docif['entity'][i][1] == n_entity)):\n",
        "                            \n",
        "                            print('\\n\\n\\n-----', doc_id)\n",
        "                            print('--Before')\n",
        "                            for key in docif:\n",
        "                                if key not in ['doc_id', 'text']:\n",
        "                                    print(docif[key][i], end='\\t')\n",
        "\n",
        "                            del docif['relation'][i][j]\n",
        "\n",
        "                            print('\\n\\n--After delete error relation')\n",
        "                            for key in docif:\n",
        "                                if key not in ['doc_id', 'text']:\n",
        "                                    print(docif[key][i], end='\\t')\n",
        "                \n",
        "                    \n",
        "                            # nu sau khi xa relation b li m relation list thnh rng th bin relation list thnh None\n",
        "                            if (len(docif['relation'][i]) == 0):\n",
        "                                docif['relation'][i] = None\n",
        "\n",
        "                                print('\\n\\n--After removed empty relation')\n",
        "                                for key in docif:\n",
        "                                    if key not in ['doc_id', 'text']:\n",
        "                                        print(docif[key][i], end='\\t')\n",
        "\n",
        "                            break\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6QzF73ElLzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f168eede-8f11-42ff-c999-8fa06ad5ffde"
      },
      "source": [
        "raw_tdata_new_v4 = copy.deepcopy(raw_tdata_new_v3)\n",
        "\n",
        "# u vo l cc thuc tnh trong dng cha relation cn xa, \n",
        "# cn nhiu nh ny  chn chnh xc c relation cn xa\n",
        "# relation_name, relation_direction ca relation cn xa\n",
        "# token, entity_id, entity_name cng dng vi relation cn xa\n",
        "# lu  l nhng ci ny phi xem trong data ri copy tay t data paste vo\n",
        "\n",
        "remove_error_relation(raw_tdata_new_v4, '23352683', 'AFFILIATION', [11, 10], 'U16', 10, 'ORGANIZATION')\n",
        "remove_error_relation(raw_tdata_new_v4, '23353950', 'PART  WHOLE', [6, 0], 'M', 0, 'LOCATION')\n",
        "remove_error_relation(raw_tdata_new_v4, '23354695', 'LOCATED', [24, 25], 'ph', 25, 'LOCATION')\n",
        "\n",
        "remove_error_relation(raw_tdata_new_v4, '23357765', 'PART  WHOLE', [15, 14], 'Vit', 14, 'LOCATION')\n",
        "remove_error_relation(raw_tdata_new_v4, '23357765', 'PART  WHOLE', [16, 14], 'Vit', 14, 'LOCATION')\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "----- 23352683\n",
            "--Before\n",
            "117\tNone\t[528, 531]\tU16\t[10, 'ORGANIZATION']\t[['AFFILIATION', 151, None, [11, 10]]]\t\n",
            "\n",
            "--After delete error relation\n",
            "117\tNone\t[528, 531]\tU16\t[10, 'ORGANIZATION']\t[]\t\n",
            "\n",
            "--After removed empty relation\n",
            "117\tNone\t[528, 531]\tU16\t[10, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "----- 23353950\n",
            "--Before\n",
            "56\tNone\t[229, 231]\tM\t[0, 'LOCATION']\t[['PART  WHOLE', 71, None, [6, 0]]]\t\n",
            "\n",
            "--After delete error relation\n",
            "56\tNone\t[229, 231]\tM\t[0, 'LOCATION']\t[]\t\n",
            "\n",
            "--After removed empty relation\n",
            "56\tNone\t[229, 231]\tM\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "\n",
            "----- 23354695\n",
            "--Before\n",
            "365\tNone\t[1655, 1658]\tph\t[25, 'LOCATION']\t[['LOCATED', 329, None, [24, 25]]]\t\n",
            "\n",
            "--After delete error relation\n",
            "365\tNone\t[1655, 1658]\tph\t[25, 'LOCATION']\t[]\t\n",
            "\n",
            "--After removed empty relation\n",
            "365\tNone\t[1655, 1658]\tph\t[25, 'LOCATION']\tNone\t\n",
            "\n",
            "\n",
            "----- 23357765\n",
            "--Before\n",
            "283\tNone\t[1238, 1242]\tVit\t[14, 'LOCATION']\t[['PART  WHOLE', 329, None, [15, 14]], ['PART  WHOLE', 332, None, [16, 14]]]\t\n",
            "\n",
            "--After delete error relation\n",
            "283\tNone\t[1238, 1242]\tVit\t[14, 'LOCATION']\t[['PART  WHOLE', 332, None, [16, 14]]]\t\n",
            "\n",
            "\n",
            "----- 23357765\n",
            "--Before\n",
            "283\tNone\t[1238, 1242]\tVit\t[14, 'LOCATION']\t[['PART  WHOLE', 332, None, [16, 14]]]\t\n",
            "\n",
            "--After delete error relation\n",
            "283\tNone\t[1238, 1242]\tVit\t[14, 'LOCATION']\t[]\t\n",
            "\n",
            "--After removed empty relation\n",
            "283\tNone\t[1238, 1242]\tVit\t[14, 'LOCATION']\tNone\t"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBr6Gx-Y2oj2"
      },
      "source": [
        "## Find all entity in a doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVl3FKeND9dk"
      },
      "source": [
        "# Tm tt c cc entity trong mt doc:\n",
        "\n",
        "def find_all_entity_in_doc(raw_data, doc_id):\n",
        "    \n",
        "    for docif in raw_data:\n",
        "\n",
        "        ######## Tm tt c cc entity trong doc hin ti:\n",
        "\n",
        "        #                      -                                          entity_1                                              -  ...\n",
        "        #                      | -                 token_1                  -  -                   token_2                -  ...|\n",
        "        ##### doc_entity_lst = [ [ [ele_id, token_id, entity_id, entity_name], [ele_id, token_id, entity_id, , entity_name], ...], ...]\n",
        "\n",
        "        ### Lu : ele_id  y l element index ca token y trong docif[\"entity\"]\n",
        "        ### ch khng phi l token_ids\n",
        "        ### lu ci ele_id thay v token_ids  truy cp token bng index nhanh hn\n",
        "        ### nu lu token_ids th phi t t token_ids tm xem token ny n nm v tr no th mi ra index (ele_id)  truy\n",
        "        ### thng th, v token_ids bt u = 1, element index bt u = 0 nn: token_ids tng ng s c: token_ids = ele_id + 1\n",
        "        ### nhng nu trong doc c subtoken th iu trn s khng c m bo\n",
        "\n",
        "        # token c entity_id ging nhau m khng ng cnh nhau (cc dng cha token khng lin tip nhau) th thuc 2 entity khc nhau. \n",
        "        # Trng hp ny l do dataset li, b lp entity_id (bn trn c entity_id = 0 ri xung di (token khng cnh nhau) li thy entity_id = 0)\n",
        "        # Hin ti mi thy c entity_id = 0 l b lp li cho nhiu entity khc nhau\n",
        "\n",
        "        \n",
        "\n",
        "        if docif['doc_id'] == doc_id:\n",
        "\n",
        "            doc_entity_lst = []\n",
        "            tmplst = []\n",
        "\n",
        "            for i in range(len(docif[\"entity\"])):\n",
        "                if docif[\"entity\"][i] != None:\n",
        "                    tmplst.append([i, docif[\"token_ids\"][i], docif[\"entity\"][i][0], docif[\"entity\"][i][1]])\n",
        "\n",
        "                    if (i < (len(docif[\"entity\"]) - 1)):\n",
        "\n",
        "                        if docif[\"entity\"][i+1] == None:\n",
        "                            doc_entity_lst.append(tmplst)\n",
        "                            tmplst = []\n",
        "\n",
        "                        elif (docif[\"entity\"][i][0] != docif[\"entity\"][i+1][0]) or (docif[\"entity\"][i][1] != docif[\"entity\"][i+1][1]):\n",
        "                            doc_entity_lst.append(tmplst)\n",
        "                            tmplst = []\n",
        "\n",
        "                    if i == (len(docif[\"entity\"]) - 1):\n",
        "                        doc_entity_lst.append(tmplst)\n",
        "\n",
        "\n",
        "            ###################### Fix li hai entity khc nhau nhng ng cnh nhau v b trng entity_id\n",
        "            # mi cp entity_id = 0 ng cnh nhau trong cc doc trong list bn di u s b tch ra thnh cc entity ring\n",
        "            # tuy nhin ta s x l tng cp mt trong tng ln x l, run_times l s cp trng trong doc\n",
        "            # nu c doc no va c cp entity_0 li va c cp khng li th phi s khng c thm list di vo m phi x l ring\n",
        "            # tr khi cp b li l cp u tin th run_times t l 1\n",
        "            # data cng khng c qu nhiu nhng cp nh ny\n",
        "            \n",
        "            doc_error_lst = [{'doc_error_id': '23351965', 'ith_er_lst': [8]},\n",
        "                             {'doc_error_id': '23352690', 'ith_er_lst': [51]},\n",
        "                             {'doc_error_id': '23352701', 'ith_er_lst': [16]},\n",
        "                             {'doc_error_id': '23352748', 'ith_er_lst': [76]},\n",
        "                             {'doc_error_id': '23352753', 'ith_er_lst': [37]},\n",
        "                             {'doc_error_id': '23352853', 'ith_er_lst': [15, 17]},\n",
        "                             {'doc_error_id': '23352878', 'ith_er_lst': [17]},\n",
        "                             {'doc_error_id': '23353786', 'ith_er_lst': [2]},\n",
        "                             {'doc_error_id': '23353891', 'ith_er_lst': [8]},\n",
        "                             {'doc_error_id': '23354619', 'ith_er_lst': [4, 6]},\n",
        "                             {'doc_error_id': '23354880', 'ith_er_lst': [113]}\n",
        "                            ]\n",
        "\n",
        "            doc_error_id_lst = [doc_error['doc_error_id'] for doc_error in doc_error_lst]\n",
        "\n",
        "            increase_ids = 0\n",
        "\n",
        "            if doc_id in doc_error_id_lst:\n",
        "                ith_doc_id = doc_error_id_lst.index(doc_id)\n",
        "\n",
        "                assert (doc_error_lst[ith_doc_id]['doc_error_id'] == doc_id), str('PRBOLEM')\n",
        "\n",
        "                #print('\\n\\n------', doc_id)\n",
        "\n",
        "                ith_er_list = sorted(doc_error_lst[ith_doc_id]['ith_er_lst'])\n",
        "\n",
        "                for irun, ith_er_id in enumerate(ith_er_list):\n",
        "                    #print('--', irun)\n",
        "                    doc_entity_lst_copy = None\n",
        "                    doc_entity_lst_copy = copy.deepcopy(doc_entity_lst)\n",
        "\n",
        "                    for ient, ent in enumerate(doc_entity_lst):\n",
        "                        if ient == (ith_er_id + increase_ids): # do b dch nn cn cng vi s id b dch\n",
        "\n",
        "                            assert ((ent[0][2] == 0) and (len(ent) > 1)), \\\n",
        "                            str('\\nWrong ith_er_lst. \\nDoc: ' + str(doc_id) + '\\nith_er_id: ' + str(ith_er_id))\n",
        "\n",
        "                            #print(doc_entity_lst_copy)\n",
        "\n",
        "                            # v d: 3 token th ch cn chy 3 - 1 = 2 ln\n",
        "                            # ln u (itk = 0) th ly token cui entity (token th 3: ent[-1]) chn vo sau v tr hin ti ca entity\n",
        "                            # ln hai (itk = 1) th ly token ngay trc token cui (token th 2 t cui ln: ent[-2]) chn vo sau v tr hin ti ca entity\n",
        "                            for itk in range(len(ent) - 1):\n",
        "                                doc_entity_lst_copy.insert((ient+1), copy.deepcopy([ent[(-1 - itk)]]))\n",
        "\n",
        "                            # cui cng th bin entity li hin ti thnh token u ca entity li hin ti l xong\n",
        "                            doc_entity_lst_copy[ient] = copy.deepcopy([ent[0]])\n",
        "\n",
        "                            #print(doc_entity_lst_copy)\n",
        "\n",
        "                            # do bn trn ta chn thm (len(ent) - 1) entity mi vo entity list\n",
        "                            # nn id ca entity b li pha sau s b tng ln lng tng ng\n",
        "                            # l tng (len(ent)-1) ca mi ent b sa trc n\n",
        "                            increase_ids += (len(ent) - 1)\n",
        "\n",
        "                            break\n",
        "                            \n",
        "                    doc_entity_lst = copy.deepcopy(doc_entity_lst_copy)\n",
        "                    #print(doc_entity_lst)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "            \n",
        "\n",
        "\n",
        "            '''\n",
        "            # Doc: 23351965\n",
        "            # hai token cnh nhau (320, 321), cng entity_id 0, cng entity_name location nhng khng phi l 1 entity <-- li data\n",
        "            # m l hai entity, v entity ny link ti entity kia.\n",
        "\n",
        "            if docif['doc_id'] == '23351965':\n",
        "                doc_entity_lst_copy = copy.deepcopy(doc_entity_lst)\n",
        "                for ient, ent in enumerate(doc_entity_lst):\n",
        "                    if (len(ent) == 2) and (ent[0][2] == 0) and (ent[1][2] == 0):\n",
        "                        #print(doc_entity_lst_copy)\n",
        "                        doc_entity_lst_copy.insert((ient+1), [doc_entity_lst_copy[ient][1]])\n",
        "                        doc_entity_lst_copy[ient] = [doc_entity_lst_copy[ient][0]]\n",
        "                        #print(doc_entity_lst_copy)\n",
        "                        \n",
        "                doc_entity_lst = copy.deepcopy(doc_entity_lst_copy)\n",
        "                #print(doc_entity_lst)\n",
        "\n",
        "\n",
        "            # Doc: 23352753  b ging bn trn\n",
        "            # hai token cnh nhau (884, 885), cng entity_id 0, cng entity_name location nhng khng phi l 1 entity <-- li data\n",
        "            # m l hai entity, v entity ny link ti entity kia.\n",
        "\n",
        "            if docif['doc_id'] == '23352753':\n",
        "                doc_entity_lst_copy_2 = copy.deepcopy(doc_entity_lst)\n",
        "                for ient, ent in enumerate(doc_entity_lst):\n",
        "                    if (len(ent) == 2) and (ent[0][2] == 0) and (ent[1][2] == 0):\n",
        "                        #print(doc_entity_lst_copy_2)\n",
        "                        doc_entity_lst_copy_2.insert((ient+1), [doc_entity_lst_copy_2[ient][1]])\n",
        "                        doc_entity_lst_copy_2[ient] = [doc_entity_lst_copy_2[ient][0]]\n",
        "                        #print(doc_entity_lst_copy_2)\n",
        "                        \n",
        "                doc_entity_lst = copy.deepcopy(doc_entity_lst_copy_2)\n",
        "                #print(doc_entity_lst)\n",
        "            \n",
        "            '''\n",
        "\n",
        "\n",
        "            \n",
        "            return doc_entity_lst\n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIA5LQry20nQ"
      },
      "source": [
        "# thng th nhng entity cnh nhau v trng id khin ta d lm thnh 1 entity\n",
        "# th nhng entity ny thng c entity_id = 0\n",
        "# nn ta s tm cc cp entity_id = 0 ny v xem cp no l li cp no khng li\n",
        "# sau  c xem trong doc ny y l li hay khng phi li (c cp khng phi li)\n",
        "# nu l cp li th s phi thm trng hp  cell code bn trn\n",
        "#  v sau dng hm tm entity bn trn s khng b li\n",
        "\n",
        "# MT LU  L: \n",
        "# V D: \n",
        "# 1-209\t935-944\tFrankfurt\t*\tLOCATION\t\n",
        "# 1-210\t945-949\t(c\t*\tLOCATION\t\n",
        "\n",
        "# 1-161\t739-743\tBali\t*\tLOCATION\t_\t_\t\n",
        "# 1-162\t744-754\t(Indonesia\t*\tLOCATION\tPART  WHOLE\t1-161\t\n",
        "               \n",
        "# gn khng chnh xc, bn trn khng c relation nhng bn di li c\n",
        "# nn nu c relation nh bn di th tch ra lm 2 entity\n",
        "# cn khng c relation nh bn trn th  n l mt entity\n",
        "# v nu khng c relation m vn tch ra lm 2 entity th label gia chng s l others, khng chnh xc\n",
        "# nu  l cng 1 entity th s hp l hn. min l  cng entity khng nh hng g\n",
        "# v trong data c rt nhiu label other gia cc location (nh M vi Anh c th l others)\n",
        "# nn nu ta chia ra th sau trong test set cng b  lm others, tc l khng hp l\n",
        "# th  thnh 1, th train data cng th m test data cng th\n",
        "# hoc ta c th tch nhng phi thm nhn part-whole vo\n",
        "\n",
        "# cng c th ton b cc entity_id = 0 cnh nhau u l cc entity khc nha, nhng nu khng phi th s khng hon ho\n",
        "# nn c th xt cc trng hp ring thay v t ng tch cc entity_0 cnh nhau thnh cc entity khc nhau\n",
        "\n",
        "def find_all_fault_entity_id_0(raw_data):\n",
        "\n",
        "    for docif in raw_data:\n",
        "        ent_lst = find_all_entity_in_doc(raw_data, docif['doc_id'])\n",
        "        \n",
        "        for i in range(len(ent_lst)):\n",
        "\n",
        "            if (len(ent_lst[i]) > 1) and (ent_lst[i][0][2] == 0):\n",
        "                print('\\n\\n------', docif['doc_id'], ' -ith: ', i)\n",
        "                for j in range(len(ent_lst[i])):\n",
        "                    first_tk_eleid = ent_lst[i][j][0]\n",
        "                    for key in docif:\n",
        "                        if key not in ['doc_id', 'text']:\n",
        "                            print(docif[key][first_tk_eleid], end='\\t')\n",
        "                    \n",
        "                    print('\\n')\n",
        "        \n",
        "\n",
        "\n",
        "                \n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POdyqoDS5dA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063be3a4-562e-4cb7-8539-b4116dde1c1f"
      },
      "source": [
        "# tn hm hi gy nhm\n",
        "# cc entity c in ra c th l li hoc l khng li, a phn l li\n",
        "# v cc enity li bn di l quyt nh khng sa\n",
        "find_all_fault_entity_id_0(raw_tdata_new_v4)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "------ 23351888  -ith:  11\n",
            "209\tNone\t[935, 944]\tFrankfurt\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "210\tNone\t[945, 949]\t(c\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23351994  -ith:  25\n",
            "452\tNone\t[1967, 1972]\tTrung\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "453\tNone\t[1973, 1976]\tVTM\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352746  -ith:  3\n",
            "23\tNone\t[94, 98]\tBali\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "24\tNone\t[99, 109]\t(Indonesia\t[0, 'LOCATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352802  -ith:  5\n",
            "75\tNone\t[336, 339]\tAFP\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "76\tNone\t[340, 346]\t/TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352804  -ith:  5\n",
            "58\tNone\t[267, 270]\tAFP\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "59\tNone\t[271, 277]\t/TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23352804  -ith:  35\n",
            "385\tNone\t[1771, 1777]\t(TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "386\tNone\t[1778, 1787]\t/Vietnam+\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23353849  -ith:  10\n",
            "72\tNone\t[353, 356]\tAFP\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "73\tNone\t[357, 363]\t/TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23353849  -ith:  43\n",
            "592\tNone\t[2793, 2799]\t(TTXVN\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "593\tNone\t[2800, 2809]\t/Vietnam+\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------ 23355817  -ith:  15\n",
            "474\tNone\t[2177, 2179]\tc\t[0, 'ORGANIZATION']\t[['AFFILIATION', 467, None, [8, 0]]]\t\n",
            "\n",
            "475\tNone\t[2180, 2184]\t(SIC\t[0, 'ORGANIZATION']\tNone\t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGR4QQfBauVE"
      },
      "source": [
        "## Fix 4: Fix li relation khng nm  dng cha token u tin trong entity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRCOfiuJa5LZ"
      },
      "source": [
        "# trong data c th xy ra trng hp, relation khng nm  token u ca mt entity\n",
        "# li ny c th do data nh vy\n",
        "# v d: relation tr ti subtoken\n",
        "# ngoi ra code bn di cng gip loi b cc relation b lp, tc l relation khng ch ch nm  token u entity th cn nm  token khc na\n",
        "# lu : hm bn di s khng thay i raw_data m s to bn copy v thay i trn bn copy ny, ri tr v bn copy ny\n",
        "\n",
        "def fix_relation_not_in_1st_token(raw_data):\n",
        "\n",
        "    raw_data_new = copy.deepcopy(raw_data)\n",
        "\n",
        "    for docif in raw_data_new:\n",
        "\n",
        "        ent_lst = find_all_entity_in_doc(raw_data, docif['doc_id'])\n",
        "\n",
        "        for i in range(len(docif['relation'])):\n",
        "            if docif['relation'][i] != None:  \n",
        "                found = False\n",
        "                for ient, ent in enumerate(ent_lst):\n",
        "                    for itoken in range(len(ent)):\n",
        "                        if i == ent[itoken][0]:\n",
        "                            found = True\n",
        "\n",
        "                            if itoken != 0:   # neu ma khong nam tai token dau cua entity\n",
        "\n",
        "                                print('\\n\\n------ ', docif['doc_id'])\n",
        "                                print('Before')\n",
        "                                for itoken_1 in range(len(ent)):\n",
        "                                    token_ele_id = ent[itoken_1][0]\n",
        "                                    for key in docif:\n",
        "                                        if key not in ['doc_id', 'text']:\n",
        "                                            print(docif[key][token_ele_id], end='\\t')\n",
        "                                    print('\\n')\n",
        "\n",
        "                                # chuyen relation len token dau\n",
        "                                if docif['relation'][ent[0][0]] != None:\n",
        "                                    for rel in docif['relation'][i]:\n",
        "                                        if rel not in docif['relation'][ent[0][0]]:   # nu khng c th mi thm vo, cn c ri th ch cn xa lp thi\n",
        "                                            docif['relation'][ent[0][0]].append(rel)\n",
        "\n",
        "                                else:\n",
        "                                    docif['relation'][ent[0][0]] = copy.deepcopy(docif['relation'][i])\n",
        "                                \n",
        "                                docif['relation'][i] = None   # chuyen xong thi xoa relation cu di\n",
        "\n",
        "                                print('After')\n",
        "                                for itoken_1 in range(len(ent)):\n",
        "                                    token_ele_id = ent[itoken_1][0]\n",
        "                                    for key in docif:\n",
        "                                        if key not in ['doc_id', 'text']:\n",
        "                                            print(docif[key][token_ele_id], end='\\t')\n",
        "                                    print('\\n')\n",
        "\n",
        "                assert (found == True), str('Not found relation in any entity')\n",
        "\n",
        "    return raw_data_new\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Mnxw1W_qy2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22bc5dad-3cc0-41d2-e03e-9697f79e7a80"
      },
      "source": [
        "raw_tdata_new_v5 = copy.deepcopy(raw_tdata_new_v4)\n",
        "raw_tdata_new_v5 = fix_relation_not_in_1st_token(raw_tdata_new_v5)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "------  23356574\n",
            "Before\n",
            "738\tNone\t[3318, 3324]\tTrng\t[18, 'ORGANIZATION']\t[['PART  WHOLE', 733, None, [17, 18]]]\t\n",
            "\n",
            "739\tNone\t[3325, 3330]\tphng\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "740\tNone\t[3331, 3335]\tGio\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "741\tNone\t[3336, 3339]\tdc\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "742\tNone\t[3340, 3346]\tchuyn\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "743\tNone\t[3347, 3353]\tnghip\t[18, 'ORGANIZATION']\t[['PART  WHOLE', 733, None, [17, 18]]]\t\n",
            "\n",
            "After\n",
            "738\tNone\t[3318, 3324]\tTrng\t[18, 'ORGANIZATION']\t[['PART  WHOLE', 733, None, [17, 18]]]\t\n",
            "\n",
            "739\tNone\t[3325, 3330]\tphng\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "740\tNone\t[3331, 3335]\tGio\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "741\tNone\t[3336, 3339]\tdc\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "742\tNone\t[3340, 3346]\tchuyn\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "743\tNone\t[3347, 3353]\tnghip\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------  23357000\n",
            "Before\n",
            "173\tNone\t[807, 810]\tvn\t[8, 'ORGANIZATION']\t[['AFFILIATION', 166, None, [7, 8]]]\t\n",
            "\n",
            "174\tNone\t[811, 816]\tphng\t[8, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "175\tNone\t[817, 821]\tkin\t[8, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "176\tNone\t[822, 826]\ttrc\t[8, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "177\tNone\t[827, 832]\t1+1>2\t[8, 'ORGANIZATION']\t[['AFFILIATION', 166, None, [7, 8]]]\t\n",
            "\n",
            "After\n",
            "173\tNone\t[807, 810]\tvn\t[8, 'ORGANIZATION']\t[['AFFILIATION', 166, None, [7, 8]]]\t\n",
            "\n",
            "174\tNone\t[811, 816]\tphng\t[8, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "175\tNone\t[817, 821]\tkin\t[8, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "176\tNone\t[822, 826]\ttrc\t[8, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "177\tNone\t[827, 832]\t1+1>2\t[8, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "\n",
            "------  23357000\n",
            "Before\n",
            "260\tNone\t[1203, 1206]\tVn\t[12, 'ORGANIZATION']\t[['AFFILIATION', 253, None, [11, 12]]]\t\n",
            "\n",
            "261\tNone\t[1207, 1212]\tphng\t[12, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "262\tNone\t[1213, 1217]\tkin\t[12, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "263\tNone\t[1218, 1222]\ttrc\t[12, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "264\tNone\t[1223, 1228]\t1+1>2\t[12, 'ORGANIZATION']\t[['AFFILIATION', 253, None, [11, 12]]]\t\n",
            "\n",
            "After\n",
            "260\tNone\t[1203, 1206]\tVn\t[12, 'ORGANIZATION']\t[['AFFILIATION', 253, None, [11, 12]]]\t\n",
            "\n",
            "261\tNone\t[1207, 1212]\tphng\t[12, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "262\tNone\t[1213, 1217]\tkin\t[12, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "263\tNone\t[1218, 1222]\ttrc\t[12, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "264\tNone\t[1223, 1228]\t1+1>2\t[12, 'ORGANIZATION']\tNone\t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQt6fgmG7foW"
      },
      "source": [
        "## Fix 5: li relation link ti token khng phi token u ca entity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzIm8E4d7rVq"
      },
      "source": [
        "# trong data, thng nhng entity c cha subtoken s b li ny\n",
        "# tc l nu trong entity ngoi token y , cn c subtoken ca 1 token no \n",
        "# th relation link ti entity ny s c stoken_id l id ca subtoken kia\n",
        "# v subtoken thng ng cui entity nn dn ti vic relation khng link ti token u ca 1 entity\n",
        "# ta s sa li cho link ti ng token u (v bn trn chng ta cng  tch, b i subid ri)\n",
        "\n",
        "# ta cn a vo thng s chnh xc ca relation cn sa\n",
        "# tc l phi xem doc b li ri ly thng tin in vo ch khng t pht hin c v link sai c nhiu l do ch khng phi mi l do ny.\n",
        "# tuy nhin ch cn in thng tin dng ch relation li ch khng cn in id token u entity ng\n",
        "\n",
        "# code bn di c th dng  i relation link t token gia entity ti token u ch khng ch l token cui ti token u\n",
        "\n",
        "def change_relation_link(raw_data, doc_id, relation_name, relation_direction, token, id_entity, n_entity):\n",
        "    \n",
        "    for docif in raw_data:\n",
        "        if docif['doc_id'] == doc_id:\n",
        "            for i in range(len(docif['relation'])):\n",
        "                if docif['relation'][i] != None:\n",
        "                    for j in range(len(docif['relation'][i])):\n",
        "\n",
        "                        if (docif['relation'][i][j][0] == relation_name) and (docif['relation'][i][j][3] == relation_direction) \\\n",
        "                        and (docif['tokens'][i] == token) and (docif['entity'][i][0] == id_entity) and ((docif['entity'][i][1] == n_entity)):\n",
        "                            \n",
        "                            print('\\n\\n\\n-----', doc_id)\n",
        "                            print('--Before change relation')\n",
        "                            for key in docif:\n",
        "                                if key not in ['doc_id', 'text']:\n",
        "                                    print(docif[key][i], end='\\t')\n",
        "\n",
        "                        \n",
        "                            ### thu thp mi entity trong doc\n",
        "                            entity_lst = find_all_entity_in_doc(raw_data, doc_id)\n",
        "\n",
        "    \n",
        "                            ### tm xem relation stoken_id nm trong entity no\n",
        "                            # th tr relation stoken_id v u entity y\n",
        "\n",
        "                            found_id = -1\n",
        "                            for ient in range(len(entity_lst)):\n",
        "                                for itoken in range(len(entity_lst[ient])):\n",
        "                                    if docif['relation'][i][j][1] == entity_lst[ient][itoken][1]:   # tm xem stoken_id thuc entity no\n",
        "                                        docif['relation'][i][j][1] = copy.deepcopy(entity_lst[ient][0][1])   # tm thy th cp nht thnh id token u entity\n",
        "                                        found_id = ient\n",
        "\n",
        "                            assert (found_id != -1), str('Not found entity has relation stoken_id')\n",
        "\n",
        "                            print('\\n\\nEntity that relation link to')\n",
        "                            for itoken in range(len(entity_lst[found_id])):\n",
        "                                token_ele_id = entity_lst[found_id][itoken][0]\n",
        "                                for key in docif:\n",
        "                                    if key not in ['doc_id', 'text']:\n",
        "                                        print(docif[key][token_ele_id], end='\\t')\n",
        "                                print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "                            print('\\n--After change relation link')\n",
        "                            for key in docif:\n",
        "                                if key not in ['doc_id', 'text']:\n",
        "                                    print(docif[key][i], end='\\t')\n",
        "                \n",
        "                    \n",
        "         \n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZm3ZC_eQ5qz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "711ec6cf-0329-4a5d-d1e0-129b87fcd414"
      },
      "source": [
        "raw_tdata_new_v6 = copy.deepcopy(raw_tdata_new_v5)\n",
        "\n",
        "change_relation_link(raw_tdata_new_v6, '23356574', 'PART  WHOLE', [18, 19], 'S', 19, 'ORGANIZATION')\n",
        "change_relation_link(raw_tdata_new_v6, '23357063', 'PART  WHOLE', [1, 2], 'Hi', 2, 'ORGANIZATION')\n",
        "change_relation_link(raw_tdata_new_v6, '23357063', 'PART  WHOLE', [1, 3], 'Vng', 3, 'LOCATION')\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "----- 23356574\n",
            "--Before change relation\n",
            "745\tNone\t[3355, 3357]\tS\t[19, 'ORGANIZATION']\t[['AFFILIATION', 733, None, [17, 19]], ['PART  WHOLE', 743, None, [18, 19]]]\t\n",
            "\n",
            "Entity that relation link to\n",
            "738\tNone\t[3318, 3324]\tTrng\t[18, 'ORGANIZATION']\t[['PART  WHOLE', 733, None, [17, 18]]]\t\n",
            "\n",
            "739\tNone\t[3325, 3330]\tphng\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "740\tNone\t[3331, 3335]\tGio\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "741\tNone\t[3336, 3339]\tdc\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "742\tNone\t[3340, 3346]\tchuyn\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "743\tNone\t[3347, 3353]\tnghip\t[18, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "--After change relation link\n",
            "745\tNone\t[3355, 3357]\tS\t[19, 'ORGANIZATION']\t[['AFFILIATION', 733, None, [17, 19]], ['PART  WHOLE', 738, None, [18, 19]]]\t\n",
            "\n",
            "\n",
            "----- 23357063\n",
            "--Before change relation\n",
            "103\tNone\t[462, 465]\tHi\t[2, 'ORGANIZATION']\t[['PART  WHOLE', 101, None, [1, 2]]]\t\n",
            "\n",
            "Entity that relation link to\n",
            "99\tNone\t[448, 451]\ttu\t[1, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "100\tNone\t[452, 455]\tCSB\t[1, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "101\tNone\t[456, 460]\t4006\t[1, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "--After change relation link\n",
            "103\tNone\t[462, 465]\tHi\t[2, 'ORGANIZATION']\t[['PART  WHOLE', 99, None, [1, 2]]]\t\n",
            "\n",
            "\n",
            "----- 23357063\n",
            "--Before change relation\n",
            "107\tNone\t[476, 480]\tVng\t[3, 'LOCATION']\t[['PART  WHOLE', 103, None, [2, 3]], ['PART  WHOLE', 101, None, [1, 3]]]\t\n",
            "\n",
            "Entity that relation link to\n",
            "99\tNone\t[448, 451]\ttu\t[1, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "100\tNone\t[452, 455]\tCSB\t[1, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "101\tNone\t[456, 460]\t4006\t[1, 'ORGANIZATION']\tNone\t\n",
            "\n",
            "\n",
            "--After change relation link\n",
            "107\tNone\t[476, 480]\tVng\t[3, 'LOCATION']\t[['PART  WHOLE', 103, None, [2, 3]], ['PART  WHOLE', 99, None, [1, 3]]]\t"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfXerQf-fQSv"
      },
      "source": [
        "# Create train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qce_9RiBsIZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "6caecd16-7d4a-40e7-e430-ebbf56bdab53"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "                    label: \n",
        "                    Du x l k hiu relation label (docif[\"relation\"]) xut hin  ch ca entity no\n",
        "\n",
        "                              Entity:                 entity_1    -    entity_2\n",
        "                    Th t trong cu:                 trc            sau\n",
        "                      \n",
        "                      Relation label:     LOCATED                         x     (per/org - loc)\n",
        "                                       IS_LOCATED         x                     (loc     - per/org)\n",
        "                                       PARTWHOLE\t                      x     (part    - whole)\n",
        "                                       WHOLE-PART         x                     (whole   - part)\n",
        "                                  PERSONALSOCIAL                               (Undirected)\n",
        "                                      AFFILIATION\t                      x     \n",
        "                                   AFFILIATION_TO         x\n",
        "                                           OTHERS                               (l nhn gi 2 entity cng 1 cu m khng c relation trong data)\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n                    label: \\n                    Du x l k hiu relation label (docif[\"relation\"]) xut hin  ch ca entity no\\n\\n                              Entity:                 entity_1    -    entity_2\\n                    Th t trong cu:                 trc            sau\\n                      \\n                      Relation label:     LOCATED                         x     (per/org - loc)\\n                                       IS_LOCATED         x                     (loc     - per/org)\\n                                       PARTWHOLE\\t                      x     (part    - whole)\\n                                       WHOLE-PART         x                     (whole   - part)\\n                                  PERSONALSOCIAL                               (Undirected)\\n                                      AFFILIATION\\t                      x     \\n                                   AFFILIATION_TO         x\\n                                           OTHERS                               (l nhn gi 2 entity cng 1 cu m khng c relation trong data)\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqEMUOpfqHZp"
      },
      "source": [
        "original_labels = ['LOCATED', 'PART  WHOLE', 'PERSONAL - SOCIAL', 'AFFILIATION']\n",
        "\n",
        "# entity cha relation nm  pha sau th l label gc\n",
        "labels = {'LOCATED': 'LOCATED', 'IS_LOCATED': 'IS_LOCATED', \n",
        "         'PART_WHOLE': 'PART_WHOLE', 'WHOLE_PART': 'WHOLE_PART', \n",
        "         'PERSONAL_SOCIAL': 'PERSONAL_SOCIAL', \n",
        "         'AFFILIATION': 'AFFILIATION', 'AFFILIATION_TO': 'AFFILIATION_TO', \n",
        "         'OTHERS': 'OTHERS'\n",
        "         }"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8crlbk4MJTlx"
      },
      "source": [
        "### Func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBWuvOzUJUCP"
      },
      "source": [
        "def split_by_colon_punc(doc_sent_tokenize):\n",
        "\n",
        "    new_doc_sent_tokenize = []\n",
        "    for isent, sent in enumerate(doc_sent_tokenize):\n",
        "        if ':' not in sent:\n",
        "            new_doc_sent_tokenize.append(sent)\n",
        "        \n",
        "        else:\n",
        "            new_sents = sent.split(\":\")\n",
        "                \n",
        "            for inew_sent, new_sent in enumerate(new_sents):\n",
        "                if inew_sent != (len(new_sents) - 1):\n",
        "                    new_doc_sent_tokenize.append(str(new_sent.lstrip() + ':'))\n",
        "                else:\n",
        "                    new_doc_sent_tokenize.append(str(new_sent.strip()))\n",
        "\n",
        "    return new_doc_sent_tokenize\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt1PwbNGJG_W"
      },
      "source": [
        "from underthesea import sent_tokenize, word_tokenize\n",
        "\n",
        "def my_sentences_tokenize(doc_id, text):\n",
        "    ######## split sentence from docif[\"text\"] using Underthesea library\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # check if sum length of all sentence < len(text)\n",
        "    len_sentences = [len(s) for s in sentences]\n",
        "    assert (sum(len_sentences) <= len(text)), str(\"\\nSentence tokenize has problem. \\nDoc: \" + docif[\"doc_id\"])\n",
        "\n",
        "\n",
        "\n",
        "    ######\n",
        "    ### trong doc ny vic chia sentence bng Underthesea b li dn ti vic mt entity nm  2 cu.\n",
        "    new_sentences = []\n",
        "    \n",
        "    if doc_id == \"23352748\":\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (sent[-3:] != \"St.\") and (sent[:4] != \"Mary\"):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (sent[-3:] == \"St.\") and (sentences[isent + 1][:4] == \"Mary\"):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "        \n",
        "    ### trong cc doc bn di vic chia sentence bng Underthesea b li dn ti vic mt relation link ti mt entity thuc cu khc.\n",
        "    ### thng do sau tn ngi vit tt c du chm\n",
        "    \n",
        "    elif doc_id == \"23351426\":\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (sent != \"c bit, Nguyn c quan h tnh cm vi ch H.T.H.\") and (sent != \"(qu H Ni , ch ca hng thi trang).\"):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (sent == \"c bit, Nguyn c quan h tnh cm vi ch H.T.H.\") and (sentences[isent + 1] == \"(qu H Ni , ch ca hng thi trang).\"):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "\n",
        "    \n",
        "    elif doc_id == \"23351515\":\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (sent != \"C quan ny cng ang khn trng iu tra, lm r ci cht ca ch N.T.T.\") and (sent != \"(SN 1983, ng x Vn Hng , huyn Vn Ninh ), v ca ngi n ng t vong trong v tai nn trn.\"):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (sent == \"C quan ny cng ang khn trng iu tra, lm r ci cht ca ch N.T.T.\") and (sentences[isent + 1] == \"(SN 1983, ng x Vn Hng , huyn Vn Ninh ), v ca ngi n ng t vong trong v tai nn trn.\"):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "        \n",
        "        new_sentences_1 = []\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"...\" not in sent):\n",
        "                new_sentences_1.append(sent)\n",
        "            \n",
        "            elif (\"...\" in sent):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_1.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences_1.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences_1)\n",
        "        sentences = copy.deepcopy(new_sentences_1)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "    \n",
        "    elif doc_id == \"23353874\":\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"sau thi gian iu tr ti BV, ch Trn Th .\" not in sent) and (\"(35 tui, ng qun Th c )\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"sau thi gian iu tr ti BV, ch Trn Th .\" in sent) and (\"(35 tui, ng qun Th c )\" in sentences[isent + 1]):\n",
        "                new_sentences.append(str(sent + ' ' + sentences[isent + 1]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    ######\n",
        "    # c mt s doc c ...\n",
        "\n",
        "    elif doc_id == '23351164':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"x l ngay Tip  PV\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"x l ngay Tip  PV\" in sent):\n",
        "                tmppp = sent.find(\" Tip  PV\")\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "        new_sentences_3 = []\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('.' in sent) and ('.' != sent[-2:]):\n",
        "                new_sents = sent.split('.')\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_3.append(str(new_sent.lstrip() + '.'))\n",
        "                    else:\n",
        "                        new_sentences_3.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ('.' not in sent) or ('.' == sent[-2:]):\n",
        "                new_sentences_3.append(sent)\n",
        "            \n",
        "        \n",
        "        #print(new_sentences_3)\n",
        "        sentences = copy.deepcopy(new_sentences_3)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == '23354055':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "\n",
        "            if (\"\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"\" in sent):\n",
        "                new_sents = sent.split(\"\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + ''))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "        new_sentences_3 = []\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('.' in sent) and ('.' != sent[-2:]):\n",
        "                new_sents = sent.split('.')\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_3.append(str(new_sent.lstrip() + '.'))\n",
        "                    else:\n",
        "                        new_sentences_3.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ('.' not in sent) or ('.' == sent[-2:]):\n",
        "                new_sentences_3.append(sent)\n",
        "            \n",
        "        \n",
        "        #print(new_sentences_3)\n",
        "        sentences = copy.deepcopy(new_sentences_3)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "    elif doc_id == '23354045':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "\n",
        "            if (\"...\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"...\" in sent):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "        new_sentences_3 = []\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('.' in sent) and ('.' != sent[-2:]):\n",
        "                new_sents = sent.split('.')\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_3.append(str(new_sent.lstrip() + '.'))\n",
        "                    else:\n",
        "                        new_sentences_3.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ('.' not in sent) or ('.' == sent[-2:]):\n",
        "                new_sentences_3.append(sent)\n",
        "            \n",
        "        \n",
        "        #print(new_sentences_3)\n",
        "        sentences = copy.deepcopy(new_sentences_3)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == '23351841':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"ban PH. Bc th ca anh\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"ban PH. Bc th ca anh\" in sent):\n",
        "                tmppp = sent.find(\". Bc th ca anh\")\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    elif doc_id == '23351612':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"Cavani v Neymar tranh nhau\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"Cavani v Neymar tranh nhau\" in sent):\n",
        "                tmppp = sent.find(\"' V Neymar tranh\")\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    elif doc_id == '23354796':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"nh vy Hn 1 thng\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"nh vy Hn 1 thng\" in sent):\n",
        "                tmppp = sent.find(\" Hn\")\n",
        "                assert (tmppp > 0), str('ERROR')\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    elif doc_id == '23353976':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"_\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"_\" in sent):\n",
        "                sent = sent.replace('_', '')\n",
        "                sent = sent.strip()\n",
        "                new_sentences.append(sent)\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "    ### nhng doc m ta ch tch theo \n",
        "    # cc doc cc cu ch c  hoc mt s doc c hai loi nhng ta ch chia cu c \n",
        "    elif doc_id in ['23351214', '23351260', '23351430', '23351433', '23351578', \\\n",
        "                    '23351607', '23351645', '23351647', '23351719', '23351967', \\\n",
        "                    '23351987', '23351990', '23352659', '23352663', '23352675', \\\n",
        "                    '23352681', '23352702', '23352751', '23352755', '23352774', \\\n",
        "                    '23352781', '23352874', '23354130', '23354460', \\\n",
        "                    '23354803', '23354816', '23354880', '23354935', '23354956', \\\n",
        "                    '23355773', '23355917', '23357489', '23358011', '23356622', \\\n",
        "                    '23352856', '23353830', '23354538', \\\n",
        "                    '23354619', '23356574', '23355228']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "\n",
        "            if ((doc_id != '23352856') and (doc_id != '23353830') \\\n",
        "                and (doc_id != '23354538') and (doc_id != '23354619') \\\n",
        "                and (doc_id != '23356574') and (doc_id != '23355228')):\n",
        "                assert ('...' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            if (\"\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"\" in sent):\n",
        "                new_sents = sent.split(\"\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + ''))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "    ### nhng doc m ta ch tch theo ...\n",
        "    # cc doc cc cu ch c ... hoc mt s doc c hai loi nhng ta ch chia cu c ...\n",
        "    elif doc_id in ['23351435', '23351516', \\\n",
        "                    '23351542', '23351579', '23351581', '23351615', '23351888', \\\n",
        "                    '23351945', '23351956', '23351994', '23352725', '23352754', \\\n",
        "                    '23352857', '23353779', '23353780', '23353857', '23353950', \\\n",
        "                    '23354318', '23354953', '23354982', '23356494', \\\n",
        "                    '23356724', '23357000', \\\n",
        "                    '23351636', '23352753', '23353931', '23354320', '23354699', '23358261', '23351422', '23351427']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ((doc_id != '23351636') and (doc_id != '23352753') and (doc_id != '23353931') \\\n",
        "                and (doc_id != '23354320') and (doc_id != '23354699') and (doc_id != '23358261') \\\n",
        "                and (doc_id != '23351422') and (doc_id != '23351427')):   # doc nay co ca 2 nhung ta chi chia theo ...\n",
        "                assert ('' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            if (doc_id == '23351422') and ('_' in sent):\n",
        "                #print(sent)\n",
        "                sent = sent.replace('_', '')\n",
        "                sent = sent.strip()\n",
        "                #print(sent)\n",
        "\n",
        "            if (\"...\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"...\" in sent):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "    ### nhng doc m ta tch theo c 2\n",
        "    elif doc_id in ['23351887', '23353846', '23353860', '23355656', '23356998']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            \n",
        "\n",
        "            if ((\"...\" not in sent) and ('' not in sent)):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif ((\"...\" in sent) and ('' not in sent)):\n",
        "                new_sents = sent.split(\"...\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '...'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ((\"...\" not in sent) and ('' in sent)):\n",
        "                new_sents = sent.split(\"\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + ''))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "\n",
        "            elif ((\"...\" in sent) and ('' in sent)):\n",
        "                new_sents = sent.replace('...', '')\n",
        "                new_sents = new_sents.split('')\n",
        "\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ######\n",
        "    # trong doc ny vic chia cu b li, 2 cu b gp thnh 1 cu\n",
        "    # nhng cu ny thng c k t: .\n",
        "    \n",
        "    elif doc_id in ['23351307', '23352704', '23353830', '23353878', \\\n",
        "                    '23356329', '23356902', '23356933', '23357491', '23357809']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if ('.' in sent) and ('.' != sent[-2:]):\n",
        "                new_sents = sent.split('.')\n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '.'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ('.' not in sent) or ('.' == sent[-2:]):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "    \n",
        "\n",
        "\n",
        "    elif doc_id == '23351946':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if (\"du lch.. (clip\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"du lch.. (clip\" in sent):\n",
        "                tmppp = sent.find(\". (clip\")\n",
        "                new_sentences.append(str(sent[:(tmppp+1)]))\n",
        "                new_sentences.append(str(sent[(tmppp+2):]))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "        new_sentences_5 = []\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "\n",
        "            if (\"\" not in sent):\n",
        "                new_sentences_5.append(sent)\n",
        "            \n",
        "            elif (\"\" in sent):\n",
        "                new_sents = sent.split(\"\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences_5.append(str(new_sent.lstrip() + ''))\n",
        "                    else:\n",
        "                        new_sentences_5.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences_5)\n",
        "        sentences = copy.deepcopy(new_sentences_5)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id == '23354082':\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            \n",
        "            if ((\"..\" not in sent) and ('' not in sent)):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif ((\"..\" in sent) and ('' not in sent)):\n",
        "                new_sents = sent.split(\"..\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '..'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "            \n",
        "            elif ((\"..\" not in sent) and ('' in sent)):\n",
        "                new_sents = sent.split(\"\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + ''))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "\n",
        "            elif ((\"..\" in sent) and ('' in sent)):\n",
        "                assert False, str('my_sentence_tokenize. Problem in doc: ' + doc_id)\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    elif doc_id in ['23351985', '23352682', '23352822', '23354085', '23356858']:\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "\n",
        "            if doc_id not in ['23354085']:\n",
        "                assert (('...' not in sent) and (\"\" not in sent)), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            if (\"..\" not in sent):\n",
        "                new_sentences.append(sent)\n",
        "            \n",
        "            elif (\"..\" in sent):\n",
        "                new_sents = sent.split(\"..\")\n",
        "                \n",
        "                for inew_sent, new_sent in enumerate(new_sents):\n",
        "                    if inew_sent != (len(new_sents) - 1):\n",
        "                        new_sentences.append(str(new_sent.lstrip() + '..'))\n",
        "                    else:\n",
        "                        new_sentences.append(str(new_sent.strip()))\n",
        "        \n",
        "        #print(new_sentences)\n",
        "        sentences = copy.deepcopy(new_sentences)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    # tach dau .\n",
        "    \n",
        "    if doc_id in ['23351436', '23351566', '23351960', '23352693', '23352701', '23353721', '23357095', '23357394', '23352663', '23356902']:\n",
        "        \n",
        "        # ith_sent l th t cc cu cn tch du . trong doc (th t cu bt u t 0)\n",
        "        # ith_dot l s th t ca cc du . trong cu m ti cc du . ny ta s tch cu thnh cc phn khc nhau\n",
        "        # ith_dot cn m th t cn thn bng tay (th t du . bt u t 1)\n",
        "        # ith_dot l mt list ca list. list th n ca ith_dot l danh sch v tr nhng du chm m ta s dng  tch cu th n tng n\n",
        "        # trong ith_sent   <- cn lu   ng th t, v ith_sent cn xp theo th t tng dn\n",
        "        # cn lm vy v c th 1 doc c nhiu cn cn tch, ri trong cc cu ny li c cu c nhiu du . cn tch\n",
        "\n",
        "        #  tng: ta duyt cc cu trong doc, da vo ith_sent_lst  bit cu no cn tch du .\n",
        "        # cu no khng cn tch th ta thm lun vo danh sch cc cu trong doc\n",
        "        # cu no cn tch th: ta da tip vo ith_dot_lst  bit ta s tch ti nhng du . no trong cu\n",
        "        # v d cu cn tch ti 2 du .: th 2 v th 3 trong cu (-> t 1 cu tch thnh 3 cu)\n",
        "        # ta s tm v tr index ca cc du . ny trong cu cn tch\n",
        "        # ri da vo index   ct cu thnh cc phn cn chia\n",
        "\n",
        "        doc_nfix_lst = [{'doc_id': '23351436', 'ith_sent_lst': [0], 'ith_dot_lst': [[2]]},\n",
        "                        {'doc_id': '23351566', 'ith_sent_lst': [5], 'ith_dot_lst': [[4]]},\n",
        "                        {'doc_id': '23351960', 'ith_sent_lst': [3], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '23352693', 'ith_sent_lst': [10, 13, 17], 'ith_dot_lst': [[1], [1], [1]]},\n",
        "                        {'doc_id': '23352701', 'ith_sent_lst': [4], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '23353721', 'ith_sent_lst': [5, 8, 19], 'ith_dot_lst': [[1], [1], [1, 2]]},\n",
        "                        {'doc_id': '23357095', 'ith_sent_lst': [5], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '23357394', 'ith_sent_lst': [7], 'ith_dot_lst': [[1]]},\n",
        "                        {'doc_id': '23352663', 'ith_sent_lst': [2], 'ith_dot_lst': [[1]]},   # contain small three dot\n",
        "                        {'doc_id': '23356902', 'ith_sent_lst': [36], 'ith_dot_lst': [[1]]}   # contain .\n",
        "                        ]\n",
        "        \n",
        "        ##### to find pos of ith dot\n",
        "        def find_ith_dot_pos(haystack, needle, n):\n",
        "            start = haystack.find(needle)\n",
        "            while start >= 0 and n > 1:\n",
        "                start = haystack.find(needle, start+len(needle))\n",
        "                n -= 1\n",
        "            return start\n",
        "        #####\n",
        "\n",
        "\n",
        "        crr_doc_nfix = None\n",
        "        for doc_nfix in doc_nfix_lst:\n",
        "            if doc_nfix['doc_id'] == doc_id:\n",
        "                crr_doc_nfix = doc_nfix\n",
        "\n",
        "        new_sentences_6 = []\n",
        "        #print(sentences)\n",
        "        for isent, sent in enumerate(sentences):\n",
        "            if doc_id not in ['23353721', '23357394', '23352663']:\n",
        "                assert (('...' not in sent) and (\"\" not in sent)), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "            assert (len(crr_doc_nfix['ith_sent_lst']) == len(crr_doc_nfix['ith_dot_lst'])), \\\n",
        "            str('\\nLEN ith_sent_lst not equal to LEN ith_dot_lst. \\nDoc: ' + doc_id)\n",
        "\n",
        "            if isent not in crr_doc_nfix['ith_sent_lst']:\n",
        "                new_sentences_6.append(sent)\n",
        "            \n",
        "            else:\n",
        "                \n",
        "                assert ('...' not in sent), str('\\nDoc contain two types of three dot. Doc: ' + doc_id)\n",
        "\n",
        "                crr_ith_sent = crr_doc_nfix['ith_sent_lst'].index(isent)\n",
        "\n",
        "                ith_dot_pos_lst = [-1, (len(sent)-1)]\n",
        "\n",
        "                for ith_dot in crr_doc_nfix['ith_dot_lst'][crr_ith_sent]:\n",
        "                    dot_pos = find_ith_dot_pos(sent, '.', ith_dot)\n",
        "\n",
        "                    assert (dot_pos >= 0), str('\\nNot found ith dot. \\nDoc: ' + doc_id + '\\nSent: ' + sent)\n",
        "\n",
        "                    ith_dot_pos_lst.insert(-1, dot_pos)\n",
        "                \n",
        "                for iith in range(len(ith_dot_pos_lst) - 1):\n",
        "                    correct_sent = sent[(ith_dot_pos_lst[iith] + 1):(ith_dot_pos_lst[iith+1] + 1)]\n",
        "                    new_sentences_6.append(str(correct_sent).strip())\n",
        "\n",
        "\n",
        "        #print(new_sentences_6)\n",
        "        sentences = copy.deepcopy(new_sentences_6)\n",
        "        #print(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    if doc_id not in ['23351164', '23351436', '23351511', '23351515', '23351556', \\\n",
        "                      '23351579', '23351952', '23351959', '23351990', '23352665', \\\n",
        "                      '23352750', '23352753', '23352769', '23352896', '23353967', \\\n",
        "                      '23354055', '23354082', '23354320', '23354336', '23354879', \\\n",
        "                      '23355434', '23357233', '23357752']:\n",
        "        sentences = copy.deepcopy(split_by_colon_punc(sentences))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return sentences\n",
        "\n",
        "    ###### \n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_riTWvT_OHxy"
      },
      "source": [
        "def get_sentence_entities(docif, doc_entity_lst, sent, sspos, espos):\n",
        "    \n",
        "    sen_entity_lst = []\n",
        "\n",
        "    for i in range(len(doc_entity_lst)):   # each entity\n",
        "        first_token_eid = doc_entity_lst[i][0][0]\n",
        "        last_token_eid = doc_entity_lst[i][-1][0]\n",
        "            \n",
        "        # nu im u ca cu < im u ca token u v im cui ca token cui < im cui ca cu\n",
        "        # th entity ny l entity ca cu ny\n",
        "        if (sspos <= docif[\"pos\"][first_token_eid][0]) and (docif[\"pos\"][last_token_eid][1] <= espos):\n",
        "            sen_entity_lst.append(doc_entity_lst[i])\n",
        "\n",
        "        # im u ca entity < im u ca cu nhng im cui ca entity li ln hn im u ca cu\n",
        "        # c li: 1 entity nhng thuc 2 cu, tc l 1 phn ca entity thuc cu trc, phn cn li li thuc cu ang xt.\n",
        "        # iu ny c th do chia cu bng Underthesea c vn  hoc dataset c vn \n",
        "        elif (docif[\"pos\"][first_token_eid][0] < sspos) and (sspos < docif[\"pos\"][last_token_eid][1]):\n",
        "                \n",
        "        # trong dataset c li ny. \n",
        "        # tuy nhin khng c nhiu, nn  hiu thm v dataset, ti ch sa chnh xc cc li ny\n",
        "        # v  sa bn trn\n",
        "\n",
        "            assert False, str(\"\\n--- An entity belongs to two sentences instead of just one. (Error Code 1) \\nIn doc: \" + docif[\"doc_id\"] + \"\\nSentence: \" + repr(sent))\n",
        "                \n",
        "        # im u ca entity < im cui ca cu nhng im cui ca entity li ln hn im cui ca cu\n",
        "        # c li: 1 entity nhng thuc 2 cu, tc l 1 phn ca entity thuc cu ang xt, phn cn li li thuc cu sau.\n",
        "        # iu ny c th do chia cu bng Underthesea c vn  hoc dataset c vn \n",
        "        elif (docif[\"pos\"][first_token_eid][0] < espos) and (espos < docif[\"pos\"][last_token_eid][1]):\n",
        "                \n",
        "            assert False, str(\"\\nAn entity belongs to two sentences instead of just one. (Error Code 2) \\nIn doc: \" + docif[\"doc_id\"] + \"\\nSentence: \" + repr(sent))\n",
        "\n",
        "\n",
        "    # c th xy ra trng hp chia cu b li, mt cu to b chia thnh hai cu nh\n",
        "    # mi cu nh li cha cc entity\n",
        "    # nhng entity cu nh ny link ti cu nh kia -> li\n",
        "    # hoc trong data c li, entity cu ny link ti cu khc.\n",
        "    # nn cn xem xem cc relation trong cu c link ti cc entity tm thy trong cu khng\n",
        "        \n",
        "    # hay relation gia 2 entity l ng, nhng stoken_id trong relation khng tr vo token u tin ca entity id kia\n",
        "    # m li tr vo token gia hoc cui entity kia ( fix li ny bn trn)\n",
        "\n",
        "    # v relation ch link ti token_ids ca token u tin trong entity khc\n",
        "    # nn ta s thu thp danh sch token_ids ca cc token u tin cc entity trong cu\n",
        "    first_tkids_lst = []\n",
        "    for i in range(len(sen_entity_lst)):\n",
        "        first_tkids_lst.append(sen_entity_lst[i][0][1])\n",
        "\n",
        "\n",
        "    for i in range(len(sen_entity_lst)):\n",
        "        first_tkeid = sen_entity_lst[i][0][0]\n",
        "\n",
        "        if docif['relation'][first_tkeid] != None:   # tng relation trong cu\n",
        "            for j in range(len(docif['relation'][first_tkeid])):\n",
        "                if docif['relation'][first_tkeid][j][1] not in first_tkids_lst:\n",
        "                        \n",
        "                    '''\n",
        "                    print(str('\\nSentence tokenize has problem. \\nDoc: ' + docif['doc_id'] + '\\nRelation stoken ID: ' + str(docif['relation'][first_tkeid][j][1])  + ' \\nprvSent: ' + sentences[isent-1] + '\\nSent: ' + sent))\n",
        "\n",
        "                    print(docif['relation'][first_tkeid])\n",
        "                    '''\n",
        "\n",
        "                    assert False, \\\n",
        "                    str('\\nSentence tokenize has problem. \\nDoc: ' + docif['doc_id'] + '\\nRelation stoken ID: ' + str(docif['relation'][first_tkeid][j][1])  + ' \\nprvSent: ' + sentences[isent-1] + '\\nSent: ' + sent)\n",
        "\n",
        "\n",
        "\n",
        "    return sen_entity_lst\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZremDvk8tP7"
      },
      "source": [
        "def relation_name_to_sentence_label(relation_name, relation_entity):\n",
        "\n",
        "    sentence_label = None\n",
        "    # nu entity cha relation l entity 1 th label ngc li\n",
        "    if relation_entity == 1:\n",
        "        if relation_name == 'LOCATED':\n",
        "            sentence_label = 'IS_LOCATED'\n",
        "\n",
        "        elif relation_name == 'PART  WHOLE':\n",
        "            sentence_label = 'WHOLE_PART'\n",
        "\n",
        "        elif relation_name == 'PERSONAL - SOCIAL':\n",
        "            sentence_label = 'PERSONAL_SOCIAL'\n",
        "\n",
        "        elif relation_name == 'AFFILIATION':\n",
        "            sentence_label = 'AFFILIATION_TO'\n",
        "\n",
        "        else:\n",
        "            assert False, str('UNKNOW RELATION NAME: ' + relation_name)\n",
        "    \n",
        "    # nu entity cha relation l entity 2 th label gi nguyn\n",
        "    elif relation_entity == 2:\n",
        "        if relation_name == 'LOCATED':\n",
        "            sentence_label = 'LOCATED'\n",
        "\n",
        "        elif relation_name == 'PART  WHOLE':\n",
        "            sentence_label = 'PART_WHOLE'\n",
        "\n",
        "        elif relation_name == 'PERSONAL - SOCIAL':\n",
        "            sentence_label = 'PERSONAL_SOCIAL'\n",
        "\n",
        "        elif relation_name == 'AFFILIATION':\n",
        "            sentence_label = 'AFFILIATION'\n",
        "\n",
        "        else:\n",
        "            assert False, str('UNKNOW RELATION NAME: ' + relation_name)\n",
        "\n",
        "    else:\n",
        "        assert False, (\"Unexpect relation_entity. Expect 1 or 2 but got: \" + relation_entity + \".\")\n",
        "\n",
        "    \n",
        "    return sentence_label"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyIgy3tfLDTs"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKFuhu7TYFB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ccc99bb-ee5b-47d3-e6b7-d548a22f7ce3"
      },
      "source": [
        "# To train data (cu cha cp entity v label)\n",
        "\n",
        "tdata = []\n",
        "\n",
        "raw_data = raw_tdata_new_v6\n",
        "\n",
        "sent_id = 0\n",
        "\n",
        "for docif in raw_data:\n",
        "\n",
        "    # find all entity in current doc    \n",
        "    doc_entity_lst = find_all_entity_in_doc(raw_data, docif['doc_id'])\n",
        "\n",
        "    # if whole doc has 0 or 1 entity -->  no relation in this doc --> skip\n",
        "    # there is many doc like this (like doc has only 3 columns,...)\n",
        "    if len(doc_entity_lst) <= 1:\n",
        "        continue\n",
        "\n",
        "\n",
        "    text = docif[\"text\"]\n",
        "    sentences = my_sentences_tokenize(docif['doc_id'], text)\n",
        "\n",
        "    \n",
        "    ######## extract training sentence\n",
        " \n",
        "    pre_espos = 0   # end of pre sentence\n",
        "\n",
        "    for isent, sent in enumerate(sentences):\n",
        "\n",
        "        sentif = {}\n",
        "        '''\n",
        "        sentif[\"doc_id\"] = docif[\"doc_id\"]\n",
        "        sentif[\"sentence\"] = sent\n",
        "        '''\n",
        "        ###### sentence position\n",
        "        # tm v tr ca cu  da vo  bit entity (cc tokens) thuc cu no\n",
        "\n",
        "        # text may have two indentical sentences\n",
        "        # so we have to find start position of current sentence in the rest of the text that not contain previous sentences.\n",
        "\n",
        "        assert (text[pre_espos:].find(sent) >= 0), str(\"Position has problem. \\nDoc: \" + docif[\"doc_id\"] + \"\\nCurrent sentence: \" + sent)\n",
        "\n",
        "        sspos = text[pre_espos:].find(sent) + pre_espos\n",
        "        espos = sspos + len(sent)\n",
        "\n",
        "        # update pre_espos\n",
        "        pre_espos = espos\n",
        "        \n",
        "        assert (sent == text[sspos:espos]), str(\"Position founded is not matched in text. \\nDoc: \" + docif[\"doc_id\"] + \"\\nCurrent sentence: \" + sent)\n",
        "\n",
        "        '''\n",
        "        sentif[\"spos\"] = [sspos, espos]\n",
        "        '''\n",
        "\n",
        "        ###### get all entity in current sentence\n",
        "        \n",
        "        sen_entity_lst = get_sentence_entities(docif, doc_entity_lst, sent, sspos, espos)\n",
        "\n",
        "        # if current sentence has 0 or 1 entity -> no relation availabel to classify -> skip\n",
        "        if len(sen_entity_lst) <= 1: \n",
        "            continue\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "        ###### create n*(n-1)/2 sentence\n",
        "        \n",
        "        for ient, ent_1 in enumerate(sen_entity_lst):\n",
        "            for jent, ent_2 in enumerate(sen_entity_lst[(ient+1):]):\n",
        "                \n",
        "                first_tkeid_ent1 = ent_1[0][0]   # dng cha token u trong entity\n",
        "                first_tkeid_ent2 = ent_2[0][0]\n",
        "                \n",
        "                last_tkeid_ent1 = ent_1[-1][0]   # dng cha token cui trong entity\n",
        "                last_tkeid_ent2 = ent_2[-1][0]\n",
        "\n",
        "                if (docif['entity'][first_tkeid_ent1][1] != \"MISCELLANEOUS\") and (docif['entity'][first_tkeid_ent2][1] != \"MISCELLANEOUS\"):\n",
        "                    \n",
        "                    # pos ca entity trong doc: start ca token u v end ca token cui trong entity\n",
        "                    ent1_pos_doc = [docif['pos'][first_tkeid_ent1][0], docif['pos'][last_tkeid_ent1][1]]\n",
        "                    ent2_pos_doc = [docif['pos'][first_tkeid_ent2][0], docif['pos'][last_tkeid_ent2][1]]\n",
        "\n",
        "                    # pos ca entity trong cu cha entity\n",
        "                    ent1_pos_sent = [(ent1_pos_doc[0] - sspos), (ent1_pos_doc[1] - sspos)]\n",
        "                    ent2_pos_sent = [(ent2_pos_doc[0] - sspos), (ent2_pos_doc[1] - sspos)]\n",
        "\n",
        "                    \n",
        "                    # kim tra xem pos trong doc v sent c khp, tr v cng entity khng\n",
        "                    assert (sent[ent1_pos_sent[0]:ent1_pos_sent[1]] == text[ent1_pos_doc[0]:ent1_pos_doc[1]]), \\\n",
        "                    str('Entity 1: pos_doc and pos_sent not matched. \\nDoc: ' + docif['doc_id'] + '\\nSent: ' + sent)\n",
        "\n",
        "                    assert (sent[ent2_pos_sent[0]:ent2_pos_sent[1]] == text[ent2_pos_doc[0]:ent2_pos_doc[1]]), \\\n",
        "                    str('Entity 1: pos_doc and pos_sent not matched. \\nDoc: ' + docif['doc_id'] + '\\nSent: ' + sent)\n",
        "\n",
        "\n",
        "                    ent1_text = sent[ent1_pos_sent[0]:ent1_pos_sent[1]]\n",
        "                    ent2_text = sent[ent2_pos_sent[0]:ent2_pos_sent[1]]\n",
        "\n",
        "                    ###############\n",
        "                    # kim tra xem mi token trong entity  c mt trong entity ly t pos hay cha\n",
        "                    for itk, tk in enumerate(ent_1):\n",
        "                        eid_tk = tk[0]\n",
        "\n",
        "                        if itk < (len(ent_1) - 1):\n",
        "                            eid_n_tk = ent_1[itk+1][0]\n",
        "\n",
        "                            assert (docif['pos'][eid_tk][1] < docif['pos'][eid_n_tk][0]), \\\n",
        "                            str(\"Position not increase. Doc: \" + docif['doc_id'] + \"\\nSent: \" + sent + \"\\ncrr-pos: \" + str(docif['pos'][eid_tk][1]) + \"\\nnpos: \" + str(docif['pos'][eid_ntk][0]))\n",
        "\n",
        "\n",
        "                        assert (ent1_pos_doc[0] <= docif['pos'][eid_tk][0]) and (docif['pos'][eid_tk][1] <= ent1_pos_doc[1]), \\\n",
        "                        str('Entity\\'s token pos not inside entity pos')\n",
        "                    \n",
        "                    ###############\n",
        "\n",
        "                    ##########\n",
        "\n",
        "                    assert (ent_1[0][1] == docif['token_ids'][first_tkeid_ent1]), str('NOT MATCHED entity first token id')\n",
        "                    assert (ent_2[0][1] == docif['token_ids'][first_tkeid_ent2]), str('NOT MATCHED entity first token id')\n",
        "\n",
        "                    sentence_label = None\n",
        "\n",
        "                    # nu c 2 entity khng c relation\n",
        "                    if (docif['relation'][first_tkeid_ent1] == None) and (docif['relation'][first_tkeid_ent2] == None):\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                    # nu c 2 entity c relation\n",
        "                    elif (docif['relation'][first_tkeid_ent1] != None) and (docif['relation'][first_tkeid_ent2] != None):\n",
        "                        \n",
        "                        # mc nh l OTHERS, nu bn di tm thy relation link ti th s c thay i\n",
        "                        # cn nu bn di tm khng thy (tc l khng c relation) th s khng b thay i v vn l OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                        for rel_1 in docif['relation'][first_tkeid_ent1]:\n",
        "                            if rel_1[1] == ent_2[0][1]:   # relation  entity 1 link ti token u entity 2\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_1[0], 1)\n",
        "\n",
        "                                # kiem tra relation direction\n",
        "                                # do relation nam o ent_1 nen direction la: [ent_2, ent_1]\n",
        "                                if rel_1[3] != None:\n",
        "                                    assert (rel_1[3] == [ent_2[0][2], ent_1[0][2]]), str('CODE 1: Not match direction')\n",
        "                                \n",
        "\n",
        "                        for rel_2 in docif['relation'][first_tkeid_ent2]:\n",
        "                            if rel_2[1] == ent_1[0][1]:   # relation  entity 2 link ti token u entity 1\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_2[0], 2)\n",
        "\n",
        "                                # do relation nam o ent_2 nen direction la: [ent_1, ent_2]\n",
        "                                if rel_2[3] != None:\n",
        "                                    assert (rel_2[3] == [ent_1[0][2], ent_2[0][2]]), str('CODE 2: Not match direction')\n",
        "        \n",
        "\n",
        "                    # nu entity 1 c relation, entity 2 khng c\n",
        "                    elif (docif['relation'][first_tkeid_ent1] != None) and (docif['relation'][first_tkeid_ent2] == None):\n",
        "                        \n",
        "                        # mc nh l OTHERS, nu bn di tm thy relation link ti th s c thay i\n",
        "                        # cn nu bn di tm khng thy (tc l khng c relation) th s khng b thay i v vn l OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "\n",
        "                        for rel_1 in docif['relation'][first_tkeid_ent1]:\n",
        "                            if rel_1[1] == ent_2[0][1]:   # relation  entity 1 link ti token u entity 2\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_1[0], 1)\n",
        "\n",
        "                                # do relation nam o ent_1 nen direction la: [ent_2, ent_1]\n",
        "                                if rel_1[3] != None:\n",
        "                                    assert (rel_1[3] == [ent_2[0][2], ent_1[0][2]]), str('CODE 3: Not match direction')\n",
        "\n",
        "                                \n",
        "\n",
        "                    # nu entity 2 c relation, entity 1 khng c\n",
        "                    elif (docif['relation'][first_tkeid_ent1] == None) and (docif['relation'][first_tkeid_ent2] != None):\n",
        "                        \n",
        "                        # mc nh l OTHERS, nu bn di tm thy relation link ti th s c thay i\n",
        "                        # cn nu bn di tm khng thy (tc l khng c relation) th s khng b thay i v vn l OTHERS.\n",
        "                        sentence_label = labels['OTHERS']\n",
        "                        \n",
        "                        for rel_2 in docif['relation'][first_tkeid_ent2]:\n",
        "                            if rel_2[1] == ent_1[0][1]:   # relation  entity 2 link ti token u entity 1\n",
        "                                sentence_label = relation_name_to_sentence_label(rel_2[0], 2)\n",
        "\n",
        "                                # do relation nam o ent_2 nen direction la: [ent_1, ent_2]\n",
        "                                if rel_2[3] != None:\n",
        "                                    assert (rel_2[3] == [ent_1[0][2], ent_2[0][2]]), str('CODE 4: Not match direction')\n",
        "\n",
        "                        \n",
        "                    ##########\n",
        "\n",
        "                    sentif[\"doc_id\"] = docif[\"doc_id\"]\n",
        "\n",
        "                    sent_id += 1\n",
        "                    sentif['sent_id'] = sent_id\n",
        "\n",
        "                    sentif[\"sentence\"] = sent\n",
        "                    sentif[\"spos\"] = [sspos, espos]\n",
        "\n",
        "                    entity_1 = {'text': copy.deepcopy(ent1_text), 'pos': copy.deepcopy(ent1_pos_sent)}\n",
        "                    entity_2 = {'text': copy.deepcopy(ent2_text), 'pos': copy.deepcopy(ent2_pos_sent)}\n",
        "\n",
        "                    sentif['entity_1'] = copy.deepcopy(entity_1)\n",
        "                    sentif['entity_2'] = copy.deepcopy(entity_2)\n",
        "\n",
        "                    sentif['label'] = copy.deepcopy(sentence_label)\n",
        "\n",
        "                    \n",
        "                    \n",
        "                    # may dong tren co the khong co copy.deepcopy nhung dong ben duoi khong co la bi loi\n",
        "                    tdata.append(copy.deepcopy(sentif))\n",
        "\n",
        "print(\"DONE\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AVi-oJSXQdY",
        "outputId": "5a0298d6-4844-41c7-f032-ed684cf664fa"
      },
      "source": [
        "count_label_others = 0\n",
        "\n",
        "for tdata_point in tdata:\n",
        "    if tdata_point['label'] == 'OTHERS':\n",
        "        count_label_others += 1\n",
        "\n",
        "print(count_label_others)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz_22I3CZvr9",
        "outputId": "9ac7bb8c-748b-475f-ef79-6f0bfe5a2391"
      },
      "source": [
        "print('Count of labels that is not OTHERS: ', (len(tdata) - count_label_others))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of labels that is not OTHERS:  2645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3O1aoGj796m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c127ca-8a4a-49be-9f05-ab745231c3d8"
      },
      "source": [
        "print(len(tdata))\n",
        "print(*tdata[0:15], sep='\\n')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15666\n",
            "{'doc_id': '23351113', 'sent_id': 1, 'sentence': 'nh minh ha Th trng B GD&T Nguyn Th Ngha  c  kin v vn  ny.', 'spos': [311, 388], 'entity_1': {'text': 'B GD&T', 'pos': [24, 32]}, 'entity_2': {'text': 'Nguyn Th Ngha', 'pos': [33, 49]}, 'label': 'AFFILIATION_TO'}\n",
            "{'doc_id': '23351113', 'sent_id': 2, 'sentence': 'ng Nguyn Tng Lm Sp ti, B GD&T s tng cng thanh, kim tra  c bin php chn chnh vic thc hin quy nh iu l ca Ban i din cha m hc sinh.', 'spos': [894, 1054], 'entity_1': {'text': 'Nguyn Tng Lm', 'pos': [4, 19]}, 'entity_2': {'text': 'B GD&T', 'pos': [29, 37]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 3, 'sentence': 'Cng theo Th trng Nguyn Th Ngha , trc nhng bin tng nh hin ti, B GD&T nghin cu c th b quy nh ny  trnh hin tng lch lut.', 'spos': [1220, 1370], 'entity_1': {'text': 'Nguyn Th Ngha', 'pos': [21, 37]}, 'entity_2': {'text': 'B GD&T', 'pos': [77, 85]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 4, 'sentence': 'Lin quan n vn  lnh o ngnh Gio dc trao i, thy Nguyn Vn nh  Hiu trng Trng THPT Ph in , ng Thp  chia s:', 'spos': [1371, 1504], 'entity_1': {'text': 'Nguyn Vn nh', 'pos': [60, 75]}, 'entity_2': {'text': 'Trng THPT Ph in', 'pos': [90, 110]}, 'label': 'AFFILIATION'}\n",
            "{'doc_id': '23351113', 'sent_id': 5, 'sentence': 'Lin quan n vn  lnh o ngnh Gio dc trao i, thy Nguyn Vn nh  Hiu trng Trng THPT Ph in , ng Thp  chia s:', 'spos': [1371, 1504], 'entity_1': {'text': 'Nguyn Vn nh', 'pos': [60, 75]}, 'entity_2': {'text': 'ng Thp', 'pos': [113, 122]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 6, 'sentence': 'Lin quan n vn  lnh o ngnh Gio dc trao i, thy Nguyn Vn nh  Hiu trng Trng THPT Ph in , ng Thp  chia s:', 'spos': [1371, 1504], 'entity_1': {'text': 'Trng THPT Ph in', 'pos': [90, 110]}, 'entity_2': {'text': 'ng Thp', 'pos': [113, 122]}, 'label': 'LOCATED'}\n",
            "{'doc_id': '23351113', 'sent_id': 7, 'sentence': 'Nhn nh  tng ca B GD&T b quyn c thu tin ca Ban i din cha m hc sinh vo thi im hin nay l kh ph hp, thy Nguyn Vn nh a l do:', 'spos': [1914, 2071], 'entity_1': {'text': 'B GD&T', 'pos': [22, 30]}, 'entity_2': {'text': 'Nguyn Vn nh', 'pos': [131, 146]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 8, 'sentence': 'Cng v cu chuyn lm thu lin quan n Ban i din cha m hc sinh, TS Nguyn Tng Lm  Ch tch Hi Tm l gio dc H Ni , nguyn Hiu trng Trng THPT inh Tin Hong (H Ni )  cho rng:', 'spos': [2627, 2825], 'entity_1': {'text': 'Nguyn Tng Lm', 'pos': [74, 89]}, 'entity_2': {'text': 'Hi Tm l gio dc H Ni', 'pos': [101, 127]}, 'label': 'AFFILIATION'}\n",
            "{'doc_id': '23351113', 'sent_id': 9, 'sentence': 'Cng v cu chuyn lm thu lin quan n Ban i din cha m hc sinh, TS Nguyn Tng Lm  Ch tch Hi Tm l gio dc H Ni , nguyn Hiu trng Trng THPT inh Tin Hong (H Ni )  cho rng:', 'spos': [2627, 2825], 'entity_1': {'text': 'Nguyn Tng Lm', 'pos': [74, 89]}, 'entity_2': {'text': 'Trng THPT inh Tin Hong', 'pos': [149, 176]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 10, 'sentence': 'Cng v cu chuyn lm thu lin quan n Ban i din cha m hc sinh, TS Nguyn Tng Lm  Ch tch Hi Tm l gio dc H Ni , nguyn Hiu trng Trng THPT inh Tin Hong (H Ni )  cho rng:', 'spos': [2627, 2825], 'entity_1': {'text': 'Nguyn Tng Lm', 'pos': [74, 89]}, 'entity_2': {'text': '(H Ni', 'pos': [177, 184]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 11, 'sentence': 'Cng v cu chuyn lm thu lin quan n Ban i din cha m hc sinh, TS Nguyn Tng Lm  Ch tch Hi Tm l gio dc H Ni , nguyn Hiu trng Trng THPT inh Tin Hong (H Ni )  cho rng:', 'spos': [2627, 2825], 'entity_1': {'text': 'Hi Tm l gio dc H Ni', 'pos': [101, 127]}, 'entity_2': {'text': 'Trng THPT inh Tin Hong', 'pos': [149, 176]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 12, 'sentence': 'Cng v cu chuyn lm thu lin quan n Ban i din cha m hc sinh, TS Nguyn Tng Lm  Ch tch Hi Tm l gio dc H Ni , nguyn Hiu trng Trng THPT inh Tin Hong (H Ni )  cho rng:', 'spos': [2627, 2825], 'entity_1': {'text': 'Hi Tm l gio dc H Ni', 'pos': [101, 127]}, 'entity_2': {'text': '(H Ni', 'pos': [177, 184]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351113', 'sent_id': 13, 'sentence': 'Cng v cu chuyn lm thu lin quan n Ban i din cha m hc sinh, TS Nguyn Tng Lm  Ch tch Hi Tm l gio dc H Ni , nguyn Hiu trng Trng THPT inh Tin Hong (H Ni )  cho rng:', 'spos': [2627, 2825], 'entity_1': {'text': 'Trng THPT inh Tin Hong', 'pos': [149, 176]}, 'entity_2': {'text': '(H Ni', 'pos': [177, 184]}, 'label': 'LOCATED'}\n",
            "{'doc_id': '23351113', 'sent_id': 14, 'sentence': 'Nhn mnh B GD&T  a ra hng dn nhng khon nh trng c v khng c thu, TS Nguyn Tng Lm cho rng, hin chng ta ang ln ln rt nhiu th v tt c u  ln Ban ph huynh.', 'spos': [3981, 4172], 'entity_1': {'text': 'B GD&T', 'pos': [10, 18]}, 'entity_2': {'text': 'Nguyn Tng Lm', 'pos': [89, 104]}, 'label': 'OTHERS'}\n",
            "{'doc_id': '23351164', 'sent_id': 15, 'sentence': 'Qung Bnh : Ct tc lng hnh, him ha rnh rp cu Long i v dng sng?', 'spos': [0, 76], 'entity_1': {'text': 'Qung Bnh', 'pos': [0, 10]}, 'entity_2': {'text': 'cu Long i', 'pos': [50, 62]}, 'label': 'WHOLE_PART'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0mBtqdZthaa"
      },
      "source": [
        "# Write to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTM1DITfqJtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bd77e43-5938-4a17-a284-14f566c64f34"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xLBzf43i3NC"
      },
      "source": [
        "# write to file\n",
        "import codecs\n",
        "import json\n",
        "\n",
        "with codecs.open('train_data.json', 'w', encoding='utf-8') as fout:\n",
        "    json.dump(tdata, fout, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72boeFzXgq-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3864947-e9e1-4d97-8f5a-57b31cb4135f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B8JEv6kDzuV"
      },
      "source": [
        "!mkdir \"/gdrive/MyDrive/VLSP2020_RE\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ng7tJuLD4_V"
      },
      "source": [
        "!mkdir \"/gdrive/MyDrive/VLSP2020_RE/json_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU0dYmmshRFZ"
      },
      "source": [
        "!cp -i train_data.json \"/gdrive/MyDrive/VLSP2020_RE/json_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4ikf014Wj7R",
        "outputId": "3d85f5b0-087c-4169-814e-205aaa3f93f7"
      },
      "source": [
        "import filecmp\n",
        "filecmp.cmp('train_data.json', '/gdrive/MyDrive/VLSP2020_RE/json_data/train_data.json')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    }
  ]
}